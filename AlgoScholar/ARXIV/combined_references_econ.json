[
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2005.07972",
        "category": "econ",
        "title": "Conformal Prediction: a Unified Review of Theory and New Challenges",
        "abstract": "In this work we provide a review of basic ideas and novel developments about Conformal Prediction -- an innovative distribution-free, non-parametric forecasting method, based on minimal assumptions -- that is able to yield in a very straightforward way predictions sets that are valid in a statistical sense also in in the finite sample case. The in-depth discussion provided in the paper covers the theoretical underpinnings of Conformal Prediction, and then proceeds to list the more advanced developments and adaptations of the original idea.",
        "references": [
            {
                "arxivId": "2107.00527",
                "title": "Distribution-Free Prediction Bands for Multivariate Functional Time Series: an Application to the Italian Gas Market",
                "abstract": "Uncertainty quantification in forecasting represents a topic of great importance in energy trading, as understanding the status of the energy market would enable traders to directly evaluate the impact of their own offers/bids. To this end, we propose a scalable procedure that outputs closed-form simultaneous prediction bands for multivariate functional response variables in a time series setting, which is able to guarantee performance bounds in terms of unconditional coverage and asymptotic exactness, both under some conditions. After evaluating its performance on synthetic data, the method is used to build multivariate prediction bands for daily demand and offer curves in the Italian gas market."
            },
            {
                "arxivId": "2106.01792",
                "title": "Conformal prediction bands for multivariate functional data",
                "abstract": null
            },
            {
                "arxivId": "2102.06746",
                "title": "The Importance of Being a Band: Finite-Sample Exact Distribution-Free Prediction Sets for Functional Data",
                "abstract": "Functional Data Analysis represents a field of growing interest in statistics. Despite several studies have been proposed leading to fundamental results, the problem of obtaining valid and efficient prediction sets has not been thoroughly covered. Indeed, the great majority of methods currently in the literature rely on strong distributional assumptions (e.g, Gaussianity), dimension reduction techniques and/or asymptotic arguments. In this work, we propose a new nonparametric approach in the field of Conformal Prediction based on a new family of nonconformity measures inducing conformal predictors able to create closed-form finite-sample valid or exact prediction sets under very minimal distributional assumptions. In addition, our proposal ensures that the prediction sets obtained are bands, an essential feature in the functional setting that allows the visualization and interpretation of such sets. The procedure is also fast, scalable, does not rely on functional dimension reduction techniques and allows the user to select different nonconformity measures depending on the problem at hand always obtaining valid bands. Within this family of measures, we propose also a specific measure leading to prediction bands asymptotically no less efficient than those obtained by not modulating."
            },
            {
                "arxivId": "2102.07967",
                "title": "Distribution-free conditional median inference",
                "abstract": "We consider the problem of constructing confidence intervals for the median of a response $Y \\in \\mathbb{R}$ conditional on features $X \\in \\mathbb{R}^d$ in a situation where we are not willing to make any assumption whatsoever on the underlying distribution of the data $(X,Y)$. We propose a method based upon ideas from conformal prediction and establish a theoretical guarantee of coverage while also going over particular distributions where its performance is sharp. Additionally, we prove an equivalence between confidence intervals for the conditional median and confidence intervals for the response variable, resulting in a lower bound on the length of any possible conditional median confidence interval. This lower bound is independent of sample size and holds for all distributions with no point masses."
            },
            {
                "arxivId": "1910.08105",
                "title": "Multi-level conformal clustering: A distribution-free technique for clustering and anomaly detection",
                "abstract": null
            },
            {
                "arxivId": "1905.02928",
                "title": "Predictive inference with the jackknife+",
                "abstract": "This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to K-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk [2015] and we discuss connections."
            },
            {
                "arxivId": "1904.06019",
                "title": "Conformal Prediction Under Covariate Shift",
                "abstract": "We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between these two distributions is known---or, in practice, can be estimated accurately with access to a large set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more generally, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems."
            },
            {
                "arxivId": "1903.04684",
                "title": "The limits of distribution-free conditional predictive inference",
                "abstract": "\n We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally rather than marginally. Existing methods such as conformal prediction offer marginal coverage guarantees, where predictive coverage holds on average over all possible test points, but this is not sufficient for many practical applications where we would like to know that our predictions are valid for a given individual, not merely on average over a population. On the other hand, exact conditional inference guarantees are known to be impossible without imposing assumptions on the underlying distribution. In this work, we aim to explore the space in between these two and examine what types of relaxations of the conditional coverage property would alleviate some of the practical concerns with marginal coverage guarantees while still being possible to achieve in a distribution-free setting."
            },
            {
                "arxivId": "1802.06300",
                "title": "Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data",
                "abstract": "We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and accounts for potential serial dependence by including block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods. When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score."
            },
            {
                "arxivId": "1709.06233",
                "title": "Discretized conformal prediction for efficient distribution\u2010free inference",
                "abstract": "In regression problems where there is no known true underlying model, conformal prediction methods enable prediction intervals to be constructed without any assumptions on the distribution of the underlying data, except that the training and test data are assumed to be exchangeable. However, these methods bear a heavy computational cost\u2014and, to be carried out exactly, the regression algorithm would need to be fitted infinitely many times. In practice, the conformal prediction method is run by simply considering only a finite grid of finely spaced values for the response variable. This paper develops discretized conformal prediction algorithms that are guaranteed to cover the target value with the desired probability and that offer a trade\u2010off between computational cost and prediction accuracy. Copyright \u00a9 2018 John Wiley & Sons, Ltd."
            },
            {
                "arxivId": "1708.00427",
                "title": "Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy",
                "abstract": "Conformal prediction is a general method that converts almost any point predictor to a prediction set. The resulting set keeps good statistical properties of the original estimator under standard assumptions, and guarantees valid average coverage even when the model is misspecified. A main challenge in applying conformal prediction in modern applications is efficient computation, as it generally requires an exhaustive search over the entire output space. In this paper we develop an exact and computationally efficient conformalization of the Lasso and elastic net. The method makes use of a novel piecewise linear homotopy of the Lasso solution under perturbation of a single input sample point. As a by-product, we provide a simpler and better justified online Lasso algorithm, which may be of independent interest. Our derivation also reveals an interesting accuracy-stability trade-off in conformal inference, which is analogous to the bias-variance trade-off in traditional parameter estimation. The practical performance of the new algorithm is demonstrated using both synthetic and real data examples."
            },
            {
                "arxivId": "1611.09933",
                "title": "Trimmed Conformal Prediction for High-Dimensional Models",
                "abstract": "In regression, conformal prediction is a general methodology to construct prediction intervals in a distribution-free manner. Although conformal prediction guarantees strong statistical property for predictive inference, its inherent computational challenge has attracted the attention of researchers in the community. In this paper, we propose a new framework, called Trimmed Conformal Prediction (TCP), based on two stage procedure, a trimming step and a prediction step. The idea is to use a preliminary trimming step to substantially reduce the range of possible values for the prediction interval, and then applying conformal prediction becomes far more efficient. As is the case of conformal prediction, TCP can be applied to any regression method, and further offers both statistical accuracy and computational gains. For a specific example, we also show how TCP can be implemented in the sparse regression setting. The experiments on both synthetic and real data validate the empirical performance of TCP."
            },
            {
                "arxivId": "1604.04173",
                "title": "Distribution-Free Predictive Inference for Regression",
                "abstract": "ABSTRACT We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows for the construction of a prediction band for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite-sample marginal coverage even when these assumptions do not hold. We analyze and compare, both empirically and theoretically, the two major variants of our conformal framework: full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called rank-one-out conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with locally varying length, to adapt to heteroscedasticity in the data. Finally, we propose a model-free notion of variable importance, called leave-one-covariate-out or LOCO inference. Accompanying this article is an R package conformalInference that implements all of the proposals we have introduced. In the spirit of reproducibility, all of our empirical results can also be easily (re)generated using this package."
            },
            {
                "arxivId": "1404.2083",
                "title": "Efficiency of conformalized ridge regression",
                "abstract": "Conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms. The method has a guaranteed coverage probability under the standard IID assumption regardless of whether the assumptions (often considerably more restrictive) of the underlying algorithm are satisfied. However, for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied, the conformal predictor loses little in efficiency as compared with the underlying algorithm (whereas being a conformal predictor, it has the stronger guarantee of validity). In this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of Bayesian ridge regression; we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard Bayesian assumptions are satisfied."
            },
            {
                "arxivId": "1401.3880",
                "title": "Regression Conformal Prediction with Nearest Neighbours",
                "abstract": "In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours Regression (k-NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k-Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures."
            },
            {
                "arxivId": "1302.6452",
                "title": "A conformal prediction approach to explore functional data",
                "abstract": null
            },
            {
                "arxivId": "1209.2673",
                "title": "Conditional validity of inductive conformal predictors",
                "abstract": null
            },
            {
                "arxivId": "1208.0806",
                "title": "Cross-conformal predictors",
                "abstract": null
            },
            {
                "arxivId": "2312.09606",
                "title": "Reliable prediction intervals with regression neural networks",
                "abstract": null
            },
            {
                "arxivId": "0902.1970",
                "title": "Sparse conformal predictors",
                "abstract": null
            },
            {
                "arxivId": "0706.3188",
                "title": "A tutorial on conformal prediction",
                "abstract": "Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability e, together with a method that makes a prediction \u0177 of a label y, it produces a set of labels, typically containing \u0177, that also contains y with probability 1 \u0096 e. Conformal prediction can be applied to any method for producing \u0177: a nearest-neighbor method, a support-vector machine, ridge regression, etc. \n \nConformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 \u0096 e of the time, even though they are based on an accumulating data set rather than on independent data sets. \n \nIn addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. \n \nThis tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005)."
            },
            {
                "arxivId": "0906.3123",
                "title": "On-line predictive linear regression",
                "abstract": "We consider the on-line predictive version of the standard problem of linear regression; the goal is to predict each consecutive response given the corresponding explanatory variables and all the previous observations. The standard treatment of prediction in linear regression analysis has two drawbacks: (1) the classical prediction intervals guarantee that the probability of error is equal to the nominal significance level $\\varepsilon$, but this property per se does not imply that the long-run frequency of error is close to $\\varepsilon$; (2) it is not suitable for prediction of complex systems as it assumes that the number of observations exceeds the number of parameters. We state a general result showing that in the on-line protocol the frequency of error for the classical prediction intervals does equal the nominal significance level, up to statistical fluctuations. We also describe alternative regression models in which informative prediction intervals can be found before the number of observations exceeds the number of parameters. One of these models, which only assumes that the observations are independent and identically distributed, is popular in machine learning but greatly underused in the statistical theory of regression."
            },
            {
                "arxivId": "1301.7375",
                "title": "Learning by Transduction",
                "abstract": "We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2011.04306",
        "category": "econ",
        "title": "Ordinal Intensity-Efficient Allocations",
        "abstract": "We study the assignment problem in situations where, in addition to having ordinal preferences, agents also have *ordinal intensities*: they can make simple and internally consistent comparisons such as\"I prefer $a$ to $b$ more than I prefer $c$ to $d$\"without necessarily being able to quantify them. In this new informational social-choice environment we first introduce a rank-based criterion that enables interpersonal comparability of such ordinal intensities. Building on this criterion, we define an allocation to be *\"intensity-efficient\"* if it is Pareto efficient with respect to the preferences induced by the agents' intensities and also such that, when another allocation assigns the same pairs of items to the same pairs of agents but in a\"flipped\"way, the former allocation assigns the commonly preferred item in every such pair to the agent who prefers it more. We present some first results on the (non-)existence of such allocations without imposing restrictions on preferences or intensities other than strictness.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2112.06363",
        "category": "econ",
        "title": "Risk and optimal policies in bandit experiments",
        "abstract": "We provide a decision theoretic analysis of bandit experiments under local asymptotics. Working within the framework of diffusion processes, we define suitable notions of asymptotic Bayes and minimax risk for these experiments. For normally distributed rewards, the minimal Bayes risk can be characterized as the solution to a second-order partial differential equation (PDE). Using a limit of experiments approach, we show that this PDE characterization also holds asymptotically under both parametric and non-parametric distributions of the rewards. The approach further describes the state variables it is asymptotically sufficient to restrict attention to, and thereby suggests a practical strategy for dimension reduction. The PDEs characterizing minimal Bayes risk can be solved efficiently using sparse matrix routines or Monte-Carlo methods. We derive the optimal Bayes and minimax policies from their numerical solutions. These optimal policies substantially dominate existing methods such as Thompson sampling; the risk of the latter is often twice as high.",
        "references": [
            {
                "arxivId": "2105.09232",
                "title": "Diffusion Approximations for Thompson Sampling",
                "abstract": "We study the behavior of Thompson sampling from the perspective of weak convergence. In the regime where the gaps between arm means scale as $1/\\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson sampling evolve according to discrete versions of SDEs and random ODEs. As $n \\to \\infty$, we show that the dynamics converge weakly to solutions of the corresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855) independently proposed this regime and developed similar SDE and random ODE approximations for Thompson sampling in the multi-armed bandit setting.) Our weak convergence theory, which covers both multi-armed and linear bandit settings, is developed from first principles using the Continuous Mapping Theorem and can be directly adapted to analyze other sampling-based bandit algorithms, for example, algorithms using the bootstrap for exploration. We also establish an invariance principle for multi-armed bandits with gaps scaling as $1/\\sqrt{n}$ -- for Thompson sampling and related algorithms involving posterior approximation or the bootstrap, the weak diffusion limits are in general the same regardless of the specifics of the reward distributions or the choice of prior. In particular, as suggested by the classical Bernstein-von Mises normal approximation for posterior distributions, the weak diffusion limits generally coincide with the limit for normally-distributed rewards and priors."
            },
            {
                "arxivId": "1901.01193",
                "title": "Improved order 1/4 convergence for piecewise constant policy approximation of stochastic control problems",
                "abstract": "In N.V. Krylov, Approximating value functions for controlled degenerate diffusion processes by using piece-wise constant policies, Electron. J. Probab., 4(2), 1999, it is proved under standard assumptions that the value functions of controlled diffusion processes can be approximated with order 1/6 error by those with controls which are constant on uniform time intervals. In this note we refine the proof and show that the provable rate can be improved to 1/4, which is optimal in our setting. Moreover, we demonstrate the improvements this implies for error estimates derived by similar techniques for approximation schemes, bringing these in line with the best available results from the PDE literature."
            },
            {
                "arxivId": "1602.08448",
                "title": "Simple Bayesian Algorithms for Best Arm Identification",
                "abstract": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. Just as the multiarmed bandit problem crystallizes the tradeoff between exploration and exploitation, this \u201cpure exploration\u201d variant crystallizes the challenge of rapidly gathering information before committing to a final decision. The paper proposes several simple Bayesian algorithms for allocating measurement effort and, by characterizing fundamental asymptotic limits on the performance of any algorithm, formalizes a sense in which these seemingly naive algorithms are the best possible."
            },
            {
                "arxivId": "1302.1611",
                "title": "Bounded regret in stochastic multi-armed bandits",
                "abstract": "We study the stochastic multi-armed bandit problem when one knows the value $\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the smallest positive gap $\\Delta$. We propose a new randomized policy that attains a regret {\\em uniformly bounded over time} in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is not possible if one only knows $\\mu^{(\\star)}$"
            },
            {
                "arxivId": "math/0601636",
                "title": "Error bounds for monotone approximation schemes for parabolic Hamilton-Jacobi-Bellman equations",
                "abstract": ". We obtain nonsymmetric upper and lower bounds on the rate of convergence of general monotone approximation/numerical schemes for parabolic Hamilton-Jacobi-Bellman equations by introducing a new notion of consistency. Our results are robust and general - they improve and extend earlier results by Krylov, Barles, and Jakobsen. We apply our general results to various schemes including Crank-Nicholson type finite difference schemes, splitting methods, and the classical approximation by piecewise constant controls. In the first two cases our results are new, and in the last two cases the results are obtained by a new method which we develop here."
            },
            {
                "arxivId": "math/0411248",
                "title": "The Rate of Convergence of Finite-Difference Approximations for Bellman Equations with Lipschitz Coefficients",
                "abstract": null
            },
            {
                "arxivId": "math/9207212",
                "title": "User\u2019s guide to viscosity solutions of second order partial differential equations",
                "abstract": "The notion of viscosity solutions of scalar fully nonlinear partial differential equations of second order provides a framework in which startling comparison and uniqueness theorems, existence theorems, and theorems about continuous dependence may now be proved by very efficient and striking arguments. The range of important applications of these results is enormous. This article is a self-contained exposition of the basic theory of viscosity solutions"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17329",
        "category": "econ",
        "title": "Assessing public perception of car automation in Iran: Acceptance and willingness to pay for adaptive cruise control",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17334",
        "category": "econ",
        "title": "Efficient estimation of parameters in marginal in semiparametric multivariate models",
        "abstract": "We consider a general multivariate model where univariate marginal distributions are known up to a common parameter vector and we are interested in estimating that vector without assuming anything about the joint distribution, except for the marginals. If we assume independence between the marginals and maximize the resulting quasi-likelihood, we obtain a consistent but inefficient estimate. If we assume a parametric copula (other than independence) we obtain a full MLE, which is efficient but only under correct copula specification and badly biased if the copula is misspecified. Instead we propose a sieve MLE estimator which improves over OMLE but does not suffer the drawbacks of the full MLE. We model the unknown part of the joint distribution using the Bernstein-Kantorovich polynomial copula and assess the resulting improvement over QMLE and over misspecified FMLE in terms of relative efficiency and robustness. We derive the asymptotic distribution of the new estimator and show that it reaches the semiparametric efficiency bound. Simulations suggest that the sieve MLE can be almost as efficient as FMLE relative to QMLE provided there is enough dependence between the marginals. An application using insurance company loss and expense data demonstrates empirical relevance of the estimator.",
        "references": [
            {
                "arxivId": "1110.3572",
                "title": "Information bounds for Gaussian copulas.",
                "abstract": "Often of primary interest in the analysis of multivariate data are the copula parameters describing the dependence among the variables, rather than the univariate marginal distributions. Since the ranks of a multivariate dataset are invariant to changes in the univariate marginal distributions, rank-based estimators are natural candidates for semiparametric copula estimation. Asymptotic information bounds for such estimators can be obtained from an asymptotic analysis of the rank likelihood, i.e. the probability of the multivariate ranks. In this article, we obtain limiting normal distributions of the rank likelihood for Gaussian copula models. Our results cover models with structured correlation matrices, such as exchangeable or circular correlation models, as well as unstructured correlation matrices. For all Gaussian copula models, the limiting distribution of the rank likelihood ratio is shown to be equal to that of a parametric likelihood ratio for an appropriately chosen multivariate normal model. This implies that the semiparametric information bounds for rank-based estimators are the same as the information bounds for estimators based on the full data, and that the multivariate normal distributions are least favorable."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17384",
        "category": "econ",
        "title": "Modeling how and why aquatic vegetation removal can free rural households from poverty-disease traps",
        "abstract": "Infectious disease can reduce labor productivity and incomes, trapping subpopulations in a vicious cycle of ill health and poverty. Efforts to boost African farmers' agricultural production through fertilizer use can inadvertently promote the growth of aquatic vegetation that hosts disease vectors. Recent trials established that removing aquatic vegetation habitat for snail intermediate hosts reduces schistosomiasis infection rates in children, while converting the harvested vegetation into compost boosts agricultural productivity and incomes. Our model illustrates how this ecological intervention changes the feedback between the human and natural systems, potentially freeing rural households from poverty-disease traps. We develop a bioeconomic model that interacts an analytical microeconomic model of agricultural households' behavior, health status and incomes over time with a dynamic model of schistosomiasis disease ecology. We calibrate the model with field data from northern Senegal. We show analytically and via simulation that local conversion of invasive aquatic vegetation to compost changes the feedbacks among interlinked disease, aquatic and agricultural systems, reducing schistosomiasis infection and increasing incomes relative to the current status quo, in which villagers rarely remove vegetation. Aquatic vegetation removal disrupts the poverty-disease trap by reducing habitat for snails that vector the infectious helminth and by promoting production of compost that returns to agricultural soils nutrients that currently leach into surface water from on-farm fertilizer applications. The result is healthier people, more productive labor, cleaner water, more productive agriculture, and higher incomes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17391",
        "category": "econ",
        "title": "Education Policy and Intergenerational Educational Persistence: Evidence from rural Benin",
        "abstract": "This paper employs a nonlinear difference-in-differences approach to empirically examine the Maximally Maintained Inequality (MMI) hypothesis in rural Benin. The findings of this study confirm the MMI hypothesis. In particular, it is observed that when 76% of educated parents choose to educate their daughters in the absence of educational programs, in contrast to only 37% among non-educated parents, the average impact of tuition fee subsidy on enrollment probability in primary schools stands at 3.8\\% for non-educated households and 0.27% for educated households. Conversely, in cases where only 27% of educated parents decide to educate their daughters without education programs, the average effect of tuition fee waivers on enrollment probability in primary schools increases to 19.64\\% for non-educated households and 24\\% for educated households. From the analysis of household education decisions influenced by a preference for education and budget constraints, three key conclusions emerge to explain the mechanism behind the MMI. Firstly, when the income advantage of educated households compared to non-educated households is significantly high, irrespective of the level of their preference advantage, reducing the financial cost of education induces a greater shift in education decisions among non-educated households. Secondly, in situations where educated households do not possess an income advantage relative to non-educated households, the reduction in education-related financial costs leads to a more pronounced change in education decisions among educated households. Lastly, for the low-income advantage of educated households, as the income advantage of educated households increases, non-educated households respond more to education policy than educated parents, if the preference advantage of educated households is relatively smaller.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17402",
        "category": "econ",
        "title": "Coordinating Resource Allocation during Product Transitions Using a Multifollower Bilevel Programming Model",
        "abstract": "We study the management of product transitions in a semiconductor manufacturing firm that requires the coordination of resource allocation decisions by multiple, autonomous Product Divisions using a multi-follower bilevel model to capture the hierarchical and decentralized nature of this decision process. Corporate management, acting as the leader, seeks to maximize the firm's total profit over a finite horizon. The followers consist of multiple Product Divisions that must share manufacturing and engineering resources to develop, produce and sell products in the market. Each Product Division needs engineering capacity to develop new products, and factory capacity to produce products for sale while also producing the prototypes and samples needed for the product development process. We model this interdependency between Product Divisions as a generalized Nash equilibrium problem at the lower level and propose a reformulation where Corporate Management acts as the leader to coordinate the resource allocation decisions. We then derive an equivalent single-level reformulation and develop a cut-and-column generation algorithm. Extensive computational experiments evaluate the performance of the algorithm and provide managerial insights on how key parameters and the distribution of decision authority affect system performance.",
        "references": [
            {
                "arxivId": "2212.06984",
                "title": "Market Mechanisms for Low-Carbon Electricity Investments: A Game-Theoretical Analysis",
                "abstract": "Electricity markets are transforming from the dominance of conventional energy resources (CERs), e.g., fossil fuels, to low-carbon energy resources (LERs), e.g., renewables and energy storage. This work examines market mechanisms to incentivize LER investments, while ensuring adequate market revenues for investors, guiding investors' strategic investments towards social optimum, and protecting consumers from scarcity prices. To reduce the impact of excessive scarcity prices, we present a new market mechanism, which consists of a Penalty payment for lost load, a supply Incentive, and an energy price Uplift (PIU). We establish a game-theoretical framework to analyze market equilibrium. We prove that one Nash equilibrium under the penalty payment and supply incentive can reach the social optimum given quadratic supply costs of CERs. Although the price uplift can ensure adequate revenues, the resulting system cost deviates from the social optimum while the gap decreases as more CERs retire. Furthermore, under the traditional marginal-cost pricing (MCP) mechanism, investors may withhold investments to cause scarcity prices, but such behavior is absent under the PIU mechanism. Simulation results show that the PIU mechanism can reduce consumers' costs by over 30% compared with the MCP mechanism by reducing excessive revenues of low-cost CERs from scarcity prices."
            },
            {
                "arxivId": "2104.06496",
                "title": "A framework for generalized Benders\u2019 decomposition and its application to multilevel optimization",
                "abstract": null
            },
            {
                "arxivId": "1807.07814",
                "title": "Interactive Supercomputing on 40,000 Cores for Machine Learning and Data Analysis",
                "abstract": "Interactive massively parallel computations are critical for machine learning and data analysis. These computations are a staple of the MIT Lincoln Laboratory Supercomputing Center (LLSC) and has required the LLSC to develop unique interactive supercomputing capabilities. Scaling interactive machine learning frameworks, such as TensorFlow, and data analysis environments, such as MATLAB/Octave, to tens of thousands of cores presents many technical challenges \u2013 in particular, rapidly dispatching many tasks through a scheduler, such as Slurm, and starting many instances of applications with thousands of dependencies. Careful tuning of launches and prepositioning of applications overcome these challenges and allow the launching of thousands of tasks in seconds on a 40,000-core supercomputer. Specifically, this work demonstrates launching 32,000 TensorFlow processes in 4 seconds and launching 262,000 Octave processes in 40 seconds. These capabilities allow researchers to rapidly explore novel machine learning architecture and data analysis algorithms."
            },
            {
                "arxivId": "1707.06196",
                "title": "A projection-based reformulation and decomposition algorithm for global optimization of a class of mixed integer bilevel linear programs",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17448",
        "category": "econ",
        "title": "What you know or who you know? The role of intellectual and social capital in opportunity recognition",
        "abstract": "The recognition of business opportunities is the first stage in the entrepreneurial process. This article analyses the effects of individuals\u2019 possession of and access to knowledge on the probability of recognizing good business opportunities in their area of residence. The authors use an eclectic theoretical framework, consisting of intellectual and social capital concepts. In particular, they analyse the role of individuals\u2019 educational level, their perception that they have the right knowledge and skills to start a business, whether they own and manage a firm, their contacts with other entrepreneurs, and whether they have been business angels. The hypotheses proposed here are tested using data collected for the GEM project in Spain in 2007. The results show that individuals\u2019 access to external knowledge through the social networks in which they participate, is fundamental for developing the capacity to recognize new business opportunities.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17578",
        "category": "econ",
        "title": "Similarity and Comparison Complexity",
        "abstract": "Some choice options are more difficult to compare than others. This paper develops a theory of what makes a comparison complex, and how comparison complexity generates systematic mistakes in choice. In our model, options are easier to compare when they 1) share similar features, holding fixed their value difference, and 2) are closer to dominance. We show how these two postulates yield tractable measures of comparison complexity in the domains of multiattribute, lottery, and intertemporal choice. Using experimental data on binary choices, we demonstrate that our complexity measures predict choice errors, choice inconsistency, and cognitive uncertainty across all three domains. We then show how canonical anomalies in choice and valuation, such as context effects, preference reversals, and apparent probability weighting and present bias in the valuation of risky and intertemporal prospects, can be understood as responses to comparison complexity.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17595",
        "category": "econ",
        "title": "Marginal treatment effects in the absence of instrumental variables",
        "abstract": "We propose a method for defining, identifying, and estimating the marginal treatment effect (MTE) without imposing the instrumental variable (IV) assumptions of independence, exclusion, and separability (or monotonicity). Under a new definition of the MTE based on reduced-form treatment error that is statistically independent of the covariates, we find that the relationship between the MTE and standard treatment parameters holds in the absence of IVs. We provide a set of sufficient conditions ensuring the identification of the defined MTE in an environment of essential heterogeneity. The key conditions include a linear restriction on potential outcome regression functions, a nonlinear restriction on the propensity score, and a conditional mean independence restriction that will lead to additive separability. We prove this identification using the notion of semiparametric identification based on functional form. We suggest consistent semiparametric estimation procedures, and provide an empirical application for the Head Start program to illustrate the usefulness of the proposed method in analyzing heterogenous causal effects when IVs are elusive.",
        "references": [
            {
                "arxivId": "1709.09284",
                "title": "Sharp Bounds and Testability of a Roy Model of STEM Major Choices",
                "abstract": "We analyze the empirical content of the Roy model, stripped down to sector-specific unobserved heterogeneity and self-selection on the basis of potential outcomes. We characterize sharp bounds on the joint distribution of potential outcomes and testable implications of the Roy model. We apply these bounds to derive a measure of departure from Roy self-selection, so as to identify prime targets for intervention. Special emphasis is put on the case of binary outcomes. We analyze a Roy model of college major choice in Canada and Germany and take a new look at the underrepresentation of women in science, technology, engineering, and mathematics."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17886",
        "category": "econ",
        "title": "Filipino Use of Designer and Luxury Perfumes: A Pilot Study of Consumer Behavior",
        "abstract": "This study investigates the usage patterns and purposes of designer perfumes among Filipino consumers, employing purposive and snowball sampling methods as non-probability sampling techniques. Data was collected using Google Forms, and the majority of respondents purchased full bottles of designer perfumes from retailers, wholesalers, and physical stores, with occasional\"blind purchases.\"Daily usage was common, with respondents applying an average of 5.88 sprays in the morning, favoring fresh scent notes and Eau De Parfum concentration. They tended to alternate perfumes daily, selecting different scent profiles according to the Philippine climate. The study reveals that Filipino respondents primarily use designer perfumes to achieve a pleasant and fresh fragrance. Additionally, these perfumes play a role in boosting self-esteem, elevating mood, and enhancing personal presentation. Some respondents reported fewer common applications, such as using perfume to address insomnia and migraines. Overall, the research highlights the significant role of perfume in the grooming routine of Filipino consumers. This study represents the first attempt to comprehend perfume usage patterns and purposes specifically within the Filipino context. Consequently, its findings are invaluable for manufacturers and marketers targeting the Filipino market, providing insights into consumer preferences and motivations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17909",
        "category": "econ",
        "title": "Regularizing Discrimination in Optimal Policy Learning with Distributional Targets",
        "abstract": "A decision maker typically (i) incorporates training data to learn about the relative effectiveness of the treatments, and (ii) chooses an implementation mechanism that implies an\"optimal\"predicted outcome distribution according to some target functional. Nevertheless, a discrimination-aware decision maker may not be satisfied achieving said optimality at the cost of heavily discriminating against subgroups of the population, in the sense that the outcome distribution in a subgroup deviates strongly from the overall optimal outcome distribution. We study a framework that allows the decision maker to penalize for such deviations, while allowing for a wide range of target functionals and discrimination measures to be employed. We establish regret and consistency guarantees for empirical success policies with data-driven tuning parameters, and provide numerical results. Furthermore, we briefly illustrate the methods in two empirical settings.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-01.json",
        "arxivId": "2401.17971",
        "category": "econ",
        "title": "Let's Roll Back! The Challenging Task of Regulating Temporary Contracts",
        "abstract": "In this paper, we evaluate the impact of a reform introduced in Italy in 2018 (Decreto Dignit\\`a), which increased the rigidity of employment protection legislation (EPL) of temporary contracts, rolling back previous policies, to reduce job instability. We use longitudinal labour force data from 2016 to 2019 and adopt a time-series technique within a Rubin Casual Model (RCM) framework to estimate the causal effect of the reform. We find that the reform was successful in reducing persistence into temporary employment and increasing the flow from temporary to permanent employment, in particular among women and young workers in the North of Italy, with significant effects on the stocks of permanent employment (+), temporary employment (-) and unemployment (-). However, this positive outcome came at the cost of higher persistence into inactivity, lower outflows from unemployment to temporary employment and higher outflows from unemployment to inactivity among males and low-educated workers.",
        "references": [
            {
                "arxivId": "1706.07840",
                "title": "Time Series Experiments and Causal Estimands: Exact Randomization Tests and Trading",
                "abstract": "Abstract We define causal estimands for experiments on single time series, extending the potential outcome framework to dealing with temporal data. Our approach allows the estimation of a broad class of these estimands and exact randomization-based p-values for testing causal effects, without imposing stringent assumptions. We further derive a general central limit theorem that can be used to conduct conservative tests and build confidence intervals for causal effects. Finally, we provide three methods for generalizing our approach to multiple units that are receiving the same class of treatment, over time. We test our methodology on simulated \u201cpotential autoregressions,\u201d which have a causal interpretation. Our methodology is partially inspired by data from a large number of experiments carried out by a financial company who compared the impact of two different ways of trading equity futures contracts. We use our methodology to make causal statements about their trading methods. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2005.12600",
        "category": "econ",
        "title": "The Race between Technology and Woman: Changes in Gender and Skill Premia in OECD Countries",
        "abstract": "The era of technological change entails complex patterns of changes in wages and employment. We develop a unified framework to measure the contribution of technological change embodied in new capital to changes in the relative wages and income shares of different types of labor. We obtain the aggregate elasticities of substitution by estimating and aggregating sectoral production function parameters with cross-country and cross-industry panel data from OECD countries. We show that advances in information, communication, and computation technologies contribute significantly to narrowing the gender wage gap, widening the skill wage gap, and declining labor shares.",
        "references": [
            {
                "arxivId": "1904.09857",
                "title": "ICT Capital\u2013Skill Complementarity and Wage Inequality: Evidence from OECD Countries",
                "abstract": null
            },
            {
                "arxivId": "1806.01221",
                "title": "Quasi-Experimental Shift-Share Research Designs",
                "abstract": "Many studies use shift-share (or \"Bartik\") instruments, which average a set of shocks with exposure share weights. We provide a new econometric framework for shift-share instrumental variable (SSIV) regressions in which identification follows from the quasi-random assignment of shocks, while exposure shares are allowed to be endogenous. The framework is motivated by an equivalence result: the orthogonality between a shift-share instrument and an unobserved residual can be represented as the orthogonality between the underlying shocks and a shock-level unobservable. SSIV regression coefficients can similarly be obtained from an equivalent shock-level regression, motivating shock-level conditions for their consistency. We discuss and illustrate several practical insights of this framework in the setting of Autor et al. (2013), estimating the effect of Chinese import competition on manufacturing employment across U.S. commuting zones."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2010.03898",
        "category": "econ",
        "title": "Consistent Specification Test of the Quantile Autoregression",
        "abstract": "This paper proposes a test for the joint hypothesis of correct dynamic specification and no omitted latent factors for the Quantile Autoregression. If the composite null is rejected we proceed to disentangle the cause of rejection, i.e., dynamic misspecification or an omitted variable. We establish the asymptotic distribution of the test statistics under fairly weak conditions and show that factor estimation error is negligible. A Monte Carlo study shows that the suggested tests have good finite sample properties. Finally, we undertake an empirical illustration of modelling GDP growth and CPI inflation in the United Kingdom, where we find evidence that factor augmented models are correctly specified in contrast with their non-augmented counterparts when it comes to GDP growth, while also exploring the asymmetric behaviour of the growth and inflation distributions.",
        "references": [
            {
                "arxivId": "1911.02173",
                "title": "Quantile Factor Models",
                "abstract": "Quantile factor models (QFM) represent a new class of factor models for high\u2010dimensional panel data. Unlike approximate factor models (AFM), which only extract mean factors, QFM also allow unobserved factors to shift other relevant parts of the distributions of observables. We propose a quantile regression approach, labeled Quantile Factor Analysis (QFA), to consistently estimate all the quantile\u2010dependent factors and loadings. Their asymptotic distributions are established using a kernel\u2010smoothed version of the QFA estimators. Two consistent model selection criteria, based on information criteria and rank minimization, are developed to determine the number of factors at each quantile. QFA estimation remains valid even when the idiosyncratic errors exhibit heavy\u2010tailed distributions. An empirical application illustrates the usefulness of QFA by highlighting the role of extra factors in the forecasts of U.S. GDP growth and inflation rates using a large set of predictors."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2205.07256",
        "category": "econ",
        "title": "Market-Based Asset Price Probability",
        "abstract": "We consider the randomness of market trade values and volumes as the origin of asset price stochasticity. We define the first four market-based price statistical moments that depend on statistical moments and correlations of market trade values and volumes. Market-based price statistical moments coincide with conventional frequency-based ones if all trade volumes are constant during the time averaging interval. We present approximations of market-based price probability by a finite number of price statistical moments. We consider the consequences of the use of market-based price statistical moments for asset-pricing models and Value-at-Risk. We show that the use of volume weighted average price results in zero price-volume correlations. We derive market-based correlations between price and squares of volume and between squares of price and volume. To forecast market-based price volatility at horizon T one should predict the first two statistical moments of market trade values and volumes and their correlations at the same horizon T.",
        "references": [
            {
                "arxivId": "1509.08503",
                "title": "Volume Weighted Average Price Optimal Execution",
                "abstract": "We study the problem of optimal execution of a trading order under Volume Weighted Average Price (VWAP) benchmark, from the point of view of a risk-averse broker. The problem consists in minimizing mean-variance of the slippage, with quadratic transaction costs. We devise multiple ways to solve it, in particular we study how to incorporate the information coming from the market during the schedule. Most related works in the literature eschew the issue of imperfect knowledge of the total market volume. We instead incorporate it in our model. We validate our method with extensive simulation of order execution on real NYSE market data. Our proposed solution, using a simple model for market volumes, reduces by 10% the VWAP deviation RMSE of the standard \"static\" solution (and can simultaneously reduce transaction costs)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2208.14829",
        "category": "econ",
        "title": "The Optimality of (Stochastic) Veto Delegation",
        "abstract": "We analyze the optimal delegation problem between a principal and an agent, assuming that the latter has state-independent preferences. We demonstrate that if the principal is more risk-averse than the agent toward non-status quo options, an optimal mechanism is a {\\em veto mechanism}. In a veto mechanism, the principal uses veto (i.e., maintaining the status quo) to balance the agent's incentives and does not randomize among non-status quo options. We characterize the optimal veto mechanism in a one-dimensional setting. In the solution, the principal uses veto only when the state surpasses a critical threshold.",
        "references": [
            {
                "arxivId": "2208.11835",
                "title": "Optimal Delegation in a Multidimensional World",
                "abstract": "In this paper, we investigate an optimal delegation model with multidimensional actions and multidimensional states of the world. The principal is responsible for taking an action, and her payoff is influenced by an unknown state of the world, which represents the agent's private information. No monetary transfers are available. Since the principal's and agent's preferences are not aligned, the agent is unwilling to simply disclose the state. The principal can employ arbitrary mechanisms, including stochastic ones, and her objective is to maximize her expected payoff."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2209.11444",
        "category": "econ",
        "title": "Treatment Effects with Multidimensional Unobserved Heterogeneity: Identification of the Marginal Treatment Effect",
        "abstract": "This paper establishes sufficient conditions for the identification of the marginal treatment effects with multivalued treatments. Our model is based on a multinomial choice model with utility maximization. Our MTE generalizes the MTE defined in Heckman and Vytlacil (2005) in binary treatment models. As in the binary case, we can interpret the MTE as the treatment effect for persons who are indifferent between two treatments at a particular level. Our MTE enables one to obtain the treatment effects of those with specific preference orders over the choice set. Further, our results can identify other parameters such as the marginal distribution of potential outcomes.",
        "references": [
            {
                "arxivId": "2010.04385",
                "title": "Identification of multi-valued treatment effects with unobserved heterogeneity",
                "abstract": null
            },
            {
                "arxivId": "1805.00057",
                "title": "Identifying Effects of Multivalued Treatments",
                "abstract": "Multivalued treatment models have only been studied so far under restrictive assumptions: ordered choice, or more recently unordered monotonicity. We show how marginal treatment effects can be identified in a more general class of models. Our results rely on two main assumptions: treatment assignment must be a measurable function of threshold-crossing rules; and enough continuous instruments must be available. On the other hand, we do not require any kind of monotonicity condition. We illustrate our approach on several commonly used models; and we also discuss the identification power of discrete instruments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2401.09718",
        "category": "econ",
        "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy",
        "abstract": "Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It also has the potential to help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity.",
        "references": [
            {
                "arxivId": "2304.11771",
                "title": "Generative AI at Work",
                "abstract": "We study the staggered introduction of a generative AI-based conversational assistant using data from 5,000 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14 percent on average, with the greatest impact on novice and low-skilled workers, and minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the potentially tacit knowledge of more able workers and helps newer workers move down the experience curve. In addition, we show that AI assistance improves customer sentiment, reduces requests for managerial intervention, and improves employee retention."
            },
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2401.17688",
        "category": "econ",
        "title": "Wages and Capital returns in a generalized P\\'olya urn",
        "abstract": "It is a widely observed phenomenon that wealth is distributed significantly more unequal than wages. In this paper we study this phenomenon using a new extension of P\\'olyas urn, modelling wealth growth through wages and capital returns. We focus in particular on the role of increasing return rates on capital, which have been identified as a main driver of inequality, and labor share, the second main parameter of our model. We fit the parameters from real-world data in Germany, so that simulation results reproduce the empirical wealth distribution and recent dynamics in Germany quite accurately, and are essentially independent from initial conditions. Our model is simple enough to allow for a detailed mathematical analysis and provides interesting predictions for future developments and on the importance of wages and capital returns for wealth aggregation. We also provide an extensive discussion of the robustness of our results and the plausibility of the main assumptions used in our model.",
        "references": [
            {
                "arxivId": "2401.09871",
                "title": "Wealth dynamics in a multi-aggregate closed monetary system",
                "abstract": "We examine the statistical properties of a closed monetary economy with multi-aggregates interactions. Building upon Yakovenko's single-agent monetary model (Dragulescu and Yakovenko, 2000), we investigate the joint equilibrium distribution of aggregate size and wealth. By comparing theoretical and simulated data, we validate our findings and investigate the influence of both micro dynamics and macro characteristics of the system on the distribution. Additionally, we analyze the system's convergence towards equilibrium under various conditions. Our laboratory model may offer valuable insights into macroeconomic phenomena allowing to reproduce typical wealth distribution features observed in real economy."
            },
            {
                "arxivId": "2311.15263",
                "title": "Strong limit theorems for step-reinforced random walks",
                "abstract": "A step-reinforced random walk is a discrete-time non-Markovian process with long range memory. At each step, with a fixed probability p, the positively step-reinforced random walk repeats one of its preceding steps chosen uniformly at random, and with complementary probability 1-p, it has an independent increment. The negatively step-reinforced random walk follows the same reinforcement algorithm but when a step is repeated its sign is also changed. Strong laws of large numbers and strong invariance principles are established for positively and negatively step-reinforced random walks in this work. Our approach relies on two general theorems on invariance principle for martingale difference sequences and a truncation argument. As by-products of our main results, the law of iterated logarithm and the functional central limit theorem are also obtained for step-reinforced random walks."
            },
            {
                "arxivId": "2303.01210",
                "title": "Asymptotics of generalized P\\'olya urns with non-linear feedback",
                "abstract": "Generalized P\\'olya urns with non-linear feedback are an established probabilistic model to describe the dynamics of growth processes with reinforcement, a generic example being competition of agents in evolving markets. It is well known which conditions on the feedback mechanism lead to monopoly where a single agent achieves full market share, and various further results for particular feedback mechanisms have been derived from different perspectives. In this paper we provide a comprehensive account of the possible asymptotic behaviour for a large general class of feedback, and describe in detail how monopolies emerge in a transition from sub-linear to super-linear feedback via hierarchical states close to linearity. We further distinguish super- and sub-exponential feedback, which show conceptually interesting differences to understand the monopoly case, and study robustness of the asymptotics with respect to initial conditions, heterogeneities and small changes of the feedback mechanisms. Finally, we derive a scaling limit for the full time evolution of market shares in the limit of diverging initial market size, including the description of typical fluctuations and extending previous results in the context of stochastic approximation."
            },
            {
                "arxivId": "2302.03677",
                "title": "Wealth distribution on a dynamic complex network",
                "abstract": "We present an agent-based model of microscopic wealth exchange in a dynamic network to study the topological features associated with economic inequality. The model evolves through two alternating processes, the conservative exchange of wealth between connected agents and the rewiring of connections, which depends on the wealth of the agents. The two dynamics are interrelated; from the dynamics of wealth a complex network emerges and the network in turn dictates who interacts with whom. We study the time evolution and the economic and topological asymptotic characteristics of the model for different values of a social protection factor $f$, which favors the poorest agent in each wealth transaction. In the case of $f=0$, our results show condensation of wealth and connections in a few agents, in accordance with the mean field models with respect to wealth. Low values of $f$ favor agents from the middle and upper classes, leading to the formation of hubs in the network. As $f$ increases, the network restriction on exchanges gives rise to an egalitarian society different from the results outside the midfield network."
            },
            {
                "arxivId": "2107.02169",
                "title": "A study of UK household wealth through empirical analysis and a non-linear Kesten process",
                "abstract": "We study the wealth distribution of UK households through a detailed analysis of data from wealth surveys and rich lists, and propose a non-linear Kesten process to model the dynamics of household wealth. The main features of our model are that we focus on wealth growth and disregard exchange, and that the rate of return on wealth is increasing with wealth. The linear case with wealth-independent return rate has been well studied, leading to a log-normal wealth distribution in the long time limit which is essentially independent of initial conditions. We find through theoretical analysis and simulations that the non-linearity in our model leads to more realistic power-law tails, and can explain an apparent two-tailed structure in the empirical wealth distribution of the UK and other countries. Other realistic features of our model include an increase in inequality over time, and a stronger dependence on initial conditions compared to linear models."
            },
            {
                "arxivId": "2102.01268",
                "title": "Simulation of a generalized asset exchange model with economic growth and wealth distribution.",
                "abstract": "The agent-based yard-sale model of wealth inequality is generalized to incorporate exponential economic growth and its distribution. The distribution of economic growth is nonuniform and is determined by the wealth of each agent and a parameter \u03bb. Our numerical results indicate that the model has a critical point at \u03bb=1 between a phase for \u03bb<1 with economic mobility and exponentially growing wealth of all agents and a nonstationary phase for \u03bb\u22651 with wealth condensation and no mobility. We define the energy of the system and show that the system can be considered to be in thermodynamic equilibrium for \u03bb<1. Our estimates of various critical exponents are consistent with a mean-field theory [see W. Klein et al., following paper, Phys. Rev. E 104, 014151 (2021)10.1103/PhysRevE.104.014151]. The exponents do not obey the usual scaling laws unless a combination of parameters that we refer to as the Ginzburg parameter is held fixed as the phase transition is approached. The model illustrates that both poorer and richer agents benefit from economic growth if its distribution does not favor the richer agents too strongly. This work and the following theoretical paper contribute to our understanding of whether the methods of equilibrium statistical mechanics can be applied to economic systems."
            },
            {
                "arxivId": "1904.05875",
                "title": "Wealth distribution models with regulations: Dynamics and equilibria",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00172",
        "category": "econ",
        "title": "The Fourier-Malliavin Volatility (FMVol) MATLAB\u00ae Library",
        "abstract": "This paper presents the Fourier-Malliavin Volatility (FMVol) estimation library for MATLAB. This library includes functions that implement Fourier- Malliavin estimators (see Malliavin and Mancino (2002, 2009)) of the volatility and co-volatility of continuous stochastic volatility processes and second-order quantities, like the quarticity (the squared volatility), the volatility of volatility and the leverage (the covariance between changes in the process and changes in its volatility). The Fourier-Malliavin method is fully non-parametric, does not require equally-spaced observations and is robust to measurement errors, or noise, without any preliminary bias correction or pre-treatment of the observations. Further, in its multivariate version, it is intrinsically robust to irregular and asynchronous sampling. Although originally introduced for a specific application in financial econometrics, namely the estimation of asset volatilities, the Fourier-Malliavin method is a general method that can be applied whenever one is interested in reconstructing the latent volatility and second-order quantities of a continuous stochastic volatility process from discrete observations.",
        "references": [
            {
                "arxivId": "1910.06660",
                "title": "Stochastic leverage effect in high-frequency data: a Fourier based analysis",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00184",
        "category": "econ",
        "title": "The Heterogeneous Aggregate Valence Analysis (HAVAN) Model: A Flexible Approach to Modeling Unobserved Heterogeneity in Discrete Choice Analysis",
        "abstract": "This paper introduces the Heterogeneous Aggregate Valence Analysis (HAVAN) model, a novel class of discrete choice models. We adopt the term\"valence'' to encompass any latent quantity used to model consumer decision-making (e.g., utility, regret, etc.). Diverging from traditional models that parameterize heterogeneous preferences across various product attributes, HAVAN models (pronounced\"haven\") instead directly characterize alternative-specific heterogeneous preferences. This innovative perspective on consumer heterogeneity affords unprecedented flexibility and significantly reduces simulation burdens commonly associated with mixed logit models. In a simulation experiment, the HAVAN model demonstrates superior predictive performance compared to state-of-the-art artificial neural networks. This finding underscores the potential for HAVAN models to improve discrete choice modeling capabilities.",
        "references": [
            {
                "arxivId": "2302.09871",
                "title": "Attitudes and Latent Class Choice Models using Machine learning",
                "abstract": "Latent Class Choice Models (LCCM) are extensions of discrete choice models (DCMs) that capture unobserved heterogeneity in the choice process by segmenting the population based on the assumption of preference similarities. We present a method of efficiently incorporating attitudinal indicators in the specification of LCCM, by introducing Artificial Neural Networks (ANN) to formulate latent variables constructs. This formulation overcomes structural equations in its capability of exploring the relationship between the attitudinal indicators and the decision choice, given the Machine Learning (ML) flexibility and power in capturing unobserved and complex behavioural features, such as attitudes and beliefs. All of this while still maintaining the consistency of the theoretical assumptions presented in the Generalized Random Utility model and the interpretability of the estimated parameters. We test our proposed framework for estimating a Car-Sharing (CS) service subscription choice with stated preference data from Copenhagen, Denmark. The results show that our proposed approach provides a complete and realistic segmentation, which helps design better policies."
            },
            {
                "arxivId": "2210.10875",
                "title": "logitr: Fast Estimation of Multinomial and Mixed Logit Models with Preference Space and Willingness-to-Pay Space Utility Parameterizations",
                "abstract": "This paper introduces the logitr R package for fast maximum likelihood estimation of multinomial logit and mixed logit models with unobserved heterogeneity across individuals, which is modeled by allowing parameters to vary randomly over individuals according to a chosen distribution. The package is faster than other similar packages such as mlogit, gmnl, mixl, and apollo, and it supports utility models specified with\"preference space\"or\"willingness to pay (WTP) space\"parameterizations, allowing for the direct estimation of marginal WTP. The typical procedure of computing WTP post-estimation using a preference space model can lead to unreasonable distributions of WTP across the population in mixed logit models. The paper provides a discussion of some of the implications of each utility parameterization for WTP estimates. It also highlights some of the design features that enable logitr's performant estimation speed and includes a benchmarking exercise with similar packages. Finally, the paper highlights additional features that are designed specifically for WTP space models, including a consistent user interface for specifying models in either space and a parallelized multi-start optimization loop, which is particularly useful for searching the solution space for different local minima when estimating models with non-convex log-likelihood functions."
            },
            {
                "arxivId": "2109.12042",
                "title": "Combining Discrete Choice Models and Neural Networks through Embeddings: Formulation, Interpretability and Performance",
                "abstract": null
            },
            {
                "arxivId": "2101.11948",
                "title": "Choice modelling in the age of machine learning",
                "abstract": null
            },
            {
                "arxivId": "2002.00922",
                "title": "A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00192",
        "category": "econ",
        "title": "Finite- and Large-Sample Inference for Ranks using Multinomial Data with an Application to Ranking Political Parties",
        "abstract": "It is common to rank different categories by means of preferences that are revealed through data on choices. A prominent example is the ranking of political candidates or parties using the estimated share of support each one receives in surveys or polls about political attitudes. Since these rankings are computed using estimates of the share of support rather than the true share of support, there may be considerable uncertainty concerning the true ranking of the political candidates or parties. In this paper, we consider the problem of accounting for such uncertainty by constructing confidence sets for the rank of each category. We consider both the problem of constructing marginal confidence sets for the rank of a particular category as well as simultaneous confidence sets for the ranks of all categories. A distinguishing feature of our analysis is that we exploit the multinomial structure of the data to develop confidence sets that are valid in finite samples. We additionally develop confidence sets using the bootstrap that are valid only approximately in large samples. We use our methodology to rank political parties in Australia using data from the 2019 Australian Election Survey. We find that our finite-sample confidence sets are informative across the entire ranking of political parties, even in Australian territories with few survey respondents and/or with parties that are chosen by only a small share of the survey respondents. In contrast, the bootstrap-based confidence sets may sometimes be considerably less informative. These findings motivate us to compare these methods in an empirically-driven simulation study, in which we conclude that our finite-sample confidence sets often perform better than their large-sample, bootstrap-based counterparts, especially in settings that resemble our empirical application.",
        "references": [
            {
                "arxivId": "2012.12550",
                "title": "Invidious Comparisons: Ranking and Selection as Compound Decisions",
                "abstract": "There is an innate human tendency, one might call it the \u201cleague table mentality,\u201d to construct rankings. Schools, hospitals, sports teams, movies, and myriad other objects are ranked even though their inherent multi\u2010dimensionality would suggest that\u2014at best\u2014only partial orderings were possible. We consider a large class of elementary ranking problems in which we observe noisy, scalar measurements of merit for \n n objects of potentially heterogeneous precision and are asked to select a group of the objects that are \u201cmost meritorious.\u201d The problem is naturally formulated in the compound decision framework of Robbins's (1956) empirical Bayes theory, but it also exhibits close connections to the recent literature on multiple testing. The nonparametric maximum likelihood estimator for mixture models (Kiefer and Wolfowitz (1956)) is employed to construct optimal ranking and selection rules. Performance of the rules is evaluated in simulations and an application to ranking U.S. kidney dialysis centers.\n"
            },
            {
                "arxivId": "0911.3749",
                "title": "Using the bootstrap to quantify the authority of an empirical ranking",
                "abstract": "The bootstrap is a popular and convenient method for quantifying the authority of an empirical ordering of attributes, for example of a ranking of the performance of institutions or of the influence of genes on a response variable. In the first of these examples, the number, p, of quantities being ordered is sometimes only moderate in size; in the second it can be very large, often much greater than sample size. However, we show that in both types of problem the conventional bootstrap can produce inconsistency. Moreover, the standard n-out-of-n bootstrap estimator of the distribution of an empirical rank may not converge in the usual sense; the estimator may converge in distribution, but not in probability. Nevertheless, in many cases the bootstrap correctly identifies the support of the asymptotic distribution of ranks. In some contemporary problems, bootstrap prediction intervals for ranks are particularly long, and in this context, we also quantify the accuracy of bootstrap methods, showing that the standard bootstrap gets the order of magnitude of the interval right, but not the constant multiplier of interval length. The m-out-of-n bootstrap can improve performance and produce statistical consistency, but it requires empirical choice of m; we suggest a tuning solution to this problem. We show that in genomic examples, where it might be expected that the standard, \"synchronous\" bootstrap will successfully accommodate non-independence of vector components, that approach can produce misleading results. An \"independent component\" bootstrap can overcome these difficulties, even in cases where components are not strictly independent."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00394",
        "category": "econ",
        "title": "Random partitions, potential of the Shapley value, and games with externalities",
        "abstract": "The Shapley value equals a player's contribution to the potential of a game. The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players. This computation integrates the coalition formation of all players and readily extends to games with externalities. We investigate those potential functions for games with externalities that can be computed this way. It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al. (2007, J. Econ. Theory 135, 339-356), is unique in the following sense. It is obtained as a the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00567",
        "category": "econ",
        "title": "Stochastic convergence in per capita CO2 emissions. An approach from nonlinear stationarity analysis",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00787",
        "category": "econ",
        "title": "Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning",
        "abstract": "Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.",
        "references": [
            {
                "arxivId": "2210.08569",
                "title": "Limited or Biased: Modeling Sub-Rational Human Investors in Financial Markets",
                "abstract": "Human decision-making in real-life deviates significantly from the optimal decisions made by fully rational agents, primarily due to computational limitations or psychological biases. While existing studies in behavioral finance have discovered various aspects of human sub-rationality, there lacks a comprehensive framework to transfer these findings into an adaptive human model applicable across diverse financial market scenarios. In this study, we introduce a flexible model that incorporates five different aspects of human sub-rationality using reinforcement learning. Our model is trained using a high-fidelity multi-agent market simulator, which overcomes limitations associated with the scarcity of labeled data of individual investors. We evaluate the behavior of sub-rational human investors using hand-crafted market scenarios and SHAP value analysis, showing that our model accurately reproduces the observations in the previous studies and reveals insights of the driving factors of human behavior. Finally, we explore the impact of sub-rationality on the investor's Profit and Loss (PnL) and market quality. Our experiments reveal that bounded-rational and prospect-biased human behaviors improve liquidity but diminish price efficiency, whereas human behavior influenced by myopia, optimism, and pessimism reduces market liquidity."
            },
            {
                "arxivId": "2206.08781",
                "title": "Reinforcement Learning for Economic Policy: A New Frontier?",
                "abstract": "Agent-based computational economics is a field with a rich academic history, yet one which has struggled to enter mainstream policy design toolboxes, plagued by the challenges associated with representing a complex and dynamic reality. The field of Reinforcement Learning (RL), too, has a rich history, and has recently been at the centre of several exponential developments. Modern RL implementations have been able to achieve unprecedented levels of sophistication, handling previously unthinkable degrees of complexity. This review surveys the historical barriers of classical agent-based techniques in economic modelling, and contemplates whether recent developments in RL can overcome any of them."
            },
            {
                "arxivId": "2206.05568",
                "title": "Bounded strategic reasoning explains crisis emergence in multi-agent market games",
                "abstract": "The efficient market hypothesis (EMH), based on rational expectations and market equilibrium, is the dominant perspective for modelling economic markets. However, the most notable critique of the EMH is the inability to model periods of out-of-equilibrium dynamics without significant external news. When such dynamics emerge endogenously, the traditional economic frameworks prove insufficient. This work offers an alternate perspective explaining the endogenous emergence of punctuated out-of-equilibrium dynamics based on bounded rational agents. In a concise market entrance game, we show how boundedly rational strategic reasoning can lead to endogenously emerging crises, exhibiting fat tails in returns. We also show how other common stylized facts, such as clustered volatility, arise due to agent diversity (or lack thereof) and the varying learning updates across the agents. This work explains various stylized facts and crisis emergence in economic markets, in the absence of any external news, based on agent interactions and bounded rational reasoning."
            },
            {
                "arxivId": "2006.13085",
                "title": "Calibration of Shared Equilibria in General Sum Partially Observable Markov Games",
                "abstract": "Training multi-agent systems (MAS) to achieve realistic equilibria gives us a useful tool to understand and model real-world systems. We consider a general sum partially observable Markov game where agents of different types share a single policy network, conditioned on agent-specific information. This paper aims at i) formally understanding equilibria reached by such agents, and ii) matching emergent phenomena of such equilibria to real-world targets. Parameter sharing with decentralized execution has been introduced as an efficient way to train multiple agents using a single policy network. However, the nature of resulting equilibria reached by such agents is not yet understood: we introduce the novel concept of \\textit{Shared equilibrium} as a symmetric pure Nash equilibrium of a certain Functional Form Game (FFG) and prove convergence to the latter for a certain class of games using self-play. In addition, it is important that such equilibria satisfy certain constraints so that MAS are \\textit{calibrated} to real world data for practical use: we solve this problem by introducing a novel dual-Reinforcement Learning based approach that fits emergent behaviors of agents in a Shared equilibrium to externally-specified targets, and apply our methods to a $n$-player market example. We do so by calibrating parameters governing distributions of agent types rather than individual agents, which allows both behavior differentiation among agents and coherent scaling of the shared policy network to multiple agents."
            },
            {
                "arxivId": "1910.01913",
                "title": "If MaxEnt RL is the Answer, What is the Question?",
                "abstract": "Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods."
            },
            {
                "arxivId": "1909.07528",
                "title": "Emergent Tool Use From Multi-Agent Autocurricula",
                "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests."
            },
            {
                "arxivId": "1906.00729",
                "title": "Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games",
                "abstract": "We study the global convergence of policy optimization for finding the Nash equilibria (NE) in zero-sum linear quadratic (LQ) games. To this end, we first investigate the landscape of LQ games, viewing it as a nonconvex-nonconcave saddle-point problem in the policy space. Specifically, we show that despite its nonconvexity and nonconcavity, zero-sum LQ games have the property that the stationary point of the objective function with respect to the linear feedback control policies constitutes the NE of the game. Building upon this, we develop three projected nested-gradient methods that are guaranteed to converge to the NE of the game. Moreover, we show that all of these algorithms enjoy both globally sublinear and locally linear convergence rates. Simulation results are also provided to illustrate the satisfactory convergence properties of the algorithms. To the best of our knowledge, this work appears to be the first one to investigate the optimization landscape of LQ games, and provably show the convergence of policy optimization methods to the Nash equilibria. Our work serves as an initial step toward understanding the theoretical aspects of policy-based reinforcement learning algorithms for zero-sum Markov games in general."
            },
            {
                "arxivId": "1901.09216",
                "title": "Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning",
                "abstract": "Though limited in real-world decision making, most multi-agent reinforcement learning (MARL) models assume perfectly rational agents -- a property hardly met due to individual's cognitive limitation and/or the tractability of the decision problem. In this paper, we introduce generalized recursive reasoning (GR2) as a novel framework to model agents with different \\emph{hierarchical} levels of rationality; our framework enables agents to exhibit varying levels of ``thinking'' ability thereby allowing higher-level agents to best respond to various less sophisticated learners. We contribute both theoretically and empirically. On the theory side, we devise the hierarchical framework of GR2 through probabilistic graphical models and prove the existence of a perfect Bayesian equilibrium. Within the GR2, we propose a practical actor-critic solver, and demonstrate its convergent property to a stationary point in two-player games through Lyapunov analysis. On the empirical side, we validate our findings on a variety of MARL benchmarks. Precisely, we first illustrate the hierarchical thinking process on the Keynes Beauty Contest, and then demonstrate significant improvements compared to state-of-the-art opponent modeling baselines on the normal-form games and the cooperative navigation benchmark."
            },
            {
                "arxivId": "1801.01290",
                "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
            },
            {
                "arxivId": "1506.02438",
                "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
                "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. \nOur approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
            },
            {
                "arxivId": "1204.6481",
                "title": "Thermodynamics as a theory of decision-making with information-processing costs",
                "abstract": "Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-02.json",
        "arxivId": "2402.00788",
        "category": "econ",
        "title": "EU-28\u2019s progress toward the 2020 renewable energy share: a club convergence analysis",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-05.json",
        "arxivId": "2401.16942",
        "category": "econ",
        "title": "Robust Price Discrimination",
        "abstract": "We consider a model of third-degree price discrimination, in which the seller has a valuation for the product which is unknown to the market designer, who aims to maximize the buyers' surplus by revealing information regarding the buyer's valuation to the seller. Our main result shows that the regret is bounded by $U^*(0)/e$, where $U^*(0)$ is the optimal buyer surplus in the case where the seller has zero valuation for the product. This bound is attained by randomly drawing a seller valuation and applying the segmentation of Bergemann et al. (2015) with respect to the drawn valuation. We show that the $U^*(0)/e$ bound is tight in the case of binary buyer valuation.",
        "references": [
            {
                "arxivId": "2105.09375",
                "title": "Calibrated Click-Through Auctions",
                "abstract": "We analyze the optimal information design in a click-through auction with stochastic click-through rates and known valuations per click. The auctioneer takes as given the auction rule of the click-through auction, namely the generalized second-price auction. Yet, the auctioneer can design the information flow regarding the click-through rates among the bidders. We require that the information structure to be calibrated in the learning sense. With this constraint, the auction needs to rank the ads by a product of the value and a calibrated prediction of the click-through rates. The task of designing an optimal information structure is thus reduced to the task of designing an optimal calibrated prediction. We show that in a symmetric setting with uncertainty about the click-through rates, the optimal information structure attains both social efficiency and surplus extraction. The optimal information structure requires private (rather than public) signals to the bidders. It also requires correlated (rather than independent) signals, even when the underlying uncertainty regarding the click-through rates is independent. Beyond symmetric settings, we show that the optimal information structure requires partial information disclosure, and achieves only partial surplus extraction."
            },
            {
                "arxivId": "1912.05770",
                "title": "Algorithmic Price Discrimination",
                "abstract": "We consider a generalization of the third degree price discrimination problem studied in Bergemann et al. (2015), where an intermediary between the buyer and the seller can design market segments to maximize any linear combination of consumer surplus and seller revenue. Unlike in Bergemann et al. (2015), we assume that the intermediary only has partial information about the buyer's value. We consider three different models of information, with increasing order of difficulty. In the first model, we assume that the intermediary's information allows him to construct a probability distribution of the buyer's value. Next we consider the sample complexity model, where we assume that the intermediary only sees samples from this distribution. Finally, we consider a bandit online learning model, where the intermediary can only observe past purchasing decisions of the buyer, rather than her exact value. For each of these models, we present algorithms to compute optimal or near optimal market segmentation."
            },
            {
                "arxivId": "1802.07407",
                "title": "Third-Party Data Providers Ruin Simple Mechanisms",
                "abstract": "Motivated by the growing prominence of third-party data providers in online marketplaces, this paper studies the impact of the presence of third-party data providers on mechanism design. When no data provider is present, it has been shown that simple mechanisms are \"good enough'' -- they can achieve a constant fraction of the revenue of optimal mechanisms. The results in this paper demonstrate that this is no longer true in the presence of a third-party data provider who can provide the bidder with a signal that is correlated with the item type. Specifically, even with a single seller, a single bidder, and a single item of uncertain type for sale, the strategies of pricing each item-type separately (the analog of item pricing for multi-item auctions) and bundling all item-types under a single price (the analog of grand bundling) can both simultaneously be a logarithmic factor worse than the optimal revenue. Further, in the presence of a data provider, item-type partitioning mechanisms---a more general class of mechanisms which divide item-types into disjoint groups and offer prices for each group---still cannot achieve within a $\u0142og \u0142og$ factor of the optimal revenue. Thus, our results highlight that the presence of a data-provider forces the use of more complicated mechanisms in order to achieve a constant fraction of the optimal revenue."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2105.09254",
        "category": "econ",
        "title": "Multiply Robust Causal Mediation Analysis with Continuous Treatments",
        "abstract": "In many applications, researchers are interested in the direct and indirect causal effects of a treatment or exposure on an outcome of interest. Mediation analysis offers a rigorous framework for identifying and estimating these causal effects. For binary treatments, efficient estimators for the direct and indirect effects are presented in Tchetgen Tchetgen and Shpitser (2012) based on the influence function of the parameter of interest. These estimators possess desirable properties, such as multiple-robustness and asymptotic normality, while allowing for slower than root-n rates of convergence for the nuisance parameters. However, in settings involving continuous treatments, these influence function-based estimators are not readily applicable without making strong parametric assumptions. In this work, utilizing a kernel-smoothing approach, we propose an estimator suitable for settings with continuous treatments inspired by the influence function-based estimator of Tchetgen Tchetgen and Shpitser (2012). Our proposed approach employs cross-fitting, relaxing the smoothness requirements on the nuisance functions, and allowing them to be estimated at slower rates than the target parameter. Additionally, similar to influence function-based estimators, our proposed estimator is multiply robust and asymptotically normal, making it applicable for inference in settings where a parametric model cannot be assumed.",
        "references": [
            {
                "arxivId": "1802.06037",
                "title": "Policy Evaluation and Optimization with Continuous Treatments",
                "abstract": "We study the problem of policy evaluation and learning from batched contextual bandit data when treatments are continuous, going beyond previous work on discrete treatments. Previous work for discrete treatment/action spaces focuses on inverse probability weighting (IPW) and doubly robust (DR) methods that use a rejection sampling approach for evaluation and the equivalent weighted classification problem for learning. In the continuous setting, this reduction fails as we would almost surely reject all observations. To tackle the case of continuous treatments, we extend the IPW and DR approaches to the continuous setting using a kernel function that leverages treatment proximity to attenuate discrete rejection. Our policy estimator is consistent and we characterize the optimal bandwidth. The resulting continuous policy optimizer (CPO) approach using our estimator achieves convergent regret and approaches the best-in-class policy for learnable policy classes. We demonstrate that the estimator performs well and, in particular, outperforms a discretization-based benchmark. We further study the performance of our policy optimizer in a case study on personalized dosing based on a dataset of Warfarin patients, their covariates, and final therapeutic doses. Our learned policy outperforms benchmarks and nears the oracle-best linear policy."
            },
            {
                "arxivId": "1210.4654",
                "title": "Semiparametric Theory for Causal Mediation Analysis: efficiency bounds, multiple robustness, and sensitivity analysis.",
                "abstract": "Whilst estimation of the marginal (total) causal effect of a point exposure on an outcome is arguably the most common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon evaluating the total effect of the exposure, investigators routinely wish to make inferences about the direct or indirect pathways of the effect of the exposure not through or through a mediator variable that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies, that produce double robust and highly efficient estimates of the marginal total causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable."
            },
            {
                "arxivId": "1301.2300",
                "title": "Direct and Indirect Effects",
                "abstract": "The direct effect of one event on another can be defined and measured by holding constant all intermediate variables between the two. Indirect effects present conceptual and practical difficulties (in nonlinear models), because they cannot be isolated by holding certain variables constant. This paper presents a new way of defining the effect transmitted through a restricted set of paths, without controlling variables on the remaining paths. This permits the assessment of a more natural type of direct and indirect effects, one that is applicable in both linear and nonlinear models and that has broader policy-related interpretations. The paper establishes conditions under which such assessments can be estimated consistently from experimental and nonexperimental data, and thus extends path-analytic techniques to nonlinear and nonparametric models."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2210.03639",
        "category": "econ",
        "title": "Quality of Life and the Experience of Context",
        "abstract": "I propose that quality of life can be compared despite the difference in values across cultures when it is experienced at the sensory and perceptual level. I argue that an approach to assessing quality of life which focuses on an individual's ability to organize his or her context by perceiving positive constellations of factors in the environment and his or her ability to achieve valuable acts and realize valuable states of being is more meaningful than the approaches of metrics which focus directly, and often solely, on the means of living and the means of freedom. Because the felt experience of quality of life is derived from a constellation of factors which make up the indivisible structure of a milieu, the experience of quality of life cannot be regarded as a subjective experience. Through the example of how different frequencies, and mixtures of frequencies, of light are perceived as colour by the eye, I demonstrate that the human cognitive apparatus, because of its relation to the object that is measured, apprehends different scales of quantity as degrees of quality. I show that lived experience is the result of a selective relationality with one's environment and that the experience of quality has something to do with the perception of entities in their interrelated and networked nature as wholes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2302.09168",
        "category": "econ",
        "title": "Screening Signal-Manipulating Agents via Contests",
        "abstract": "We study the design of screening mechanisms subject to competition and manipulation. A social planner has limited resources to allocate to multiple agents using only signals manipulable through unproductive effort. We show that the welfare-maximizing mechanism takes the form of a contest and characterize the optimal contest. We apply our results to two settings: either the planner has one item or a number of items proportional to the number of agents. We show that in both settings, with sufficiently many agents, a winner-takes-all contest is never optimal. In particular, the planner always benefits from randomizing the allocation to some agents.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2303.01921",
        "category": "econ",
        "title": "Trusting: Alone and together",
        "abstract": "We study the problem of an agent continuously faced with the decision of placing or not placing trust in an institution. The agent makes use of Bayesian learning in order to estimate the institution's true trustworthiness and makes the decision to place trust based on myopic rationality. Using elements from random walk theory, we explicitly derive the probability that such an agent ceases placing trust at some point in the relationship, as well as the expected time spent placing trust conditioned on their discontinuation thereof. We then continue by modelling two truster agents, each in their own relationship to the institution. We consider two natural models of communication between them. In the first (``observable rewards'') agents disclose their experiences with the institution with one another, while in the second (``observable actions'') agents merely witness the actions of their neighbour, i.e., placing or not placing trust. Under the same assumptions as in the single agent case, we describe the evolution of the beliefs of agents under these two different communication models. Both the probability of ceasing to place trust and the expected time in the system elude explicit expressions, despite there being only two agents. We therefore conduct a simulation study in order to compare the effect of the different kinds of communication on the trust dynamics. We find that a pair of agents in both communication models has a greater chance of learning the true trustworthiness of an institution than a single agent. Communication between agents promotes the formation of long term trust with a trustworthy institution as well as the timely exit from a trust relationship with an untrustworthy institution. Contrary to what one might expect, we find that having less information (observing each other's actions instead of experiences) can sometimes be beneficial to the agents.",
        "references": [
            {
                "arxivId": "2404.14953",
                "title": "Dynamic pricing with Bayesian updates from online reviews",
                "abstract": "When launching new products, firms face uncertainty about market reception. Online reviews provide valuable information not only to consumers but also to firms, allowing firms to adjust the product characteristics, including its selling price. In this paper, we consider a pricing model with online reviews in which the quality of the product is uncertain, and both the seller and the buyers Bayesianly update their beliefs to make purchasing&pricing decisions. We model the seller's pricing problem as a basic bandits' problem and show a close connection with the celebrated Catalan numbers, allowing us to efficiently compute the overall future discounted reward of the seller. With this tool, we analyze and compare the optimal static and dynamic pricing strategies in terms of the probability of effectively learning the quality of the product."
            },
            {
                "arxivId": "2112.14265",
                "title": "Learning in Repeated Interactions on Networks",
                "abstract": "We study how long-lived, rational, exponentially discounting agents learn in a social network. In every period, each agent observes the past actions of his neighbors, receives a private signal, and chooses an action with the objective of matching the state. Since agents behave strategically, and since their actions depend on higher order beliefs, it is difficult to characterize equilibrium behavior. Nevertheless, we show that regardless of the size and shape of the network, and the patience of the agents, the equilibrium speed of learning is bounded from above by a constant that only depends on the private signal distribution. The full paper is available at https://arxiv.org/abs/2112.14265."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2308.08630",
        "category": "econ",
        "title": "Cooperation and interdependence in global science funding",
        "abstract": "Investments in research and development are key to scientific and economic growth and to the well-being of society. Scientific research demands significant resources making national scientific investment a crucial driver of scientific production. As scientific production becomes increasingly multinational, it is critical to study how nations' scientific activities are funded both domestically and internationally. By tracing research grants acknowledged in scholarly publications, our study reveals a shifting duopoly of China and the United States in the global funding landscape, with a contrasting funding pattern; while China has surpassed the United States in publications with acknowledged domestic and international funding, the United States largely maintains its role as the most important global research partner. Our results also highlight the precarity of low- and middle-income countries to global funding disruptions. By revealing the complex interdependence and collaboration between countries in the global scientific enterprise, this work informs future studies investigating the national and global scientific enterprise and how funding leads to both productive cooperation and vulnerable dependencies.",
        "references": [
            {
                "arxivId": "2104.10812",
                "title": "The latent structure of global scientific development",
                "abstract": null
            },
            {
                "arxivId": "2001.04697",
                "title": "Funding information in Web of Science: an updated overview",
                "abstract": null
            },
            {
                "arxivId": "1604.04896",
                "title": "Funding Data from Publication Acknowledgments: Coverage, Uses, and Limitations",
                "abstract": "This article contributes to the development of methods for analysing research funding systems by exploring the robustness and comparability of emerging approaches to generate funding landscapes useful for policy making. We use a novel data set of manually extracted and coded data on the funding acknowledgements of 7,510 publications representing UK cancer research in the year 2011 and compare these \u201creference data\u201d with funding data provided by Web of Science (WoS) and MEDLINE/PubMed. Findings show high recall (around 93%) of WoS funding data. By contrast, MEDLINE/PubMed data retrieved less than half of the UK cancer publications acknowledging at least one funder. Conversely, both databases have high precision (+90%): That is, few cases of publications with no acknowledgment to funders are identified as having funding data. Nonetheless, funders acknowledged in UK cancer publications were not correctly listed by MEDLINE/PubMed and WoS in around 75% and 32% of the cases, respectively. Reference data on the UK cancer research funding system are used as a case study to demonstrate the utility of funding data for strategic intelligence applications (e.g., mapping of funding landscape and co\u2010funding activity, comparison of funders' research portfolios)."
            },
            {
                "arxivId": "1604.04780",
                "title": "Characterization, description, and considerations for the use of funding acknowledgement data in Web of Science",
                "abstract": null
            },
            {
                "arxivId": "0911.1044",
                "title": "Macro-level indicators of the relations between research funding and research output",
                "abstract": null
            },
            {
                "arxivId": "0904.2389",
                "title": "Extracting the multiscale backbone of complex weighted networks",
                "abstract": "A large number of complex systems find a natural abstraction in the form of weighted networks whose nodes represent the elements of the system and the weighted edges identify the presence of an interaction and its relative strength. In recent years, the study of an increasing number of large-scale networks has highlighted the statistical heterogeneity of their interaction pattern, with degree and weight distributions that vary over many orders of magnitude. These features, along with the large number of elements and links, make the extraction of the truly relevant connections forming the network's backbone a very challenging problem. More specifically, coarse-graining approaches and filtering techniques come into conflict with the multiscale nature of large-scale systems. Here, we define a filtering method that offers a practical procedure to extract the relevant connection backbone in complex multiscale networks, preserving the edges that represent statistically significant deviations with respect to a null model for the local assignment of weights to edges. An important aspect of the method is that it does not belittle small-scale interactions and operates at all scales defined by the weight distribution. We apply our method to real-world network instances and compare the obtained results with alternative backbone extraction techniques."
            },
            {
                "arxivId": "0903.5254",
                "title": "Comparing Bibliometric Statistics Obtained from the Web of Science and Scopus",
                "abstract": "For more than 40 years, the Institute for Scientific Information (ISI, now part of Thomson Reuters) produced the only available bibliographic databases from which bibliometricians could compile large-scale bibliometric indicators. ISI's citation indexes, now regrouped under the Web of Science (WoS), were the major sources of bibliometric data until 2004, when Scopus was launched by the publisher Reed Elsevier. For those who perform bibliometric analyses and comparisons of countries or institutions, the existence of these two major databases raises the important question of the comparability and stability of statistics obtained from different data sources. This paper uses macro-level bibliometric indicators to compare results obtained from the WoS and Scopus. It shows that the correlations between the measures obtained with both databases for the number of papers and the number of citations received by countries, as well as for their ranks, are extremely high (R2 > .99). There is also a very high correlation when countries' papers are broken down by field. The paper thus provides evidence that indicators of scientific production and citations at the country level are stable and largely independent of the database."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2310.13681",
        "category": "econ",
        "title": "RealFM: A Realistic Mechanism to Incentivize Federated Participation and Contribution",
        "abstract": "Edge device participation in federating learning (FL) is typically studied under the lens of device-server communication (e.g., device dropout) and assumes an undying desire from edge devices to participate in FL. As a result, current FL frameworks are flawed when implemented in realistic settings, with many encountering the free-rider dilemma. In a step to push FL towards realistic settings, we propose RealFM: the first federated mechanism that (1) realistically models device utility, (2) incentivizes data contribution and device participation, (3) provably removes the free-rider dilemma, and (4) relaxes assumptions on data homogeneity, data sharing, and monetary reward payments. Compared to previous FL mechanisms, RealFM allows for a non-linear relationship between model accuracy and utility, which improves the utility gained by the server and participating devices. On real-world data, RealFM improves device and server utility, as well as data contribution, by over 3 and 4 magnitudes respectively compared to baselines.",
        "references": [
            {
                "arxivId": "2207.04557",
                "title": "Mechanisms that Incentivize Data Sharing in Federated Learning",
                "abstract": "Federated learning is typically considered a beneficial technology which allows 1 multiple agents to collaborate with each other, improve the accuracy of their models, 2 and solve problems which are otherwise too data-intensive / expensive to be solved 3 individually. However, under the expectation that other agents will share their 4 data, rational agents may be tempted to engage in detrimental behavior such as 5 free-riding where they contribute no data but still enjoy an improved model. In 6 this work, we propose a framework to analyze the behavior of such rational data 7 generators. We first show how a naive scheme leads to catastrophic levels of 8 free-riding where the benefits of data sharing are completely eroded. Then, using 9 ideas from contract theory, we introduce accuracy shaping based mechanisms to 10 maximize the amount of data generated by each agent. These provably prevent 11 free-riding without needing any payment mechanism. 12"
            },
            {
                "arxivId": "2203.11751",
                "title": "FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction",
                "abstract": "Federated learning (FL) allows multiple clients to collectively train a high-performance global model without sharing their private data. However, the key challenge in federated learning is that the clients have significant statistical heterogeneity among their local data distributions, which would cause inconsistent optimized local models on the clientside. To address this fundamental dilemma, we propose a novel federated learning algorithm with local drift decoupling and correction (FedDC). Our FedDC only introduces lightweight modifications in the local training phase, in which each client utilizes an auxiliary local drift variable to track the gap between the local model parameter and the global model parameters. The key idea of FedDC is to utilize this learned local drift variable to bridge the gap, i.e., conducting consistency in parameter-level. The experiment results and analysis demonstrate that FedDC yields expediting convergence and better performance on various image classification tasks, robust in partial participation settings, non-iid data, and heterogeneous clients."
            },
            {
                "arxivId": "2111.11850",
                "title": "Incentive Mechanisms for Federated Learning: From Economic and Game Theoretic Perspective",
                "abstract": "Federated learning (FL) becomes popular and has shown great potentials in training large-scale machine learning (ML) models without exposing the owners\u2019 raw data. In FL, the data owners can train ML models based on their local data and only send the model updates rather than raw data to the model owner for aggregation. To improve learning performance in terms of model accuracy and training completion time, it is essential to recruit sufficient participants. Meanwhile, the data owners are rational and may be unwilling to participate in the collaborative learning process due to the resource consumption. To address the issues, there have been various works recently proposed to motivate the data owners to contribute their resources. In this paper, we provide a comprehensive review for the economic and game theoretic approaches proposed in the literature to design various schemes for incentivizing data owners to participate in FL training process. In particular, we first present the fundamentals and background of FL, economic theories commonly used in incentive mechanism design. Then, we review applications of game theory and economic approaches applied for incentive mechanisms design of FL. Finally, we highlight some open issues and future research directions concerning incentive mechanism design of FL."
            },
            {
                "arxivId": "2108.05568",
                "title": "A Contract Theory based Incentive Mechanism for Federated Learning",
                "abstract": "Federated learning (FL) serves as a data privacy-preserved machine learning paradigm, and realizes the collaborative model trained by distributed clients. To accomplish an FL task, the task publisher needs to pay financial incentives to the FL server and FL server offloads the task to the contributing FL clients. It is challenging to design proper incentives for the FL clients due to the fact that the task is privately trained by the clients. This paper aims to propose a contract theory based FL task training model towards minimizing incentive budget subject to clients being individually rational (IR) and incentive compatible (IC) in each FL training round. We design a two-dimensional contract model by formally defining two private types of clients, namely data quality and computation effort. To effectively aggregate the trained models, a contract-based aggregator is proposed. We analyze the feasible and optimal contract solutions to the proposed contract model. %Experimental results demonstrate that the proposed framework and contract model can effective improve the generation accuracy of FL tasks. Experimental results show that the generalization accuracy of the FL tasks can be improved by the proposed incentive mechanism where contract-based aggregation is applied."
            },
            {
                "arxivId": "2106.15406",
                "title": "A Comprehensive Survey of Incentive Mechanism for Federated Learning",
                "abstract": "Federated learning utilizes various resources provided by participants to collaboratively train a global model, which potentially address the data privacy issue of machine learning. In such promising paradigm, the performance will be deteriorated without sufficient training data and other resources in the learning process. Thus, it is quite crucial to inspire more participants to contribute their valuable resources with some payments for federated learning. In this paper, we present a comprehensive survey of incentive schemes for federate learning. Specifically, we identify the incentive problem in federated learning and then provide a taxonomy for various schemes. Subsequently, we summarize the existing incentive mechanisms in terms of the main techniques, such as Stackelberg game, auction, contract theory, Shapley value, reinforcement learning, blockchain. By reviewing and comparing some impressive results, we figure out three directions for the future study."
            },
            {
                "arxivId": "2011.11660",
                "title": "Differentially Private Learning Needs Better Features (or Much More Data)",
                "abstract": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area."
            },
            {
                "arxivId": "2011.10464",
                "title": "A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning",
                "abstract": "Federated learning (FL) is an emerging practical framework for effective and scalable machine learning among multiple participants, such as end users, organizations and companies. However, most existing FL or distributed learning frameworks have not well addressed two important issues together: collaborative fairness and adversarial robustness (e.g. free-riders and malicious participants). In conventional FL, all participants receive the global model (equal rewards), which might be unfair to the high-contributing participants. Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious adversaries could game the system to access the global model for free or to sabotage it. In this paper, we propose a novel Robust and Fair Federated Learning (RFFL) framework to achieve collaborative fairness and adversarial robustness simultaneously via a reputation mechanism. RFFL maintains a reputation for each participant by examining their contributions via their uploaded gradients (using vector similarity) and thus identifies non-contributing or malicious participants to be removed. Our approach differentiates itself by not requiring any auxiliary/validation dataset. Extensive experiments on benchmark datasets show that RFFL can achieve high fairness and is very robust to different types of adversaries while achieving competitive predictive accuracy."
            },
            {
                "arxivId": "2010.01264",
                "title": "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients",
                "abstract": "Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient."
            },
            {
                "arxivId": "2009.06192",
                "title": "A Principled Approach to Data Valuation for Federated Learning",
                "abstract": null
            },
            {
                "arxivId": "2008.12161",
                "title": "Collaborative Fairness in Federated Learning",
                "abstract": null
            },
            {
                "arxivId": "2010.12797",
                "title": "Collaborative Machine Learning with Incentive-Aware Model Rewards",
                "abstract": "Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets."
            },
            {
                "arxivId": "2006.11901",
                "title": "Free-rider Attacks on Model Aggregation in Federated Learning",
                "abstract": "Free-rider attacks on federated learning consist in dissimulating participation to the federated learning process with the goal of obtaining the final aggregated model without actually contributing with any data. We introduce here the first theoretical and experimental analysis of free-rider attacks on federated learning schemes based on iterative parameters aggregation, such as FedAvg or FedProx, and provide formal guarantees for these attacks to converge to the aggregated models of the fair participants. We first show that a straightforward implementation of this attack can be simply achieved by not updating the local parameters during the iterative federated optimization. As this attack can be detected by adopting simple countermeasures at the server level, we subsequently study more complex disguising schemes based on stochastic updates of the free-rider parameters. We demonstrate the proposed strategies on a number of experimental scenarios, in both iid and non-iid settings. We conclude by providing recommendations to avoid free-rider attacks in real world applications of federated learning, especially in sensitive domains where security of data and models is critical."
            },
            {
                "arxivId": "2004.03877",
                "title": "Towards Federated Learning in UAV-Enabled Internet of Vehicles: A Multi-Dimensional Contract-Matching Approach",
                "abstract": "Coupled with the rise of Deep Learning, the wealth of data and enhanced computation capabilities of Internet of Vehicles (IoV) components enable effective Artificial Intelligence (AI) based models to be built. Beyond ground data sources, Unmanned Aerial Vehicles (UAVs) based service providers for data collection and AI model training, i.e., Drones-as-a-Service (DaaS), is becoming increasingly popular in recent years. However, the stringent regulations governing data privacy potentially impedes data sharing across independently owned UAVs. To this end, we propose the adoption of a Federated Learning (FL) based approach to enable privacy-preserving collaborative Machine Learning across a federation of independent DaaS providers for the development of IoV applications, e.g., for traffic prediction and car park occupancy management. Given the information asymmetry and incentive mismatches between the UAVs and model owners, we leverage on the self-revealing properties of a multi-dimensional contract to ensure truthful reporting of the UAV types, while accounting for the multiple sources of heterogeneity, e.g., in sensing, computation, and transmission costs. Then, we adopt the Gale-Shapley algorithm to match the lowest cost UAV to each subregion. The simulation results validate the incentive compatibility of our contract design, and shows the efficiency of our matching, thus guaranteeing profit maximization for the model owner amid information asymmetry."
            },
            {
                "arxivId": "2001.08996",
                "title": "Mechanism Design for Multi-Party Machine Learning",
                "abstract": "In a multi-party machine learning system, different parties cooperate on optimizing towards better models by sharing data in a privacy-preserving way. A major challenge in learning is the incentive issue. For example, if there is competition among the parties, one may strategically hide his data to prevent other parties from getting better models. \nIn this paper, we study the problem through the lens of mechanism design and incorporate the features of multi-party learning in our setting. First, each agent's valuation has externalities that depend on others' types and actions. Second, each agent can only misreport a type lower than his true type, but not the other way round. We call this setting interdependent value with type-dependent action spaces. We provide the optimal truthful mechanism in the quasi-monotone utility setting. We also provide necessary and sufficient conditions for truthful mechanisms in the most general case. Finally, we show the existence of such mechanisms is highly affected by the market growth rate and provide empirical analysis."
            },
            {
                "arxivId": "1912.00818",
                "title": "Federated Learning with Personalization Layers",
                "abstract": "The emerging paradigm of federated learning strives to enable collaborative training of machine learning models on the network edge without centrally aggregating raw data and hence, improving data privacy. This sharply deviates from traditional machine learning and necessitates the design of algorithms robust to various sources of heterogeneity. Specifically, statistical heterogeneity of data across user devices can severely degrade the performance of standard federated averaging for traditional machine learning applications like personalization with deep learning. This paper pro-posesFedPer, a base + personalization layer approach for federated training of deep feedforward neural networks, which can combat the ill-effects of statistical heterogeneity. We demonstrate effectiveness ofFedPerfor non-identical data partitions ofCIFARdatasetsand on a personalized image aesthetics dataset from Flickr."
            },
            {
                "arxivId": "1911.12560",
                "title": "Free-riders in Federated Learning: Attacks and Defenses",
                "abstract": "Federated learning is a recently proposed paradigm that enables multiple clients to collaboratively train a joint model. It allows clients to train models locally, and leverages the parameter server to generate a global model by aggregating the locally submitted gradient updates at each round. Although the incentive model for federated learning has not been fully developed, it is supposed that participants are able to get rewards or the privilege to use the final global model, as a compensation for taking efforts to train the model. Therefore, a client who does not have any local data has the incentive to construct local gradient updates in order to deceive for rewards. In this paper, we are the first to propose the notion of free rider attacks, to explore possible ways that an attacker may construct gradient updates, without any local training data. Furthermore, we explore possible defenses that could detect the proposed attacks, and propose a new high dimensional detection method called STD-DAGMM, which particularly works well for anomaly detection of model parameters. We extend the attacks and defenses to consider more free riders as well as differential privacy, which sheds light on and calls for future research in this field."
            },
            {
                "arxivId": "1910.03581",
                "title": "FedMD: Heterogenous Federated Learning via Model Distillation",
                "abstract": "Federated learning enables the creation of a powerful centralized model without compromising data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants."
            },
            {
                "arxivId": "1707.02968",
                "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
                "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 \u00d7 or 100 \u00d7 ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between \u2018enormous data\u2019 and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets."
            },
            {
                "arxivId": "1602.05629",
                "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
                "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. \nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2311.02467",
        "category": "econ",
        "title": "Individualized Policy Evaluation and Learning under Clustered Network Interference",
        "abstract": "While there now exists a large literature on policy evaluation and learning, much of prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference may lead to biased policy evaluation and ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network interference (also known as partial interference) where clusters of units are sampled from a population and units may influence one another within each cluster. Unlike previous methods that impose strong restrictions on spillover effects, the proposed methodology only assumes a semiparametric structural model where each unit's outcome is an additive function of individual treatments within the cluster. Under this model, we propose an estimator that can be used to evaluate the empirical performance of an ITR. We show that this estimator is substantially more efficient than the standard inverse probability weighting estimator, which does not impose any assumption about spillover effects. We derive the finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to the improved performance of learned policies. Finally, we conduct simulation and empirical studies to illustrate the advantages of the proposed methodology.",
        "references": [
            {
                "arxivId": "2312.03268",
                "title": "Design-based inference for generalized network experiments with stochastic interventions",
                "abstract": "A growing number of scholars and data scientists are conducting randomized experiments to analyze causal relationships in network settings where units influence one another. A dominant methodology for analyzing these network experiments has been design-based, leveraging randomization of treatment assignment as the basis for inference. In this paper, we generalize this design-based approach so that it can be applied to more complex experiments with a variety of causal estimands with different target populations. An important special case of such generalized network experiments is a bipartite network experiment, in which the treatment assignment is randomized among one set of units and the outcome is measured for a separate set of units. We propose a broad class of causal estimands based on stochastic intervention for generalized network experiments. Using a design-based approach, we show how to estimate the proposed causal quantities without bias, and develop conservative variance estimators. We apply our methodology to a randomized experiment in education where a group of selected students in middle schools are eligible for the anti-conflict promotion program, and the program participation is randomized within this group. In particular, our analysis estimates the causal effects of treating each student or his/her close friends, for different target populations in the network. We find that while the treatment improves the overall awareness against conflict among students, it does not significantly reduce the total number of conflicts."
            },
            {
                "arxivId": "2309.07476",
                "title": "Causal inference in network experiments: regression-based analysis and design-based properties",
                "abstract": "Investigating interference or spillover effects among units is a central task in many social science problems. Network experiments are powerful tools for this task, which avoids endogeneity by randomly assigning treatments to units over networks. However, it is non-trivial to analyze network experiments properly without imposing strong modeling assumptions. Previously, many researchers have proposed sophisticated point estimators and standard errors for causal effects under network experiments. We further show that regression-based point estimators and standard errors can have strong theoretical guarantees if the regression functions and robust standard errors are carefully specified to accommodate the interference patterns under network experiments. We first recall a well-known result that the Hajek estimator is numerically identical to the coefficient from the weighted-least-squares fit based on the inverse probability of the exposure mapping. Moreover, we demonstrate that the regression-based approach offers three notable advantages: its ease of implementation, the ability to derive standard errors through the same weighted-least-squares fit, and the capacity to integrate covariates into the analysis, thereby enhancing estimation efficiency. Furthermore, we analyze the asymptotic bias of the regression-based network-robust standard errors. Recognizing that the covariance estimator can be anti-conservative, we propose an adjusted covariance estimator to improve the empirical coverage rates. Although we focus on regression-based point estimators and standard errors, our theory holds under the design-based framework, which assumes that the randomness comes solely from the design of network experiments and allows for arbitrary misspecification of the regression models."
            },
            {
                "arxivId": "2307.08840",
                "title": "Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War",
                "abstract": "Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over. To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk that a new algorithmic policy leads to worse outcomes for subgroups of individual units and then averages this over the distribution of subgroups. We also propose a Bayesian policy learning framework that maximizes the posterior expected value while controlling the posterior expected ACRisk. This framework separates the estimation of heterogeneous treatment effects from policy optimization, enabling flexible estimation of effects and optimization over complex policy classes. We characterize the resulting chance-constrained optimization problem as a constrained linear programming problem. Our analysis shows that compared to the actual algorithm used during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors."
            },
            {
                "arxivId": "2303.11507",
                "title": "Optimal individualized treatment rule for combination treatments under budget constraints",
                "abstract": "\n The individualized treatment rule (ITR), which recommends an optimal treatment based on individual characteristics, has drawn considerable interest from many areas such as precision medicine, personalized education, and personalized marketing. Existing ITR estimation methods mainly adopt 1 of 2 or more treatments. However, a combination of multiple treatments could be more powerful in various areas. In this paper, we propose a novel double encoder model (DEM) to estimate the ITR for combination treatments. The proposed double encoder model is a nonparametric model which not only flexibly incorporates complex treatment effects and interaction effects among treatments but also improves estimation efficiency via the parameter-sharing feature. In addition, we tailor the estimated ITR to budget constraints through a multi-choice knapsack formulation, which enhances our proposed method under restricted-resource scenarios. In theory, we provide the value reduction bound with or without budget constraints, and an improved convergence rate with respect to the number of treatments under the DEM. Our simulation studies show that the proposed method outperforms the existing ITR estimation in various settings. We also demonstrate the superior performance of the proposed method in patient-derived xenograft data that recommends optimal combination treatments to shrink the tumour size of the colorectal cancer."
            },
            {
                "arxivId": "2212.09900",
                "title": "Policy learning \"without\" overlap: Pessimism and generalized empirical Bernstein's inequality",
                "abstract": "This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities. In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weighted (AIPW)-type estimators using knowledge of the behavior policies for collecting the offline data. Without assuming any uniform overlap condition, we establish a data-dependent upper bound for the suboptimality of our algorithm, which depends only on (i) the overlap for the optimal policy, and (ii) the complexity of the policy class. As an implication, for adaptively collected data, we ensure efficient policy learning as long as the propensities for optimal actions are lower bounded over time, while those for suboptimal ones are allowed to diminish arbitrarily fast. In our theoretical analysis, we develop a new self-normalized concentration inequality for IPW estimators, generalizing the well-known empirical Bernstein's inequality to unbounded and non-i.i.d. data."
            },
            {
                "arxivId": "2208.13323",
                "title": "Safe Policy Learning under Regression Discontinuity Designs with Multiple Cutoffs",
                "abstract": "The regression discontinuity (RD) design is widely used for program evaluation with observational data. The primary focus of the existing literature has been the estimation of the local average treatment effect at the existing treatment cutoff. In contrast, we consider policy learning under the RD design. Because the treatment assignment mechanism is deterministic, learning better treatment cutoffs requires extrapolation. We develop a robust optimization approach to finding optimal treatment cutoffs that improve upon the existing ones. We first decompose the expected utility into point-identifiable and unidentifiable components. We then propose an efficient doubly-robust estimator for the identifiable parts. To account for the unidentifiable components, we leverage the existence of multiple cutoffs that are common under the RD design. Specifically, we assume that the heterogeneity in the conditional expectations of potential outcomes across different groups vary smoothly along the running variable. Under this assumption, we minimize the worst case utility loss relative to the status quo policy. The resulting new treatment cutoffs have a safety guarantee that they will not yield a worse overall outcome than the existing cutoffs. Finally, we establish the asymptotic regret bounds for the learned policy using semi-parametric efficiency theory. We apply the proposed methodology to empirical and simulated data sets."
            },
            {
                "arxivId": "2208.05553",
                "title": "Exploiting neighborhood interference with low-order interactions under unit randomized design",
                "abstract": "Abstract Network interference, where the outcome of an individual is affected by the treatment assignment of those in their social network, is pervasive in real-world settings. However, it poses a challenge to estimating causal effects. We consider the task of estimating the total treatment effect (TTE), or the difference between the average outcomes of the population when everyone is treated versus when no one is, under network interference. Under a Bernoulli randomized design, we provide an unbiased estimator for the TTE when network interference effects are constrained to low-order interactions among neighbors of an individual. We make no assumptions on the graph other than bounded degree, allowing for well-connected networks that may not be easily clustered. We derive a bound on the variance of our estimator and show in simulated experiments that it performs well compared with standard estimators for the TTE. We also derive a minimax lower bound on the mean squared error of our estimator, which suggests that the difficulty of estimation can be characterized by the degree of interactions in the potential outcomes model. We also prove that our estimator is asymptotically normal under boundedness conditions on the network degree and potential outcomes model. Central to our contribution is a new framework for balancing model flexibility and statistical complexity as captured by this low-order interactions structure."
            },
            {
                "arxivId": "2205.12803",
                "title": "Estimating the total treatment effect in randomized experiments with unknown network structure",
                "abstract": "Significance In many domains, we want to estimate the total treatment effect (TTE) in situations where we suspect network interference is present. However, we often cannot measure the network or the implied dependency structure. Surprisingly, we are able to develop principles for designing randomized experiments without knowledge of the network, showing that under reasonable conditions one can nonetheless estimate the TTE, accounting for interference on the unknown network. The proposed design principles, and related estimator, work with a broad class of outcome models. Our estimator has low variance under simple randomized designs, resulting in an efficient and practical solution for estimating total treatment effect in the presence of complex network effects. We detail the assumptions under which the proposed methods work and discuss situations when they may fail."
            },
            {
                "arxivId": "2111.09932",
                "title": "Minimum Resource Threshold Policy Under Partial Interference",
                "abstract": "When developing policies for prevention of infectious diseases, policymakers often set specific, outcome-oriented targets to achieve. For example, when developing a vaccine allocation policy, policymakers may want to distribute them so that at least a certain fraction of individuals in a census block are disease-free and spillover effects due to interference within blocks are accounted for. The paper proposes methods to estimate a block-level treatment policy that achieves a pre-defined, outcome-oriented target while accounting for spillover effects due to interference. Our policy, the minimum resource threshold policy (MRTP), suggests the minimum fraction of treated units required within a block to meet or exceed the target level of the outcome. We estimate the MRTP from empirical risk minimization using a novel, nonparametric, doubly robust loss function. We then characterize statistical properties of the estimated MRTP in terms of the excess risk bound. We apply our methodology to design a water, sanitation, and hygiene allocation policy for Senegal with the goal of increasing the proportion of households with no children experiencing diarrhea to a level exceeding a specified threshold. Our policy outperforms competing policies and offers new approaches to design allocation policies, especially in international development for communicable diseases."
            },
            {
                "arxivId": "2109.11679",
                "title": "Safe Policy Learning through Extrapolation: Application to Pre-trial Risk Assessment",
                "abstract": "Algorithmic recommendations and decisions have become ubiquitous in today\u2019s society. Many of these and other data-driven policies, especially in the realm of public policy, are based on known, deterministic rules to ensure their transparency and interpretability. For example, algorithmic pre-trial risk assessments, which serve as our motivating application, provide relatively simple, deterministic classification scores and recommendations to help judges make release decisions. How can we use the data based on existing deterministic policies to learn new and better policies? Unfortunately, prior methods for policy learning are not applicable because they require existing policies to be stochastic rather than deterministic. We develop a robust optimization approach that partially identifies the expected utility of a policy, and then finds an optimal policy by minimizing the worst-case regret. The resulting policy is conservative but has a statistical safety guarantee, allowing the policy-maker to limit the probability of producing a worse outcome than the existing policy. We extend this approach to common and important settings where humans make decisions with the aid of algorithmic recommendations. Lastly, we apply the proposed methodology to a unique field experiment on pre-trial risk assessment instruments. We derive new classification and recommendation rules that retain the transparency and interpretability of the existing instrument while potentially leading to better overall outcomes at a lower cost."
            },
            {
                "arxivId": "2104.03802",
                "title": "Average Direct and Indirect Causal Effects under Interference",
                "abstract": "We propose a definition for the average indirect effect of a binary treatment in the potential outcomes model for causal inference under cross-unit interference. Our definition is analogous to the standard definition of the average direct effect, and can be expressed without needing to compare outcomes across multiple randomized experiments. We show that the proposed indirect effect satisfies a decomposition theorem whereby, in a Bernoulli trial, the sum of the average direct and indirect effects always corresponds to the effect of a policy intervention that infinitesimally increases treatment probabilities. We also consider a number of parametric models for interference, and find that our non-parametric indirect effect remains a natural estimand when re-expressed in the context of these models."
            },
            {
                "arxivId": "2103.06392",
                "title": "Design and Analysis of Bipartite Experiments Under a Linear Exposure-response Model",
                "abstract": "A bipartite experiment consists of one set of units being assigned treatments and another set of units for which we measure outcomes. The two sets of units are connected by a bipartite graph, governing how the treated units can affect the outcome units. The bipartite framework naturally arises in marketplace experiments where, for example, experimenters may seek to investigate the effect of discounting goods on buyer behavior. In this paper, we consider estimation of the average total treatment effect in the bipartite experimental framework under a linear exposure-response model. We introduce the Exposure Reweighted Linear (ERL) estimator, and show that the estimator is unbiased, consistent and asymptotically normal, provided that the bipartite graph is sufficiently sparse. To facilitate inference, we introduce an unbiased and consistent estimator of the variance of the ERL point estimator. In addition, we introduce a cluster-based design, Exposure-Design, that uses heuristics to increase the precision of the ERL estimator by realizing a desirable exposure distribution. Finally, we demonstrate the application of the described methodology to marketplace experiments using a publicly available Amazon user-item review dataset. The full version of the paper is available at: https://arxiv.org/abs/2103.06392."
            },
            {
                "arxivId": "2012.04055",
                "title": "Who should get vaccinated? Individualized allocation of vaccines over SIR network",
                "abstract": null
            },
            {
                "arxivId": "2008.00707",
                "title": "Heterogeneous Treatment and Spillover Effects Under Clustered Network Interference",
                "abstract": "The bulk of causal inference studies rules out the presence of interference between units. However, in many real-world settings units are interconnected by social, physical or virtual ties and the effect of a treatment can spill from one unit to other connected individuals in the network. In these settings, interference should be taken into account to avoid biased estimates of the treatment effect, but it can also be leveraged to save resources and provide the intervention to a lower percentage of the population where the treatment is more effective and where the effect can spill over to other susceptible individuals. In fact, different people might respond differently not only to the treatment received but also to the treatment received by their network contacts. Understanding the heterogeneity of treatment and spillover effects can help policy-makers in the scale-up phase of the intervention, it can guide the design of targeting strategies with the ultimate goal of making the interventions more cost-effective, and it might even allow generalizing the level of treatment spillover effects in other populations. In this paper, we develop a machine learning method that makes use of tree-based algorithms and an Horvitz-Thompson estimator to assess the heterogeneity of treatment and spillover effects with respect to individual, neighborhood and network characteristics in the context of clustered network interference. We illustrate how the proposed binary tree methodology performs in a Monte Carlo simulation study. Additionally, we provide an application on a randomized experiment aimed at assessing the heterogeneous effects of information sessions on the uptake of a new weather insurance policy in rural China."
            },
            {
                "arxivId": "2007.13302",
                "title": "Random graph asymptotics for treatment effect estimation under network interference",
                "abstract": "The network interference model for causal inference places all experimental units at the vertices of an undirected exposure graph, such that treatment assigned to one unit may affect the outcome of another unit if and only if these two units are connected by an edge. This model has recently gained popularity as means of incorporating interference effects into the Neyman--Rubin potential outcomes framework; and several authors have considered estimation of various causal targets, including the direct and indirect effects of treatment. In this paper, we consider large-sample asymptotics for treatment effect estimation under network interference in a setting where the exposure graph is a random draw from a graphon. When targeting the direct effect, we show that---in our setting---popular estimators are considerably more accurate than existing results suggest, and provide a central limit theorem in terms of moments of the graphon. Meanwhile, when targeting the indirect effect, we leverage our generative assumptions to propose a consistent estimator in a setting where no other consistent estimators are currently available. We also show how our results can be used to conduct a practical assessment of the sensitivity of randomized study inference to potential interference effects. Overall, our results highlight the promise of random graph asymptotics in understanding the practicality and limits of causal inference under network interference."
            },
            {
                "arxivId": "2004.08950",
                "title": "Efficient Semiparametric Estimation of Network Treatment Effects Under Partial Interference",
                "abstract": "There has been growing interest in causal inference to study treatment effects under interference. While many estimators have been proposed, there is little work on studying efficiency-related optimality properties of these estimators. To this end, the paper presents semiparametrically efficient and doubly robust estimation of network treatment effects under interference. We focus on partial interference where study units are partitioned into non-overlapping clusters and there is interference within clusters, but not across clusters. We derive the efficient influence function and the semiparametric efficiency bound for a family of network causal effects that include the direct and the indirect/spillover effects. We also adapt M-estimation theory to interference settings and propose M-estimators which are locally efficient and doubly robust. We conclude by presenting some limited results on adaptive estimation to interference patterns, or commonly referred to as exposure mapping."
            },
            {
                "arxivId": "1910.10862",
                "title": "A\u00a0graph\u2010theoretic approach to randomization tests of causal effects under general\n\n\n interference",
                "abstract": "Interference exists when a unit's outcome depends on another unit's treatment assignment. For example, intensive policing on one street could have a spillover effect on neighbouring streets. Classical randomization tests typically break down in this setting because many null hypotheses of interest are no longer sharp under interference. A promising alternative is to instead construct a conditional randomization test on a subset of units and assignments for which a given null hypothesis is sharp. Finding these subsets is challenging, however, and existing methods are limited to special cases or have limited power. In this paper, we propose valid and easy\u2010to\u2010implement randomization tests for a general class of null hypotheses under arbitrary interference between units. Our key idea is to represent the hypothesis of interest as a bipartite graph between units and assignments, and to find an appropriate biclique of this graph. Importantly, the null hypothesis is sharp within this biclique, enabling conditional randomization\u2010based tests. We also connect the size of the biclique to statistical power. Moreover, we can apply off\u2010the\u2010shelf graph clustering methods to find such bicliques efficiently and at scale. We illustrate our approach in settings with clustered interference and show advantages over methods designed specifically for that setting. We then apply our method to a large\u2010scale policing experiment in Medell\u00edn, Colombia, where interference has a spatial structure."
            },
            {
                "arxivId": "1906.10258",
                "title": "Policy Targeting under Network Interference",
                "abstract": "\n This paper studies the problem of optimally allocating treatments in the presence of spillover effects, using information from a (quasi-)experiment. I introduce a method that maximizes the sample analog of average social welfare when spillovers occur. I construct semi-parametric welfare estimators with known and unknown propensity scores and cast the optimization problem into a mixed-integer linear program, which can be solved using off-the-shelf algorithms. I derive a strong set of guarantees on regret, i.e., the difference between the maximum attainable welfare and the welfare evaluated at the estimated policy. The proposed method presents attractive features for applications: (i) it does not require network information of the target population; (ii) it exploits heterogeneity in treatment effects for targeting individuals; (iii) it does not rely on the correct specification of a particular structural model; and (iv) it accommodates constraints on the policy function. An application for targeting information on social networks illustrates the advantages of the method."
            },
            {
                "arxivId": "1905.10116",
                "title": "Semi-Parametric Efficient Policy Learning with Continuous Actions",
                "abstract": "We consider off-policy evaluation and optimization with continuous action spaces. We focus on observational data where the data collection policy is unknown and needs to be estimated from data. We take a semi-parametric approach where the value function takes a known parametric form in the treatment, but we are agnostic on how it depends on the observed contexts. We propose a doubly robust off-policy estimate for this setting and show that off-policy optimization based on this doubly robust estimate is robust to estimation errors of the policy function or the regression model. We also show that the variance of our off-policy estimate achieves the semi-parametric efficiency bound. Our results also apply if the model does not satisfy our semi-parametric form but rather we measure regret in terms of the best projection of the true value function to this functional space. Our work extends prior approaches of policy optimization from observational data that only considered discrete actions. We provide an experimental evaluation of our method in a synthetic data example motivated by optimal personalized pricing."
            },
            {
                "arxivId": "1905.05389",
                "title": "Experimental Evaluation of Individualized Treatment Rules",
                "abstract": "Abstract The increasing availability of individual-level data has led to numerous applications of individualized (or personalized) treatment rules (ITRs). Policy makers often wish to empirically evaluate ITRs and compare their relative performance before implementing them in a target population. We propose a new evaluation metric, the population average prescriptive effect (PAPE). The PAPE compares the performance of ITR with that of non-individualized treatment rule, which randomly treats the same proportion of units. Averaging the PAPE over a range of budget constraints yields our second evaluation metric, the area under the prescriptive effect curve (AUPEC). The AUPEC represents an overall performance measure for evaluation, like the area under the receiver and operating characteristic curve (AUROC) does for classification, and is a generalization of the QINI coefficient used in uplift modeling. We use Neyman\u2019s repeated sampling framework to estimate the PAPE and AUPEC and derive their exact finite-sample variances based on random sampling of units and random assignment of treatment. We extend our methodology to a common setting, in which the same experimental data are used to both estimate and evaluate ITRs. In this case, our variance calculation incorporates the additional uncertainty due to random splits of data used for cross-validation. The proposed evaluation metrics can be estimated without requiring modeling assumptions, asymptotic approximation, or resampling methods. As a result, it is applicable to any ITR including those based on complex machine learning algorithms. The open-source software package is available for implementing the proposed methodology. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1810.04778",
                "title": "Offline Multi-Action Policy Learning: Generalization and Optimization",
                "abstract": "As a result of digitization of the economy, more and more decision makers from a wide range of domains have gained the ability to target products, services, and information provision based on individual characteristics. Examples include selecting offers, prices, advertisements, or emails to send to consumers, choosing a bid to submit in a contextual first-price auctions, and determining which medication to prescribe to a patient. The key to enabling this is to learn a treatment policy from historical observational data in a sample-efficient way, hence uncovering the best personalized treatment choice recommendation. In \u201cOffline Policy Learning: Generalization and Optimization,\u201d Z. Zhou, S. Athey, and S. Wager provide a sample-optimal policy learning algorithm that is computationally efficient and that learns a tree-based treatment policy from observational data. In our quest toward fully automated personalization, the work provides a theoretically sound and practically implementable approach."
            },
            {
                "arxivId": "1806.07422",
                "title": "Doubly robust estimation in observational studies with partial interference",
                "abstract": "Interference occurs when the treatment (or exposure) of one individual affects the outcomes of others. In some settings, it may be reasonable to assume that individuals can be partitioned into clusters such that there is no interference between individuals in different clusters, that is, there is partial interference. In observational studies with partial interference, inverse probability weighted (IPW) estimators have been something else different possible treatment effects. However, the validity of IPW estimators depends on the propensity score being known or correctly modelled. Alternatively, one can estimate the treatment effect using an outcome regression model. In this paper, we propose doubly robust (DR) estimators that utilize both models and are consistent and asymptotically normal if either model, but not necessarily both, is correctly specified. Empirical results are presented to demonstrate the DR property of the proposed estimators and the efficiency gain of DR over IPW estimators when both models are correctly specified. The different estimators are illustrated using data from a study examining the effects of cholera vaccination in Bangladesh."
            },
            {
                "arxivId": "1711.04834",
                "title": "Causal inference from observational studies with clustered interference, with application to a cholera vaccine study",
                "abstract": "Inferring causal effects from an observational study is challenging because participants are not randomized to treatment. Observational studies in infectious disease research present the additional challenge that one participant's treatment may affect another participant's outcome, i.e., there may be interference. In this paper recent approaches to defining causal effects in the presence of interference are considered, and new causal estimands designed specifically for use with observational studies are proposed. Previously defined estimands target counterfactual scenarios in which individuals independently select treatment with equal probability. However, in settings where there is interference between individuals within clusters, it may be unlikely that treatment selection is independent between individuals in the same cluster. The proposed causal estimands instead describe counterfactual scenarios in which the treatment selection correlation structure is the same as in the observed data distribution, allowing for within-cluster dependence in the individual treatment selections. These estimands may be more relevant for policy-makers or public health officials who desire to quantify the effect of increasing the proportion of treated individuals in a population. Inverse probability-weighted estimators for these estimands are proposed. The large-sample properties of the estimators are derived, and a simulation study demonstrating the finite-sample performance of the estimators is presented. The proposed methods are illustrated by analyzing data from a study of cholera vaccination in over 100,000 individuals in Bangladesh."
            },
            {
                "arxivId": "1708.08171",
                "title": "Spillover Effects in the Presence of Unobserved Networks",
                "abstract": "Abstract When experimental subjects can interact with each other, the outcome of one individual may be affected by the treatment status of others. In many social science experiments, such spillover effects may occur through multiple networks, for example, through both online and offline face-to-face networks in a Twitter experiment. Thus, to understand how people use different networks, it is essential to estimate the spillover effect in each specific network separately. However, the unbiased estimation of these network-specific spillover effects requires an often-violated assumption that researchers observe all relevant networks. We show that, unlike conventional omitted variable bias, bias due to unobserved networks remains even when treatment assignment is randomized and when unobserved networks and a network of interest are independently generated. We then develop parametric and nonparametric sensitivity analysis methods, with which researchers can assess the potential influence of unobserved networks on causal findings. We illustrate the proposed methods with a simulation study based on a real-world Twitter network and an empirical application based on a network field experiment in China."
            },
            {
                "arxivId": "1705.07384",
                "title": "Balanced Policy Evaluation and Learning",
                "abstract": "We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success."
            },
            {
                "arxivId": "1612.01205",
                "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits",
                "abstract": "We study the off-policy evaluation problem---estimating the value of a target policy using data collected by another policy---under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse propensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is suboptimal. We then propose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of data sets, often outperforming prior work by orders of magnitude."
            },
            {
                "arxivId": "1609.04464",
                "title": "Peer Encouragement Designs in Causal Inference with Partial Interference and Identification of Local Average Network Effects",
                "abstract": "In non-network settings, encouragement designs have been widely used to analyze causal effects of a treatment, policy, or intervention on an outcome of interest when randomizing the treatment was considered impractical or when compliance to treatment cannot be perfectly enforced. Unfortunately, such questions related to treatment compliance have received less attention in network settings and the most well-studied experimental design in networks, the two-stage randomization design, requires perfect compliance with treatment. The paper proposes a new experimental design called peer encouragement design to study network treatment effects when enforcing treatment randomization is not feasible. The key idea in peer encouragement design is the idea of personalized encouragement, which allows point-identification of familiar estimands in the encouragement design literature. The paper also defines new causal estimands, local average network effects, that can be identified under the new design and analyzes the effect of non-compliance behavior in randomized experiments on networks."
            },
            {
                "arxivId": "1608.06805",
                "title": "Analyzing Two-Stage Experiments in the Presence of Interference",
                "abstract": "ABSTRACT Two-stage randomization is a powerful design for estimating treatment effects in the presence of interference; that is, when one individual\u2019s treatment assignment affects another individual\u2019s outcomes. Our motivating example is a two-stage randomized trial evaluating an intervention to reduce student absenteeism in the School District of Philadelphia. In that experiment, households with multiple students were first assigned to treatment or control; then, in treated households, one student was randomly assigned to treatment. Using this example, we highlight key considerations for analyzing two-stage experiments in practice. Our first contribution is to address additional complexities that arise when household sizes vary; in this case, researchers must decide between assigning equal weight to households or equal weight to individuals. We propose unbiased estimators for a broad class of individual- and household-weighted estimands, with corresponding theoretical and estimated variances. Our second contribution is to connect two common approaches for analyzing two-stage designs: linear regression and randomization inference. We show that, with suitably chosen standard errors, these two approaches yield identical point and variance estimates, which is somewhat surprising given the complex randomization scheme. Finally, we explore options for incorporating covariates to improve precision. We confirm our analytic results via simulation studies and apply these methods to the attendance study, finding substantively meaningful spillover effects."
            },
            {
                "arxivId": "1605.04812",
                "title": "Off-policy evaluation for slate recommendation",
                "abstract": "This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy's performance. A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learning-to-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased---these conditions are weaker than prior heuristics for slate evaluation---and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators."
            },
            {
                "arxivId": "1506.02084",
                "title": "Exact p-Values for Network Interference",
                "abstract": "ABSTRACT We study the calculation of exact p-values for a large class of nonsharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit\u2019s treatment status on another according to the distance between units, for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (e.g., based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, e.g., only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2402.01648",
        "category": "econ",
        "title": "Forecasting Imports in OECD Member Countries and Iran by Using Neural Network Algorithms of LSTM",
        "abstract": "Artificial Neural Networks (ANN) which are a branch of artificial intelligence, have shown their high value in lots of applications and are used as a suitable forecasting method. Therefore, this study aims at forecasting imports in OECD member selected countries and Iran for 20 seasons from 2021 to 2025 by means of ANN. Data related to the imports of such countries collected over 50 years from 1970 to 2019 from valid resources including World Bank, WTO, IFM,the data turned into seasonal data to increase the number of collected data for better performance and high accuracy of the network by using Diz formula that there were totally 200 data related to imports. This study has used LSTM to analyse data in Pycharm. 75% of data considered as training data and 25% considered as test data and the results of the analysis were forecasted with 99% accuracy which revealed the validity and reliability of the output. Since the imports is consumption function and since the consumption is influenced during Covid-19 Pandemic, so it is time-consuming to correct and improve it to be influential on the imports, thus the imports in the years after Covid-19 Pandemic has had a fluctuating trend.",
        "references": [
            {
                "arxivId": "2106.15397",
                "title": "Automated Evolutionary Approach for the Design of Composite Machine Learning Pipelines",
                "abstract": null
            },
            {
                "arxivId": "1907.03055",
                "title": "PANNA: Properties from Artificial Neural Network Architectures",
                "abstract": null
            },
            {
                "arxivId": "1503.04069",
                "title": "LSTM: A Search Space Odyssey",
                "abstract": "Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2402.01785",
        "category": "econ",
        "title": "DoubleMLDeep: Estimation of Causal Effects with Multimodal Data",
        "abstract": "This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.",
        "references": [
            {
                "arxivId": "2402.04674",
                "title": "Hyperparameter Tuning for Causal Inference with Double Machine Learning: A Simulation Study",
                "abstract": "Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks. While there is an extensive literature on tuning ML learners for prediction, there is only little guidance available on tuning ML learners for causal machine learning and how to select among different ML learners. In this paper, we empirically assess the relationship between the predictive performance of ML methods and the resulting causal estimation based on the Double Machine Learning (DML) approach by Chernozhukov et al. (2018). DML relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge. We provide empirical insights on the role of hyperparameter tuning and other practical decisions for causal estimation with DML. First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning. Second, we investigate how the choice of ML methods and hyperparameters, including recent AutoML frameworks, impacts the estimation performance for a causal parameter of interest. Third, we assess to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics."
            },
            {
                "arxivId": "2207.08815",
                "title": "Why do tree-based models still outperform deep learning on tabular data?",
                "abstract": "While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner."
            },
            {
                "arxivId": "2109.00725",
                "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond",
                "abstract": "Abstract A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1"
            },
            {
                "arxivId": "2107.13782",
                "title": "Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions",
                "abstract": null
            },
            {
                "arxivId": "2106.01342",
                "title": "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training",
                "abstract": "Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners. However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems. Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks."
            },
            {
                "arxivId": "2104.03220",
                "title": "DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python",
                "abstract": "DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org."
            },
            {
                "arxivId": "2103.09603",
                "title": "DoubleML - An Object-Oriented Implementation of Double Machine Learning in R",
                "abstract": "The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods."
            },
            {
                "arxivId": "2010.11929",
                "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
            },
            {
                "arxivId": "1910.03771",
                "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}."
            },
            {
                "arxivId": "1907.11692",
                "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            {
                "arxivId": "1901.09036",
                "title": "Orthogonal Statistical Learning",
                "abstract": "We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be estimated from data (a \"nuisance model\"). We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target model and one for the nuisance model. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning literature to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate the case where the target parameter belongs to a complex nonparametric class. We characterize conditions on the metric entropy such that oracle rates---rates of the same order as if we knew the nuisance model---are achieved. We also analyze the rates achieved by specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance in the literature: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data."
            },
            {
                "arxivId": "1809.09953",
                "title": "Deep Neural Networks for Estimation and Inference: Application to Causal Effects and Other Semiparametric Estimands",
                "abstract": "We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second\u2010step inference after first\u2010step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now\u2010common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed\u2010width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression\u2010type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing."
            },
            {
                "arxivId": "1802.02163",
                "title": "How to make causal inferences using texts",
                "abstract": "Text as data techniques offer a great promise: the ability to inductively discover measures that are useful for testing social science theories with large collections of text. Nearly all text-based causal inferences depend on a latent representation of the text, but we show that estimating this latent representation from the data creates underacknowledged risks: we may introduce an identification problem or overfit. To address these risks, we introduce a split-sample workflow for making rigorous causal inferences with discovered measures as treatments or outcomes. We then apply it to estimate causal effects from an experiment on immigration attitudes and a study on bureaucratic responsiveness."
            },
            {
                "arxivId": "1708.06633",
                "title": "Nonparametric regression using deep neural networks with ReLU activation function",
                "abstract": "Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to $\\log n$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates."
            },
            {
                "arxivId": "1206.5538",
                "title": "Representation Learning: A Review and New Perspectives",
                "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."
            },
            {
                "arxivId": "0801.1095",
                "title": "SIMULTANEOUS ANALYSIS OF LASSO AND DANTZIG SELECTOR",
                "abstract": "We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the l p estimation loss for 1 \u2264 p \u2264 2 in the linear model when the number of variables can be much larger than the sample size."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-06.json",
        "arxivId": "2402.02535",
        "category": "econ",
        "title": "Data-driven Policy Learning for a Continuous Treatment",
        "abstract": "This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable. Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare. We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension. This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension. Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias. This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation. To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques. This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficiency. By utilizing precise maximal and concentration inequalities, we derive sharper regret bounds than those currently available in the literature. In instances where the propensity score is unknown, we adopt the doubly robust (DR) moment condition tailored to the continuous treatment setting. In alignment with the binary-treatment case, the DR welfare regret closely parallels the IPW welfare regret, given the fast convergence of nuisance estimators.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-07.json",
        "arxivId": "1902.00976",
        "category": "econ",
        "title": "Bayesian Elicitation.",
        "abstract": "How can a receiver design an information structure in order to elicit information from a sender? We study how a decision-maker can acquire more information from an agent by reducing her own ability to observe what the agent transmits. Intuitively, when the two parties' preferences are not perfectly aligned, this garbling relaxes the sender's concern that the receiver will use her information to the sender's disadvantage. We characterize the optimal information structure for the receiver. The main result is that under broad conditions, the receiver can do just as well as if she could commit to a rule mapping the sender's message to actions: information design is just as good as full commitment. Similarly, we show that these conditions guarantee that ex ante information acquisition always benefits the receiver, even though this learning might actually lower the receiver's expected payoff in the absence of garbling. We illustrate these effects in a range of economically relevant examples.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-07.json",
        "arxivId": "2006.09587",
        "category": "econ",
        "title": "Adaptive, Rate-Optimal Hypothesis Testing in Nonparametric IV Models",
        "abstract": "We propose a new adaptive hypothesis test for inequality (e.g., monotonicity, convexity) and equality (e.g., parametric, semiparametric) restrictions on a structural function in a nonparametric instrumental variables (NPIV) model. Our test statistic is based on a modified leave-one-out sample analog of a quadratic distance between the restricted and unrestricted sieve NPIV estimators. We provide computationally simple, data-driven choices of sieve tuning parameters and Bonferroni adjusted chi-squared critical values. Our test adapts to the unknown smoothness of alternative functions in the presence of unknown degree of endogeneity and unknown strength of the instruments. It attains the adaptive minimax rate of testing in $L^2$. That is, the sum of its type I error uniformly over the composite null and its type II error uniformly over nonparametric alternative models cannot be improved by any other hypothesis test for NPIV models of unknown regularities. Confidence sets in $L^2$ are obtained by inverting the adaptive test. Simulations confirm that our adaptive test controls size and its finite-sample power greatly exceeds existing non-adaptive tests for monotonicity and parametric restrictions in NPIV models. Empirical applications to test for shape restrictions of differentiated products demand and of Engel curves are presented.",
        "references": [
            {
                "arxivId": "1909.10129",
                "title": "SPECIFICATION TESTING IN NONPARAMETRIC INSTRUMENTAL QUANTILE REGRESSION",
                "abstract": "There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model with endogenous regressors, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable variable. Under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. A failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. This article develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specified. Our test statistic is asymptotically normally distributed under correct specification and consistent against any alternative model. In addition, test statistics to justify the model simplification are established. Finite sample properties are examined in a Monte Carlo study and an empirical illustration is provided."
            },
            {
                "arxivId": "1508.03365",
                "title": "Optimal Sup-Norm Rates and Uniform Inference on Nonlinear Functionals of Nonparametric IV Regression",
                "abstract": "This paper makes several important contributions to the literature about nonparametric instrumental variables (NPIV) estimation and inference on a structural function h0 and functionals of h0. First, we derive sup\u2010norm convergence rates for computationally simple sieve NPIV (series two\u2010stage least squares) estimators of h0 and its derivatives. Second, we derive a lower bound that describes the best possible (minimax) sup\u2010norm rates of estimating h0 and its derivatives, and show that the sieve NPIV estimator can attain the minimax rates when h0 is approximated via a spline or wavelet sieve. Our optimal sup\u2010norm rates surprisingly coincide with the optimal root\u2010mean\u2010squared rates for severely ill\u2010posed problems, and are only a logarithmic factor slower than the optimal root\u2010mean\u2010squared rates for mildly ill\u2010posed problems. Third, we use our sup\u2010norm rates to establish the uniform Gaussian process strong approximations and the score bootstrap uniform confidence bands (UCBs) for collections of nonlinear functionals of h0 under primitive conditions, allowing for mildly and severely ill\u2010posed problems. Fourth, as applications, we obtain the first asymptotic pointwise and uniform inference results for plug\u2010in sieve t\u2010statistics of exact consumer surplus (CS) and deadweight loss (DL) welfare functionals under low\u2010level conditions when demand is estimated via sieve NPIV. Our real data application of UCBs for exact CS and DL functionals of gasoline demand reveals interesting patterns and is applicable to other goods markets.\n\nSeries two\u2010stage least squares optimal sup\u2010norm convergence rates uniform Gaussian process strong approximation score bootstrap uniform confidence bands nonlinear welfare functionals nonparametric demand with endogeneity C13 C14 C36"
            },
            {
                "arxivId": "1507.05270",
                "title": "Nonparametric Instrumental Variable Estimation Under Monotonicity",
                "abstract": "The ill\u2010posedness of the nonparametric instrumental variable (NPIV) model leads to estimators that may suffer from poor statistical performance. In this paper, we explore the possibility of imposing shape restrictions to improve the performance of the NPIV estimators. We assume that the function to be estimated is monotone and consider a sieve estimator that enforces this monotonicity constraint. We define a constrained measure of ill\u2010posedness that is relevant for the constrained estimator and show that, under a monotone IV assumption and certain other mild regularity conditions, this measure is bounded uniformly over the dimension of the sieve space. This finding is in stark contrast to the well\u2010known result that the unconstrained sieve measure of ill\u2010posedness that is relevant for the unconstrained estimator grows to infinity with the dimension of the sieve space. Based on this result, we derive a novel non\u2010asymptotic error bound for the constrained estimator. The bound gives a set of data\u2010generating processes for which the monotonicity constraint has a particularly strong regularization effect and considerably improves the performance of the estimator. The form of the bound implies that the regularization effect can be strong even in large samples and even if the function to be estimated is steep, particularly so if the NPIV model is severely ill\u2010posed. Our simulation study confirms these findings and reveals the potential for large performance gains from imposing the monotonicity constraint."
            },
            {
                "arxivId": "1909.10133",
                "title": "Goodness-of-fit tests based on series estimators in nonparametric instrumental regression",
                "abstract": null
            },
            {
                "arxivId": "1212.6757",
                "title": "TESTING REGRESSION MONOTONICITY IN ECONOMETRIC MODELS",
                "abstract": "Monotonicity is a key qualitative prediction of a wide array of economic models derived via robust comparative statics. It is therefore important to design effective and practical econometric methods for testing this prediction in empirical analysis. This article develops a general nonparametric framework for testing monotonicity of a regression function. Using this framework, a broad class of new tests is introduced, which gives an empirical researcher a lot of flexibility to incorporate ex ante information she might have. The article also develops new methods for simulating critical values, which are based on the combination of a bootstrap procedure and new selection algorithms. These methods yield tests that have correct asymptotic size and are asymptotically nonconservative. It is also shown how to obtain an adaptive and rate optimal test that has the best attainable rate of uniform consistency against models whose regression function has Lipschitz-continuous first-order derivatives and that automatically adapts to the unknown smoothness of the regression function. Simulations show that the power of the new tests in many cases significantly exceeds that of some prior tests, e.g., that of Ghosal, Sen, and Van der Vaart (2000)."
            },
            {
                "arxivId": "0709.2003",
                "title": "ON RATE OPTIMALITY FOR ILL-POSED INVERSE PROBLEMS IN ECONOMETRICS",
                "abstract": "In this paper we clarify the relations between the existing sets of regularity conditions for convergence rates of nonparametric indirect regression (NPIR) and nonparametric instrumental variables (NPIV) regression models. We establish minimax risk lower bounds in mean integrated squared error loss for the NPIR and NPIV models under two basic regularity conditions: the approximation number and the link condition. We show that both a simple projection estimator for the NPIR model and a sieve minimum distance estimator for the NPIV model can achieve the minimax risk lower bounds and are rate optimal uniformly over a large class of structure functions, allowing for mildly ill-posed and severely ill-posed cases."
            },
            {
                "arxivId": "math/0607342",
                "title": "Asymptotic equivalence for nonparametric regression with multivariate and random design",
                "abstract": "We show that nonparametric regression is asymptotically equivalent, in Le Cam's sense, to a sequence of Gaussian white noise experiments as the number of observations tends to infinity. We propose a general constructive framework, based on approximation spaces, which allows asymptotic equivalence to be achieved, even in the cases of multivariate and random design."
            },
            {
                "arxivId": "math/0605462",
                "title": "Adaptive Confidence Balls",
                "abstract": "Adaptive confidence balls are constructed for individual resolution levels as well as the entire mean vector in a multiresolution framework. Finite sample lower bounds are given for the minimum expected squared radius for confidence balls with a prespecified confidence level. The confidence balls are centered on adaptive estimators based on special local block thresholding rules. The radius is derived from an analysis of the loss of this adaptive estimator. In addition adaptive honest confidence balls are constructed which have guaranteed coverage probability over all of R N and expected squared radius adapting over a maximum range of Besov bodies."
            },
            {
                "arxivId": "math/0605473",
                "title": "Adaptive nonparametric confidence sets",
                "abstract": "We construct honest confidence regions for a Hilbert space-valued parameter in various statistical models. The confidence sets can be centered at arbitrary adaptive estimators, and have diameter which adapts optimally to a given selection of models. The latter adaptation is necessarily limited in scope. We review the notion of adaptive confidence regions, and relate the optimal rates of the diameter of adaptive confidence regions to the minimax rates for testing and estimation. Applications include the finite normal mean model, the white noise model, density estimation and regression with random design. \u00a9 Institute of Mathematical Statistics, 2006."
            },
            {
                "arxivId": "math/0505640",
                "title": "Data-Driven Rate-Optimal Specification Testing in Regression Models",
                "abstract": "We propose new data-driven smooth tests for a parametric regression function. The smoothing parameter is selected through a new criterion that favors a large smoothing parameter under the null hypothesis. The resulting test is adaptive rate-optimal and consistent against Pitman local alternatives approaching the parametric model at a rate arbitrarily close to 1/sqrt(n). Asymptotic critical values come from the standard normal distribution and bootstrap can be used in small samples. A general formalization allows to consider a large class of linear smoothing methods, which can be tailored for detection of additive alternatives."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-07.json",
        "arxivId": "2102.01587",
        "category": "econ",
        "title": "Games on Endogenous Networks",
        "abstract": "We study network games in which players choose both the partners with whom they associate and an action level (e.g., effort) that creates spillovers for those partners. We introduce a framework and two solution concepts, extending standard approaches for analyzing each choice in isolation: Nash equilibrium in actions and pairwise stability in links. Our main results show that, under suitable order conditions on incentives, stable networks take simple forms. The first condition concerns whether links create positive or negative payoff spillovers. The second concerns whether actions are strategic complements to links, or strategic substitutes. Together, these conditions yield a taxonomy of the relationship between network structure and economic primitives organized around two network architectures: ordered overlapping cliques and nested split graphs. We apply our model to understand the consequences of competition for status, to microfound matching models that assume clique formation, and to interpret empirical findings that highlight unintended consequences of group design.",
        "references": [
            {
                "arxivId": "2302.05831",
                "title": "On the Difficulty of Characterizing Network Formation with Endogenous Behavior",
                "abstract": "Bolletta (2021, Math. Soc. Sci. 114:1-10) studies a model in which a network is strategically formed and then agents play a linear best-response investment game in it. The model is motivated by an application in which people choose both their study partners and their levels of educational effort. Agents have different one-dimensional types $\\unicode{x2013}$ private returns to effort. A main result claims that pairwise Nash stable networks have a locally complete structure consisting of possibly overlapping cliques: if two agents are linked, they are part of a clique composed of all agents with types between theirs. We offer a counterexample showing that the claimed characterization is incorrect, highlight where the analysis errs, and discuss implications for network formation models."
            },
            {
                "arxivId": "2008.03102",
                "title": "Pricing Group Membership",
                "abstract": "We consider a model where agents differ in their `types' which determines their voluntary contribution towards a public good. We analyze what the equilibrium composition of groups are under centralized and centralized choice. We show that there exists a top-down sorting equilibrium i.e. an equilibrium where there exists a set of prices which leads to groups that can be ordered by level of types, with the first k types in the group with the highest price and so on. This exists both under decentralized and centralized choosing. We also analyze the model with endogenous group size and examine under what conditions is top-down sorting socially efficient. We illustrate when integration (i.e. mixing types so that each group's average type if the same) is socially better than top-down sorting. Finally, we show that top down sorting is efficient even when groups compete among themselves"
            },
            {
                "arxivId": "1901.00373",
                "title": "Nash Equilibria on (Un)Stable Networks",
                "abstract": "In response to a change, individuals may choose to follow the responses of their friends or, alternatively, to change their friends. To model these decisions, consider a game where players choose their behaviors and friendships. In equilibrium, players internalize the need for consensus in forming friendships and choose their optimal strategies on subsets of \n k players\u2014a form of bounded rationality. The \n k\u2010player consensual dynamic delivers a probabilistic ranking of a game's equilibria, and via a varying \n k, facilitates estimation of such games.\n \n Applying the model to adolescents' smoking suggests that: (a) the response of the friendship network to changes in tobacco price amplifies the intended effect of price changes on smoking, (b) racial desegregation of high schools decreases the overall smoking prevalence, (c) peer effect complementarities are substantially stronger between smokers compared to between nonsmokers."
            },
            {
                "arxivId": "1611.07658",
                "title": "A Network Formation Model Based on Subgraphs",
                "abstract": "We develop a new class of random-graph models for the statistical estimation of network formation that allow for substantial correlation in links. Various subgraphs (e.g., links, triangles, cliques, stars) are generated and their union results in a network. The challenge in estimating the frequencies with which subgraphs 'truly' form is that subgraphs can overlap and may also incidentally generate new subgraphs, and so the true rate of formation of the subgraphs cannot generally be inferred just by counting their presence in the resulting network. We provide estimation techniques for recovering the rates at which the underlying subgraphs were formed from the observation of a single (large) network. We provide results on identification of the true underlying rates of subgraph formation from various statistics, as well as a new Central Limit Theorem for correlated random variables that establishes asymptotic normality for our estimators. We also show that if the network is sparse enough then direct counts of subgraphs are consistent and asymptotically normal estimators. We illustrate the models with applications."
            },
            {
                "arxivId": "1605.04470",
                "title": "The Friendship Paradox and Systematic Biases in Perceptions and Social Norms",
                "abstract": "The \u201cfriendship paradox\u201d (first noted by Feld in 1991) refers to the fact that, on average, people have strictly fewer friends than their friends have. I show that this oversampling of more popular people can lead people to perceive more engagement than exists in the overall population. This feeds back to amplify engagement in behaviors that involve complementarities. Also, people with the greatest proclivity for a behavior choose to interact the most, leading to further feedback and amplification. These results are consistent with studies finding overestimation of peer consumption of alcohol, cigarettes, and drugs and with resulting high levels of drug and alcohol consumption."
            },
            {
                "arxivId": "physics/0603023",
                "title": "Nonequilibrium phase transition in the coevolution of networks and opinions.",
                "abstract": "Models of the convergence of opinion in social systems have been the subject of considerable recent attention in the physics literature. These models divide into two classes, those in which individuals form their beliefs based on the opinions of their neighbors in a social network of personal acquaintances, and those in which, conversely, network connections form between individuals of similar beliefs. While both of these processes can give rise to realistic levels of agreement between acquaintances, practical experience suggests that opinion formation in the real world is not a result of one process or the other, but a combination of the two. Here we present a simple model of this combination, with a single parameter controlling the balance of the two processes. We find that the model undergoes a continuous phase transition as this parameter is varied, from a regime in which opinions are arbitrarily diverse to one in which most individuals hold the same opinion."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-09.json",
        "arxivId": "2402.02402",
        "category": "econ",
        "title": "Machine Learning Analysis of the Impact of Increasing the Minimum Wage on Income Inequality in Spain from 2001 to 2021",
        "abstract": "This paper analyzes the impact of the National Minimum Wage from 2001 to 2021. The MNW increased from 505.7/month (2001) to 1,108.3/month (2021). Using the data provided by the Spanish Tax Administration Agency, databases that represent the entire population studied can be analyzed. More accurate results and more efficient predictive models are provided by these counts. This work is characterized by the database used, which is a national census and not a sample or projection. Therefore, the study reflects results and analyses based on historical data from the Spanish Salary Census 2001-2021. Various machine-learning models show that income inequality has been reduced by raising the minimum wage. Raising the minimum wage has not led to inflation or increased unemployment. On the contrary, it has been consistent with increased net employment, contained prices, and increased corporate profit margins. The most important conclusion is that an increase in the minimum wage in the period analyzed has led to an increase in the wealth of the country, increasing employment and company profits, and is postulated, under the conditions analyzed, as an effective method for the redistribution of wealth.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-12.json",
        "arxivId": "2401.07689",
        "category": "econ",
        "title": "Impermanent Loss Conditions: An Analysis of Decentralized Exchange Platforms",
        "abstract": "Decentralized exchanges are widely used platforms for trading crypto assets. The most common types work with automated market makers (AMM), allowing traders to exchange assets without needing to find matching counterparties. Thereby, traders exchange against asset reserves managed by smart contracts. These assets are provided by liquidity providers in exchange for a fee. Static analysis shows that small price changes in one of the assets can result in losses for liquidity providers. Despite the success of AMMs, it is claimed that liquidity providers often suffer losses. However, the literature does not adequately consider the dynamic effects of fees over time. Therefore, we investigate the impermanent loss problem in a dynamic setting using Monte Carlo simulations. Our findings indicate that price changes do not necessarily lead to losses. Fees paid by traders and arbitrageurs are equally important. In this respect, we can show that an arbitrage-friendly environment benefits the liquidity provider. Thus, we suggest that AMM developers should promote an arbitrage-friendly environment rather than trying to prevent arbitrage.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-12.json",
        "arxivId": "2402.06024",
        "category": "econ",
        "title": "A Generalization of Arrow's Impossibility Theorem Through Combinatorial Topology",
        "abstract": "We present a generalization of Arrow's impossibility theorem and prove it using a combinatorial topology framework. Instead of assuming the unrestricted domain, we introduce a domain restriction called the class of polarization and diversity over triples. The domains in this class are defined by requiring profiles in which society is strongly, but not completely, polarized over how to rank triples of alternatives, as well as some profiles that violate the value-restriction condition. To prove this result, we use the combinatorial topology approach started by Rajsbaum and Ravent\\'os-Pujol in the ACM Symposium on Principles of Distributed Computing (PODC) 2022, which in turn is based on the algebraic topology framework started by Baryshnikov in 1993. While Rajsbaum and Ravent\\'os-Pujol employed this approach to study Arrow's impossibility theorem and domain restrictions for the case of two voters and three alternatives, we extend it for the general case of any finite number of alternatives and voters. Although allowing for arbitrary (finite) alternatives and voters results in simplicial complexes of high dimension, our findings demonstrate that these complexes can be effectively analyzed by examining their $2$$\\unicode{x2013}$skeleton, even within the context of domain restrictions at the level of the $2$$\\unicode{x2013}$skeleton.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2103.01412",
        "category": "econ",
        "title": "Some Finite Sample Properties of the Sign Test",
        "abstract": "This paper contains two finite-sample results concerning the sign test. First, we show that the sign-test is unbiased with independent, non-identically distributed data for both one-sided and two-sided hypotheses. The proof for the two-sided case is based on a novel argument that relates the derivatives of the power function to a regular bipartite graph. Unbiasedness then follows from the existence of perfect matchings on such graphs. Second, we provide a simple theoretical counterexample to show that the sign test over-rejects when the data exhibits correlation. Our results can be useful for understanding the properties of approximate randomization tests in settings with few clusters.",
        "references": [
            {
                "arxivId": "2105.01008",
                "title": "A Modified Randomization Test for the Level of Clustering",
                "abstract": "Suppose a researcher observes individuals within a county within a state. Given concerns about correlation across individuals, it is common to group observations into clusters and conduct inference treating observations across clusters as roughly independent. However, a researcher that has chosen to cluster at the county level may be unsure of their decision, given knowledge that observations are independent across states. This paper proposes a modified randomization test as a robustness check for the chosen level of clustering in a linear regression setting. Existing tests require either the number of states or number of counties to be large. Our method is designed for settings with few states and few counties. While the method is conservative, it has competitive power in settings that may be relevant to empirical work."
            },
            {
                "arxivId": "1803.07951",
                "title": "Testing continuity of a density via g-order statistics in the regression discontinuity design",
                "abstract": "In the regression discontinuity design (RDD), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., McCrary (2008). In this paper we propose an approximate sign test for continuity of a density at a point based on the so-called g-order statistics, and study its properties under two complementary asymptotic frameworks. In the first asymptotic framework, the number q of observations local to the cut-off is fixed as the sample size n diverges to infinity, while in the second framework q diverges to infinity slowly as n diverges to infinity. Under both of these frameworks, we show that the test we propose is asymptotically valid in the sense that it has limiting rejection probability under the null hypothesis not exceeding the nominal level. More importantly, the test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, and exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity. In a simulation study, we find that the approximate sign test provides good control of the rejection probability under the null hypothesis while remaining competitive under the alternative hypothesis. We finally apply our test to the design in Lee (2008), a well-known application of the RDD to study incumbency advantage."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2105.02569",
        "category": "econ",
        "title": "Machine Collaboration",
        "abstract": "We propose a new ensemble framework for supervised learning, called machine collaboration (MaC), using a collection of possibly heterogeneous base learning methods (hereafter, base machines) for prediction tasks. Unlike bagging/stacking (a parallel and independent framework) and boosting (a sequential and top\u2010down framework), MaC is a type of circular and recursive learning framework. The circular and recursive nature helps the base machines to transfer information circularly and update their structures and parameters accordingly. The theoretical result on the risk bound of the estimator from MaC reveals that the circular and recursive feature can help MaC reduce risk via a parsimonious ensemble. We conduct extensive experiments on MaC using both simulated data and 119 benchmark real datasets. The results demonstrate that in most cases, MaC performs significantly better than several other state\u2010of\u2010the\u2010art methods, including classification and regression trees, neural networks, stacking, and boosting.",
        "references": [
            {
                "arxivId": "2006.08855",
                "title": "RaSE: Random Subspace Ensemble Classification",
                "abstract": "We propose a flexible ensemble classification framework, Random Subspace Ensemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate many weak learners, where each weak learner is a base classifier trained in a subspace optimally selected from a collection of random subspaces. To conduct subspace selection, we propose a new criterion, ratio information criterion (RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis includes the risk and Monte-Carlo variance of RaSE classifier, establishing the screening consistency and weak consistency of RIC, and providing an upper bound for the misclassification rate of RaSE classifier. In addition, we show that in a high-dimensional framework, the number of random subspaces needs to be very large to guarantee that a subspace covering signals is selected. Therefore, we propose an iterative version of RaSE algorithm and prove that under some specific conditions, a smaller number of generated random subspaces are needed to find a desirable subspace through iteration. An array of simulations under various models and real-data applications demonstrate the effectiveness and robustness of the RaSE classifier and its iterative version in terms of low misclassification rate and accurate feature ranking. The RaSE algorithm is implemented in the R package RaSEn on CRAN."
            },
            {
                "arxivId": "1703.00512",
                "title": "PMLB: a large benchmark suite for machine learning evaluation and comparison",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2107.02780",
        "category": "econ",
        "title": "Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy",
        "abstract": "The US Census Bureau will deliberately corrupt data sets derived from the 2020 US Census, enhancing the privacy of respondents while potentially reducing the precision of economic analysis. To investigate whether this trade-off is inevitable, we formulate a semiparametric model of causal inference with high dimensional corrupted data. We propose a procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. We prove consistency and Gaussian approximation by finite sample arguments, with a rate of $n^{ 1/2}$ for semiparametric estimands that degrades gracefully for nonparametric estimands. Our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and empirically validate. Our analysis provides nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. Calibrated simulations verify the coverage of our data cleaning adjusted confidence intervals and demonstrate the relevance of our results for Census-derived data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2203.16816",
        "category": "econ",
        "title": "Budget-Constrained Auctions with Unassured Priors: Strategic Equivalence and Structural Properties",
        "abstract": "In today's online advertising markets, it is common for advertisers to set long-term budgets. Correspondingly, advertising platforms adopt budget control methods to ensure that advertisers' payments lie within their budgets. Most budget control methods rely on the value distributions of advertisers. However, due to the complex advertising landscape and potential privacy concerns, the platform hardly learns advertisers' true priors. Thus, it is crucial to understand how budget control auction mechanisms perform under unassured priors. This work answers this problem from multiple aspects. We consider the unassured prior game among the seller and all buyers induced by different mechanisms in the stochastic model. We restrict the parameterized mechanisms to satisfy the budget-extracting condition, which maximizes the seller's revenue by extracting buyers' budgets as effectively as possible. Our main result shows that the Bayesian revenue-optimal mechanism and the budget-extracting bid-discount first-price mechanism yield the same set of Nash equilibrium outcomes in the unassured prior game. This implies that simple mechanisms can be as robust as the optimal mechanism under unassured priors in the budget-constrained setting. In the symmetric case, we further show that all these five (budget-extracting) mechanisms share the same set of possible outcomes. We further dig into the structural properties of these mechanisms. We characterize sufficient and necessary conditions on the budget-extracting parameter tuple for bid-discount/pacing first-price auctions. Meanwhile, when buyers do not take strategic behaviors, we exploit the dominance relationships of these mechanisms by revealing their intrinsic structures.",
        "references": [
            {
                "arxivId": "2205.08674",
                "title": "Budget Pacing in Repeated Auctions: Regret and Efficiency without Convergence",
                "abstract": "We study the aggregate welfare and individual regret guarantees of dynamic \\emph{pacing algorithms} in the context of repeated auctions with budgets. Such algorithms are commonly used as bidding agents in Internet advertising platforms. We show that when agents simultaneously apply a natural form of gradient-based pacing, the liquid welfare obtained over the course of the learning dynamics is at least half the optimal expected liquid welfare obtainable by any allocation rule. Crucially, this result holds \\emph{without requiring convergence of the dynamics}, allowing us to circumvent known complexity-theoretic obstacles of finding equilibria. This result is also robust to the correlation structure between agent valuations and holds for any \\emph{core auction}, a broad class of auctions that includes first-price, second-price, and generalized second-price auctions. For individual guarantees, we further show such pacing algorithms enjoy \\emph{dynamic regret} bounds for individual value maximization, with respect to the sequence of budget-pacing bids, for any auction satisfying a monotone bang-for-buck property."
            },
            {
                "arxivId": "2110.11855",
                "title": "Auctions between Regret-Minimizing Agents",
                "abstract": "We analyze a scenario in which software agents implemented as regret-minimizing algorithms engage in a repeated auction on behalf of their users. We study first-price and second-price auctions, as well as their generalized versions (e.g., as those used for ad auctions). Using both theoretical analysis and simulations, we show that, surprisingly, in second-price auctions the players have incentives to misreport their true valuations to their own learning agents, while in first-price auctions it is a dominant strategy for all players to truthfully report their valuations to their agents."
            },
            {
                "arxivId": "2107.10923",
                "title": "Throttling Equilibria in Auction Markets",
                "abstract": "Throttling is a popular method of budget management for online ad auctions in which the platform modulates the participation probability of an advertiser in order to smoothly spend her budget across many auctions. In this work, we investigate the setting in which all of the advertisers simultaneously employ throttling to manage their budgets, and we do so for both first-price and second-price auctions. We analyze the structural and computational properties of the resulting equilibria. For first-price auctions, we show that a unique equilibrium always exists, is well-behaved and can be computed efficiently via tatonnement-style decentralized dynamics. In contrast, for second-price auctions, we prove that even though an equilibrium always exists, the problem of finding an equilibrium is PPAD-complete, there can be multiple equilibria, and it is NP-hard to find the revenue maximizing one. We also compare the equilibrium outcomes of throttling to those of multiplicative pacing, which is the other most popular and well-studied method of budget management. Finally, we characterize the Price of Anarchy of these equilibria for liquid welfare by showing that it is at most 2 for both first-price and second-price auctions, and demonstrating that our bound is tight."
            },
            {
                "arxivId": "2106.09503",
                "title": "The Parity Ray Regularizer for Pacing in Auction Markets",
                "abstract": "Budget-management systems are one of the key components of modern auction markets. Internet advertising platforms typically offer advertisers the possibility to pace the rate at which their budget is depleted, through budget-pacing mechanisms. We focus on multiplicative pacing mechanisms in an online setting in which a bidder is repeatedly confronted with a series of advertising opportunities. After collecting bids, each item is then allocated through a single-item, second-price auction. If there were no budgetary constraints, bidding truthfully would be an optimal choice for the advertiser. However, since their budget is limited, the advertiser may want to shade their bid downwards in order to preserve their budget for future opportunities, and to spread expenditures evenly over time. The literature on online pacing problems mostly focuses on the setting in which the bidder optimizes an additive separable objective, such as the total click-through rate or the revenue of the allocation. In many settings, however, bidders may also care about other objectives which oftentimes are non-separable. We study the frequent case in which the utility of a (proxy) bidder depends on the rewards obtained from items they are allocated, and on the distance of the realized distribution of impressions from a target distribution. We introduce a novel regularizer which can describe those distributional preferences, while keeping the problem tractable. We show that this regularizer can be integrated into an existing online mirror descent scheme with minor modifications, attaining the optimal order of sub-linear regret compared to the optimal allocation in hindsight when inputs are drawn independently, from an unknown distribution. Moreover, we show that our approach can easily be incorporated in standard existing pacing systems that are not usually built for this objective. The effectiveness of our algorithm in internet advertising applications is confirmed by numerical experiments on real-world data."
            },
            {
                "arxivId": "2103.13969",
                "title": "The Complexity of Pacing for Second-Price Auctions",
                "abstract": "Budget constraints are ubiquitous in online advertisement auctions. To manage these constraints and smooth out the expenditure across auctions, the bidders (or the platform on behalf of them) often employ pacing: each bidder is assigned a multiplier between 0 and 1, and her bid on each item is multiplicatively scaled down by the multiplier. This naturally gives rise to a game in which each bidder strategically selects a multiplier. The appropriate notion of equilibrium in this game is the pacing equilibrium. In this work, we show that the problem of finding an approximate pacing equilibrium is PPAD-complete for second-price auctions. This resolves an open question of[2]. As a consequence of our hardness result, we show that the tatonnement-style budget-management dynamics introduced by[1] is unlikely to converge efficiently for repeated second-price auctions. Our hardness result also implies the existence of a refinement of supply-aware market equilibria which is hard to compute with simple linear utilities."
            },
            {
                "arxivId": "2009.06136",
                "title": "Convergence Analysis of No-Regret Bidding Algorithms in Repeated Auctions",
                "abstract": "The connection between games and no-regret algorithms has been widely studied in the literature. A fundamental result is that when all players play no-regret strategies, this produces a sequence of actions whose time-average is a coarse-correlated equilibrium of the game. However, much less is known about equilibrium selection in the case that multiple equilibria exist. In this work, we study the convergence of no-regret bidding algorithms in auctions. Besides being of theoretical interest, bidding dynamics in auctions is an important question from a practical viewpoint as well. We study the repeated game between bidders in which a single item is sold at each time step and the bidder's value is drawn from an unknown distribution. We show that if the bidders use any mean-based learning rule then the bidders converge with high probability to the truthful pure Nash Equilibrium in a second price auction, in VCG auction in the multi-slot setting and to the Bayesian Nash equilibrium in a first price auction. We note mean-based algorithms cover a wide variety of known no-regret algorithms such as Exp3, UCB, \\epsilon-Greedy etc. Also, we analyze the convergence of the individual iterates produced by such learning algorithms, as opposed to the time-average of the sequence. Our experiments corroborate our theoretical findings and also find a similar convergence when we use other strategies such as Deep Q-Learning."
            },
            {
                "arxivId": "1811.07166",
                "title": "Pacing Equilibrium in First-Price Auction Markets",
                "abstract": "In the isolated auction of a single item, second price is often preferable to first price in properties of theoretical interest. Unfortunately, single items are rarely sold in true isolation, so considering the broader context is critical when adopting a pricing strategy. In this paper, we show that this context is important in a model centrally relevant to Internet advertising: when items (ad impressions) are individually auctioned within the context of a larger system that is managing budgets, theory offers surprising support for using a first price auction to sell each individual item. In particular, first price auctions offer theoretical guarantees of equilibrium uniqueness, monotonicity, and other desirable properties, as well as efficient computability as the solution to the well-studied Eisenberg-Gale convex program. We also use simulations to demonstrate that while there are incentives to misreport in thin markets (where budgets aren't constraining), a bidder's incentive to deviate vanishes in thick markets."
            },
            {
                "arxivId": "1510.01210",
                "title": "Trading Networks with Bilateral Contracts",
                "abstract": "We consider general networks of bilateral contracts that include supply chains. We define a new stability concept, called trail stability, and show that any network of bilateral contracts has a trail-stable outcome whenever agents' preferences satisfy full substitutability. Trail stability is a natural extension of chain stability, but is a stronger solution concept in general contract networks. Trail-stable outcomes are not immune to deviations of arbitrary sets of firms. In fact, we show that outcomes satisfying an even more demanding stability property -- full trail stability -- always exist. We pin down conditions under which trail-stable and fully trail-stable outcomes have a lattice structure. We then completely describe the relationships between all stability concepts. When contracts specify trades and prices, we also show that competitive equilibrium exists in networked markets even in the absence of fully transferrable utility. The competitive equilibrium outcome is trail-stable."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2210.15841",
        "category": "econ",
        "title": "How to sample and when to stop sampling: The generalized Wald problem and minimax policies",
        "abstract": "The aim of this paper is to develop techniques for incorporating the cost of information into experimental design. Specifically, we study sequential experiments where sampling is costly and a decision-maker aims to determine the best treatment for full scale implementation by (1) adaptively allocating units to two possible treatments, and (2) stopping the experiment when the expected welfare (inclusive of sampling costs) from implementing the chosen treatment is maximized. Working under the diffusion limit, we describe the optimal policies under the minimax regret criterion. Under small cost asymptotics, the same policies are also optimal under parametric and non-parametric distributions of outcomes. The minimax optimal sampling rule is just the Neyman allocation; it is independent of sampling costs and does not adapt to previous outcomes. The decision-maker stops sampling when the average difference between the treatment outcomes, multiplied by the number of observations collected until that point, exceeds a specific threshold. The results derived here also apply to best arm identification with two arms.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2302.07935",
        "category": "econ",
        "title": "Market-Based Probability of Stock Returns",
        "abstract": "Markets possess all available information on stock returns. The randomness of market trade determines the statistics of stock returns. This paper describes the dependence of the first four market-based statistical moments of stock returns on statistical moments and correlations of current and past trade values. The mean return of trades during the averaging period coincides with Markowitz's definition of portfolio value weighted return. We derive the market-based volatility of return and return-value correlations. We present approximations of the characteristic functions and probability measures of stock return by a finite number of market-based statistical moments. To forecast market-based average return or volatility of return, one should predict the statistical moments and correlations of current and past market trade values at the same time horizon.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2309.03784",
        "category": "econ",
        "title": "On the minimal simplex economy",
        "abstract": "In our previous paper we proved that every affine economy has a competitive equilibrium. We define a simplex economy as an affine economy consisting of a stochastic allocation (defining the initial endowments) and a variation with repetition of the number of commodities taking the number of consumers (representing the preferences). We show that a competitive equilibrium can be intrinsically computed in any minimal simplex economy.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2309.07664",
        "category": "econ",
        "title": "Computer says \u2018no\u2019: Exploring systemic bias in ChatGPT using an audit approach",
        "abstract": null,
        "references": [
            {
                "arxivId": "2304.11771",
                "title": "Generative AI at Work",
                "abstract": "We study the staggered introduction of a generative AI-based conversational assistant using data from 5,000 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14 percent on average, with the greatest impact on novice and low-skilled workers, and minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the potentially tacit knowledge of more able workers and helps newer workers move down the experience curve. In addition, we show that AI assistance improves customer sentiment, reduces requests for managerial intervention, and improves employee retention."
            },
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            },
            {
                "arxivId": "2303.08774",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
            },
            {
                "arxivId": "2303.01157",
                "title": "How will Language Modelers like ChatGPT Affect Occupations and Industries?",
                "abstract": "Recent dramatic increases in AI language modeling capabilities has led to many questions about the effect of these technologies on the economy. In this paper we present a methodology to systematically assess the extent to which occupations, industries and geographies are exposed to advances in AI language modeling capabilities. We find that the top occupations exposed to language modeling include telemarketers and a variety of post-secondary teachers such as English language and literature, foreign language and literature, and history teachers. We find the top industries exposed to advances in language modeling are legal services and securities, commodities, and investments. We also find a positive correlation between wages and exposure to AI language modeling."
            },
            {
                "arxivId": "2103.11790",
                "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do",
                "abstract": null
            },
            {
                "arxivId": "1802.04422",
                "title": "A comparative study of fairness-enhancing interventions in machine learning",
                "abstract": "Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought."
            },
            {
                "arxivId": "1710.02926",
                "title": "When Should You Adjust Standard Errors for Clustering?",
                "abstract": "\n Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, e.g., by state but not by gender, and in observational studies, but not in completely randomized experiments? (ii) Why is conventional clustering an \u201call-or-nothing\u201d adjustment, while within-cluster correlations can be strong or extremely weak? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional cluster standard errors can be severely inflated, and propose new variance estimators that correct for this bias."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2310.12863",
        "category": "econ",
        "title": "A remark on moment-dependent phase transitions in high-dimensional Gaussian approximations",
        "abstract": "In this article, we study the critical growth rates of dimension below which Gaussian critical values can be used for hypothesis testing but beyond which they cannot. We are particularly interested in how these growth rates depend on the number of moments that the observations possess.",
        "references": [
            {
                "arxivId": "2205.09691",
                "title": "High-Dimensional Data Bootstrap",
                "abstract": "This article reviews recent progress in high-dimensional bootstrap. We first review high-dimensional central limit theorems for distributions of sample mean vectors over the rectangles, bootstrap consistency results in high dimensions, and key techniques used to establish those results. We then review selected applications of high-dimensional bootstrap: construction of simultaneous confidence sets for high-dimensional vector parameters, multiple hypothesis testing via step-down, postselection inference, intersection bounds for partially identified parameters, and inference on best policies in policy evaluation. Finally, we also comment on a couple of future research directions."
            },
            {
                "arxivId": "2012.09513",
                "title": "Nearly optimal central limit theorem and bootstrap approximations in high dimensions",
                "abstract": "In this paper, we derive new, nearly optimal bounds for the Gaussian approximation to scaled averages of $n$ independent high-dimensional centered random vectors $X_1,\\dots,X_n$ over the class of rectangles in the case when the covariance matrix of the scaled average is non-degenerate. In the case of bounded $X_i$'s, the implied bound for the Kolmogorov distance between the distribution of the scaled average and the Gaussian vector takes the form $$C (B^2_n \\log^3 d/n)^{1/2} \\log n,$$ where $d$ is the dimension of the vectors and $B_n$ is a uniform envelope constant on components of $X_i$'s. This bound is sharp in terms of $d$ and $B_n$, and is nearly (up to $\\log n$) sharp in terms of the sample size $n$. In addition, we show that similar bounds hold for the multiplier and empirical bootstrap approximations. Moreover, we establish bounds that allow for unbounded $X_i$'s, formulated solely in terms of moments of $X_i$'s. Finally, we demonstrate that the bounds can be further improved in some special smooth and zero-skewness cases."
            },
            {
                "arxivId": "2009.13673",
                "title": "High-dimensional CLT for Sums of Non-degenerate Random Vectors: $n^{-1/2}$-rate",
                "abstract": "In this note, we provide a Berry--Esseen bounds for rectangles in high-dimensions when the random vectors have non-singular covariance matrices. Under this assumption of non-singularity, we prove an $n^{-1/2}$ scaling for the Berry--Esseen bound for sums of mean independent random vectors with a finite third moment. The proof is essentially the method of compositions proof of multivariate Berry--Esseen bound from Senatov (2011). Similar to other existing works (Kuchibhotla et al. 2018, Fang and Koike 2020a), this note considers the applicability and effectiveness of classical CLT proof techniques for the high-dimensional case."
            },
            {
                "arxivId": "2009.06004",
                "title": "Central limit theorem and bootstrap approximation in high dimensions: Near 1/n rates via implicit smoothing",
                "abstract": "Non-asymptotic bounds for Gaussian and bootstrap approximation have recently attracted significant interest in high-dimensional statistics. This paper studies Berry-Esseen bounds for such approximations with respect to the multivariate Kolmogorov distance, in the context of a sum of $n$ random vectors that are $p$-dimensional and i.i.d. Up to now, a growing line of work has established bounds with mild logarithmic dependence on $p$. However, the problem of developing corresponding bounds with near $n^{-1/2}$ dependence on $n$ has remained largely unresolved. Within the setting of random vectors that have sub-Gaussian or sub-exponential entries, this paper establishes bounds with near $n^{-1/2}$ dependence, for both Gaussian and bootstrap approximation. In addition, the proofs are considerably distinct from other recent approaches and make use of an\"implicit smoothing\"operation in the Lindeberg interpolation."
            },
            {
                "arxivId": "2009.00339",
                "title": "Large-dimensional central limit theorem with fourth-moment error bounds on convex sets and balls",
                "abstract": "We prove the large-dimensional Gaussian approximation of a sum of $n$ independent random vectors in $\\mathbb{R}^d$ together with fourth-moment error bounds on convex sets and Euclidean balls. We show that compared with classical third-moment bounds, our bounds can achieve improved and, in the case of balls, optimal dependence $d=o(n)$ on dimension. We discuss an application to the bootstrap. The proof is by recent advances in Stein's method."
            },
            {
                "arxivId": "2008.04389",
                "title": "Central Limit Theorem in high dimensions: The optimal bound on dimension growth rate",
                "abstract": "<p>In this article, we try to give an answer to the simple question: \u201c<italic>What is the optimal growth rate of the dimension <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"p\">\n  <mml:semantics>\n    <mml:mi>p</mml:mi>\n    <mml:annotation encoding=\"application/x-tex\">p</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> as a function of the sample size <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"n\">\n  <mml:semantics>\n    <mml:mi>n</mml:mi>\n    <mml:annotation encoding=\"application/x-tex\">n</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> for which the Central Limit Theorem (CLT) holds uniformly over the collection of <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"p\">\n  <mml:semantics>\n    <mml:mi>p</mml:mi>\n    <mml:annotation encoding=\"application/x-tex\">p</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>-dimensional hyper-rectangles ?\u201d</italic>. Specifically, we are interested in the normal approximation of suitably scaled versions of the sum <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"sigma-summation Underscript i equals 1 Overscript n Endscripts upper X Subscript i\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:munderover>\n        <mml:mo>\u2211<!-- \u2211 --></mml:mo>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi>i</mml:mi>\n          <mml:mo>=</mml:mo>\n          <mml:mn>1</mml:mn>\n        </mml:mrow>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi>n</mml:mi>\n        </mml:mrow>\n      </mml:munderover>\n      <mml:msub>\n        <mml:mi>X</mml:mi>\n        <mml:mi>i</mml:mi>\n      </mml:msub>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">\\sum _{i=1}^{n}X_i</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> in <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"script upper R Superscript p\">\n  <mml:semantics>\n    <mml:msup>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">R</mml:mi>\n      </mml:mrow>\n      <mml:mi>p</mml:mi>\n    </mml:msup>\n    <mml:annotation encoding=\"application/x-tex\">\\mathcal {R}^p</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> uniformly over the class of hyper-rectangles <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"script upper A Superscript r e Baseline equals left-brace product Underscript j equals 1 Overscript p Endscripts left-bracket a Subscript j Baseline comma b Subscript j Baseline right-bracket intersection script upper R colon negative normal infinity less-than-or-equal-to a Subscript j Baseline less-than-or-equal-to b Subscript j Baseline less-than-or-equal-to normal infinity comma j equals 1 comma ellipsis comma p right-brace\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:msup>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">A</mml:mi>\n        </mml:mrow>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi>r</mml:mi>\n          <mml:mi>e</mml:mi>\n        </mml:mrow>\n      </mml:msup>\n      <mml:mo>=</mml:mo>\n      <mml:mo fence=\"false\" stretchy=\"false\">{</mml:mo>\n      <mml:munderover>\n        <mml:mo>\u220f<!-- \u220f --></mml:mo>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi>j</mml:mi>\n          <mml:mo>=</mml:mo>\n          <mml:mn>1</mml:mn>\n        </mml:mrow>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi>p</mml:mi>\n        </mml:mrow>\n      </mml:munderover>\n      <mml:mo stretchy=\"false\">[</mml:mo>\n      <mml:msub>\n        <mml:mi>a</mml:mi>\n        <mml:mi>j</mml:mi>\n      </mml:msub>\n      <mml:mo>,</mml:mo>\n      <mml:msub>\n        <mml:mi>b</mml:mi>\n        <mml:mi>j</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">]</mml:mo>\n      <mml:mo>\u2229<!-- \u2229 --></mml:mo>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">R</mml:mi>\n      </mml:mrow>\n      <mml:mo>:</mml:mo>\n      <mml:mo>\u2212<!-- \u2212 --></mml:mo>\n      <mml:mi mathvariant=\"normal\">\u221e<!-- \u221e --></mml:mi>\n      <mml:mo>\u2264<!-- \u2264 --></mml:mo>\n      <mml:msub>\n        <mml:mi>a</mml:mi>\n        <mml:mi>j</mml:mi>\n      </mml:msub>\n      <mml:mo>\u2264<!-- \u2264 --></mml:mo>\n      <mml:msub>\n        <mml:mi>b</mml:mi>\n        <mml:mi>j</mml:mi>\n      </mml:msub>\n      <mml:mo>\u2264<!-- \u2264 --></mml:mo>\n      <mml:mi mathvariant=\"normal\">\u221e<!-- \u221e --></mml:mi>\n      <mml:mo>,</mml:mo>\n      <mml:mi>j</mml:mi>\n      <mml:mo>=</mml:mo>\n      <mml:mn>1</mml:mn>\n      <mml:mo>,</mml:mo>\n      <mml:mo>\u2026<!-- \u2026 --></mml:mo>\n      <mml:mo>,</mml:mo>\n      <mml:mi>p</mml:mi>\n      <mml:mo fence=\"false\" stretchy=\"false\">}</mml:mo>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">\\mathcal {A}^{re}=\\{\\prod _{j=1}^{p}[a_j,b_j]\\cap \\mathcal {R}:-\\infty \\leq a_j\\leq b_j \\leq \\infty , j=1,\\ldots ,p\\}</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>, where <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"upper X 1 comma ellipsis comma upper X Subscript n Baseline\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:msub>\n        <mml:mi>X</mml:mi>\n        <mml:mn>1</mml:mn>\n      </mml:msub>\n      <mml:mo>,</mml:mo>\n      <mml:mo>\u2026<!-- \u2026 --></mml:mo>\n      <mml:mo>,</mml:mo>\n      <mml:msub>\n        <mml:mi>X</mml:mi>\n        <mml:mi>n</mml:mi>\n      </mml:msub>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">X_1,\\dots ,X_n</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> are independent <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"p minus\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>p</mml:mi>\n      <mml:mo>\u2212<!-- \u2212 --></mml:mo>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">p-</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>dimensional random vectors with each having independent and identically distributed (iid) components. We investigate the optimal cut-off rate of <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"log p\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>log</mml:mi>\n      <mml:mo>\u2061<!-- \u2061 --></mml:mo>\n      <mml:mi>p</mml:mi>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">\\log p</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> below which the uniform CLT holds and above which it fails. According to some recent results of Chernozukov et al. [Ann. Probab. 45 (2017), pp. 2309\u20132352], it is well known that the CLT holds uniformly over <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"script upper A Superscript r e\">\n  <mml:semantics>\n    <mml:msup>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">A</mml:mi>\n      </mml:mrow>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi>r</mml:mi>\n        <mml:mi>e</mml:mi>\n      </mml:mrow>\n    </mml:msup>\n    <mml:annotation encoding=\"application/x-tex\">\\mathcal {A}^{re}</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> if <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"log p equals o left-parenthesis n Superscript 1 slash 7 Baseline right-parenthesis\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>log</mml:mi>\n      <mml:mo>\u2061<!-- \u2061 --></mml:mo>\n      <mml:mi>p</mml:mi>\n      <mml:mo>=</mml:mo>\n      <mml:mi>o</mml:mi>\n      <mml:mstyle scriptlevel=\"0\">\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mo maxsize=\"1.2em\" minsize=\"1.2em\">(</mml:mo>\n        </mml:mrow>\n      </mml:mstyle>\n      <mml:msup>\n        <mml:mi>n</mml:mi>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mn>1</mml:mn>\n          <mml:mrow class=\"MJX-TeXAtom-ORD\">\n            <mml:mo>/</mml:mo>\n          </mml:mrow>\n          <mml:mn>7</mml:mn>\n        </mml:mrow>\n      </mml:msup>\n      <mml:mstyle scriptlevel=\"0\">\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mo maxsize=\"1.2em\" minsize=\"1.2em\">)</mml:mo>\n        </mml:mrow>\n      </mml:mstyle>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">\\log p=o\\big (n^{1/7}\\big )</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>. They also conjectured that for CLT to hold uniformly over <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"script upper A Superscript r e\">\n  <mml:semantics>\n    <mml:msup>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">A</mml:mi>\n      </mml:mrow>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi>r</mml:mi>\n        <mml:mi>e</mml:mi>\n      </mml:mrow>\n    </mml:msup>\n    <mml:annotation encoding=\"application/x-tex\">\\mathcal {A}^{re}</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>, the optimal rate is <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"log p equals o left-parenthesis n Superscript 1 slash 3 Baseline right-parenthesis\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>log</mml:mi>\n      <mml:mo>\u2061<!-- \u2061 --></mml:mo>\n      <mml:mi>p</mml:mi>\n      <mml:mo>=</mml:mo>\n      <mml:mi>o</mml:mi>\n      <mml:mstyle scriptlevel=\"0\">\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mo maxsize=\"1.2em\" minsize=\"1.2em\">(</mml:mo>\n        </mml:mrow>\n      </mml:mstyle>\n      <mml:msup>\n        <mml:mi>n</mml:mi>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mn>1</mml:mn>\n          <"
            },
            {
                "arxivId": "1212.6906",
                "title": "Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors",
                "abstract": "We derive a Gaussian approximation result for the maximum of a sum of high dimensional random vectors. Specifically, we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the Gaussian random vectors with the same covariance matrices as the original vectors. This result applies when the dimension of random vectors (p) is large compared to the sample size (n); in fact, p can be much larger than n, without restricting correlations of the coordinates of these vectors. We also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional Gaussian random vectors obtained by multiplying the original vectors with i.i.d. Gaussian multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure. Here too, p can be large or even much larger than n. These distributional approximations, either Gaussian or conditional Gaussian, yield a high-quality approximation to the distribution of the original maximum, often with approximation error decreasing polynomially in the sample size, and hence are of interest in many applications. We demonstrate how our Gaussian approximations and the multiplier bootstrap can be used for modern high dimensional estimation, multiple hypothesis testing, and adaptive specification testing. All these results contain non-asymptotic bounds on approximation errors."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.02303",
        "category": "econ",
        "title": "Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium",
        "abstract": "The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.",
        "references": [
            {
                "arxivId": "2301.02276",
                "title": "Statistical Inference and A/B Testing for First-Price Pacing Equilibria",
                "abstract": "We initiate the study of statistical inference and A/B testing for first-price pacing equilibria (FPPE). The FPPE model captures the dynamics resulting from large-scale first-price auction markets where buyers use pacing-based budget management. Such markets arise in the context of internet advertising, where budgets are prevalent. We propose a statistical framework for the FPPE model, in which a limit FPPE with a continuum of items models the long-run steady-state behavior of the auction platform, and an observable FPPE consisting of a finite number of items provides the data to estimate primitives of the limit FPPE, such as revenue, Nash social welfare (a fair metric of efficiency), and other parameters of interest. We develop central limit theorems and asymptotically valid confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of our estimators. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Numerical simulations verify our central limit theorems, and empirical coverage rates for our confidence intervals agree with our theory."
            },
            {
                "arxivId": "2204.01884",
                "title": "Policy Learning with Competing Agents",
                "abstract": "Decision makers often aim to learn a treatment assignment policy under a capacity constraint on the number of agents that they can treat. When agents can respond strategically to such policies, competition arises, complicating estimation of the optimal policy. In this paper, we study capacity-constrained treatment assignment in the presence of such interference. We consider a dynamic model where the decision maker allocates treatments at each time step and heterogeneous agents myopically best respond to the previous treatment assignment policy. When the number of agents is large but finite, we show that the threshold for receiving treatment under a given policy converges to the policy's mean-field equilibrium threshold. Based on this result, we develop a consistent estimator for the policy gradient. In a semi-synthetic experiment with data from the National Education Longitudinal Study of 1988, we demonstrate that this estimator can be used for learning capacity-constrained policies in the presence of strategic behavior."
            },
            {
                "arxivId": "2103.12936",
                "title": "Online Market Equilibrium with Application to Fair Division",
                "abstract": "Computing market equilibria is a problem of both theoretical and applied interest. Much research to date focuses on the case of static Fisher markets with full information on buyers' utility functions and item supplies. Motivated by real-world markets, we consider an online setting: individuals have linear, additive utility functions; items arrive sequentially and must be allocated and priced irrevocably. We define the notion of an online market equilibrium in such a market as time-indexed allocations and prices which guarantee buyer optimality and market clearance in hindsight. We propose a simple, scalable and interpretable allocation and pricing dynamics termed as PACE. When items are drawn i.i.d. from an unknown distribution (with a possibly continuous support), we show that PACE leads to an online market equilibrium asymptotically. In particular, PACE ensures that buyers' time-averaged utilities converge to the equilibrium utilities w.r.t. a static market with item supplies being the unknown distribution and that buyers' time-averaged expenditures converge to their per-period budget. Hence, many desirable properties of market equilibrium-based fair division such as no envy, Pareto optimality, and the proportional-share guarantee are also attained asymptotically in the online setting. Next, we extend the dynamics to handle quasilinear buyer utilities, which gives the first online algorithm for computing first-price pacing equilibria. Finally, numerical experiments on real and synthetic datasets show that the dynamics converges quickly under various metrics."
            },
            {
                "arxivId": "2002.05670",
                "title": "Experimental Design in Two-Sided Platforms: An Analysis of Bias",
                "abstract": "We develop an analytical framework to study experimental design in two-sided platforms. In the settings we consider, customers rent listings; rented listings are occupied for some amount of time, then become available. Platforms typically use two common designs to study interventions in such settings: customer-side randomization (CR), and listing-side randomization (LR), along with associated estimators. We develop a stochastic model and associated mean field limit to capture dynamics in such systems, and use our model to investigate how the performance of these estimators is affected by interference effects between listings and between customers. Good experimental design depends on market balance: we show that in highly demand-constrained markets, CR is unbiased, while LR is biased; conversely, in highly supply-constrained markets, LR is unbiased, while CR is biased. We also study a design based on two-sided randomization (TSR) where both customers and listings are randomized to treatment and control, and show that appropriate choices of such designs can be unbiased in both extremes of market balance, and also yield low bias in intermediate regimes of market balance. Full paper available at: https://arxiv.org/abs/2002.05670."
            },
            {
                "arxivId": "1609.06654",
                "title": "Convex Program Duality, Fisher Markets, and Nash Social Welfare",
                "abstract": "We study Fisher markets and the problem of maximizing the Nash social welfare (NSW), and show\u00a0several closely related new results. In particular, we obtain: A new integer program for the NSW maximization problem whose fractional relaxation has a\u00a0bounded integrality gap. In contrast, the natural integer program has an unbounded integrality gap. An improved, and tight, factor 2 analysis of the algorithm of [7]; in turn showing that the integrality\u00a0gap of the above relaxation is at most 2. The approximation factor shown by [7] was 2e\u00a01/e \u2248 2.89. A lower bound of e\u00a01/e \u2248 1.44 on the integrality gap of this relaxation. New convex programs for natural generalizations of linear Fisher markets and proofs that these\u00a0markets admit rational equilibria. These results were obtained by establishing connections between previously known disparate results,\u00a0and they help uncover their mathematical underpinnings. We show a formal connection between the\u00a0convex programs of Eisenberg and Gale and that of Shmyrev, namely that their duals are equivalent up\u00a0to a change of variables. Both programs capture equilibria of linear Fisher markets. By adding suitable\u00a0constraints to Shmyrev\u2019s program, we obtain a convex program that captures equilibria of the spendingrestricted\u00a0market model defined by [7] in the context of the NSW maximization problem. Further, adding\u00a0certain integral constraints to this program we get the integer program for the NSW mentioned above. The basic tool we use is convex programming duality. In the special case of convex programs with\u00a0linear constraints (but convex objectives), we show a particularly simple way of obtaining dual programs,\u00a0putting it almost at par with linear program duality. This simple way of finding duals has been used\u00a0subsequently for many other applications."
            },
            {
                "arxivId": "1511.08748",
                "title": "A New Class of Combinatorial Markets with Covering Constraints: Algorithms and Applications",
                "abstract": "We introduce a new class of combinatorial markets in which agents have covering constraints over resources required and are interested in delay minimization. Our market model is applicable to several settings including scheduling and communicating over a network. This model is quite different from the traditional models, to the extent that neither do the classical equilibrium existence results seem to apply to it nor do any of the efficient algorithmic techniques developed to compute equilibria. In particular, our model does not satisfy the condition of non-satiation, which is used critically to show the existence of equilibria in traditional market models and we observe that our set of equilibrium prices could be a connected, non-convex set. We give a proof of the existence of equilibria and a polynomial time algorithm for finding one, drawing heavily on techniques from LP duality and submodular minimization. Finally, we show that our model inherits many of the fairness properties of traditional equilibrium models as well as new models, such as CEEI."
            },
            {
                "arxivId": "1502.07571",
                "title": "Online Fair Division: Analysing a Food Bank Problem",
                "abstract": "We study an online model of fair division designed to capture features of a real world charity problem. We consider two simple mechanisms for this model in which agents simply declare what items they like. We analyse axiomatic properties of these mechanisms such as strategy-proofness and envy freeness. Finally, we perform a competitive analysis and compute the price of anarchy."
            },
            {
                "arxivId": "1404.1097",
                "title": "Competitive algorithms from competitive equilibria: non-clairvoyant scheduling under polyhedral constraints",
                "abstract": "We introduce and study a general scheduling problem that we term the Packing Scheduling problem (PSP). In this problem, jobs can have different arrival times and sizes; a scheduler can process job j at rate xj, subject to arbitrary packing constraints over the set of rates (x) of the outstanding jobs. The PSP framework captures a variety of scheduling problems, including the classical problems of unrelated machines scheduling, broadcast scheduling, and scheduling jobs of different parallelizability. It also captures scheduling constraints arising in diverse modern environments ranging from individual computer architectures to data centers. More concretely, PSP models multidimensional resource requirements and parallelizability, as well as network bandwidth requirements found in data center scheduling. In this paper, we design non-clairvoyant online algorithms for PSP and its special cases -- in this setting, the scheduler is unaware of the sizes of jobs. Our results are summarized as follows. \u2022 For minimizing total weighted completion time, we show a O(1)-competitive algorithm. Surprisingly, we achieve this result by applying the well-known Proportional Fairness algorithm (PF) to perform allocations each time instant. Though PF has been extensively studied in the context of maximizing fairness in resource allocation, we present the first analysis in adversarial and general settings for optimizing job latency. Our result is also the first O(1)-competitive algorithm for weighted completion time for several classical non-clairvoyant scheduling problems. \u2022For minimizing total weighted flow time, for any constant \u03b5 > 0, any O(n1---\u03b5)-competitive algorithm requires extra speed (resource augmentation) compared to the offline optimum. We show that PF is a O(log n)-speed O(log n)-competitive non-clairvoyant algorithm, where n is the total number of jobs. We further show that there is an instance of PSP for which no non-clairvoyant algorithm can be O(n1---\u03b5)-competitive with o(\u221alog n) speed. \u2022For the classical problem of minimizing total flow time for unrelated machines in the non-clairvoyant setting, we present the first online algorithm which is scalable ((1 + \u03b5)-speed O(1)-competitive for any constant \u03b5 > 0). No non-trivial results were known for this setting, and the previous scalable algorithm could handle only related machines. We develop new algorithmic techniques to handle the unrelated machines setting that build on a new single machine scheduling policy. Since unrelated machine scheduling is a special case of PSP, when contrasted with the lower bound for PSP, our result also shows that PSP is significantly harder than perhaps the most general classical scheduling settings. Our results for PSP show that instantaneous fair scheduling algorithms can also be effective tools for minimizing the overall job latency, even when the scheduling decisions are non-clairvoyant and constrained by general packing constraints."
            },
            {
                "arxivId": "1109.4204",
                "title": "Moment Consistency of the Exchangeably Weighted Bootstrap for Semiparametric M\u2010estimation",
                "abstract": "The bootstrap variance estimate is widely used in semiparametric inferences. However, its theoretical validity is a well\u2010known open problem. In this paper, we provide a first theoretical study on the bootstrap moment estimates in semiparametric models. Specifically, we establish the bootstrap moment consistency of the Euclidean parameter, which immediately implies the consistency of t\u2010type bootstrap confidence set. It is worth pointing out that the only additional cost to achieve the bootstrap moment consistency in contrast with the distribution consistency is to simply strengthen the L1 maximal inequality condition required in the latter to the Lp maximal inequality condition for p\u22651. The general Lp multiplier inequality developed in this paper is also of independent interest. These general conclusions hold for the bootstrap methods with exchangeable bootstrap weights, for example, non\u2010parametric bootstrap and Bayesian bootstrap. Our general theory is illustrated in the celebrated Cox regression model."
            },
            {
                "arxivId": "1105.1976",
                "title": "A consistent bootstrap procedure for the maximum score estimator",
                "abstract": null
            },
            {
                "arxivId": "0906.1310",
                "title": "Bootstrap consistency for general semiparametric $M$-estimation",
                "abstract": "Consider $M$-estimation in a semiparametric model that is characterized by a Euclidean parameter of interest and an infinite-dimensional nuisance parameter. As a general purpose approach to statistical inferences, the bootstrap has found wide applications in semiparametric $M$-estimation and, because of its simplicity, provides an attractive alternative to the inference approach based on the asymptotic distribution theory. The purpose of this paper is to provide theoretical justifications for the use of bootstrap as a semiparametric inferential tool. We show that, under general conditions, the bootstrap is asymptotically consistent in estimating the distribution of the $M$-estimate of Euclidean parameter; that is, the bootstrap distribution asymptotically imitates the distribution of the $M$-estimate. We also show that the bootstrap confidence set has the asymptotically correct coverage probability. These general conclusions hold, in particular, when the nuisance parameter is not estimable at root-$n$ rate, and apply to a broad class of bootstrap methods with exchangeable bootstrap weights. This paper provides a first general theoretical study of the bootstrap in semiparametric models."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.06758",
        "category": "econ",
        "title": "Measuring the Dunkelflaute: How (not) to analyze variable renewable energy shortage",
        "abstract": "As variable renewable energy sources increasingly gain importance in global energy systems, there is a growing interest in understanding periods of variable renewable energy shortage (``Dunkelflauten''). Defining, quantifying, and comparing such shortage events across different renewable generation technologies and locations presents a surprisingly intricate challenge. Various approaches exist in different bodies of literature, such as hydrology, wind and solar energy analysis, or energy system modeling. The subject of interest in previous analyses ranges from single technologies in specific locations to diverse technology portfolios across multiple regions, focusing either on supply from variable renewables or its mismatch with electricity demand. We provide an overview of methods for quantifying variable renewable energy shortage. We explain and critically discuss the merits and challenges of different approaches for defining and identifying shortage events and propose further methodological improvements for more accurate shortage determination. Additionally, we elaborate on comparability requirements for multi-technological and multi-regional energy shortage analysis. In doing so, we aim to contribute to unifying disparate methodologies, harmonizing terminologies, and providing guidance for future research.",
        "references": [
            {
                "arxivId": "2301.11096",
                "title": "How flexible electrification can integrate fluctuating renewables",
                "abstract": null
            },
            {
                "arxivId": "2211.16419",
                "title": "Geographical balancing of wind power decreases storage needs in a 100% renewable European power sector",
                "abstract": null
            },
            {
                "arxivId": "2208.07302",
                "title": "Power sector effects of green hydrogen production in Germany",
                "abstract": null
            },
            {
                "arxivId": "2103.03442",
                "title": "Sector coupling via hydrogen to lower the cost of energy system decarbonization",
                "abstract": "Sector coupling via H2 could significantly reduce the cost of energy system decarbonization by providing extra flexibility for variable renewable energy integration."
            },
            {
                "arxivId": "1801.05290",
                "title": "Synergies of sector coupling and transmission reinforcement in a cost-optimised, highly renewable European energy system",
                "abstract": null
            },
            {
                "arxivId": "1707.08164",
                "title": "Opening the black box of energy modelling: strategies and lessons learned",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.06765",
        "category": "econ",
        "title": "Perfect Bayesian Persuasion",
        "abstract": "A sender commits to an experiment to persuade a receiver. Accounting for the sender's experiment-choice incentives, and not presupposing a receiver tie-breaking rule when indifferent, we characterize when the sender's equilibrium payoff is unique and so coincides with her\"Bayesian persuasion\"value. A sufficient condition in finite models is that every action which is receiver-optimal at some belief is uniquely optimal at some other belief -- a generic property. We similarly show the equilibrium sender payoff is typically unique in ordered models. In an extension, we show uniqueness generates robustness to imperfect sender commitment.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07190",
        "category": "econ",
        "title": "Transporte Aereo: Economia e Politicas Publicas",
        "abstract": "This book, written in Portuguese, presents a comprehensive analysis of the air transport industry in Brazil, highlighting its vital importance to the country's economy. It explores the sector's complexity, from economic characteristics to interaction with the national aeronautical industry, through the specialization of the workforce and market demand analysis. The book delves into the economic regulation of air transport, tracing the evolution from periods of strict regulation to phases of liberalization and deregulation, and examines market dynamics, focusing on concentration and competitiveness. It also analyzes demand and supply through case studies, investigating everything from tourists traveling within Brazil to the coverage of the national territory and the prices of air tickets. Finally, the book proposes principles for the regulation and public policies of the air sector, emphasizing the priority of the passenger, the business environment, access to air transport, and economic efficiency, culminating in the advocacy for a free market, but with protection for competition and the consumer.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07227",
        "category": "econ",
        "title": "Time-Delayed Game Strategy Analysis Among Japan, Other Nations, and the International Atomic Energy Agency in the Context of Fukushima Nuclear Wastewater Discharge Decision",
        "abstract": "This academic paper examines the strategic interactions between Japan, other nations, and the International Atomic Energy Agency (IAEA) regarding Japan's decision to release treated nuclear wastewater from the Fukushima Daiichi Nuclear Power Plant into the sea. It introduces a payoff matrix and time-delay elements in replicator dynamic equations to mirror real-world decision-making delays. The paper analyzes the stability of strategies and conditions for different stable states using characteristic roots of a linearized system and numerical simulations. It concludes that time delays significantly affect decision-making stability and evolution trajectories in nuclear wastewater disposal strategies. The study highlights the importance of efficient wastewater treatment technology, the impact of export tax revenue losses on Japan's strategies, and the role of international cooperation. The novelty of the research lies in integrating time-delay elements from ocean dynamics and governmental decision-making into the game-theoretical model.",
        "references": [
            {
                "arxivId": "1404.0994",
                "title": "Evolutionary game theory using agent-based methods.",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07235",
        "category": "econ",
        "title": "Scientific Talent Leaks Out of Funding Gaps",
        "abstract": "We study how delays in NIH grant funding affect the career outcomes of research personnel. Using comprehensive earnings and tax records linked to university transaction data along with a difference-in-differences design, we find that a funding interruption of more than 30 days has a substantial effect on job placements for personnel who work in labs with a single NIH R01 research grant, including a 3 percentage point (40%) increase in the probability of not working in the US. Incorporating information from the full 2020 Decennial Census and data on publications, we find that about half of those induced into nonemployment appear to permanently leave the US and are 90% less likely to publish in a given year, with even larger impacts for trainees (postdocs and graduate students). Among personnel who continue to work in the US, we find that interrupted personnel earn 20% less than their continuously-funded peers, with the largest declines concentrated among trainees and other non-faculty personnel (such as staff and undergraduates). Overall, funding delays account for about 5% of US nonemployment in our data, indicating that they have a meaningful effect on the scientific labor force at the national level.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07266",
        "category": "econ",
        "title": "Spillover effects of US monetary policy on emerging markets amidst uncertainty",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07277",
        "category": "econ",
        "title": "Bundling Demand in K-12 Broadband Procurement",
        "abstract": "We evaluate the effects of bundling demand for broadband internet by K-12 schools. In 2014, New Jersey switched from decentralized procurements to a new procurement system that bundled schools into four regional groups. Using an event study approach, we find that, on average, prices for participants decreased by one-third, and broadband speed purchased increased sixfold. We bound the change in school expenditures due to the program and find that participants saved at least as much as their total\"E-rate\"subsidy from the federal government. Under weak assumptions on demand, we show that participating schools experienced large welfare gains.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07322",
        "category": "econ",
        "title": "Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis",
        "abstract": "Online A/B testing is widely used in the internet industry to inform decisions on new feature roll-outs. For online marketplaces (such as advertising markets), standard approaches to A/B testing may lead to biased results when buyers operate under a budget constraint, as budget consumption in one arm of the experiment impacts performance of the other arm. To counteract this interference, one can use a budget-split design where the budget constraint operates on a per-arm basis and each arm receives an equal fraction of the budget, leading to ``budget-controlled A/B testing.'' Despite clear advantages of budget-controlled A/B testing, performance degrades when budget are split too small, limiting the overall throughput of such systems. In this paper, we propose a parallel budget-controlled A/B testing design where we use market segmentation to identify submarkets in the larger market, and we run parallel experiments on each submarket. Our contributions are as follows: First, we introduce and demonstrate the effectiveness of the parallel budget-controlled A/B test design with submarkets in a large online marketplace environment. Second, we formally define market interference in first-price auction markets using the first price pacing equilibrium (FPPE) framework. Third, we propose a debiased surrogate that eliminates the first-order bias of FPPE, drawing upon the principles of sensitivity analysis in mathematical programs. Fourth, we derive a plug-in estimator for the surrogate and establish its asymptotic normality. Fifth, we provide an estimation procedure for submarket parallel budget-controlled A/B tests. Finally, we present numerical examples on semi-synthetic data, confirming that the debiasing technique achieves the desired coverage properties.",
        "references": [
            {
                "arxivId": "2402.02303",
                "title": "Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium",
                "abstract": "The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory."
            },
            {
                "arxivId": "2311.10679",
                "title": "Non-uniform Bid-scaling and Equilibria for Different Auctions: An Empirical Study",
                "abstract": "In recent years, the growing adoption of autobidding has motivated the study of auction design with value-maximizing auto-bidders. It is known that under mild assumptions, uniform bid-scaling is an optimal bidding strategy in truthful auctions, e.g., Vickrey-Clarke-Groves auction (VCG), and the price of anarchy for VCG is $2$. However, for other auction formats like First-Price Auction (FPA) and Generalized Second-Price auction (GSP), uniform bid-scaling may not be an optimal bidding strategy, and bidders have incentives to deviate to adopt strategies with non-uniform bid-scaling. Moreover, FPA can achieve optimal welfare if restricted to uniform bid-scaling, while its price of anarchy becomes $2$ when non-uniform bid-scaling strategies are allowed. All these price of anarchy results have been focused on welfare approximation in the worst-case scenarios. To complement theoretical understandings, we empirically study how different auction formats (FPA, GSP, VCG) with different levels of non-uniform bid-scaling perform in an autobidding world with a synthetic dataset for auctions. Our empirical findings include: * For both uniform bid-scaling and non-uniform bid-scaling, FPA is better than GSP and GSP is better than VCG in terms of both welfare and profit; * A higher level of non-uniform bid-scaling leads to lower welfare performance in both FPA and GSP, while different levels of non-uniform bid-scaling have no effect in VCG. Our methodology of synthetic data generation may be of independent interest."
            },
            {
                "arxivId": "2310.14983",
                "title": "Causal clustering: design of cluster experiments under network interference",
                "abstract": "This paper studies the design of cluster experiments to estimate the global treatment effect in the presence of network spillovers. We provide a framework to choose the clustering that minimizes the worst-case mean-squared error of the estimated global effect. We show that optimal clustering solves a novel penalized min-cut optimization problem computed via off-the-shelf semi-definite programming algorithms. Our analysis also characterizes simple conditions to choose between any two cluster designs, including choosing between a cluster or individual-level randomization. We illustrate the method's properties using unique network data from the universe of Facebook's users and existing data from a field experiment."
            },
            {
                "arxivId": "2111.02468",
                "title": "Robust Auction Design in the Auto-bidding World",
                "abstract": "In classic auction theory, reserve prices are known to be effective for improving revenue for the auctioneer against quasi-linear utility maximizing bidders. The introduction of reserve prices, however, usually do not help improve total welfare of the auctioneer and the bidders. In this paper, we focus on value maximizing bidders with return on spend constraints -- a paradigm that has drawn considerable attention recently as more advertisers adopt auto-bidding algorithms in advertising platforms -- and show that the introduction of reserve prices has a novel impact on the market. Namely, by choosing reserve prices appropriately the auctioneer can improve not only the total revenue but also the total welfare. Our results also demonstrate that reserve prices are robust to bidder types, i.e., reserve prices work well for different bidder types, such as value maximizers and utility maximizers, without using bidder type information. We generalize these results for a variety of auction mechanisms such as VCG, GSP, and first-price auctions. Moreover, we show how to combine these results with additive boosts to improve the welfare of the outcomes of the auction further. Finally, we complement our theoretical observations with an empirical study confirming the effectiveness of these ideas using data from online advertising auctions."
            },
            {
                "arxivId": "2110.13814",
                "title": "Bidders' Responses to Auction Format Change in Internet Display Advertising Auctions",
                "abstract": "We study actual bidding behavior when a new auction format gets introduced into the marketplace. More specifically, we investigate this question using a novel dataset on internet display advertising auctions that exploits a staggered adoption by different publishers (sellers) of first-price auctions (FPAs), instead of the traditional second-price auctions (SPAs). Event study regression estimates indicate that, immediately after the auction format change, the revenue per sold impression (price) jumped considerably for the treated publishers relative to the control publishers, ranging from 35% to 75% of the pre-treatment price level of the treatment group. Further, we observe that in later auction format changes the increase in the price levels under FPAs relative to price levels under SPAs dissipates over time, reminiscent of the celebrated revenue equivalence theorem. A possible interpretation of these facts is initially insufficient bid shading after the format change rather than an immediate shift to a new Bayesian Nash equilibrium. The gradual decrease in prices can be interpreted as the result of bidders' learning to shade their bids. We also present suggestive evidence that bidders' sophistication may have impacted their response to the auction format change. Our work constitutes one of the first field studies on bidders' responses to auction format changes, providing an important complement to theoretical model predictions. As such, it provides valuable information to auction designers when considering the implementation of different formats. The full version of the paper is available at: https://arxiv.org/abs/2110.13814"
            },
            {
                "arxivId": "2103.06392",
                "title": "Design and Analysis of Bipartite Experiments Under a Linear Exposure-response Model",
                "abstract": "A bipartite experiment consists of one set of units being assigned treatments and another set of units for which we measure outcomes. The two sets of units are connected by a bipartite graph, governing how the treated units can affect the outcome units. The bipartite framework naturally arises in marketplace experiments where, for example, experimenters may seek to investigate the effect of discounting goods on buyer behavior. In this paper, we consider estimation of the average total treatment effect in the bipartite experimental framework under a linear exposure-response model. We introduce the Exposure Reweighted Linear (ERL) estimator, and show that the estimator is unbiased, consistent and asymptotically normal, provided that the bipartite graph is sufficiently sparse. To facilitate inference, we introduce an unbiased and consistent estimator of the variance of the ERL point estimator. In addition, we introduce a cluster-based design, Exposure-Design, that uses heuristics to increase the precision of the ERL estimator by realizing a desirable exposure distribution. Finally, we demonstrate the application of the described methodology to marketplace experiments using a publicly available Amazon user-item review dataset. The full version of the paper is available at: https://arxiv.org/abs/2103.06392."
            },
            {
                "arxivId": "2012.08591",
                "title": "Network Experimentation at Scale",
                "abstract": "We describe our network experimentation framework, deployed at Facebook, which accounts for interference between experimental units. We document this system, including the design and estimation procedures, and detail insights we have gained from the many experiments that have used this system at scale. In our estimation procedure, we introduce a cluster-based regression adjustment that substantially improves precision for estimating global treatment effects, as well as a procedure to test for interference. With our regression adjustment, we find that imbalanced clusters can better account for interference than balanced clusters without sacrificing accuracy. In addition, we show that logging exposure to a treatment can result in additional variance reduction. Interference is a widely acknowledged issue in online field experiments, yet there is less evidence from real-world experiments demonstrating interference in online settings. We fill this gap by describing two case studies that capture significant network effects and highlight the value of this experimentation framework."
            },
            {
                "arxivId": "1305.6979",
                "title": "Graph cluster randomization: network exposure to multiple universes",
                "abstract": "A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how graph cluster randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified. Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a restricted-growth condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-13.json",
        "arxivId": "2402.07521",
        "category": "econ",
        "title": "A step towards the integration of machine learning and small area estimation",
        "abstract": "The use of machine-learning techniques has grown in numerous research areas. Currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis. However, the usage of these methods in survey sampling including small area estimation is still very limited. Therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. Therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. We study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. What is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. The solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. The simulation studies are based on a real, longitudinal dataset, freely available from the Polish Local Data Bank, where the prediction problem of subpopulation characteristics in the last period, with\"borrowing strength\"from other subpopulations and time periods, is considered.",
        "references": [
            {
                "arxivId": "2201.10933",
                "title": "Flexible domain prediction using mixed effects random forests",
                "abstract": "This paper promotes the use of random forests as versatile tools for estimating spatially disaggregated indicators in the presence of small area\u2010specific sample sizes. Small area estimators are predominantly conceptualised within the regression\u2010setting and rely on linear mixed models to account for the hierarchical structure of the survey data. In contrast, machine learning methods offer non\u2010linear and non\u2010parametric alternatives, combining excellent predictive performance and a reduced risk of model\u2010misspecification. Mixed effects random forests combine advantages of regression forests with the ability to model hierarchical dependencies. This paper provides a coherent framework based on mixed effects random forests for estimating small area averages and proposes a non\u2010parametric bootstrap estimator for assessing the uncertainty of the estimates. We illustrate advantages of our proposed methodology using Mexican income\u2010data from the state Nuevo Le\u00f3n. Finally, the methodology is evaluated in model\u2010based and design\u2010based simulations comparing the proposed methodology to traditional regression\u2010based approaches for estimating small area averages."
            },
            {
                "arxivId": "2103.00834",
                "title": "Improving the Output Quality of Official Statistics Based on Machine Learning Algorithms",
                "abstract": "Abstract National statistical institutes currently investigate how to improve the output quality of official statistics based on machine learning algorithms. A key issue is concept drift, that is, when the joint distribution of independent variables and a dependent (categorical) variable changes over time. Under concept drift, a statistical model requires regular updating to prevent it from becoming biased. However, updating a model asks for additional data, which are not always available. An alternative is to reduce the bias by means of bias correction methods. In the article, we focus on estimating the proportion (base rate) of a category of interest and we compare two popular bias correction methods: the misclassification estimator and the calibration estimator. For prior probability shift (a specific type of concept drift), we investigate the two methods analytically as well as numerically. Our analytical results are expressions for the bias and variance of both methods. As numerical result, we present a decision boundary for the relative performance of the two methods. Our results provide a better understanding of the effect of prior probability shift on output quality. Consequently, we may recommend a novel approach on how to use machine learning algorithms in the context of official statistics."
            },
            {
                "arxivId": "2002.09736",
                "title": "Model-Assisted Estimation Through Random Forests in Finite Population Sampling",
                "abstract": "Abstract In surveys, the interest lies in estimating finite population parameters such as population totals and means. In most surveys, some auxiliary information is available at the estimation stage. This information may be incorporated in the estimation procedures to increase their precision. In this article, we use random forests (RFs) to estimate the functional relationship between the survey variable and the auxiliary variables. In recent years, RFs have become attractive as National Statistical Offices have now access to a variety of data sources, potentially exhibiting a large number of observations on a large number of variables. We establish the theoretical properties of model-assisted procedures based on RFs and derive corresponding variance estimators. A model-calibration procedure for handling multiple survey variables is also discussed. The results of a simulation study suggest that the proposed point and estimation procedures perform well in terms of bias, efficiency and coverage of normal-based confidence intervals, in a wide variety of settings. Finally, we apply the proposed methods using data on radio audiences collected by M\u00e9diam\u00e9trie, a French audience company. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-14.json",
        "arxivId": "2305.12179",
        "category": "econ",
        "title": "Commodity-specific triads in the Dutch inter-industry production network",
        "abstract": null,
        "references": [
            {
                "arxivId": "2111.15248",
                "title": "Reconstructing firm-level interactions in the Dutch input\u2013output network from production constraints",
                "abstract": null
            },
            {
                "arxivId": "2103.05623",
                "title": "The physics of financial networks",
                "abstract": null
            },
            {
                "arxivId": "2101.07818",
                "title": "Simultaneous supply and demand constraints in input\u2013output networks: the case of Covid-19 in Germany, Italy, and Spain",
                "abstract": "Natural and anthropogenic disasters frequently affect both the supply and demand sides of an economy. A striking recent example is the Covid-19 pandemic which has created severe disruptions to economic output in most countries. These direct shocks to supply and demand will propagate downstream and upstream through production networks. Given the exogenous shocks, we derive a lower bound on total shock propagation. We find that even in this best case scenario network effects substantially amplify the initial shocks. To obtain more realistic model predictions, we study the propagation of shocks bottom-up by imposing different rationing rules on industries if they are not able to satisfy incoming demand. Our results show that economic impacts depend strongly on the emergence of input bottlenecks, making the rationing assumption a key variable in predicting adverse economic impacts. We further establish that the magnitude of initial shocks and network density heavily influence model predictions."
            },
            {
                "arxivId": "2012.02677",
                "title": "Reconstructing Networks",
                "abstract": "Complex networks datasets often come with the problem of missing information: interactions data that have not been measured or discovered, may be affected by errors, or are simply hidden because of privacy issues. This Element provides an overview of the ideas, methods and techniques to deal with this problem and that together define the field of network reconstruction. Given the extent of the subject, we shall focus on the inference methods rooted in statistical physics and information theory. The discussion will be organized according to the different scales of the reconstruction task, that is, whether the goal is to reconstruct the macroscopic structure of the network, to infer its mesoscale properties, or to predict the individual microscopic connections."
            },
            {
                "arxivId": "2012.01265",
                "title": "Weighted network motifs as random walk patterns",
                "abstract": "Over the last two decades, network theory has shown to be a fruitful paradigm in understanding the organization and functioning of real-world complex systems. One technique helpful to this endeavor is identifying functionally influential subgraphs, shedding light on underlying evolutionary processes. Such overrepresented subgraphs, motifs, have received much attention in simple networks, where edges are either on or off. However, for weighted networks, motif analysis is still undeveloped. Here, we proposed a novel methodology\u2014based on a random walker taking a fixed maximum number of steps\u2014to study weighted motifs of limited size. We introduce a sink node to balance the network and allow the detection of configurations within an a priori fixed number of steps for the random walker. We applied this approach to different real networks and selected a specific null model based on maximum-entropy to test the significance of weighted motifs occurrence. We found that identified similarities enable the classifications of systems according to functioning mechanisms associated with specific configurations: economic networks exhibit close patterns while differentiating from ecological systems without any a priori assumption."
            },
            {
                "arxivId": "2001.11125",
                "title": "Testing biological network motif significance with exponential random graph models",
                "abstract": null
            },
            {
                "arxivId": "1909.01274",
                "title": "In Search of Lost Edges: A Case Study on Reconstructing FInancial Networks",
                "abstract": "To capture the systemic complexity of international financial systems, network data is an important prerequisite. However, dyadic data is often not available, raising the need for methods that allow for reconstructing networks based on limited information. In this paper, we are reviewing different methods that are designed for the estimation of matrices from their marginals and potentially exogenous information. This includes a general discussion of the available methodology that provides edge probabilities as well as models that are focussed on the reconstruction of edge values. Besides summarizing the advantages, shortfalls and computational issues of the approaches, we put them into a competitive comparison using the SWIFT (Society for Worldwide Interbank Financial Telecommunication) MT 103 payment messages network (MT 103: Single Customer Credit Transfer). This network is not only economically meaningful but also fully observed which allows for an extensive competitive horse race of methods. The comparison concerning the binary reconstruction is divided into an evaluation of the edge probabilities and the quality of the reconstructed degree structures. Furthermore, the accuracy of the predicted edge values is investigated. To test the methods on different topologies, the application is split into two parts. The first part considers the full MT 103 network, being an illustration for the reconstruction of large, sparse financial networks. The second part is concerned with reconstructing a subset of the full network, representing a dense medium-sized network. Regarding substantial outcomes, it can be found that no method is superior in every respect and that the preferred model choice highly depends on the goal of the analysis, the presumed network structure and the availability of exogenous information."
            },
            {
                "arxivId": "1811.09829",
                "title": "A faster horse on a safer trail: generalized inference for the efficient reconstruction of weighted networks",
                "abstract": "Due to the interconnectedness of financial entities, estimating certain key properties of a complex financial system, including the implied level of systemic risk, requires detailed information about the structure of the underlying network of dependencies. However, since data about financial linkages are typically subject to confidentiality, network reconstruction techniques become necessary to infer both the presence of connections and their intensity. Recently, several \u2018horse races\u2019 have been conducted to compare the performance of the available financial network reconstruction methods. These comparisons were based on arbitrarily chosen metrics of similarity between the real network and its reconstructed versions. Here we establish a generalized maximum-likelihood approach to rigorously define and compare weighted reconstruction methods. Our generalization uses the maximization of a certain conditional entropy to solve the problem represented by the fact that the density-dependent constraints required to reliably reconstruct the network are typically unobserved and, therefore, cannot enter directly, as sufficient statistics, in the likelihood function. The resulting approach admits as input any reconstruction method for the purely binary topology and, conditionally on the latter, exploits the available partial information to infer link weights. We find that the most reliable method is obtained by \u2018dressing\u2019 the best-performing binary method with an exponential distribution of link weights having a properly density-corrected and link-specific mean value and propose two safe (i.e. unbiased in the sense of maximum conditional entropy) variants of it. While the one named CReMA is perfectly general (as a particular case, it can place optimal weights on a network if the bare topology is known), the one named CReMB is recommended both in case of full uncertainty about the network topology and if the existence of some links is certain. In these cases, the CReMB is faster and reproduces empirical networks with highest generalized likelihood among the considered competing models."
            },
            {
                "arxivId": "1810.07774",
                "title": "How production networks amplify economic growth",
                "abstract": "Significance Technological improvement is the most important cause of long-term economic growth. We study the effects of technology improvement in the setting of a production network, in which each producer buys input goods and converts them to other goods, selling the product to households or other producers. We show how this network amplifies the effects of technological improvements as they propagate along chains of production. Longer production chains for an industry bias it toward faster price reduction, and longer production chains for a country bias it toward faster growth. These predictions are in good agreement with data and improve with the passage of time, demonstrating a key influence of production chains in price change and output growth over the long term. Technological improvement is the most important cause of long-term economic growth. In standard growth models, technology is treated in the aggregate, but an economy can also be viewed as a network in which producers buy goods, convert them to new goods, and sell the production to households or other producers. We develop predictions for how this network amplifies the effects of technological improvements as they propagate along chains of production, showing that longer production chains for an industry bias it toward faster price reduction and that longer production chains for a country bias it toward faster growth. These predictions are in good agreement with data from the World Input Output Database and improve with the passage of time. The results show that production chains play a major role in shaping the long-term evolution of prices, output growth, and structural change."
            },
            {
                "arxivId": "1810.05095",
                "title": "The statistical physics of real-world networks",
                "abstract": null
            },
            {
                "arxivId": "1809.06057",
                "title": "Cumulative effects of triadic closure and homophily in social networks",
                "abstract": "Dynamic interplay of choice homophily and triadic closure amplifies homophily and creates core-peripheries in social networks. Social network structure has often been attributed to two network evolution mechanisms\u2014triadic closure and choice homophily\u2014which are commonly considered independently or with static models. However, empirical studies suggest that their dynamic interplay generates the observed homophily of real-world social networks. By combining these mechanisms in a dynamic model, we confirm the longheld hypothesis that choice homophily and triadic closure cause induced homophily. We estimate how much observed homophily in friendship and communication networks is amplified due to triadic closure. We find that cumulative effects of homophily amplification can also lead to the widely documented core-periphery structure of networks, and to memory of homophilic constraints (equivalent to hysteresis in physics). The model shows that even small individual bias may prompt network-level changes such as segregation or core group dominance. Our results highlight that individual-level mechanisms should not be analyzed separately without considering the dynamics of society as a whole."
            },
            {
                "arxivId": "1506.00348",
                "title": "Enhanced Gravity Model of Trade: Reconciling Macroeconomic and Network Models",
                "abstract": "The structure of the International Trade Network (ITN), whose nodes and links represent world countries and their trade relations respectively, affects key economic processes worldwide, including globalization, economic integration, industrial production, and the propagation of shocks and instabilities. Characterizing the ITN via a simple yet accurate model is an open problem. The traditional Gravity Model (GM) successfully reproduces the volume of trade between connected countries, using macroeconomic properties such as GDP, geographic distance, and possibly other factors. However, it predicts a network with complete or homogeneous topology, thus failing to reproduce the highly heterogeneous structure of the ITN. On the other hand, recent maximum-entropy network models successfully reproduce the complex topology of the ITN, but provide no information about trade volumes. Here we integrate these two currently incompatible approaches via the introduction of an Enhanced Gravity Model (EGM) of trade. The EGM is the simplest model combining the GM with the network approach within a maximum-entropy framework. Via a unified and principled mechanism that is transparent enough to be generalized to any economic network, the EGM provides a new econometric framework wherein trade probabilities and trade volumes can be separately controlled by any combination of dyadic and country-specific macroeconomic variables. The model successfully reproduces both the global topology and the local link weights of the ITN, parsimoniously reconciling the conflicting approaches. It also indicates that the probability that any two countries trade a certain volume should follow a geometric or exponential distribution with an additional point mass at zero volume."
            },
            {
                "arxivId": "1302.2063",
                "title": "Early-warning signals of topological collapse in interbank networks",
                "abstract": null
            },
            {
                "arxivId": "1201.1215",
                "title": "Triadic motifs and dyadic self-organization in the World Trade Network",
                "abstract": null
            },
            {
                "arxivId": "1103.1243",
                "title": "Randomizing world trade. I. A binary network analysis.",
                "abstract": "The international trade network (ITN) has received renewed multidisciplinary interest due to recent advances in network theory. However, it is still unclear whether a network approach conveys additional, nontrivial information with respect to traditional international-economics analyses that describe world trade only in terms of local (first-order) properties. In this and in a companion paper, we employ a recently proposed randomization method to assess in detail the role that local properties have in shaping higher-order patterns of the ITN in all its possible representations (binary or weighted, directed or undirected, aggregated or disaggregated by commodity) and across several years. Here we show that, remarkably, the properties of all binary projections of the network can be completely traced back to the degree sequence, which is therefore maximally informative. Our results imply that explaining the observed degree sequence of the ITN, which has not received particular attention in economic theory, should instead become one the main focuses of models of trade."
            },
            {
                "arxivId": "0908.1143",
                "title": "How small are building blocks of complex networks",
                "abstract": "Network motifs are small building blocks of complex networks. Statistically significant motifs often perform network-specific functions. However, the precise nature of the connection between motifs and the global structure and function of networks remains elusive. Here we show that the global structure of some real networks is statistically determined by the probability of connections within motifs of size at most 3, once this probability accounts for node degrees. The connectivity profiles of node triples in these networks capture all their local and global properties. This finding impacts methods relying on motif statistical significance, and enriches our understanding of the elementary forces that shape the structure of complex networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-14.json",
        "arxivId": "2306.11154",
        "category": "econ",
        "title": "A Truth Serum for Eliciting Self-Evaluations in Scientific Reviews",
        "abstract": "This paper designs a simple, efficient and truthful mechanism to to elicit self-evaluations about items jointly owned by owners. A key application of this mechanism is to improve the peer review of large scientific conferences where a paper often has multiple authors and many authors have multiple papers. Our mechanism is designed to generate an entirely new source of review data truthfully elicited from paper owners, and can be used to augment the traditional approach of eliciting review data only from peer reviewers. Our approach starts by partitioning all submissions of a conference into disjoint blocks, each of which shares a common set of co-authors. We then elicit the ranking of the submissions from each author and employ isotonic regression to produce adjusted review scores that align with both the reported ranking and the raw review scores. Under certain conditions, truth-telling by all authors is a Nash equilibrium for any valid partition of the overlapping ownership sets. We prove that to ensure truthfulness for such isotonic regression based mechanisms, partitioning the authors into blocks and eliciting only ranking information independently from each block is necessary. This leave the optimization of block partition as the only room for maximizing the estimation efficiency of our mechanism, which is a computationally intractable optimization problem in general. Fortunately, we develop a nearly linear-time greedy algorithm that provably finds a performant partition with appealing robust approximation guarantees. Extensive experiments on both synthetic data and real-world conference review data demonstrate the effectiveness of this owner-assisted calibration mechanism.",
        "references": [
            {
                "arxivId": "2304.11160",
                "title": "The Isotonic Mechanism for Exponential Family Estimation",
                "abstract": "In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism to exponential family distributions. This mechanism generates adjusted scores that closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, implementing this mechanism does not require knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question involves only pairwise comparisons between her submissions, thus indicating the optimality of ranking in truthful information elicitation. Moreover, we show that the adjusted scores improve dramatically the estimation accuracy compared to the original scores and achieve nearly minimax optimality when the ground-truth scores have bounded total variation. We conclude the paper by presenting experiments conducted on the ICML 2023 ranking data, which show significant estimation gain using the Isotonic Mechanism."
            },
            {
                "arxivId": "2304.01393",
                "title": "Every Author as First Author",
                "abstract": "We propose a new standard for writing author names on papers and in bibliographies, which places every author as a first author -- superimposed. This approach enables authors to write papers as true equals, without any advantage given to whoever's name happens to come first alphabetically (for example). We develop the technology for implementing this standard in LaTeX, BibTeX, and HTML; show several examples; and discuss further advantages."
            },
            {
                "arxivId": "2207.11315",
                "title": "Tradeoffs in Preventing Manipulation in Paper Bidding for Reviewer Assignment",
                "abstract": "Many conferences rely on paper bidding as a key component of their reviewer assignment procedure. These bids are then taken into account when assigning reviewers to help ensure that each reviewer is assigned to suitable papers. However, despite the benefits of using bids, reliance on paper bidding can allow malicious reviewers to manipulate the paper assignment for unethical purposes (e.g., getting assigned to a friend's paper). Several different approaches to preventing this manipulation have been proposed and deployed. In this paper, we enumerate certain desirable properties that algorithms for addressing bid manipulation should satisfy. We then offer a high-level analysis of various approaches along with directions for future investigation."
            },
            {
                "arxivId": "2110.14802",
                "title": "You Are the Best Reviewer of Your Own Papers: An Owner-Assisted Scoring Mechanism",
                "abstract": "I consider a setting where reviewers offer very noisy scores for several items for the selection of high-quality ones (e.g., peer review of large conference proceedings), whereas the owner of these items knows the true underlying scores but prefers not to provide this information. To address this withholding of information, in this paper, I introduce the Isotonic Mechanism, a simple and efficient approach to improving imprecise raw scores by leveraging certain information that the owner is incentivized to provide. This mechanism takes the ranking of the items from best to worst provided by the owner as input, in addition to the raw scores provided by the reviewers. It reports the adjusted scores for the items by solving a convex optimization problem. Under certain conditions, I show that the owner's optimal strategy is to honestly report the true ranking of the items to her best knowledge in order to maximize the expected utility. Moreover, I prove that the adjusted scores provided by this owner-assisted mechanism are significantly more accurate than the raw scores provided by the reviewers. This paper concludes with several extensions of the Isotonic Mechanism and some refinements of the mechanism for practical consideration."
            },
            {
                "arxivId": "2110.12607",
                "title": "Least Square Calibration for Peer Reviews",
                "abstract": "Peer review systems such as conference paper review often suffer from the issue of miscalibration. Previous works on peer review calibration usually only use the ordinal information or assume simplistic reviewer scoring functions such as linear functions. In practice, applications like academic conferences often rely on manual methods, such as open discussions, to mitigate miscalibration. It remains an important question to develop algorithms that can handle different types of miscalibrations based on available prior knowledge. In this paper, we propose a flexible framework, namely least square calibration (LSC), for selecting top candidates from peer ratings. Our framework provably performs perfect calibration from noiseless linear scoring functions under mild assumptions, yet also provides competitive calibration results when the scoring function is from broader classes beyond linear functions and with arbitrary noise. On our synthetic dataset, we empirically demonstrate that our algorithm consistently outperforms the baseline which select top papers based on the highest average ratings."
            },
            {
                "arxivId": "2109.09774",
                "title": "Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment",
                "abstract": "In this paper we revisit the 2014 NeurIPS experiment that examined inconsistency in conference peer review. We determine that 50\\% of the variation in reviewer quality scores was subjective in origin. Further, with seven years passing since the experiment we find that for \\emph{accepted} papers, there is no correlation between quality scores and impact of the paper as measured as a function of citation count. We trace the fate of rejected papers, recovering where these papers were eventually published. For these papers we find a correlation between quality scores and impact. We conclude that the reviewing process for the 2014 conference was good for identifying poor papers, but poor for identifying good papers. We give some suggestions for improving the reviewing process but also warn against removing the subjective element. Finally, we suggest that the real conclusion of the experiment is that the community should place less onus on the notion of `top-tier conference publications' when assessing the quality of individual researchers. For NeurIPS 2021, the PCs are repeating the experiment, as well as conducting new ones."
            },
            {
                "arxivId": "2012.00714",
                "title": "Debiasing Evaluations That are Biased by Evaluations",
                "abstract": "It is common to evaluate a set of items by soliciting people to rate them. For example, universities ask students to rate the teaching quality of their instructors, and conference organizers ask authors of submissions to evaluate the quality of the reviews. However, in these applications, students often give a higher rating to a course if they receive higher grades in a course, and authors often give a higher rating to the reviews if their papers are accepted to the conference. In this work, we call these external factors the \"outcome\" experienced by people, and consider the problem of mitigating these outcome-induced biases in the given ratings when some information about the outcome is available. We formulate the information about the outcome as a known partial ordering on the bias. We propose a debiasing method by solving a regularized optimization problem under this ordering constraint, and also provide a carefully designed cross-validation method that adaptively chooses the appropriate amount of regularization. We provide theoretical guarantees on the performance of our algorithm, as well as experimental evaluations."
            },
            {
                "arxivId": "2011.15050",
                "title": "A Novice-Reviewer Experiment to Address Scarcity of Qualified Reviewers in Large Conferences",
                "abstract": "Conference peer review constitutes a human-computation process whose importance cannot be overstated: not only it identifies the best submissions for acceptance, but, ultimately, it impacts the future of the whole research area by promoting some ideas and restraining others. A surge in the number of submissions received by leading AI conferences has challenged the sustainability of the review process by increasing the burden on the pool of qualified reviewers which is growing at a much slower rate. In this work, we consider the problem of reviewer recruiting with a focus on the scarcity of qualified reviewers in large conferences. Specifically, we design a procedure for (i) recruiting reviewers from the population not typically covered by major conferences and (ii) guiding them through the reviewing pipeline. In conjunction with the ICML 2020 --- a large, top-tier machine learning conference --- we recruit a small set of reviewers through our procedure and compare their performance with the general population of ICML reviewers. Our experiment reveals that a combination of the recruiting and guiding mechanisms allows for a principled enhancement of the reviewer pool and results in reviews of superior quality compared to the conventional pool of reviews as evaluated by senior members of the program committee (meta-reviewers)."
            },
            {
                "arxivId": "1901.06246",
                "title": "Avoiding a Tragedy of the Commons in the Peer Review Process",
                "abstract": "Peer review is the foundation of scientific publication, and the task of reviewing has long been seen as a cornerstone of professional service. However, the massive growth in the field of machine learning has put this community benefit under stress, threatening both the sustainability of an effective review process and the overall progress of the field. In this position paper, we argue that a tragedy of the commons outcome may be avoided by emphasizing the professional aspects of this service. In particular, we propose a rubric to hold reviewers to an objective standard for review quality. In turn, we also propose that reviewers be given appropriate incentive. As one possible such incentive, we explore the idea of financial compensation on a per-review basis. We suggest reasonable funding models and thoughts on long term effects."
            },
            {
                "arxivId": "1507.06411",
                "title": "Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment",
                "abstract": "The principle of peer review is central to the evaluation of research, by ensuring that only high-quality items are funded or published. But peer review has also received criticism, as the selection of reviewers may introduce biases in the system. In 2014, the organizers of the ``Neural Information Processing Systems\\rq\\rq{} conference conducted an experiment in which $10\\%$ of submitted manuscripts (166 items) went through the review process twice. Arbitrariness was measured as the conditional probability for an accepted submission to get rejected if examined by the second committee. This number was equal to $60\\%$, for a total acceptance rate equal to $22.5\\%$. Here we present a Bayesian analysis of those two numbers, by introducing a hidden parameter which measures the probability that a submission meets basic quality criteria. The standard quality criteria usually include novelty, clarity, reproducibility, correctness and no form of misconduct, and are met by a large proportions of submitted items. The Bayesian estimate for the hidden parameter was equal to $56\\%$ ($95\\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result suggested the total acceptance rate should be increased in order to decrease arbitrariness estimates in future review processes."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-14.json",
        "arxivId": "2402.07960",
        "category": "econ",
        "title": "An Analysis of the Recovery Path of the Consumer Sector in the Post-Pandemic Era",
        "abstract": "This paper proposes a referencable pattern of the recovery of the consumption sector, a new dimension to observe and evaluate the intrinsic value of the consumption sector, and proposes the concept of sensory-based consumption and the ranking of the weights of different categories;creates the concept of digital consumption index, coupled with digital RMB index and China-style digital economy index. Finally we explain the internal logic of digital consumption as a consumption upgrade tool and a higher valuation target in the context of China's economic performance in 2022 and the Chinese government's policy in 2023, leading to the investment strategy of roller conduction effect.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-14.json",
        "arxivId": "2402.08549",
        "category": "econ",
        "title": "Competitive Revenue Extraction from Time-Discounted Transactions in the Semi-Myopic Regime",
        "abstract": "Decentralized cryptocurrencies are payment systems that rely on aligning the incentives of users and miners to operate correctly and offer a high quality of service to users. Recent literature studies the mechanism design problem of the auction serving as a cryptocurrency's transaction fee mechanism (TFM). We find that a non-myopic modelling of miners falls close to another well-known problem: that of online buffer management for packet switching. The main difference is that unlike packets which are of a fixed size throughout their lifetime, in a financial environment, user preferences (and therefore revenue extraction) may be time-dependent. We study the competitive ratio guarantees given a certain discount rate, and show how existing methods from packet scheduling, which we call\"the undiscounted case\", perform suboptimally in the more general discounted setting. Most notably, we find a novel, simple, memoryless, and optimal deterministic algorithm for the semi-myopic case, when the discount factor is up to ~0.770018. We also present a randomized algorithm that achieves better performance than the best possible deterministic algorithm, for any discount rate.",
        "references": [
            {
                "arxivId": "2206.08959",
                "title": "Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform",
                "abstract": "Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as \u00d0Apps. When engineering \u00d0Apps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective \u00d0Apps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., \u00d0App developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that Etherscan\u2019s Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStation\u2019s Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, \u00d0App developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions."
            },
            {
                "arxivId": "2201.05574",
                "title": "Empirical Analysis of EIP-1559: Transaction Fees, Waiting Times, and Consensus Security",
                "abstract": "A transaction fee mechanism (TFM) is an essential component of a blockchain protocol. However, a systematic evaluation of the real-world impact of TFMs is still absent. Using rich data from the Ethereum blockchain, the mempool, and exchanges, we study the effect of EIP-1559, one of the earliest-deployed TFMs that depart from the traditional first-price auction paradigm. We conduct a rigorous and comprehensive empirical study to examine its causal effect on blockchain transaction fee dynamics, transaction waiting times, and consensus security. Our results show that EIP-1559 improves the user experience by mitigating intrablock differences in the gas price paid and reducing users' waiting times. However, EIP-1559 has only a small effect on gas fee levels and consensus security. In addition, we find that when Ether's price is more volatile, the waiting time is significantly higher. We also verify that a larger block size increases the presence of siblings. These findings suggest new directions for improving TFMs."
            },
            {
                "arxivId": "2106.02149",
                "title": "Optimal Pricing Schemes for an Impatient Buyer",
                "abstract": "A patient seller aims to sell a good to an impatient buyer (i.e., one who discounts utility over time). The buyer will remain in the market for a period of time $T$, and her private value is drawn from a publicly known distribution. What is the revenue-optimal pricing-curve (sequence of (price, time) pairs) for the seller? Is randomization of help here? Is the revenue-optimal pricing curve computable in polynomial time? We answer these questions in this paper. We give an efficient algorithm for computing the revenue-optimal pricing curve. We show that pricing curves, that post a price at each point of time and let the buyer pick her utility maximizing time to buy, are revenue-optimal among a much broader class of sequential lottery mechanisms. I.e., mechanisms that allow the seller to post a menu of lotteries at each point of time cannot get any higher revenue than pricing curves. We also show that the even broader class of mechanisms that allow the menu of lotteries to be adaptively set, can earn strictly higher revenue than that of pricing curves, and the revenue gap can be as big as the support size of the buyer's value distribution."
            },
            {
                "arxivId": "2012.00854",
                "title": "Transaction Fee Mechanism Design for the Ethereum Blockchain: An Economic Analysis of EIP-1559",
                "abstract": "EIP-1559 is a proposal to make several tightly coupled additions to Ethereum's transaction fee mechanism, including variable-size blocks and a burned base fee that rises and falls with demand. This report assesses the game-theoretic strengths and weaknesses of the proposal and explores some alternative designs."
            },
            {
                "arxivId": "1811.02351",
                "title": "An Incentive Analysis of some Bitcoin Fee Designs",
                "abstract": "In the Bitcoin system, miners are incentivized to join the system and validate transactions through fees paid by the users. A simple \"pay your bid\" auction has been employed to determine the transaction fees. Recently, Lavi, Sattath and Zohar [LSZ17] proposed an alternative fee design, called the monopolistic price (MP) mechanism, aimed at improving the revenue for the miners. Although MP is not strictly incentive compatible (IC), they studied how close to IC the mechanism is for iid distributions, and conjectured that it is nearly IC asymptotically based on extensive simulations and some analysis. In this paper, we prove that the MP mechanism is nearly incentive compatible for any iid distribution as the number of users grows large. This holds true with respect to other attacks such as splitting bids. We also prove a conjecture in [LSZ17] that MP dominates the RSOP auction in revenue (originally defined in Goldberg et al. [GHKSW06] for digital goods). These results lend support to MP as a Bitcoin fee design candidate. Additionally, we explore some possible intrinsic correlations between incentive compatibility and revenue in general."
            },
            {
                "arxivId": "1805.05288",
                "title": "The Gap Game",
                "abstract": "Blockchain-based cryptocurrencies secure a decentralized consensus protocol by incentives. The protocol participants, called miners, generate (mine) a series of blocks, each containing monetary transactions created by system users. As incentive for participation, miners receive newly minted currency and transaction fees paid by transaction creators. Blockchain bandwidth limits lead users to pay increasing fees in order to prioritize their transactions. However, most prior work focused on models where fees are negligible. In a notable exception, Carlsten et al. [17] postulated that if incentives come only from fees then a mining gap would form~--- miners would avoid mining when the available fees are insufficient. In this work, we analyze cryptocurrency security in realistic settings, taking into account all elements of expenses and rewards. To study when gaps form, we analyze the system as a game we call the gap game. We analyze the game with a combination of symbolic and numeric analysis tools in a wide range of scenarios. Our analysis confirms Carlsten et al.'s postulate; indeed, we show that gaps form well before fees are the only incentive, and analyze the implications on security. Perhaps surprisingly, we show that different miners choose different gap sizes to optimize their utility, even when their operating costs are identical. Alarmingly, we see that the system incentivizes large miner coalitions, reducing system decentralization. We describe the required conditions to avoid the incentive misalignment, providing guidelines for future cryptocurrency design."
            },
            {
                "arxivId": "1709.08881",
                "title": "Redesigning Bitcoin\u2019s Fee Market",
                "abstract": "The Bitcoin payment system involves two agent types: users that transact with the currency and pay fees and miners in charge of authorizing transactions and securing the system in return for these fees. Two of Bitcoin\u2019s challenges are (i) securing sufficient miner revenues as block rewards decrease, and (ii) alleviating the throughput limitation due to a small maximal block size cap. These issues are strongly related as increasing the maximal block size may decrease revenue due to Bitcoin\u2019s pay-your-bid approach. To decouple them, we analyze the \u201cmonopolistic auction\u201d [16], showing (i) its revenue does not decrease as the maximal block size increases, (ii) it is resilient to an untrusted auctioneer (the miner), and (iii) simplicity for transaction issuers (bidders), as the average gain from strategic bid shading (relative to bidding one\u2019s value) diminishes as the number of bids increases."
            },
            {
                "arxivId": "1603.08177",
                "title": "Planning Problems for Sophisticated Agents with Present Bias",
                "abstract": "Present bias, the tendency to weigh costs and benefits incurred in the present too heavily, is one of the most widespread human behavioral biases. It has also been the subject of extensive study in the behavioral economics literature. While the simplest models assume that decision-making agents are naive, reasoning about the future without taking their bias into account, there is considerable evidence that people often behave in ways that are sophisticated with respect to present bias, making plans based on the belief that they will be present-biased in the future. For example, committing to a course of action to reduce future opportunities for procrastination or overconsumption are instances of sophisticated behavior in everyday life. Models of sophisticated behavior have lacked an underlying formalism that allows one to reason over the full space of multi-step tasks that a sophisticated agent might face, and this has made it correspondingly difficult to make comparative or worst-case statements about the performance of sophisticated agents in arbitrary scenarios. In this paper, we incorporate the framework of sophistication into a graph-theoretic model that we used in recent work for modeling naive agents. This new synthesis of two formalisms --- sophistication and graph-theoretic planning --- uncovers a rich structure that wasn't apparent in the earlier behavioral economics work on this problem, including a range of findings that shed new light on sophisticated behavior. In particular, our graph-theoretic model makes two kinds of new results possible. First, we give tight worst-case bounds on the performance of sophisticated agents in arbitrary multi-step tasks relative to the optimal plan, along with worst-case bounds for related questions. Second, the flexibility of our formalism makes it possible to identify new phenomena about sophisticated agents that had not been seen in prior literature: these include a surprising non-monotonic property in the use of rewards to motivate sophisticated agents; a sharp distinction in the performance of agents who overestimate versus underestimate their level of present bias; and a framework for reasoning about commitment devices that shows how certain classes of commitments can produce large gains for arbitrary tasks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-14.json",
        "arxivId": "2402.08575",
        "category": "econ",
        "title": "Heterogeneity, Uncertainty and Learning: Semiparametric Identification and Estimation",
        "abstract": "We provide semiparametric identification results for a broad class of learning models in which continuous outcomes depend on three types of unobservables: i) known heterogeneity, ii) initially unknown heterogeneity that may be revealed over time, and iii) transitory uncertainty. We consider a common environment where the researcher only has access to a short panel on choices and realized outcomes. We establish identification of the outcome equation parameters and the distribution of the three types of unobservables, under the standard assumption that unknown heterogeneity and uncertainty are normally distributed. We also show that, absent known heterogeneity, the model is identified without making any distributional assumption. We then derive the asymptotic properties of a sieve MLE estimator for the model parameters, and devise a tractable profile likelihood based estimation procedure. Monte Carlo simulation results indicate that our estimator exhibits good finite-sample properties.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2201.10673",
        "category": "econ",
        "title": "Robust comparative statics for the elasticity of intertemporal substitution",
        "abstract": "We study a general class of consumption\u2013savings problems with recursive preferences. We characterize the sign of the consumption response to arbitrary shocks in terms of the product of two sufficient statistics: the elasticity of intertemporal substitution (EIS) between contemporaneous consumption and continuation utility, and the relative elasticity of the marginal value of wealth (REMV). Under homotheticity, the REMV always equals 1, so the propensity of the agent to save or \u201cdis\u2010save\u201d is always signed by the relationship of the EIS with unity. We apply our results to derive comparative statics in classical problems of portfolio allocation, consumption\u2013savings with income risk, and entrepreneurial investment. Our results suggest empirical identification strategies for both the value of the EIS and its relationship with unity.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2204.05480",
        "category": "econ",
        "title": "Tuning parameter-free nonparametric density estimation from tabulated summary data",
        "abstract": null,
        "references": [
            {
                "arxivId": "2206.04257",
                "title": "Capital and Labor Income Pareto Exponents in the United States, 1916-2019",
                "abstract": "Accurately estimating income Pareto exponents is challenging due to limitations in data availability and the applicability of statistical methods. Using tabulated summaries of incomes from tax authorities and a recent estimation method, we estimate income Pareto exponents in U.S. for 1916-2019. We find that during the past three decades, the capital and labor income Pareto exponents have been stable at around 1.2 and 2. Our findings suggest that the top tail income and wealth inequality is higher and wealthy agents have twice as large an impact on the aggregate economy than previously thought but there is no clear trend post-1985."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2209.01146",
        "category": "econ",
        "title": "Generalized Principal-Agency: Contracts, Information, Games and Beyond",
        "abstract": "In the principal-agent problem formulated by Myerson'82, agents have private information (type) and make private decisions (action), both of which are unobservable to the principal. Myerson pointed out an elegant linear programming solution that relies on the revelation principle. This paper extends Myerson's results to a more general setting where the principal's action space can be infinite and subject to additional design constraints. Our generalized principal-agent model unifies several important design problems including contract design, information design, and Bayesian Stackelberg games, and encompasses them as special cases. We first extend the revelation principle to this general model, based on which a polynomial-time algorithm is then derived for computing the optimal mechanism for the principal. This algorithm not only implies new efficient solutions simultaneously for all the aforementioned special cases but also significantly simplifies previously known algorithms designed for special cases. Inspired by the recent interest in the algorithmic design of a single contract and menu of contracts, we study such constrained design problems to our general principal-agent model. In contrast to the above unification, our results here illustrate the other facet of diversity among different principal-agent design problems and demonstrate how their different structures can lead to different complexities: some are tractable whereas others are APX-hard. Finally, we reveal an interesting connection of our model to the problem of information acquisition for decision making and study its algorithmic properties in general.",
        "references": [
            {
                "arxivId": "2302.02873",
                "title": "Online Mechanism Design for Information Acquisition",
                "abstract": "We study the problem of designing mechanisms for \\emph{information acquisition} scenarios. This setting models strategic interactions between an uniformed \\emph{receiver} and a set of informed \\emph{senders}. In our model the senders receive information about the underlying state of nature and communicate their observation (either truthfully or not) to the receiver, which, based on this information, selects an action. Our goal is to design mechanisms maximizing the receiver's utility while incentivizing the senders to report truthfully their information. First, we provide an algorithm that efficiently computes an optimal \\emph{incentive compatible} (IC) mechanism. Then, we focus on the \\emph{online} problem in which the receiver sequentially interacts in an unknown game, with the objective of minimizing the \\emph{cumulative regret} w.r.t. the optimal IC mechanism, and the \\emph{cumulative violation} of the incentive compatibility constraints. We investigate two different online scenarios, \\emph{i.e.,} the \\emph{full} and \\emph{bandit feedback} settings. For the full feedback problem, we propose an algorithm that guarantees $\\tilde{\\mathcal O}(\\sqrt T)$ regret and violation, while for the bandit feedback setting we present an algorithm that attains $\\tilde{\\mathcal O}(T^{\\alpha})$ regret and $\\tilde{\\mathcal O}(T^{1-\\alpha/2})$ violation for any $\\alpha\\in[1/2, 1]$. Finally, we complement our results providing a tight lower bound."
            },
            {
                "arxivId": "2111.09179",
                "title": "Contracts with Private Cost per Unit-of-Effort",
                "abstract": "Economic theory distinguishes between principal-agent settings in which the agent has a private type and settings in which the agent takes a hidden action. Many practical problems, however, involve aspects of both. For example, brand X may seek to hire an influencer Y to create sponsored content to be posted on social media platform Z. This problem has a hidden action component (the brand may not be able or willing to observe the amount of effort exerted by the influencer), but also a private type component (influencers may have different costs per unit-of-effort). This \"effort\" and \"cost per unit-of-effort\" perspective naturally leads to a principal-agent problem with hidden action and single-dimensional private type, which generalizes both the classic principal-agent hidden action model of contract theory a la Grossmann and Hart [1986] and the (procurement version) of single-dimensional mechanism design a la Myerson [1983]. A natural goal in this model is to design an incentive-compatible contract, which consist of an allocation rule that maps types to actions, and a payment rule that maps types to payments for the stochastic outcomes of the chosen action. Our main contribution is an LP-duality based characterization of implementable allocation rules for this model, which applies to both discrete and continuous types. This characterization shares important features of Myerson's celebrated characterization result, but also departs from it in significant ways. We present several applications, including a polynomial-time algorithm for finding the optimal contract with a constant number of actions. This in sharp contrast to recent work on hidden action problems with multi-dimensional private information, which has shown that the problem of computing an optimal contract for constant numbers of actions is APX-hard."
            },
            {
                "arxivId": "2010.06742",
                "title": "Contracts under Moral Hazard and Adverse Selection",
                "abstract": "In the classical principal-agent problem, a principal must design a contract to incentivize an agent to perform an action on behalf of the principal. We study the classical principal-agent problem in a setting where the agent can be of one of several types (affecting the outcome of actions they might take). This combines the contract theory phenomena of \"moral hazard\" (incomplete information about actions) with that of \"adverse selection\" (incomplete information about types). We examine this problem through the computational lens. We show that in this setting it is APX-hard to compute either the profit-maximizing single contract or the profit-maximizing menu of contracts (as opposed to in the absence of types, where one can efficiently compute the optimal contract). We then show that the performance of the best linear contract scales especially well in the number of types: if agent has n available actions and T possible types, the best linear contract achieves an O(n log T) approximation of the best possible profit. Finally, we apply our framework to prove tight worst-case approximation bounds between a variety of benchmarks of mechanisms for the principal."
            },
            {
                "arxivId": "1911.04146",
                "title": "Optimal Common Contract with Heterogeneous Agents",
                "abstract": "We consider the principal-agent problem with heterogeneous agents. Previous works assume that the principal signs independent incentive contracts with every agent to make them invest more efforts on the tasks. However, in many circumstances, these contracts need to be identical for the sake of fairness. We investigate the optimal common contract problem. To our knowledge, this is the first attempt to consider this natural and important generalization. We first show this problem is NP-complete. Then we provide a dynamic programming algorithm to compute the optimal contract in $O(n^2m)$ time, where $n,m$ are the number of agents and actions, under the assumption that the agents' cost functions obey increasing difference property. At last, we generalize the setting such that each agent can choose to directly produce a reward in $[0,1]$. We provide an $O(\\log n)$-approximate algorithm for this generalization."
            },
            {
                "arxivId": "1907.04397",
                "title": "Selling Information Through Consulting",
                "abstract": "We consider a monopoly information holder selling information to a budget-constrained decision maker, who may benefit from the seller's information. The decision maker has a utility function that depends on his action and an uncertain state of the world. The seller and the buyer each observe a private signal regarding the state of the world, which may be correlated with each other. The seller's goal is to sell her private information to the buyer and extract maximum possible revenue, subject to the buyer's budget constraints. We consider three different settings with increasing generality, i.e., the seller and buyer signals can be independent, correlated, or follow a general distribution accessed through a black-box sampling oracle. For each setting, we design information selling mechanisms which are both optimal and simple in the sense that they can be naturally interpreted, have succinct representations, and can be efficiently computed. Notably, though the optimal mechanism exhibits slightly increasing complexity as the setting becomes more general, all our mechanisms share the same format of acting as a consultant who recommends the best action to the buyer but uses different and carefully designed payment rules for different settings. Each of our optimal mechanisms can be easily computed by solving a single polynomial-size linear program. This significantly simplifies exponential-size LPs solved by the Ellipsoid method in the previous work, which computes the optimal mechanisms in the same setting but without budget limit. Such simplification is enabled by our new characterizations of the optimal mechanism in the (more realistic) budget-constrained setting."
            },
            {
                "arxivId": "1811.12655",
                "title": "Prior-free Data Acquisition for Accurate Statistical Estimation",
                "abstract": "We study a data analyst's problem of acquiring data from self-interested individuals to obtain an accurate estimation of some statistic of a population, subject to an expected budget constraint. Each data holder incurs a cost, which is unknown to the data analyst, to acquire and report his data. The cost can be arbitrarily correlated with the data. The data analyst has an expected budget that she can use to incentivize individuals to provide their data. The goal is to design a joint acquisition-estimation mechanism to optimize the performance of the produced estimator, without any prior information on the underlying distribution of cost and data. We investigate two types of estimations: unbiased point estimation and confidence interval estimation. Unbiased estimators: We design a truthful, individually rational, online mechanism to acquire data from individuals and output an unbiased estimator of the population mean when the data analyst has no prior information on the cost-data distribution and individuals arrive in a random order. The performance of this mechanism matches that of the optimal mechanism, which knows the true cost distribution, within a constant factor. The performance of an estimator is evaluated by its variance under the worst-case cost-data correlation. Confidence intervals: We characterize an approximately optimal (within a factor 2) mechanism for obtaining a confidence interval of the population mean when the data analyst knows the true cost distribution at the beginning. This mechanism is efficiently computable. We then design a truthful, individually rational, online algorithm that is only worse than the approximately optimal mechanism by a constant factor. The performance of an estimator is evaluated by its expected length under the worst-case cost-data correlation."
            },
            {
                "arxivId": "1808.03713",
                "title": "Simple versus Optimal Contracts",
                "abstract": "We consider the classic principal-agent model of contract theory, in which a principal designs an outcome-dependent compensation scheme to incentivize an agent to take a costly and unobservable action. When all of the model parameters---including the full distribution over principal rewards resulting from each agent action---are known to the designer, an optimal contract can in principle be computed by linear programming. In addition to their demanding informational requirements, however, such optimal contracts are often complex and unintuitive, and do not resemble contracts used in practice. This paper examines contract theory through the theoretical computer science lens, with the goal of developing novel theory to explain and justify the prevalence of relatively simple contracts, such as linear (pure commission) contracts. First, we consider the case where the principal knows only the first moment of each action's reward distribution, and we prove that linear contracts are guaranteed to be worst-case optimal, ranging over all reward distributions consistent with the given moments. Second, we study linear contracts from a worst-case approximation perspective, and prove several tight parameterized approximation bounds."
            },
            {
                "arxivId": "1711.01295",
                "title": "Optimal Data Acquisition for Statistical Estimation",
                "abstract": "We consider a data analyst's problem of purchasing data from strategic agents to compute an unbiased estimate of a statistic of interest. Agents incur private costs to reveal their data and the costs can be arbitrarily correlated with their data. Once revealed, data are verifiable. This paper focuses on linear unbiased estimators. We design an individually rational and incentive compatible mechanism that optimizes the worst-case mean-squared error of the estimation, where the worst-case is over the unknown correlation between costs and data, subject to a budget constraint in expectation. We characterize the form of the optimal mechanism in closed-form. We further extend our results to acquiring data for estimating a parameter in regression analysis, where private costs can correlate with the values of the dependent variable but not with the values of the independent variables."
            },
            {
                "arxivId": "1204.5519",
                "title": "Optimal mechanisms for selling information",
                "abstract": "The buying and selling of information is taking place at a scale unprecedented in the history of commerce, thanks to the formation of online marketplaces for user data. Data providing agencies sell user information to advertisers to allow them to match ads to viewers more effectively. In this paper we study the design of optimal mechanisms for a monopolistic data provider to sell information to a buyer, in a model where both parties have (possibly correlated) private signals about a state of the world, and the buyer uses information learned from the seller, along with his own signal, to choose an action (e.g., displaying an ad) whose payoff depends on the state of the world.\n We provide sufficient conditions under which there is a simple one-round protocol (i.e. a protocol where the buyer and seller each sends a single message, and there is a single money transfer) achieving optimal revenue. In these cases we present a polynomial-time algorithm that computes the optimal mechanism. Intriguingly, we show that multiple rounds of partial information disclosure (interleaved by payment to the seller) are sometimes necessary to achieve optimal revenue if the buyer is allowed to abort his interaction with the seller prematurely. We also prove some negative results about the inability of simple mechanisms for selling information to approximate more complicated ones in the worst case."
            },
            {
                "arxivId": "2106.01946",
                "title": "Convex optimization",
                "abstract": "This textbook is based on lectures given by the authors at MIPT (Moscow), HSE (Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all, the authors focused on the program of a two-semester course of lectures on convex optimization, which is given to students of MIPT. The first chapter of this book contains the materials of the first semester (\"Fundamentals of convex analysis and optimization\"), the second and third chapters contain the materials of the second semester (\"Numerical methods of convex optimization\"). The textbook has a number of features. First, in contrast to the classic manuals, this book does not provide proofs of all the theorems mentioned. This allowed, on one side, to describe more themes, but on the other side, made the presentation less self-sufficient. The second important point is that part of the material is advanced and is published in the Russian educational literature, apparently for the first time. Third, the accents that are given do not always coincide with the generally accepted accents in the textbooks that are now popular. First of all, we talk about a sufficiently advanced presentation of conic optimization, including robust optimization, as a vivid demonstration of the capabilities of modern convex analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2211.13100",
        "category": "econ",
        "title": "Leverage, Endogenous Unbalanced Growth, and Asset Price Bubbles",
        "abstract": "We present a general equilibrium macro-finance model with a positive feedback loop between capital investment and land price. As leverage is relaxed beyond a critical value, through the financial accelerator, a phase transition occurs from balanced growth where land prices reflect fundamentals (present value of rents) to unbalanced growth where land prices grow faster than rents, generating land price bubbles. Unbalanced growth dynamics and bubbles are associated with financial loosening and technological progress. In an analytically tractable two-sector large open economy model with unique equilibria, financial loosening simultaneously leads to low interest rates, asset overvaluation, and top-end wealth concentration.",
        "references": [
            {
                "arxivId": "2311.03638",
                "title": "Bubble economics",
                "abstract": null
            },
            {
                "arxivId": "2307.00349",
                "title": "Unbalanced Growth, Elasticity of Substitution, and Land Overvaluation",
                "abstract": "We study the long-run behavior of land prices when land plays the dual role of factor of production and store of value. In modern economies where technological progress is faster in non-land sectors, when the elasticity of substitution in production exceeds 1 at high input levels (which always holds if non-land factors do not fully depreciate), unbalanced growth occurs and land becomes overvalued on the long-run trend relative to the fundamental value defined by the present value of land rents. Around the trend, land prices exhibit recurrent stochastic fluctuations, with expansions and contractions in the size of land overvaluation."
            },
            {
                "arxivId": "2305.08268",
                "title": "Bubble Necessity Theorem",
                "abstract": "Asset price bubbles are situations where asset prices exceed the fundamental values defined by the present value of dividends. This paper presents a conceptually new perspective: the necessity of bubbles. We establish the Bubble Necessity Theorem in a plausible general class of economic models: with faster long-run economic growth ($G$) than dividend growth ($G_d$) and counterfactual long-run autarky interest rate ($R$) below dividend growth, all equilibria are bubbly with non-negligible bubble sizes relative to the economy. This bubble necessity condition naturally arises in economies with sufficiently strong savings motives and multiple factors or sectors with uneven productivity growth."
            },
            {
                "arxivId": "2201.10673",
                "title": "Robust comparative statics for the elasticity of intertemporal substitution",
                "abstract": "We study a general class of consumption\u2013savings problems with recursive preferences. We characterize the sign of the consumption response to arbitrary shocks in terms of the product of two sufficient statistics: the elasticity of intertemporal substitution (EIS) between contemporaneous consumption and continuation utility, and the relative elasticity of the marginal value of wealth (REMV). Under homotheticity, the REMV always equals 1, so the propensity of the agent to save or \u201cdis\u2010save\u201d is always signed by the relationship of the EIS with unity. We apply our results to derive comparative statics in classical problems of portfolio allocation, consumption\u2013savings with income risk, and entrepreneurial investment. Our results suggest empirical identification strategies for both the value of the EIS and its relationship with unity."
            },
            {
                "arxivId": "1712.01431",
                "title": "Determination of Pareto Exponents in Economic Models Driven by Markov Multiplicative Processes",
                "abstract": "This article contains new tools for studying the shape of the stationary distribution of sizes in a dynamic economic system in which units experience random multiplicative shocks and are occasionally reset. Each unit has a Markov\u2010switching type, which influences their growth rate and reset probability. We show that the size distribution has a Pareto upper tail, with exponent equal to the unique positive solution to an equation involving the spectral radius of a certain matrix\u2010valued function. Under a nonlattice condition on growth rates, an eigenvector associated with the Pareto exponent provides the distribution of types in the upper tail of the size distribution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2212.00157",
        "category": "econ",
        "title": "Robust Contracts with Exploration",
        "abstract": "We study a two-period moral hazard problem; there are two agents, with action sets that are unknown to the principal. The principal contracts with each agent sequentially, and seeks to maximize the worst-case discounted sum of payoffs, where the worst case is over the possible action sets. The principal observes the action chosen by the first agent, and then offers a new contract to the second agent based on this knowledge, thus having the opportunity to explore in the first period. We introduce and compare three different notions of dynamic worst-case considerations. Within each notion, we define a suitable rule of updating and characterize the principal's optimal payoff guarantee. We find that linear contracts are robustly optimal not only in static settings, but also in dynamic environments with exploration.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2302.13937",
        "category": "econ",
        "title": "Human and Machine Intelligence in n-Person Games with Partial Knowledge: Theory and Computation",
        "abstract": "In this paper, I formalize intelligence measurement in games by introducing mechanisms that assign a real number -- interpreted as an intelligence score -- to each player in a game. This score quantifies the ex-post strategic ability of the players based on empirically observable information, such as the actions of the players, the game's outcome, strength of the players, and a reference oracle machine such as a chess-playing artificial intelligence system. Specifically, I introduce two main concepts: first, the Game Intelligence (GI) mechanism, which quantifies a player's intelligence in a game by considering not only the game's outcome but also the\"mistakes\"made during the game according to the reference machine's intelligence. Second, I define gamingproofness, a practical and computational concept of strategyproofness. To illustrate the GI mechanism, I apply it to an extensive dataset comprising over a billion chess moves, including over a million moves made by top 20 grandmasters in history. Notably, Magnus Carlsen emerges with the highest GI score among all world championship games included in the dataset. In machine-vs-machine games, the well-known chess engine Stockfish comes out on top.",
        "references": [
            {
                "arxivId": "2006.01855",
                "title": "Aligning Superhuman AI with Human Behavior: Chess as a Model System",
                "abstract": "As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of AlphaZero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making."
            },
            {
                "arxivId": "1808.06922",
                "title": "Catch-Up: A Rule That Makes Service Sports More Competitive",
                "abstract": "Abstract Service sports include two-player contests such as volleyball, badminton, and squash. We analyze four rules, including the Standard Rule (SR), in which a player continues to serve until he or she loses. The Catch-Up Rule (CR) gives the serve to the player who has lost the previous point\u2014as opposed to the player who won the previous point, as under SR. We also consider two Trailing Rules that make the server the player who trails in total score. Surprisingly, compared with SR, only CR gives the players the same probability of winning a game while increasing its expected length, thereby making it more competitive and exciting to watch. Unlike one of the Trailing Rules, CR is strategy-proof. By contrast, the rules of tennis fix who serves and when; its tiebreaker, however, keeps play competitive by being fair\u2014not favoring either the player who serves first or who serves second."
            },
            {
                "arxivId": "1109.1314",
                "title": "Measuring Intelligence through Games",
                "abstract": "Abstract Arti\ufb01cial general intelligence (AGI) refers to research aimed at tackling the full problemof arti\ufb01cial intelligence, that is, create truly intelligent agents. This sets it apart from mostAI research which aims at solving relatively narrow domains, such as character recognition,motion planning, or increasing player satisfaction in games. But how do we know when anagent is truly intelligent? A common point of reference in the AGI community is Legg andHutter\u2019s formal de\ufb01nition of universal intelligence, which has the appeal of simplicity andgenerality but is unfortunately incomputable.Games of various kinds are commonly used as benchmarks for \u201cnarrow\u201d AI research,as they are considered to have many important properties. We argue that many of theseproperties carry over to the testing of general intelligence as well. We then sketch how suchtesting could practically be carried out. The central part of this sketch is an extension ofuniversal intelligence to deal with \ufb01nite time, and the use of sampling of the space of gamesexpressed in a suitably biased game description language.Keywords: measure of intelligence, games"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2307.10983",
        "category": "econ",
        "title": "Commitment and the Dynamics of Household Labor Supply",
        "abstract": "The extent to which individuals commit to their partner for life has important implications. This paper develops a lifecycle collective model of the household, through which it characterizes behavior in three prominent alternative types of commitment: full, limited, and no commitment. We propose a test that distinguishes between all three types based on how contemporaneous and historical news affect household behavior. Our test permits heterogeneity in the degree of commitment across households. Using recent data from the Panel Study of Income Dynamics, we reject full and no commitment, while we find strong evidence for limited commitment.",
        "references": [
            {
                "arxivId": "2102.07476",
                "title": "Personality Traits and the Marriage Market",
                "abstract": "Which and how many attributes are relevant for the sorting of agents in a matching market? This paper addresses these questions by constructing indices of mutual attractiveness that aggregate information about agents\u2019 attributes. The first k indices for agents on each side of the market provide the best approximation of the matching surplus by a k-dimensional model. The methodology is applied on a unique Dutch household survey containing information about education, height, body mass index, health, attitude toward risk, and personality traits of spouses."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2310.17517",
        "category": "econ",
        "title": "Safety, In Numbers",
        "abstract": "We introduce a way to compare actions in decision problems. An action is safer than another if the set of beliefs at which the decision-maker prefers the safer action increases in size (in the set inclusion sense) as the decision-maker becomes more risk averse. We provide a full characterization of this relation and show that it is equivalent to a robust concept of single-crossing. We discuss applications to investment hedging, security design, and game theory.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2312.15326",
        "category": "econ",
        "title": "On Connected Strongly-Proportional Cake-Cutting",
        "abstract": "We investigate the problem of fairly dividing a divisible heterogeneous resource, also known as a cake, among a set of agents. We characterize the existence of an allocation in which every agent receives a contiguous piece worth strictly more than their proportional share, also known as a *strongly-proportional allocation*. The characterization is supplemented with an algorithm that determines the existence of a connected strongly-proportional allocation using at most $n \\cdot 2^{n-1}$ queries. We provide a simpler characterization for agents with strictly positive valuations, and show that the number of queries required to determine the existence of a connected strongly-proportional allocation is in $\\Theta(n^2)$. Our proofs are constructive and yield a connected strongly-proportional allocation, when it exists, using a similar number of queries.",
        "references": [
            {
                "arxivId": "2005.01982",
                "title": "Envy-free cake cutting: A polynomial number of queries with high probability",
                "abstract": "In this article we propose a probabilistic framework in order to study the fair division of a divisible good, e.g. a cake, between n players. Our framework follows the same idea than the \"Full independence model\" used in the study of fair division of indivisible goods. We show that, in this framework, there exists an envy-free division algorithm satisfying the following probability estimate:\r\n$$\\mathbb{P}\\big( C(\\mu_1, \\ldots,\\mu_n) \\geq n^{7+b}\\big) = \\mathcal{O}\\Big(n^{-\\frac{b-1}{3}+1+o(1)}\\Big),$$\r\nwhere $\\mu_1,\\ldots, \\mu_n$ correspond to the preferences of the $n$ players,\r\n$C(\\mu_1, \\ldots,\\mu_n)$ is the number of queries used by the algorithm and $b>4$.\r\nIn particular, this gives\r\n$$\\lim_{n \\rightarrow + \\infty}\\mathbb{P}\\big( C(\\mu_1, \\ldots,\\mu_n) \\geq n^{12}\\big) = 0.$$\r\nIt must be noticed that nowadays few things are known about the complexity of envy-free division algorithms. Indeed, Procaccia has given a lower bound in $\\Omega(n^2)$ and Aziz and Mackenzie have given an upper bound in $n^{n^{n^{n^{n^{n}}}}}$. As our estimate means that we have $C(\\mu_1, \\ldots, \\mu_n)<n^{12}$ with a high probability, this gives a new insight on the complexity of envy-free cake cutting algorithms.\\\\\r\nOur result follows from a study of Webb's algorithm and a theorem of Tao and Vu about the smallest singular value of a random matrix."
            },
            {
                "arxivId": "1910.14129",
                "title": "Dividing a Graphical Cake",
                "abstract": "We consider the classical cake-cutting problem where we wish to fairly divide a heterogeneous resource, often modeled as a cake, among interested agents. Work on the subject typically assumes that the cake is represented by an interval. In this paper, we introduce a generalized setting where the cake can be in the form of the set of edges of an undirected graph, allowing us to model the division of road networks. Unlike in the canonical setting, common fairness criteria such as proportionality cannot always be satisfied in our setting if each agent must receive a connected subgraph. We determine the optimal approximation of proportionality that can be obtained for any number of agents with arbitrary valuations, and exhibit a tight guarantee for each graph in the case of two agents. In addition, when more than one connected piece per agent is allowed, we establish the best egalitarian welfare guarantee for each total number of connected pieces. We also study a number of variants and extensions, including when approximate equitability is considered, or when the item to be divided is undesirable (also known as chore division)."
            },
            {
                "arxivId": "1909.07141",
                "title": "Disproportionate division",
                "abstract": "We study the disproportionate version of the classical cake\u2010cutting problem: how efficiently can we divide a cake, here [0,1], among n\u2a7e2 agents with different demands \u03b11,\u03b12,\u22ef,\u03b1n summing to 1? When all the agents have equal demands of \u03b11=\u03b12=\u22ef=\u03b1n=1/n , it is well known that there exists a fair division with n\u22121 cuts, and this is optimal. For arbitrary demands on the other hand, folklore arguments from algebraic topology show that O(nlogn) cuts suffice, and this has been the state of the art for decades. Here, we improve the state of affairs in two ways: we prove that disproportionate division may always be achieved with 3n\u22124 cuts, and also give an effective algorithm to construct such a division. We additionally offer a topological conjecture that implies that 2n\u22122 cuts suffice in general, which would be optimal."
            },
            {
                "arxivId": "1510.03170",
                "title": "Fair and Square: Cake-Cutting in Two Dimensions",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-15.json",
        "arxivId": "2312.16307",
        "category": "econ",
        "title": "Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration",
        "abstract": "We consider the setting of synthetic control methods (SCMs), a canonical approach used to estimate the treatment effect on the treated in a panel data setting. We shed light on a frequently overlooked but ubiquitous assumption made in SCMs of\"overlap\": a treated unit can be written as some combination -- typically, convex or linear combination -- of the units that remain under control. We show that if units select their own interventions, and there is sufficiently large heterogeneity between units that prefer different interventions, overlap will not hold. We address this issue by proposing a framework which incentivizes units with different preferences to take interventions they would not normally consider. Specifically, leveraging tools from information design and online learning, we propose a SCM that incentivizes exploration in panel data settings by providing incentive-compatible intervention recommendations to units. We establish this estimator obtains valid counterfactual estimates without the need for an a priori overlap assumption. We extend our results to the setting of synthetic interventions, where the goal is to produce counterfactual outcomes under all interventions, not just control. Finally, we provide two hypothesis tests for determining whether unit overlap holds for a given panel dataset.",
        "references": [
            {
                "arxivId": "2202.08426",
                "title": "Synthetic Control As Online Linear Regression",
                "abstract": "This paper notes a simple connection between synthetic control and online learning. Specifically, we recognize synthetic control as an instance of \n Follow\u2010The\u2010Leader (FTL). Standard results in online convex optimization then imply that, even when outcomes are chosen by an adversary, synthetic control predictions of counterfactual outcomes for the treated unit perform almost as well as an oracle weighted average of control units' outcomes. Synthetic control on differenced data performs almost as well as oracle weighted difference\u2010in\u2010differences, potentially making it an attractive choice in practice. We argue that this observation further supports the use of synthetic control estimators in comparative case studies.\n"
            },
            {
                "arxivId": "1904.07272",
                "title": "Introduction to Multi-Armed Bandits",
                "abstract": "Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments. The chapters are as follows: stochastic bandits, lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full Feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual Bandits; bandits and games; bandits with knapsacks; bandits and incentives."
            },
            {
                "arxivId": "1711.06940",
                "title": "Robust Synthetic Control",
                "abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. To begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. We provide the first finite sample analysis for a broader class of models, the Latent Variable Model, in contrast to Factor Models previously considered in the literature. Further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same setting, we prove that the mean-squared-error (MSE) in our prediction estimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the noise variance. Using a data aggregation method, we show that the MSE can be made as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to a consistent estimator. We also introduce a Bayesian framework to quantify the model uncertainty through posterior probabilities. Our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-16.json",
        "arxivId": "1601.00934",
        "category": "econ",
        "title": "Con fidence Intervals for Projections of Partially Identi fied Parameters",
        "abstract": "We propose a bootstrap\u2010based \n calibrated projection procedure to build confidence intervals for single components and for smooth functions of a partially identified parameter vector in moment (in)equality models. The method controls asymptotic coverage uniformly over a large class of data generating processes. The extreme points of the calibrated projection confidence interval are obtained by extremizing the value of the function of interest subject to a proper relaxation of studentized sample analogs of the moment (in)equality conditions. The degree of relaxation, or critical level, is calibrated so that the function of \n \u03b8, not \n \u03b8 itself, is uniformly asymptotically covered with prespecified probability. This calibration is based on repeatedly checking feasibility of linear programming problems, rendering it computationally attractive.\n \n Nonetheless, the program defining an extreme point of the confidence interval is generally nonlinear and potentially intricate. We provide an algorithm, based on the response surface method for global optimization, that approximates the solution rapidly and accurately, and we establish its rate of convergence. The algorithm is of independent interest for optimization problems with simple objectives and complicated constraints. An empirical application estimating an entry game illustrates the usefulness of the method. Monte Carlo simulations confirm the accuracy of the solution algorithm, the good statistical as well as computational performance of calibrated projection (including in comparison to other methods), and the algorithm's potential to greatly accelerate computation of other confidence intervals.",
        "references": [
            {
                "arxivId": "1908.09103",
                "title": "CONSTRAINT QUALIFICATIONS IN PARTIAL IDENTIFICATION",
                "abstract": "The literature on stochastic programming typically restricts attention to problems that fulfill constraint qualifications. The literature on estimation and inference under partial identification frequently restricts the geometry of identified sets with diverse high-level assumptions. These superficially appear to be different approaches to closely related problems. We extensively analyze their relation. Among other things, we show that for partial identification through pure moment inequalities, numerous assumptions from the literature essentially coincide with the Mangasarian\u2013Fromowitz constraint qualification. This clarifies the relation between well-known contributions, including within econometrics, and elucidates stringency, as well as ease of verification, of some high-level assumptions in seminal papers."
            },
            {
                "arxivId": "1605.00499",
                "title": "Monte Carlo Confidence Sets for Identified Sets",
                "abstract": "In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-16.json",
        "arxivId": "2301.00277",
        "category": "econ",
        "title": "Higher-order Refinements of Small Bandwidth Asymptotics for Density-Weighted Average Derivative Estimators",
        "abstract": "The density weighted average derivative (DWAD) of a regression function is a canonical parameter of interest in economics. Classical first-order large sample distribution theory for kernel-based DWAD estimators relies on tuning parameter restrictions and model assumptions that imply an asymptotic linear representation of the point estimator. These conditions can be restrictive, and the resulting distributional approximation may not be representative of the actual sampling distribution of the statistic of interest. In particular, the approximation is not robust to bandwidth choice. Small bandwidth asymptotics offers an alternative, more general distributional approximation for kernel-based DWAD estimators that allows for, but does not require, asymptotic linearity. The resulting inference procedures based on small bandwidth asymptotics were found to exhibit superior finite sample performance in simulations, but no formal theory justifying that empirical success is available in the literature. Employing Edgeworth expansions, this paper shows that small bandwidth asymptotic approximations lead to inference procedures with higher-order distributional properties that are demonstrably superior to those of procedures based on asymptotic linear approximations.",
        "references": [
            {
                "arxivId": "1508.02973",
                "title": "On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric Inference",
                "abstract": "ABSTRACT Nonparametric methods play a central role in modern empirical work. While they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. We study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness to tuning parameter choice. This is achieved using a novel, yet simple, Studentization, which leads to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth selectors. For interior points, we show that the mean-squared error (MSE)-optimal bandwidth for the original point estimator (before bias correction) delivers the fastest coverage error decay rate after bias correction when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is too \u201clarge.\u201d Finally, for odd-degree local polynomial regression, we show that, as with point estimation, coverage error adapts to boundary points automatically when appropriate Studentization is used; however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results are established using valid Edgeworth expansions and illustrated with simulated data. Our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard errors, have smaller coverage error and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "math/9309211",
                "title": "Decoupling Inequalities for the Tail Probabilities of Multivariate $U$-Statistics",
                "abstract": "In this paper we present a decoupling inequality that shows that multivariate $U$-statistics can be studied as sums of (conditionally) independent random variables. This result has important implications in several areas of probability and statistics including the study of random graphs and multiple stochastic integration. More precisely, we get the following result: Let $\\{X_j\\}$ be a sequence of independent random variables on a measurable space $(\\mathscr{J}, S)$ and let $\\{X^{(j)}_i\\}, j = 1,\\ldots,k$, be $k$ independent copies of $\\{X_i\\}$. Let $f_{i_1i_2\\ldots i_k}$ be families of functions of $k$ variables taking $(S \\times \\cdots \\times S)$ into a Banach space $(B, \\|\\cdots\\|)$. Then, for all $n \\geq k \\geq 2, t > 0$, there exist numerical constants $C_k$ depending on $k$ only so that $P\\bigg(\\big\\|\\sum_{1\\leq i_1\\neq i_2\\neq\\cdots\\neq i_k\\leq n} f_{i_1\\cdots i_k}(X^{(1)}_{i_1}, X^{(1)}_{i_2}, \\ldots, X^{(1)}_{i_k})\\big\\|\\geq t\\bigg)$ $\\leq C_kP\\bigg(C_k\\big\\|\\sum_{1\\leq i_1\\neq i_2\\neq\\cdots\\neq i_k\\leq n} f_{i_1\\cdots i_k}(X^{(1)}_{i_1}, X^{(2)}_{i_2}, \\ldots, X^{(k)}_{i_k})\\big\\|\\geq t\\bigg).$ The reverse bound holds if, in addition, the following symmetry condition holds almost surely: $f_{i_1i_2\\cdots i_k}(X_{i_1}, X_{i_2},\\ldots,X_{i_k}) = f_{i_{\\pi(1)}i_{\\pi(2)}\\cdots i_{\\pi(k)}} (X_{i_{\\pi(1)}, X_{i_{\\pi(2)}}, \\ldots,X_{i_{\\pi(k)}}),$ for all permutations $\\pi$ of $(1,\\ldots,k)$."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-16.json",
        "arxivId": "2401.07337",
        "category": "econ",
        "title": "Individual and Collective Welfare in Risk Sharing with Many States",
        "abstract": "We provide a quantitative assessment of welfare in the classical model of risk-sharing and exchange under uncertainty. We prove three kinds of results. First, that in an equilibrium allocation, the scope for improving individual welfare by a given margin (an $\\varepsilon$-improvement) vanishes as the number of states increases. Second, that the scope for a change in aggregate resources that may be distributed to enhance individual welfare by a given margin also vanishes. Equivalently: in an inefficient allocation, for a given level of resource sub-optimality (as measured by the coefficient of resource under-utilization), the possibilities for enhancing welfare by perturbing aggregate resources decrease exponentially to zero with the number of states. Finally, we consider efficient risk-sharing in standard models of uncertainty aversion with multiple priors, and show that, in an inefficient allocation, certain sets of priors shrink with the size of the state space.",
        "references": [
            {
                "arxivId": "1905.05165",
                "title": "The Edgeworth Conjecture with Small Coalitions and Approximate Equilibria in Large Economies",
                "abstract": "We revisit the connection between bargaining and equilibrium in exchange economies, and study its algorithmic implications. We consider bargaining outcomes to be allocations that cannot be blocked (i.e., profitably re-traded) by coalitions of small size and show that these allocations must be approximate Walrasian equilibria. Our results imply that deciding whether an allocation is approximately Walrasian can be done in polynomial time, even in economies for which finding an equilibrium is known to be computationally hard."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-16.json",
        "arxivId": "2402.09744",
        "category": "econ",
        "title": "Quantile Granger Causality in the Presence of Instability",
        "abstract": "We propose a new framework for assessing Granger causality in quantiles in unstable environments, for a fixed quantile or over a continuum of quantile levels. Our proposed test statistics are consistent against fixed alternatives, they have nontrivial power against local alternatives, and they are pivotal in certain important special cases. In addition, we show the validity of a bootstrap procedure when asymptotic distributions depend on nuisance parameters. Monte Carlo simulations reveal that the proposed test statistics have correct empirical size and high power, even in absence of structural breaks. Finally, two empirical applications in energy economics and macroeconomics highlight the applicability of our method as the new tests provide stronger evidence of Granger causality.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-16.json",
        "arxivId": "2402.09789",
        "category": "econ",
        "title": "Identification with Posterior-Separable Information Costs",
        "abstract": "I provide a model of rational inattention with heterogeneity and prove it is observationally equivalent to a state-dependent stochastic choice model subject to attention costs. I demonstrate that additive separability of unobservable heterogeneity, together with an independence assumption, suffice for the empirical model to admit a representative agent. Using conditional probabilities, I show how to identify: how covariates affect the desirability of goods, (a measure of) welfare, factual changes in welfare, and bounds on counterfactual market shares.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-19.json",
        "arxivId": "2303.06603",
        "category": "econ",
        "title": "Correlation between upstreamness and downstreamness in random global value chains",
        "abstract": "This paper is concerned with upstreamness and downstreamness of industries and countries in global value chains. Upstreamness and downstreamness measure respectively the average distance of an industrial sector from final consumption and from primary inputs, and they are computed from based on the most used global Input-Output tables databases, e.g., the World Input-Output Database (WIOD). Recently, Antr\\`as and Chor reported a puzzling and counter-intuitive finding in data from the period 1995-2011, namely that (at country level) upstreamness appears to be positively correlated with downstreamness, with a correlation slope close to $+1$. This effect is stable over time and across countries, and it has been confirmed and validated by later analyses. We first analyze a simple model of random Input/Output tables, and we show that, under minimal and realistic structural assumptions, there is a natural positive correlation emerging between upstreamness and downstreamness of the same industrial sector/country, with correlation slope equal to $+1$. This effect is robust against changes in the randomness of the entries of the I/O table and different aggregation protocols. Secondly, we perform experiments by randomly reshuffling the entries of the empirical I/O table where these puzzling correlations are detected, in such a way that the global structural constraints are preserved. Again, we find that the upstreamness and downstreamness of the same industrial sector/country are positively correlated with slope close to $+1$, even though the random reshuffling has destroyed any underlying economic information about inter-sectorial connections and trends. Our results strongly suggest that the empirically observed puzzling correlation may rather be a necessary consequence of the few structural constraints that Input/Output tables must meet.",
        "references": [
            {
                "arxivId": "2208.09430",
                "title": "Statistics of the largest eigenvalues and singular values of low-rank random matrices with non-negative entries",
                "abstract": "We compute analytically the distribution and moments of the largest eigenvalues/singular values and resolvent statistics for random matrices with (i) non-negative entries, (ii) small rank, and (iii) prescribed sums of rows/columns. Applications are discussed in the context of Mean First Passage Time of random walkers on networks, and the calculation of network\"influence\"metrics. The analytical results are corroborated by numerical simulations."
            },
            {
                "arxivId": "2106.02730",
                "title": "\"Spectrally gapped\" random walks on networks: a Mean First Passage Time formula",
                "abstract": "We derive an approximate but explicit formula for the Mean First\nPassage Time of a random walker between a source and a target node of a\ndirected and weighted network. The formula does not require any matrix\ninversion, and it takes as only input the transition probabilities into\nthe target node. It is derived from the calculation of the average\nresolvent of a deformed ensemble of random sub-stochastic matrices\nH=\\langle H\\rangle +\\delta HH=\u27e8H\u27e9+\u03b4H,\nwith \\langle H\\rangle\u27e8H\u27e9\nrank-11\nand non-negative. The accuracy of the formula depends on the spectral\ngap of the reduced transition matrix, and it is tested numerically on\nseveral instances of (weighted) networks away from the high sparsity\nregime, with an excellent agreement."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-19.json",
        "arxivId": "2401.04200",
        "category": "econ",
        "title": "Teacher bias or measurement error?",
        "abstract": "In many countries, teachers' track recommendations are used to allocate students to secondary school tracks. Previous studies have shown that students from families with low socioeconomic status (SES) receive lower track recommendations than their peers from high SES families, conditional on standardized test scores. It is often argued that this indicates teacher bias. However, this claim is invalid in the presence of measurement error in test scores. We discuss how measurement error in test scores generates a biased coefficient of the conditional SES gap, and consider three empirical strategies to address this bias. Using administrative data from the Netherlands, we find that measurement error explains 35 to 43% of the conditional SES gap in track recommendations.",
        "references": [
            {
                "arxivId": "2304.10636",
                "title": "The quality of school track assignment decisions by teachers",
                "abstract": "We study the quality of secondary school track assignment decisions in the Netherlands, using a regression discontinuity design. In 6th grade, primary school teachers assign each student to a secondary school track. If a student scores above a track-specific cutoff on the standardized end-of-primary education test, the teacher can upwardly revise this assignment. By comparing students just left and right of these cutoffs, we find that between 50-90% of the students are\"trapped in track\": these students are on the high track after four years, only if they started on the high track in first year. The remaining (minority of) students are\"always low\": they are always on the low track after four years, independently of where they started. These proportions hold for students near the cutoffs that shift from the low to the high track in first year by scoring above the cutoff. Hence, for a majority of these students the initial (unrevised) track assignment decision is too low. The results replicate across most of the secondary school tracks, from the vocational to the academic tracks, and stand out against an education system with a lot of upward and downward track mobility."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-19.json",
        "arxivId": "2402.10836",
        "category": "econ",
        "title": "Manipulation Test for Multidimensional RDD",
        "abstract": "The causal inference model proposed by Lee (2008) for the regression discontinuity design (RDD) relies on assumptions that imply the continuity of the density of the assignment (running) variable. The test for this implication is commonly referred to as the manipulation test and is regularly reported in applied research to strengthen the design's validity. The multidimensional RDD (MRDD) extends the RDD to contexts where treatment assignment depends on several running variables. This paper introduces a manipulation test for the MRDD. First, it develops a theoretical model for causal inference with the MRDD, used to derive a testable implication on the conditional marginal densities of the running variables. Then, it constructs the test for the implication based on a quadratic form of a vector of statistics separately computed for each marginal density. Finally, the proposed test is compared with alternative procedures commonly employed in applied research.",
        "references": [
            {
                "arxivId": "2301.08958",
                "title": "A Practical Introduction to Regression Discontinuity Designs: Extensions",
                "abstract": "This monograph, together with its accompanying first part Cattaneo, Idrobo and Titiunik (2020), collects and expands the instructional materials we prepared for more than $50$ short courses and workshops on Regression Discontinuity (RD) methodology that we taught between 2014 and 2023. In this second monograph, we discuss several topics in RD methodology that build on and extend the analysis of RD designs introduced in Cattaneo, Idrobo and Titiunik (2020). Our first goal is to present an alternative RD conceptual framework based on local randomization ideas. This methodological approach can be useful in RD designs with discretely-valued scores, and can also be used more broadly as a complement to the continuity-based approach in other settings. Then, employing both continuity-based and local randomization approaches, we extend the canonical Sharp RD design in multiple directions: fuzzy RD designs, RD designs with discrete scores, and multi-dimensional RD designs. The goal of our two-part monograph is purposely practical and hence we focus on the empirical analysis of RD designs."
            },
            {
                "arxivId": "1808.01398",
                "title": "Coverage error optimal confidence intervals for local polynomial regression",
                "abstract": "In this paper we develop new confidence intervals for local polynomial regression, which minimize their worse-case coverage error and length in large samples. Our results rely on novel, valid Edgeworth expansions for $t$-statistics based on local polynomial methods, which are established uniformly over relevant classes of data generating processes and interval estimators. These higher-order expansions also allow for the uniform kernel and any derivative order, significantly improving on previous technical results available in the literature. In addition, we discuss principled, inference-optimal tuning parameter (bandwidth) selection and kernel functions. The main methodological results obtained in this paper are implemented in companion {\\sf R} and \\texttt{Stata} software packages."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2005.03625",
        "category": "econ",
        "title": "Know Your Clients' Behaviours: A Cluster Analysis of Financial Transactions",
        "abstract": "In Canada, financial advisors and dealers are required by provincial securities commissions and self-regulatory organizations\u2014charged with direct regulation over investment dealers and mutual fund dealers\u2014to respectively collect and maintain know your client (KYC) information, such as their age or risk tolerance, for investor accounts. With this information, investors, under their advisor\u2019s guidance, make decisions on their investments that are presumed to be beneficial to their investment goals. Our unique dataset is provided by a financial investment dealer with over 50,000 accounts for over 23,000 clients covering the period from January 1st to August 12th 2019. We use a modified behavioral finance recency, frequency, monetary model for engineering features that quantify investor behaviours, and unsupervised machine learning clustering algorithms to find groups of investors that behave similarly. We show that the KYC information\u2014such as gender, residence region, and marital status\u2014does not explain client behaviours, whereas eight variables for trade and transaction frequency and volume are most informative. Hence, our results should encourage financial regulators and advisors to use more advanced metrics to better understand and predict investor behaviours.",
        "references": [
            {
                "arxivId": "1609.08520",
                "title": "Clustering Approaches for Financial Data Analysis: a Survey",
                "abstract": "Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, confidence of expected return, etc. Banking and financial institutes have applied different data mining techniques to enhance their business performance. Among these techniques, clustering has been considered as a significant method to capture the natural structure of data. However, there are not many studies on clustering approaches for financial data analysis. In this paper, we evaluate different clustering algorithms for analysing different financial datasets varied from time series to transactions. We also discuss the advantages and disadvantages of each method to enhance the understanding of inner structure of financial datasets as well as the capability of each clustering method in this context."
            },
            {
                "arxivId": "1201.0490",
                "title": "Scikit-learn: Machine Learning in Python",
                "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2110.13761",
        "category": "econ",
        "title": "Regime-Switching Density Forecasts Using Economists' Scenarios",
        "abstract": "We propose an approach for generating macroeconomic density forecasts that incorporate information on multiple scenarios defined by experts. We adopt a regime-switching framework in which sets of scenarios (\"views\") are used as Bayesian priors on economic regimes. Predictive densities coming from different views are then combined by optimizing objective functions of density forecasting. We illustrate the approach with an empirical application to quarterly real-time forecasts of U.S. GDP growth, in which we exploit the Fed's macroeconomic scenarios used for bank stress tests. We show that the approach achieves good accuracy in terms of average predictive scores and good calibration of forecast distributions. Moreover, it can be used to evaluate the contribution of economists' scenarios to density forecast performance.",
        "references": [
            {
                "arxivId": "2205.04216",
                "title": "Forecast combinations: An over 50-year review",
                "abstract": null
            },
            {
                "arxivId": "1106.1638",
                "title": "Combining Predictive Distributions",
                "abstract": "Predictive distributions need to be aggregated when probabilistic forecasts are merged, or when expert opinions expressed in terms of probability distributions are fused. We take a prediction space approach that applies to discrete, mixed discrete-continuous and continuous predictive distributions alike, and study combination formulas for cumulative distribution functions from the perspectives of coherence, probabilistic and conditional calibration, and dispersion. Both linear and non-linear aggregation methods are investigated, including generalized, spread-adjusted and beta-transformed linear pools. The effects and techniques are demonstrated theoretically, in simulation examples, and in case studies on density forecasts for S&P 500 returns and daily maximum temperature at Seattle-Tacoma Airport."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2202.00625",
        "category": "econ",
        "title": "Black-box Bayesian inference for economic agent-based models",
        "abstract": null,
        "references": [
            {
                "arxivId": "2101.12156",
                "title": "Sequential Monte Carlo algorithms for agent-based models of disease transmission",
                "abstract": "Agent-based models of disease transmission involve stochastic rules that specify how a number of individuals would infect one another, recover or be removed from the population. Common yet stringent assumptions stipulate interchangeability of agents and that all pairwise contact are equally likely. Under these assumptions, the population can be summarized by counting the number of susceptible and infected individuals, which greatly facilitates statistical inference. We consider the task of inference without such simplifying assumptions, in which case, the population cannot be summarized by low-dimensional counts. We design improved particle filters, where each particle corresponds to a specific configuration of the population of agents, that take either the next or all future observations into account when proposing population configurations. Using simulated data sets, we illustrate that orders of magnitude improvements are possible over bootstrap particle filters. We also provide theoretical support for the approximations employed to make the algorithms practical."
            },
            {
                "arxivId": "2101.04653",
                "title": "Benchmarking Simulation-Based Inference",
                "abstract": "Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such 'likelihood-free' algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms."
            },
            {
                "arxivId": "2011.08644",
                "title": "Generalized Posteriors in Approximate Bayesian Computation",
                "abstract": "Complex simulators have become a ubiquitous tool in many scientific disciplines, providing high-fidelity, implicit probabilistic models of natural and social phenomena. Unfortunately, they typically lack the tractability required for conventional statistical analysis. Approximate Bayesian computation (ABC) has emerged as a key method in simulation-based inference, wherein the true model likelihood and posterior are approximated using samples from the simulator. In this paper, we draw connections between ABC and generalized Bayesian inference (GBI). First, we re-interpret the accept/reject step in ABC as an implicitly defined error model. We then argue that these implicit error models will invariably be misspecified. While ABC posteriors are often treated as a necessary evil for approximating the standard Bayesian posterior, this allows us to re-interpret ABC as a potential robustification strategy. This leads us to suggest the use of GBI within ABC, a use case we explore empirically."
            },
            {
                "arxivId": "2010.10079",
                "title": "Neural Approximate Sufficient Statistics for Implicit Models",
                "abstract": "We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of likelihood function is intractable but sampling / simulating data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representation of the data. This representation is computed by a deep neural network trained by a joint statistic-posterior learning strategy. We apply our approach to both traditional approximate Bayesian computation (ABC) and recent neural likelihood approaches, boosting their performance on a range of tasks."
            },
            {
                "arxivId": "2003.00580",
                "title": "Technological Interdependencies Predict Innovation Dynamics",
                "abstract": "We propose a simple model where the innovation rate of a technological domain depends on the innovation rate of the technological domains it relies on. Using data on US patents from 1836 to 2017, we make out-of-sample predictions and find that the predictability of innovation rates can be boosted substantially when network effects are taken into account. In the case where a technology$'$s neighborhood future innovation rates are known, the average predictability gain is 28$\\%$ compared to simpler time series model which do not incorporate network effects. Even when nothing is known about the future, we find positive average predictability gains of 20$\\%$. The results have important policy implications, suggesting that the effective support of a given technology must take into account the technological ecosystem surrounding the targeted technology."
            },
            {
                "arxivId": "1912.01703",
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            {
                "arxivId": "1909.09397",
                "title": "Simulating Crowds in Real Time with Agent-Based Modelling and a Particle Filter",
                "abstract": "Agent-based modelling is a valuable approach for systems whose behaviour is driven by the interactions between distinct entities. They have shown particular promise as a means of modelling crowds of people in streets, public transport terminals, stadiums, etc. However, the methodology faces a fundamental difficulty: there are no established mechanisms for dynamically incorporating real-time data into models. This limits simulations that are inherently dynamic, such as pedestrian movements, to scenario testing of, for example, the potential impacts of new architectural configurations on movements. This paper begins to address this fundamental gap by demonstrating how a particle filter could be used to incorporate real data into an agent-based model of pedestrian movements at run time. The experiments show that it is indeed possible to use a particle filter to perform online (real time) model optimisation. However, as the number of agents increases, the number of individual particles (and hence the computational complexity) required increases exponentially. By laying the groundwork for the real-time simulation of crowd movements, this paper has implications for the management of complex environments (both nationally and internationally) such as transportation hubs, hospitals, shopping centres, etc."
            },
            {
                "arxivId": "1906.04522",
                "title": "Bayesian Estimation of Economic Simulation Models Using Neural Networks",
                "abstract": null
            },
            {
                "arxivId": "1905.07488",
                "title": "Automatic Posterior Transformation for Likelihood-Free Inference",
                "abstract": "How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference."
            },
            {
                "arxivId": "1903.00007",
                "title": "Fast likelihood-free cosmology with neural density estimators and active learning",
                "abstract": "\n Likelihood-free inference provides a framework for performing rigorous Bayesian inference using only forward simulations, properly accounting for all physical and observational effects that can be successfully included in the simulations. The key challenge for likelihood-free applications in cosmology, where simulation is typically expensive, is developing methods that can achieve high-fidelity posterior inference with as few simulations as possible. Density-estimation likelihood-free inference (DELFI) methods turn inference into a density estimation task on a set of simulated data-parameter pairs, and give orders of magnitude improvements over traditional Approximate Bayesian Computation approaches to likelihood-free inference. In this paper we use neural density estimators (NDEs) to learn the likelihood function from a set of simulated datasets, with active learning to adaptively acquire simulations in the most relevant regions of parameter space on-the-fly. We demonstrate the approach on a number of cosmological case studies, showing that for typical problems high-fidelity posterior inference can be achieved with just $\\mathcal {O}(10^3)$ simulations or fewer. In addition to enabling efficient simulation-based inference, for simple problems where the form of the likelihood is known, DELFI offers a fast alternative to MCMC sampling, giving orders of magnitude speed-up in some cases. Finally, we introduce pydelfi \u2013 a flexible public implementation of DELFI with NDEs and active learning \u2013 available at https://github.com/justinalsing/pydelfi."
            },
            {
                "arxivId": "1905.03747",
                "title": "Approximate Bayesian computation with the Wasserstein distance",
                "abstract": "A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well\u2010known approach of using order statistics within approximate Bayesian computation to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and we propose a new distance based on the Hilbert space filling curve. We provide a theoretical study of the method proposed, describing consistency as the threshold goes to 0 while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g\u2010and\u2010k distributions, a toggle switch model from systems biology, a queuing model and a L\u00e9vy\u2010driven stochastic volatility model."
            },
            {
                "arxivId": "1902.05938",
                "title": "A Comparison of Economic Agent-Based Model Calibration Methods",
                "abstract": "Interest in agent-based models of financial markets and the wider economy has increased consistently over the last few decades, in no small part due to their ability to reproduce a number of empirically-observed stylised facts that are not easily recovered by more traditional modelling approaches. Nevertheless, the agent-based modelling paradigm faces mounting criticism, focused particularly on the rigour of current validation and calibration practices, most of which remain qualitative and stylised fact-driven. While the literature on quantitative and data-driven approaches has seen significant expansion in recent years, most studies have focused on the introduction of new calibration methods that are neither benchmarked against existing alternatives nor rigorously tested in terms of the quality of the estimates they produce. We therefore compare a number of prominent ABM calibration methods, both established and novel, through a series of computational experiments in an attempt to determine the respective strengths and weaknesses of each approach and the overall quality of the resultant parameter estimates. We find that Bayesian estimation, though less popular in the literature, consistently outperforms frequentist, objective function-based approaches and results in reasonable parameter estimates in many contexts. Despite this, we also find that agent-based model calibration techniques require further development in order to definitively calibrate large-scale models. We therefore make suggestions for future research."
            },
            {
                "arxivId": "1901.10230",
                "title": "Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation",
                "abstract": "We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation. The DeepSets architecture is a special case of PEN and we can therefore also target fully exchangeable data. We employ PENs to learn summary statistics in approximate Bayesian computation (ABC). When comparing PENs to previous deep learning methods for learning summary statistics, our results are highly competitive, both considering time series and static models. Indeed, PENs provide more reliable posterior samples even when using less training data. (Less)"
            },
            {
                "arxivId": "1812.08434",
                "title": "Graph Neural Networks: A Review of Methods and Applications",
                "abstract": null
            },
            {
                "arxivId": "1809.05679",
                "title": "Graph Convolutional Networks for Text Classification",
                "abstract": "Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification."
            },
            {
                "arxivId": "1807.03039",
                "title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
                "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at this https URL"
            },
            {
                "arxivId": "1805.07226",
                "title": "Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows",
                "abstract": "We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit."
            },
            {
                "arxivId": "1805.00013",
                "title": "Constraining Effective Field Theories with Machine Learning.",
                "abstract": "We present powerful new analysis techniques to constrain effective field theories at the LHC. By leveraging the structure of particle physics processes, we extract extra information from Monte\u00a0Carlo simulations, which can be used to train neural network models that estimate the likelihood ratio. These methods scale well to processes with many observables and theory parameters, do not require any approximations of the parton shower or detector response, and can be evaluated in microseconds. We show that they allow us to put significantly stronger bounds on dimension-six operators than existing methods, demonstrating their potential to improve the precision of the LHC legacy constraints."
            },
            {
                "arxivId": "1804.06788",
                "title": "Validating Bayesian Inference Algorithms with Simulation-Based Calibration",
                "abstract": "Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce \\emph{simulation-based calibration} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software."
            },
            {
                "arxivId": "1711.01861",
                "title": "Flexible statistical inference for mechanistic models of neural dynamics",
                "abstract": "Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling."
            },
            {
                "arxivId": "1705.07057",
                "title": "Masked Autoregressive Flow for Density Estimation",
                "abstract": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks."
            },
            {
                "arxivId": "1703.06114",
                "title": "Deep Sets",
                "abstract": "In this paper, we study the problem of designing objective functions for machine learning problems defined on finite \\emph{sets}. In contrast to traditional objective functions defined for machine learning problems operating on finite dimensional vectors, the new objective functions we propose are operating on finite sets and are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \\citep{poczos13aistats}, via anomaly detection in piezometer data of embankment dams \\citep{Jung15Exploration}, to cosmology \\citep{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and image tagging."
            },
            {
                "arxivId": "1605.08695",
                "title": "TensorFlow: A system for large-scale machine learning",
                "abstract": "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications."
            },
            {
                "arxivId": "1506.06101",
                "title": "Robust Bayesian Inference via Coarsening",
                "abstract": "ABSTRACT The standard approach to Bayesian inference is based on the assumption that the distribution of the data belongs to the chosen model class. However, even a small violation of this assumption can have a large impact on the outcome of a Bayesian procedure. We introduce a novel approach to Bayesian inference that improves robustness to small departures from the model: rather than conditioning on the event that the observed data are generated by the model, one conditions on the event that the model generates data close to the observed data, in a distributional sense. When closeness is defined in terms of relative entropy, the resulting \u201ccoarsened\u201d posterior can be approximated by simply tempering the likelihood\u2014that is, by raising the likelihood to a fractional power\u2014thus, inference can usually be implemented via standard algorithms, and one can even obtain analytical solutions when using conjugate priors. Some theoretical properties are derived, and we illustrate the approach with real and simulated data using mixture models and autoregressive models of unknown order. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1202.3819",
                "title": "A comparative review of dimension reduction methods in approximate Bayesian computation",
                "abstract": "Approximate Bayesian computation (ABC) methods make use of comparisons between simulated and observed summary statistics to overcome the problem of computationally intractable likelihood functions. As the practical implementation of ABC requires computations based on vectors of summary statistics, rather than full data sets, a central question is how to derive low-dimensional summary statistics from the observed data with minimal loss of information. In this article we provide a comprehensive review and comparison of the performance of the principal methods of dimension reduction proposed in the ABC literature. The methods are split into three nonmutually exclusive classes consisting of best subset selection methods, projection techniques and regularization. In addition, we introduce two new methods of dimension reduction. The first is a best subset selection method based on Akaike and Bayesian information criteria, and the second uses ridge regression as a regularization procedure. We illustrate the performance of these dimension reduction techniques through the analysis of three challenging models and data sets."
            },
            {
                "arxivId": "0903.5480",
                "title": "The pseudo-marginal approach for efficient Monte Carlo computations",
                "abstract": "We introduce a powerful and flexible MCMC algorithm for stochastic simulation. The method builds on a pseudo-marginal method originally introduced in [Genetics 164 (2003) 1139--1160], showing how algorithms which are approximations to an idealized marginal algorithm, can share the same marginal stationary distribution as the idealized method. Theoretical results are given describing the convergence properties of the proposed method, and simple numerical examples are given to illustrate the promising empirical characteristics of the technique. Interesting comparisons with a more obvious, but inexact, Monte Carlo approximation to the marginal algorithm, are also given."
            },
            {
                "arxivId": "0901.1925",
                "title": "Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems",
                "abstract": "Approximate Bayesian computation (ABC) methods can be used to evaluate posterior distributions without having to calculate likelihoods. In this paper, we discuss and apply an ABC method based on sequential Monte Carlo (SMC) to estimate parameters of dynamical models. We show that ABC SMC provides information about the inferability of parameters and model sensitivity to changes in parameters, and tends to perform better than other ABC approaches. The algorithm is applied to several well-known biological systems, for which parameters and their credible intervals are inferred. Moreover, we develop ABC SMC as a tool for model selection; given a range of different mathematical descriptions, ABC SMC is able to choose the best model using the standard Bayesian model selection apparatus."
            },
            {
                "arxivId": "0805.2256",
                "title": "Adaptive approximate Bayesian computation",
                "abstract": "Sequential techniques can enhance the efficiency of the approximate Bayesian computation algorithm, as in Sisson et al.'s (2007) partial rejection control version. While this method is based upon the theoretical works of Del Moral et al. (2006), the application to approximate Bayesian computation results in a bias in the approximation to the posterior. An alternative version based on genuine importance sampling arguments bypasses this difficulty, in connection with the population Monte Carlo method of Cappe et al. (2004), and it includes an automatic scaling of the forward kernel. When applied to a population genetics example, it compares favourably with two other versions of the approximate algorithm. Copyright 2009, Oxford University Press."
            },
            {
                "arxivId": "0805.2368",
                "title": "A Kernel Method for the Two-Sample-Problem",
                "abstract": "We propose two statistical tests to determine if two samples are from different distributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The first test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be computed in O(m2) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2203.08014",
        "category": "econ",
        "title": "Non-Existent Moments of Earnings Growth",
        "abstract": "The literature often employs moment-based earnings risk measures like variance, skewness, and kurtosis. However, under heavy-tailed distributions, these moments may not exist in the population. Our empirical analysis reveals that population kurtosis, skewness, and variance often do not exist for the conditional distribution of earnings growth. This challenges moment-based analyses. We propose robust conditional Pareto exponents as novel earnings risk measures, developing estimation and inference methods. Using the UK New Earnings Survey Panel Dataset (NESPD) and US Panel Study of Income Dynamics (PSID), we find: 1) Moments often fail to exist; 2) Earnings risk increases over the life cycle; 3) Job stayers face higher earnings risk; 4) These patterns persist during the 2007--2008 recession and the 2015--2016 positive growth period.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2206.09574",
        "category": "econ",
        "title": "The Winner-Take-All Dilemma",
        "abstract": "We consider collective decision\u2010making when society consists of groups endowed with voting weights. Each group chooses an internal rule that specifies the allocation of its weight to alternatives as a function of its members' preferences. Under fairly general conditions, we show that the winner\u2010take\u2010all rule is a dominant strategy, while the equilibrium is Pareto dominated, highlighting the dilemma structure between optimality for each group and for the whole society. We also develop a technique for asymptotic analysis and show Pareto dominance of the proportional rule.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2207.03565",
        "category": "econ",
        "title": "With a little help from my friends: essentiality vs opportunity in group criticality",
        "abstract": "We define a notion of the criticality of a player for simple monotone games based on cooperation with other players, either to form a winning coalition or to break a winning one, with an essential role for all the players involved. We compare it with the notion of differential criticality given by Beisbart that measures power as the opportunity left by other players. We prove that our proposal satisfies an extension of the strong monotonicity introduced by Young, assigns no power to null players and does not reward free riders, and can easily be computed from the minimal winning and blocking coalitions. An application to the Italian elections is presented. Our analysis shows that the measures of group criticality defined so far cannot weigh essential players while only remaining an opportunity measure. We propose a group opportunity test to reconcile the two views.",
        "references": [
            {
                "arxivId": "2207.07302",
                "title": "Lexicographic Ranking based on Minimal Winning Coalitions",
                "abstract": "In this paper, we consider the consistency of the desirability relation with the ranking of the players in a simple game provided by some well-known solutions, in particular the Public Good Index [14] and the criticality-based ranking [1]. We de\ufb01ne a new ranking solution, the Lexicographic Ranking based on Minimal winning coalitions (LRM), strongly related to the Public Good Index being rooted in the minimal winning coalitions of the simple game, proving that it is monotonic with respect to the desirability relation [17], when it holds. A suitable characterization of the LRM solution is provided. Finally, we investigate the relation among the LRM solution and the criticality-based ranking, referring to the dual game."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2301.11237",
        "category": "econ",
        "title": "The Hazards and Benefits of Condescension in Social Learning",
        "abstract": "In a misspecified social learning setting, agents are condescending if they perceive their peers as having private information that is of lower quality than it is in reality. Applying this to a standard sequential model, we show that outcomes improve when agents are mildly condescending. In contrast, too much condescension leads to worse outcomes, as does anti-condescension.",
        "references": [
            {
                "arxivId": "2012.15007",
                "title": "Evolutionarily Stable (Mis)specifications: Theory and Applications",
                "abstract": "We introduce an evolutionary framework to evaluate competing (mis)specifications in strategic situations, focusing on which misspecifications can persist over a correct specification. Agents with heterogeneous specifications coexist in a society and repeatedly match against random opponents to play a stage game. They draw Bayesian inferences about the environment based on personal experience, so their learning depends on the distribution of specifications and matching assortativity in the society. One specification is evolutionarily stable against another if, whenever sufficiently prevalent, its adherents obtain higher expected objective payoffs than their counterparts. The learning channel leads to novel stability phenomena compared to frameworks where the heritable unit of cultural transmission is a single belief instead of a specification (i.e., set of feasible beliefs). We apply the framework to linear-quadratic-normal games where players receive correlated signals but possibly misperceive the information structure. The correct specification is not evolutionarily stable against a correlational error, whose direction depends on matching assortativity. As another application, the framework also endogenizes coarse analogy classes in centipede games. The full paper can be found at https://kevinhe.net/papers/theory_evolution.pdf"
            },
            {
                "arxivId": "2011.09640",
                "title": "Belief Error and Non-Bayesian Social Learning: An Experimental Evidence",
                "abstract": "This paper experimentally studies whether individuals hold a first-order belief that others apply Bayes\u2019 rule to incorporate private information into their beliefs, which is a fundamental assumption in many Bayesian and non-Bayesian social learning models. We design a novel experimental setting in which the first-order belief assumption implies that social information is equivalent to private information. Our main finding is that participants\u2019 reported reservation prices of social information are significantly lower than those of private information, which provides evidence that casts doubt on the first-order belief assumption. We also build a novel belief error model in which participants form a random posterior belief with a Bayesian posterior belief kernel to explain the experimental findings. The structural estimation of the model suggests that participants\u2019 sophisticated consideration of others\u2019 belief error and their exaggeration of the error both contribute to the difference in reservation prices."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2302.13066",
        "category": "econ",
        "title": "Estimating Fiscal Multipliers by Combining Statistical Identification with Potentially Endogenous Proxies",
        "abstract": "Different proxy variables used in fiscal policy SVARs lead to contradicting conclusions regarding the size of fiscal multipliers. We show that the conflicting results are due to violations of the exogeneity assumptions, i.e. the commonly used proxies are endogenously related to the structural shocks. We propose a novel approach to include proxy variables into a Bayesian non-Gaussian SVAR, tailored to accommodate for potentially endogenous proxy variables. Using our model, we show that increasing government spending is a more effective tool to stimulate the economy than reducing taxes.",
        "references": [
            {
                "arxivId": "1306.4911",
                "title": "Independent Component Analysis via Distance Covariance",
                "abstract": "ABSTRACT This article introduces a novel statistical framework for independent component analysis (ICA) of multivariate data. We propose methodology for estimating mutually independent components, and a versatile resampling-based procedure for inference, including misspecification testing. Independent components are estimated by combining a nonparametric probability integral transformation with a generalized nonparametric whitening method based on distance covariance that simultaneously minimizes all forms of dependence among the components. We prove the consistency of our estimator under minimal regularity conditions and detail conditions for consistency under model misspecification, all while placing assumptions on the observations directly, not on the latent components. U statistics of certain Euclidean distances between sample elements are combined to construct a test statistic for mutually independent components. The proposed measures and tests are based on both necessary and sufficient conditions for mutual independence. We demonstrate the improvements of the proposed method over several competing methods in simulation studies, and we apply the proposed ICA approach to two real examples and contrast it with principal component analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2305.00231",
        "category": "econ",
        "title": "Historical trend in educational homophily: U-shaped or not U-shaped? Or, how to set a criterion to choose a criterion?",
        "abstract": "Measuring changes in overall inequality between different educational groups is often performed by quantifying variations in educational marital homophily across consecutive generations. However, this task becomes challenging when the education level of marriageable individuals is generation-specific. To address this challenge, various indicators have been proposed in the assortative mating literature. In this paper, we review a set of criteria that indicators must satisfy to be considered as suitable measures of homophily and inequality. Our analytical criteria include robustness to the number of educational categories distinguished and the negative association between intergenerational mobility and homophily. Additionally, we also impose an empirical criterion on the identified qualitative historical trend in homophily between 1960 and 2010 in the US at the national and sub-national levels. Our analysis reveals that while a specific cardinal indicator meets all three criteria, many commonly applied indices do not. We propose the application of this well-performing indicator to quantify the trend in overall inequality in any country, including European countries, with available population data on couples' education level.",
        "references": [
            {
                "arxivId": "2303.05895",
                "title": "What do surveys say about the historical trend of inequality and the applicability of two table-transformation methods?",
                "abstract": "We apply a pseudo panel analysis of survey data from the years 2010 and 2017 about Americans' self-reported marital preferences and perform some formal tests on the sign and magnitude of the change in educational homophily from the generation of the early Boomers to the late Boomers, as well as from the early GenerationX to the late GenerationX. In the analysis, we control for changes in preferences over the course of the survey respondents' lives. We use the test results to decide whether the popular iterative proportional fitting (IPF) algorithm, or its alternative, the NM-method is more suitable for analyzing revealed marital preferences. These two methods construct different tables representing counterfactual joint educational distributions of couples. Thereby, they disagree on the trend of revealed preferences identified from the prevalence of homogamy by counterfactual decompositions. By finding self-reported homophily to display a U-shaped pattern, our tests reject the hypothesis that the IPF is suitable for constructing counterfactuals in general, while we cannot reject the applicability of the NM. The significance of our survey-based method-selection is due to the fact that the choice between the IPF and the NM makes a difference not only to the identified historical trend of revealed homophily, but also to what future paths of social inequality are believed to be possible."
            },
            {
                "arxivId": "2303.05515",
                "title": "The iterative proportional fitting algorithm and the NM-method: solutions for two different sets of problems",
                "abstract": "In this paper, we identify two different sets of problems. The first covers the problems that the iterative proportional fitting (IPF) algorithm was developed to solve. These concern completing a population table by using a sample. The other set concerns constructing a counterfactual population table with the purpose of comparing two populations. The IPF is commonly applied by social scientists to solve problems not only in the first set, but also in the second one. We show that while it is legitimate to use the IPF for the first set of problems, it is not the right tool to address the problems of the second kind. We promote an alternative of the IPF, the NM-method, for solving problems in the second set. We provide both theoretical and empirical comparisons of these methods."
            },
            {
                "arxivId": "2104.09141",
                "title": "Decomposition scheme matters more than you may think.",
                "abstract": "This paper promotes the application of a path-independent decomposition scheme. Besides presenting some theoretical arguments supporting this decomposition scheme, this study also illustrates the difference between the path-independent decomposition scheme and a popular sequential decomposition with an empirical application of the two schemes. The empirical application is about identifying a directly unobservable phenomenon, i.e. the changing social gap between people from different educational strata, through its effect on marriages and cohabitations. It exploits census data from four waves between 1977 and 2011 about the American, French, Hungarian, Portuguese, and Romanian societies. For some societies and periods, the outcome of the decomposition is found to be highly sensitive to the choice of the decomposition scheme. These examples illustrate the point that a careful selection of the decomposition scheme is crucial for adequately documenting the dynamics of unobservable factors."
            },
            {
                "arxivId": "2103.06991",
                "title": "A new method for identifying what Cupid's invisible hand is doing. Is it spreading color blindness while turning us more\"picky'' about spousal education?",
                "abstract": "We develop a method suitable for detecting whether racial homophily is on the rise and also whether the economic divide (i.e., the gap between individuals with different education levels and thereby with different abilities to generate income) is growing in a society. We identify these changes with the changing aggregate marital preferences over the partners' race and education level through their effects on the share of inter-racial couples and the share of educationally homogamous couples. These shares are shaped not only by preferences, but also by the distributions of marriageable men and women by traits. The method proposed is designed to control for changes in the trait distributions from one generation to another. By applying the method, we find the economic divide in the US to display a U-curve pattern between 1960 and 2010 followed by its slightly negative trend between 2010 and 2015. The identified trend of racial homophily suggests that the American society has become more and more permissive towards racial intermarriages since 1970. Finally, we refute the aggregate version of the status-cast exchange hypothesis based on the joint dynamics of the economic divide and the racial homophily."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2306.12003",
        "category": "econ",
        "title": "Difference-in-Differences with Interference",
        "abstract": "In many scenarios, such as the evaluation of place-based policies, potential outcomes are not only dependent upon the unit's own treatment but also its neighbors' treatment. Despite this,\"difference-in-differences\"(DID) type estimators typically ignore such interference among neighbors. I show in this paper that the canonical DID estimators generally fail to identify interesting causal effects in the presence of neighborhood interference. To incorporate interference structure into DID estimation, I propose doubly robust estimators for the direct average treatment effect on the treated as well as the average spillover effects under a modified parallel trends assumption. The approach in this paper relaxes common restrictions in the literature, such as partial interference and correctly specified spillover functions. Moreover, robust inference is discussed based on the asymptotic distribution of the proposed estimators.",
        "references": [
            {
                "arxivId": "2211.14354",
                "title": "A Design-Based Approach to Spatial Correlation",
                "abstract": "When observing spatial data, what standard errors should we report? With the finite population framework, we identify three channels of spatial correlation: sampling scheme, assignment design, and model specification. The Eicker-Huber-White standard error, the cluster-robust standard error, and the spatial heteroskedasticity and autocorrelation consistent standard error are compared under different combinations of the three channels. Then, we provide guidelines for whether standard errors should be adjusted for spatial correlation for both linear and nonlinear estimators. As it turns out, the answer to this question also depends on the magnitude of the sampling probability."
            },
            {
                "arxivId": "2107.12420",
                "title": "Efficient Treatment Effect Estimation in Observational Studies under Heterogeneous Partial Interference",
                "abstract": "In many observational studies in social science and medicine, individuals are connected in ways that affect the adoption and efficacy of interventions. One individual\u2019s treatment and attributes may affect another individual\u2019s treatment and outcome. In particular, this violates the commonly made stable unit treatment value assumption (SUTVA). Interference is often heterogeneous, and an individual\u2019s outcome is not only affected by how many, but also which, neighbors or connections are treated. In this paper, we propose a flexible framework for heterogeneous partial interference that partitions units into subsets based on observables. We allow interactions to be heterogeneous across subsets, but homogeneous for individuals within a subset. In this framework, we propose a class of estimators for heterogeneous direct and spillover effects from observational data that are shown to be doubly robust, asymptotically normal, and semiparametric efficient. In addition, we discuss a bias-variance tradeoff between robustness to heterogeneous interference and estimation efficiency. We further propose consistent matchingbased variance estimators and hypothesis tests to determine the appropriate specification of interference structure. We apply our methods to the Add Health data and find that regular alcohol consumption exhibits negative effects on academic performance, but the magnitude of effect varies by gender and friends\u2019 alcohol consumption."
            },
            {
                "arxivId": "2105.03737",
                "title": "Difference-in-Differences Estimation with Spatial Spillovers",
                "abstract": "Empirical work often uses treatment assigned following geographic boundaries. When the effects of treatment cross over borders, classical difference-in-differences estimation produces biased estimates for the average treatment effect. In this paper, I introduce a potential outcomes framework to model spillover effects and decompose the estimate's bias in two parts: (1) the control group no longer identifies the counterfactual trend because their outcomes are affected by treatment and (2) changes in treated units' outcomes reflect the effect of their own treatment status and the effect from the treatment status of 'close' units. I propose conditions for non-parametric identification that can remove both sources of bias and semi-parametrically estimate the spillover effects themselves including in settings with staggered treatment timing. To highlight the importance of spillover effects, I revisit analyses of three place-based interventions."
            },
            {
                "arxivId": "2104.04565",
                "title": "Tailored inference for finite populations: conditional validity and transfer across distributions",
                "abstract": "\n Parameters of sub-populations can be more relevant than those of super-populations. For example, a healthcare provider may be interested in the effect of a treatment plan for a specific subset of their patients; policymakers may be concerned with the impact of a policy in a particular state within a given population. In these cases, the focus is on a specific finite population, as opposed to an infinite super-population. Such a population can be characterized by fixing some attributes that are intrinsic to them, leaving unexplained variations like measurement error as random. Inference for a population with fixed attributes can then be modelled as inferring parameters of a conditional distribution. Accordingly, it is desirable that confidence intervals are conditionally valid for the realized population, instead of marginalizing over many possible draws of populations.\n We provide a statistical inference framework for parameters of finite populations with known attributes. Leveraging the attribute information, our estimators and confidence intervals closely target a specific finite population. When the data is from the population of interest, our confidence intervals attain asymptotic conditional validity given the attributes, and are shorter than those for super-population inference. In addition, we develop procedures to infer parameters of new populations with differing covariate distributions; the confidence intervals are also conditionally valid for the new populations under mild conditions. Our methods extend to situations where the fixed information has a weaker structure or is only partially observed. We demonstrate the validity and applicability of our methods using simulated and real-world data."
            },
            {
                "arxivId": "2103.06471",
                "title": "Causal inference with misspecified exposure mappings: separating definitions and assumptions",
                "abstract": "Exposure mappings facilitate investigations of complex causal effects when units interact in experiments. Current methods require experimenters to use the same exposure mappings both to define the effect of interest and to impose assumptions on the interference structure. However, the two roles rarely coincide in practice, and experimenters are forced to make the often questionable assumption that their exposures are correctly specified. This paper argues that the two roles exposure mappings currently serve can, and typically should, be separated, so that exposures are used to define effects without necessarily assuming that they are capturing the complete causal structure in the experiment. The paper shows that this approach is practically viable by providing conditions under which exposure effects can be precisely estimated when the exposures are misspecified. Some important questions remain open."
            },
            {
                "arxivId": "1305.6156",
                "title": "Estimating Average Causal Effects Under Interference Between Units",
                "abstract": "This paper presents a randomization-based framework for estimating causal effects under interference between units. The framework integrates three components: (i) an experimental design that defines the probability distribution of treatment assignments, (ii) a mapping that relates experimental treatment assignments to exposures received by units in the experiment, and (iii) estimands that make use of the experiment to answer questions of substantive interest. Using this framework, we develop the case of estimating average unit-level causal effects from a randomized experiment with interference of arbitrary but known form. The resulting estimators are based on inverse probability weighting. We provide randomization-based variance estimators that account for the complex clustering that can occur when interference is present. We also establish consistency and asymptotic normality under local dependence assumptions. We discuss refinements including covariate-adjusted effect estimators and ratio estimation. We illustrate and assess empirical performance with a naturalistic simulation using network data from American high schools."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2401.13812",
        "category": "econ",
        "title": "Optimal Queueing Regimes",
        "abstract": "We consider an M/M/1 queueing model where customers can strategically decide whether to join the queue or balk and when to renege. We characterize the class of queueing regimes such that, for any parameters of the model, the socially efficient behavior is an equilibrium outcome.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2401.16752",
        "category": "econ",
        "title": "Enhancing Urban Traffic Safety: An Evaluation of Taipei\u2019s Neighborhood Traffic Environment Improvement Program",
        "abstract": "In densely populated urban areas, where interactions between pedestrians, vehicles, and motorcycles are frequent and complex, traffic safety is a critical concern. This paper evaluates the Neighborhood Traffic Environment Improvement Program in Taipei, which involved painting green pedestrian paths, adjusting no-parking red/yellow lines, and painting speed limit and stop/slow signs on lanes and alleys. Exploiting staggered rollout of policy implementation and administrative traffic accident data, we found that the program reduced daytime traffic accidents by 5 percent and injuries by 8 percent, while having no significant impact on nighttime incidents. The effectiveness of the program during the day is mainly attributed to the painted green sidewalks, with adequate sunlight playing a part in the program's success. Our findings indicate that cost-effective strategies like green pedestrian lanes can be effective in areas with dense populations and high motorcycle traffic, as they improve safety by encouraging pedestrians to use marked areas and deterring vehicles from these zones.",
        "references": [
            {
                "arxivId": "2212.06080",
                "title": "Logs with Zeros? Some Problems and Solutions",
                "abstract": "\n When studying an outcome Y that is weakly positive but can equal zero (e.g. earnings), researchers frequently estimate an average treatment effect (ATE) for a \u201clog-like\u201d transformation that behaves like log\u2009(Y) for large Y but is defined at zero (e.g. log\u2009(1 + Y), $\\operatorname{arcsinh}(Y)$). We argue that ATEs for log-like transformations should not be interpreted as approximating percentage effects, since unlike a percentage, they depend on the units of the outcome. In fact, we show that if the treatment affects the extensive margin, one can obtain a treatment effect of any magnitude simply by rescaling the units of Y before taking the log-like transformation. This arbitrary unit-dependence arises because an individual-level percentage effect is not well-defined for individuals whose outcome changes from zero to nonzero when receiving treatment, and the units of the outcome implicitly determine how much weight the ATE for a log-like transformation places on the extensive margin. We further establish a trilemma: when the outcome can equal zero, there is no treatment effect parameter that is an average of individual-level treatment effects, unit invariant, and point identified. We discuss several alternative approaches that may be sensible in settings with an intensive and extensive margin, including (i) expressing the ATE in levels as a percentage (e.g. using Poisson regression), (ii) explicitly calibrating the value placed on the intensive and extensive margins, and (iii) estimating separate effects for the two margins (e.g. using Lee bounds). We illustrate these approaches in three empirical applications."
            },
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            },
            {
                "arxivId": "2108.12419",
                "title": "Revisiting event study designs: robust and efficient estimation",
                "abstract": "We develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Our method applies with time-varying controls, in triple-difference designs, and with certain non-binary treatments. We show the practical relevance of our results in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter - about half as large as benchmark estimates used to calibrate macroeconomic models - and predominantly occurs in the first month after the rebate."
            },
            {
                "arxivId": "0806.1354",
                "title": "Analysis of the Effect of Speed Limit Increases on Accident-Injury Severities",
                "abstract": "The influence of speed limits on roadway safety has been a subject of continuous debate in the State of Indiana and nationwide. In Indiana, highway-related accidents result in about 900 fatalities and forty thousand injuries annually and place an incredible social and economic burden on the state. Still, speed limits posted on highways and other roads are routinely exceeded as individual drivers try to balance safety, mobility (speed), and the risks and penalties associated with law enforcement efforts. The speed-limit/safety issue has been a matter of considerable concern in Indiana since the state raised its speed limits on rural interstates and selected multilane highways on July 1, 2005. In this paper, the influence of the posted speed limit on the severity of vehicle accidents is studied using Indiana accident data from 2004 (the year before speed limits were raised) and 2006 (the year after speed limits were raised on rural interstates and some multi-lane non-interstate routes). Statistical models of the injury severity of different types of accidents on various roadway classes were estimated. The results of the model estimations showed that, for the speed limit ranges currently used, speed limits did not have a statistically significant effect on the severity of accidents on interstate highways. However, for some non-interstate highways, higher speed limits were found to be associated with higher accident severities - suggesting that future speed limit changes, on non-interstate highways in particular, need to be carefully assessed on a case-by-case basis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2401.16754",
        "category": "econ",
        "title": "AI Oversight and Human Mistakes: Evidence from Centre Court",
        "abstract": "Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because of these costs, umpires cared twice as much about Type II errors under AI oversight.",
        "references": [
            {
                "arxivId": "2402.11157",
                "title": "The Value of Context: Human versus Black Box Evaluators",
                "abstract": "Evaluations once solely within the domain of human experts (e.g., medical diagnosis by doctors) can now also be carried out by machine learning algorithms. This raises a new conceptual question: what is the difference between being evaluated by humans and algorithms, and when should an individual prefer one form of evaluation over the other? We propose a theoretical framework that formalizes one key distinction between the two forms of evaluation: Machine learning algorithms are standardized, fixing a common set of covariates by which to assess all individuals, while human evaluators customize which covariates are acquired to each individual. Our framework defines and analyzes the advantage of this customization -- the value of context -- in environments with very high-dimensional data. We show that unless the agent has precise knowledge about the joint distribution of covariates, the value of more covariates exceeds the value of context."
            },
            {
                "arxivId": "1711.05225",
                "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning",
                "abstract": "We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.05152",
        "category": "econ",
        "title": "Is the Price Right? Reconceptualizing Price and Income Elasticity to Anticipate Price Perception Issues",
        "abstract": "Price perception by consumers represents a challenge to the ability of a business to correctly and profitably price and sell their products or services in a given market and any new target market. Complicating the perception of prices is the dynamics of price and income elasticity, both of which are key for estimating demand. This article proposes a novel and elegant identity that conceptualizes elasticity as a means of quantifying the potential for price perception problems in a market. Elasticity studies from 1990 to 2023 (n=30) were sampled to evaluate the relationship between price and income elasticity for various consumer commodities using the identity. The results suggest that, given known price and income elasticity values, a business can anticipate pricing perception problems in advance and address the potential for damaging distortion of their value proposition. Further, the business can use this insight to correctly choose a strategic pricing approach.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11072",
        "category": "econ",
        "title": "Awareness of self-control",
        "abstract": "Economists modeled self-control problems in decisions of people with the time-inconsistence preferences model. They argued that the source of self-control problems could be uncertainty and temptation. This paper uses an experimental test offered to individuals instantaneous reward and future rewards to measure awareness of self-control problems in a tempting condition and also measure the effect of commitment and flexibility cost on their welfare. The quasi-hyperbolic discounting model with time discount factor and present bias at the same time was used for making a model for measuring awareness and choice reversal conditions. The test showed 66% awareness of self-control (partially naive behaviors) in individuals. The welfare implication for individuals increased with commitment and flexibility costs. The result can be useful in marketing and policy-making fields design precisely offers for customers and society.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11134",
        "category": "econ",
        "title": "Functional Partial Least-Squares: Optimal Rates and Adaptation",
        "abstract": "We consider the functional linear regression model with a scalar response and a Hilbert space-valued predictor, a well-known ill-posed inverse problem. We propose a new formulation of the functional partial least-squares (PLS) estimator related to the conjugate gradient method. We shall show that the estimator achieves the (nearly) optimal convergence rate on a class of ellipsoids and we introduce an early stopping rule which adapts to the unknown degree of ill-posedness. Some theoretical and simulation comparison between the estimator and the principal component regression estimator is provided.",
        "references": [
            {
                "arxivId": "1709.03473",
                "title": "Is Completeness Necessary? Estimation in Nonidentified Linear Models",
                "abstract": "This paper documents the consequences of the identification failures for a class of linear ill-posed inverse models. The Tikhonov-regularized estimator converges to a well-defined limit equal to the best approximation of the structural parameter in the orthogonal complement to the null space of the operator. We illustrate that in many cases the best approximation may coincide with the structural parameter or at least may reasonably approximate it. We characterize the nonasymptotic Hilbert space norm and the uniform norm convergence rates for the best approximation. Nonidentification has important implications for the large sample distribution of the Tikhonov-regularized estimator, and we document the transition between the Gaussian and the weighted chi-squared limits. The theoretical results are illustrated for the nonparametric IV and the functional linear IV regressions and are further supported by the Monte Carlo experiments."
            },
            {
                "arxivId": "1606.07702",
                "title": "Optimal Adaptation for Early Stopping in Statistical Inverse Problems",
                "abstract": "For linear inverse problems $Y=\\mathsf{A}\\mu+\\xi$, it is classical to recover the unknown signal $\\mu$ by iterative regularisation methods $(\\widehat \\mu^{(m)}, m=0,1,\\ldots)$ and halt at a data-dependent iteration $\\tau$ using some stopping rule, typically based on a discrepancy principle, so that the weak (or prediction) squared-error $\\|\\mathsf{A}(\\widehat \\mu^{(\\tau)}-\\mu)\\|^2$ is controlled. In the context of statistical estimation with stochastic noise $\\xi$, we study oracle adaptation (that is, compared to the best possible stopping iteration) in strong squared-error $E[\\|\\hat \\mu^{(\\tau)}-\\mu\\|^2]$. \nFor a residual-based stopping rule oracle adaptation bounds are established for general spectral regularisation methods. The proofs use bias and variance transfer techniques from weak prediction error to strong $L^2$-error, as well as convexity arguments and concentration bounds for the stochastic part. Adaptive early stopping for the Landweber method is studied in further detail and illustrated numerically."
            },
            {
                "arxivId": "1611.01593",
                "title": "Operator Lipschitz functions",
                "abstract": "The goal of this survey is a comprehensive study of operator Lipschitz functions. A continuous function on the real line is said to be operator Lipschitz if for arbitrary self-adjoint operators and . Sufficient conditions and necessary conditions are given for operator Lipschitzness. The class of operator differentiable functions on is also studied. Further, operator Lipschitz functions on closed subsets of the plane are considered, and the class of commutator Lipschitz functions on such subsets is introduced. An important role for the study of such classes of functions is played by double operator integrals and Schur multipliers. Bibliography: 77 titles."
            },
            {
                "arxivId": "1405.5900",
                "title": "PLS: a new statistical insight through the prism of orthogonal polynomials",
                "abstract": "Partial Least Square (PLS) is a dimension reduction method used to remove multicollinearities in a regression model. However contrary to Principal Components Analysis (PCA) the PLS components are also choosen to be optimal for predicting the response $Y$. In this paper we provide a new and explicit formula for the residuals. We show that the residuals are completely determined by the spectrum of the design matrix and by the noise on the observations. Because few are known on the behaviour of the PLS components we also investigate their statistical properties in a regression context. New results on regression and prediction error for PLS are stated under the assumption of a low variance of the noise."
            },
            {
                "arxivId": "1205.6367",
                "title": "Methodology and theory for partial least squares applied to functional data",
                "abstract": "The partial least squares procedure was originally developed to estimate the slope parameter in multivariate parametric models. More recently it has gained popularity in the functional data literature. There, the partial least squares estimator of slope is either used to construct linear predictive models, or as a tool to project the data onto a one-dimensional quantity that is employed for further statistical analysis. Although the partial least squares approach is often viewed as an attractive alternative to projections onto the principal component basis, its properties are less well known than those of the latter, mainly because of its iterative nature. We develop an explicit formulation of partial least squares for functional data, which leads to insightful results and motivates new theory, demonstrating consistency and establishing convergence rates."
            },
            {
                "arxivId": "1112.2509",
                "title": "Adaptive functional linear regression",
                "abstract": "We consider the estimation of the slope function in functional linear regression, where scalar responses are modeled in dependence of random functions. Cardot and Johannes [J. Multivariate Anal. 101 (2010) 395\u2013408] have shown that a thresholded projection estimator can attain up to a constant minimax-rates of convergence in a general framework which allows us to cover the prediction problem with respect to the mean squared prediction error as well as the estimation of the slope function and its derivatives. This estimation procedure, however, requires an optimal choice of a tuning parameter with regard to certain characteristics of the slope function and the covariance operator associated with the functional regressor. As this information is usually inaccessible in practice, we investigate a fully data-driven choice of the tuning parameter which combines model selection and Lepski\u2019s method. It is inspired by the recent work of Goldenshluger and Lepski [Ann. Statist. 39 (2011) 1608\u20131632]. The tuning parameter is selected as minimizer of a stochastic penalized contrast function imitating Lepski\u2019s method among a random collection of admissible values. This choice of the tuning parameter depends only on the data and we show that within the general framework the resulting data-driven thresholded projection estimator can attain minimaxrates up to a constant over a variety of classes of slope functions and covariance operators. The results are illustrated considering different configurations which cover in particular the prediction problem as well as the estimation of the slope and its derivatives. A simulation study shows the reasonable performance of the fully data-driven estimation procedure."
            },
            {
                "arxivId": "1001.2089",
                "title": "EMPIRICAL RISK MINIMIZATION IN INVERSE PROBLEMS",
                "abstract": "We study estimation of a multivariate function f: R d \u2192 R when the observations are available from the function Af, where A is a known linear operator. Both the Gaussian white noise model and density estimation are studied. We define an L 2 -empirical risk functional which is used to define a \u03b4-net minimizer and a dense empirical risk minimizer. Upper bounds for the mean integrated squared error of the estimators are given. The upper bounds show how the difficulty of the estimation depends on the operator through the norm of the adjoint of the inverse of the operator and on the underlying function class through the entropy of the class. Corresponding lower bounds are also derived. As examples, we consider convolution operators and the Radon transform. In these examples, the estimators achieve the optimal rates of convergence. Furthermore, a new type of oracle inequality is given for inverse problems in additive models."
            },
            {
                "arxivId": "0902.4344",
                "title": "Smoothing splines estimators for functional linear regression",
                "abstract": "The paper considers functional linear regression, where scalar responses $Y_1,...,Y_n$ are modeled in dependence of random functions $X_1,...,X_n$. We propose a smoothing splines estimator for the functional slope parameter based on a slight modification of the usual penalty. Theoretical analysis concentrates on the error in an out-of-sample prediction of the response for a new random function $X_{n+1}$. It is shown that rates of convergence of the prediction error depend on the smoothness of the slope function and on the structure of the predictors. We then prove that these rates are optimal in the sense that they are minimax over large classes of possible slope functions and distributions of the predictive curves. For the case of models with errors-in-variables the smoothing spline estimator is modified by using a denoising correction of the covariance matrix of discretized curves. The methodology is then applied to a real case study where the aim is to predict the maximum of the concentration of ozone by using the curve of this concentration measured the preceding day."
            },
            {
                "arxivId": "0806.0533",
                "title": "Thresholding projection estimators in functional linear models",
                "abstract": null
            },
            {
                "arxivId": "math/0702650",
                "title": "Prediction in functional linear regression",
                "abstract": "Supported in part by NSF Grant DMS-03-06576 and a grant from the Australian Research Council."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11370",
        "category": "econ",
        "title": "Stable Menus of Public Goods: A Matching Problem",
        "abstract": "We study a matching problem between agents and public goods, in settings without monetary transfers. Since goods are public, they have no capacity constraints. There is no exogenously defined budget of goods to be provided. Rather, each provided good must justify its cost, leading to strong complementarities in the\"preferences\"of goods. Furthermore, goods that are in high demand given other already-provided goods must also be provided. The question of the existence of a stable solution (a menu of public goods to be provided) exhibits a rich combinatorial structure. We uncover sufficient conditions and necessary conditions for guaranteeing the existence of a stable solution, and derive both positive and negative results for strategyproof stable matching.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11371",
        "category": "econ",
        "title": "Companhias a\\'ereas s\\~ao todas iguais? A converg\\^encia dos modelos de neg\\'ocios no transporte a\\'ereo",
        "abstract": "This study discusses the literature on the convergence of business models of airlines in Brazilian air transport, focusing on the formation of flight networks. Initially, it analyzes the determinants of the network formation patterns of the\"fundamental\"business models (archetypes) of airlines in the first years after the sector's deregulation. Then, it discusses how the business models of Brazilian companies resemble these patterns. The literature highlights convergences between the network formation strategies of full-service companies in relation to older low-cost companies, in addition to business model redirections after mergers and acquisitions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11373",
        "category": "econ",
        "title": "Determinantes concorrenciais dos atrasos dos voos no aeroporto e na rota",
        "abstract": "Flight delays are a reality in the modern air industry worldwide. However, studies in the literature have investigated the competitive determinants of delays arising from factors originating at the airport and along the route separately. This work aims to present a national study that used a unifying approach from the literature, considering the local and global effects of competition on delays. The analysis took into account a phenomenon known as the\"internalization of externalities\"of airport congestion. Furthermore, it discusses the relationship between quality and competition on the route and the impacts of the entry of a low-cost carrier (LCC) on the delays of incumbent airlines in the Brazilian air market. The literature suggests that there is evidence of congestion internalization at Brazilian airports, in parallel with competition for quality at the route level. Additionally, the potential competition caused by the presence of the LCC leads to a global effect that suggests the existence of impacts other than prices on routes where it has not entered.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11374",
        "category": "econ",
        "title": "Impactos da Navega\\c{c}\\~ao Baseada em Performance nos Tempos de Voo da Avia\\c{c}\\~ao Comercial",
        "abstract": "This work presents an analysis of recent literature examining factors that influence flight times in Brazil, with special attention to the impact of new technology implementations, specifically Performance-Based Navigation (PBN). PBN procedures began to be implemented in Brazilian airspace in 2009 and represent a new concept of air navigation, using satellites to create 3D flight routes that are shorter and more precise, potentially reducing the distances flown and consequently flight times and delays. Flight times depend on various factors, such as the size and complexity of the origin and destination airports, weather conditions, aircraft models, and how full the flights are. Therefore, to assess the impact of the new procedures, the variation in this set of factors must be taken into account. The results found in the literature suggest that Performance-Based Navigation had a positive impact, reducing flight times by about 1-2%, representing a saving of tens of thousands of flight hours from the beginning of implementations in 2009 until the end of 2018.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11437",
        "category": "econ",
        "title": "The Assignment Game: New Mechanisms for Equitable Core Imputations",
        "abstract": "The set of core imputations of the assignment game forms a (non-finite) distributive lattice. So far, efficient algorithms were known for computing only its two extreme imputations; however, each of them maximally favors one side and disfavors the other side of the bipartition, leading to inequitable profit sharing. Another issue is that a sub-coalition consisting of one player (or a set of players from the same side of the bipartition) can make zero profit, therefore a core imputation is not obliged to give them any profit. Hence core imputations make no fairness guarantee at the level of individual agents. This raises the question of computing {\\em more equitable core imputations}. In this paper, we give combinatorial (i.e., the mechanism does not invoke an LP-solver) polynomial time mechanisms for computing the leximin and leximax core imputations for the assignment game. These imputations achieve ``fairness'' in different ways: whereas leximin tries to make poor agents more rich, leximax tries to make rich agents less rich. In general, the two imputations are different. Our mechanisms were derived by a suitable adaptation of the classical primal-dual paradigm from combinatorial optimization. The ``engine'' driving them involves recent insights, obtained via complementarity, into core imputations \\cite{Va.New-characterizations} and the pristine combinatorial structure of matching. We have identified other natural games which could benefit from our approach.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11464",
        "category": "econ",
        "title": "Weighted Myerson value for Network games",
        "abstract": "We study the weighted Myerson value for Network games extending a similar concept for communication situations. Network games, unlike communication situations, treat direct and indirect links among players differently and distinguish their effects in both worth generation and allocation processes. The weighted Myerson value is an allocation rule for Network games that generalizes the Myerson value of Network games. Here, the players are assumed to have some weights measuring their capacity to form links with other players. Two characterization of the weighted Myerson value are provided. Finally, we propose a bidding mechanism to show that the weighted Myerson value is a subgame-perfect Nash equilibrium under a non-cooperative framework.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11579",
        "category": "econ",
        "title": "Assessment of low-carbon tourism development from multi-aspect analysis: a case study of the Yellow River Basin, China",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-20.json",
        "arxivId": "2402.11754",
        "category": "econ",
        "title": "Attraction Via Prices and Information",
        "abstract": "We study the ramifications of increased commitment power for information provision in an oligopolistic market with search frictions. Although prices are posted and, therefore, guide search, if firms cannot commit to information provision policies, there is no active search at equilibrium so consumers visit (and purchase from) at most one firm. If firms can guide search by both their prices and information policies, there exists a unique symmetric equilibrium exhibiting price dispersion and active search. Nevertheless, when the market is thin, consumers prefer the former case, which features intense price competition. Firms always prefer the latter.",
        "references": [
            {
                "arxivId": "2303.13409",
                "title": "Persuaded Search",
                "abstract": "We consider sequential search by an agent who cannot observe the quality of goods but can acquire information by buying signals from a profit-maximizing principal with limited commitment power. The principal can charge higher prices for more informative signals in any period, but high prices in the future discourage continued search by the agent, thereby reducing the principal's future profits. A unique stationary equilibrium outcome exists, and we show that the principal $(i)$ induces the socially efficient stopping rule, $(ii)$ extracts the full surplus, and $(iii)$ persuades the agent against settling for marginal goods, extending the duration of surplus extraction. However, introducing an additional, free source of information can lead to inefficiency in equilibrium."
            },
            {
                "arxivId": "1802.09396",
                "title": "Attraction versus Persuasion: Information Provision in Search Markets",
                "abstract": "We consider a model of oligopolistic competition in a market with search frictions, in which competing firms with products of unknown quality advertise how much information a consumer\u2019s visit will glean. In the unique symmetric equilibrium of this game, the countervailing incentives of attraction and persuasion yield a payoff function for each firm that is linear in the firm\u2019s realized effective value. If the expected quality of the products is sufficiently high (or competition is sufficiently fierce), this corresponds to full information: firms provide the first-best level of information. If not, this corresponds to information dispersion: firms randomize over signals."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2305.01464",
        "category": "econ",
        "title": "Large Global Volatility Matrix Analysis Based on Structural Information",
        "abstract": "In this paper, we develop a novel large volatility matrix estimation procedure for analyzing global financial markets. Practitioners often use lower-frequency data, such as weekly or monthly returns, to address the issue of different trading hours in the international financial market. However, this approach can lead to inefficiency due to information loss. To mitigate this problem, our proposed method, called Structured Principal Orthogonal complEment Thresholding (Structured-POET), incorporates observation structural information for both global and national factor models. We establish the asymptotic properties of the Structured-POET estimator, and also demonstrate the drawbacks of conventional covariance matrix estimation procedures when using lower-frequency data. Finally, we apply the Structured-POET estimator to an out-of-sample portfolio allocation study using international stock market data.",
        "references": [
            {
                "arxivId": "2210.16042",
                "title": "Eigenvalue tests for the number of latent factors in short panels",
                "abstract": "This paper studies new tests for the number of latent factors in a large cross-sectional factor model with small time dimension. These tests are based on the eigenvalues of variance-covariance matrices of (possibly weighted) asset returns, and rely on either an assumption of spherical errors, or instrumental variables for factor betas. We establish the asymptotic distributional results using expansion theorems based on perturbation theory for symmetric matrices. Our framework accommodates semi-strong factors in the systematic components. We propose a novel statistical test for weak factors against strong or semi-strong factors. We provide an empirical application to US equity data. Evidence for a different number of latent factors according to market downturns and market upturns, is statistically ambiguous in the considered subperiods. In particular, our results contradicts the common wisdom of a single factor model in bear markets. \u2217This paper underlies the Halbert White Jr. Memorial JFEC invited lecture given by Patrick Gagliardini at the Annual Society for Financial Econometrics Conference on June 25th 2022 at the University of Cambridge. We thank the JFEC Editors Allan Timmermann and Fabio Trojani for the invitation, the discussants Alexei Onatski and Markus Pelger for very insightful and constructive comments, as well as G. Genoni, L. Mancini and participants at the Annual SoFiE conference 2022 and at seminars at the Universities of Geneva and Warwick for helpful remarks. \u2020University of Geneva and Swiss Finance Institute. \u2021Universit\u00e0 della Svizzera italiana (USI, Lugano) and Swiss Finance Institute. E-mail address: patrick.gagliardini@usi.ch. \u00a7University of Geneva and Swiss Finance Institute. 1 ar X iv :2 21 0. 16 04 2v 1 [ ec on .E M ] 2 8 O ct 2 02 2"
            },
            {
                "arxivId": "2102.12783",
                "title": "Next generation models for portfolio risk management: An approach using financial big data",
                "abstract": "This paper proposes a dynamic process of portfolio risk measurement to address potential information loss. The proposed model takes advantage of financial big data to incorporate out-of-target-portfolio information that may be missed when one considers the Value at Risk (VaR) measures only from certain assets of the portfolio. We investigate how the curse of dimensionality can be overcome in the use of financial big data and discuss where and when benefits occur from a large number of assets. In this regard, the proposed approach is the first to suggest the use of financial big data to improve the accuracy of risk analysis. We compare the proposed model with benchmark approaches and empirically show that the use of financial big data improves small portfolio risk analysis. Our findings are useful for portfolio managers and financial regulators, who may seek for an innovation to improve the accuracy of portfolio risk estimation. \u2217corresponding author. Tel: +82 2 958 3448. E-mail addresses: kwjung@postech.ac.kr (K. Jung), donggyukim@kaist.ac.kr (D. Kim), ysh93@kaist.ac.kr (S. Yu) 1 ar X iv :2 10 2. 12 78 3v 2 [ qfi n. R M ] 6 J an 2 02 2 JEL classification: C13, C32, C55, C58."
            },
            {
                "arxivId": "1206.0613",
                "title": "Factor modeling for high-dimensional time series: inference for the number of factors",
                "abstract": "This paper deals with the factor modeling for high-dimensional time series based on a dimension-reduction viewpoint. Under stationary settings, the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a nonnegative definite matrix, and is therefore applicable when the dimension of time series is on the order of a few thousands. Asymptotic properties of the proposed method are investigated under two settings: (i) the sample size goes to infinity while the dimension of time series is fixed; and (ii) both the sample size and the dimension of time series go to infinity together. In particular, our estimators for zero-eigenvalues enjoy faster convergence (or slower divergence) rates, hence making the estimation for the number of factors easier. In particular, when the sample size and the dimension of time series go to infinity together, the estimators for the eigenvalues are no longer consistent. However, our estimator for the number of the factors, which is based on the ratios of the estimated eigenvalues, still works fine. Furthermore, this estimation shows the so-called \u201cblessing of dimensionality\u201d property in the sense that the performance of the estimation may improve when the dimension of time series increases. A two-step procedure is investigated when the factors are of different degrees of strength. Numerical illustration with both simulated and real data is also reported."
            },
            {
                "arxivId": "1201.0175",
                "title": "Large covariance estimation by thresholding principal orthogonal complements",
                "abstract": "The paper deals with the estimation of a high dimensional covariance with a conditional sparsity structure and fast diverging eigenvalues. By assuming a sparse error covariance matrix in an approximate factor model, we allow for the presence of some cross\u2010sectional correlation even after taking out common but unobservable factors. We introduce the principal orthogonal complement thresholding method \u2018POET\u2019 to explore such an approximate factor structure with sparsity. The POET\u2010estimator includes the sample covariance matrix, the factor\u2010based covariance matrix, the thresholding estimator and the adaptive thresholding estimator as specific examples. We provide mathematical insights when the factor analysis is approximately the same as the principal component analysis for high dimensional data. The rates of convergence of the sparse residual covariance matrix and the conditional sparse covariance matrix are studied under various norms. It is shown that the effect of estimating the unknown factors vanishes as the dimensionality increases. The uniform rates of convergence for the unobserved factors and their factor loadings are derived. The asymptotic results are also verified by extensive simulation studies. Finally, a real data application on portfolio allocation is presented."
            },
            {
                "arxivId": "1105.4292",
                "title": "High Dimensional Covariance Matrix Estimation in Approximate Factor Models",
                "abstract": "The variance covariance matrix plays a central role in the inferential theories of high dimensional factor models in finance and economics. Popular regularization methods of directly exploiting sparsity are not directly applicable to many financial problems. Classical methods of estimating the covariance matrices are based on the strict factor models, assuming independent idiosyncratic components. This assumption, however, is restrictive in practical applications. By assuming sparse error covariance matrix, we allow the presence of the cross-sectional correlation even after taking out common factors, and it enables us to combine the merits of both methods. We estimate the sparse covariance using the adaptive thresholding technique as in Cai and Liu (2011), taking into account the fact that direct observations of the idiosyncratic components are unavailable. The impact of high dimensionality on the covariance matrix estimation based on the factor structure is then studied."
            },
            {
                "arxivId": "1011.3027",
                "title": "Introduction to the non-asymptotic analysis of random matrices",
                "abstract": "This is a tutorial on some basic non-asymptotic methods and concepts in random matrix theory. The reader will learn several tools for the analysis of the extreme singular values of random matrices with independent rows or columns. Many of these methods sprung off from the development of geometric functional analysis since the 1970's. They have applications in several fields, most notably in theoretical computer science, statistics and signal processing. A few basic applications are covered in this text, particularly for the problem of estimating covariance matrices in statistics and for validating probabilistic constructions of measurement matrices in compressed sensing. These notes are written particularly for graduate students and beginning researchers in different areas, including functional analysts, probabilists, theoretical statisticians, electrical engineers, and theoretical computer scientists."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12401",
        "category": "econ",
        "title": "An\\'alise das estrat\\'egias de planejamento de tempos de voo pelas companhias a\\'ereas",
        "abstract": "This study explores the approaches used by airlines in setting flight times. It highlights the need to balance operational and strategic factors, such as optimizing the use of resources - including aircraft, crew, and fuel - and managing the risks related to delays and congestion. The work details a national analysis focused on domestic flights, investigating the factors that influence companies to adjust scheduled flight times and the impact of this practice on punctuality. The results indicate that decisions about flight time are influenced by both operational and strategic aspects, being affected by competition and the sector's policies on punctuality and slot allocation. Furthermore, it was found that adding extra time is an effective strategy for reducing delays, although it may conceal system deficiencies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12403",
        "category": "econ",
        "title": "A literatura das receitas comerciais em aeroportos: discuss\\~oes e principais descobertas",
        "abstract": "The exploration of existing commercial opportunities has been increasing the share of commercial (non-aeronautical) revenues in the total revenues of airports worldwide. These revenues, also called commercial, non-aeronautical, or non-tariff revenues, come from rentals, duty-free shops, food and beverage sales, parking, advertising, etc. In other words, everything that is not the main business of the airport. Consequently, the dependence on commercial revenues has also become increasingly important, and airport managers are interested in understanding how to improve their financial results with this new revenue source, as it not only improves financial outcomes but also optimizes passengers' time and money consumption options at airports. Therefore, this chapter seeks to discuss the main determinants of commercial revenues at national airports, analyzing the possible impacts of the behavior of passengers from low-cost carriers (LCC) on these revenues, combined with other determining factors.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12561",
        "category": "econ",
        "title": "Robust Appointment Scheduling with Waiting Time Guarantees",
        "abstract": "Appointment scheduling problems under uncertainty encounter a fundamental trade-off between cost minimization and customer waiting times. Most existing studies address this trade-off using a weighted sum approach, which puts little emphasis on individual waiting times and, thus, customer satisfaction. In contrast, we study how to minimize total cost while providing waiting time guarantees to all customers. Given box uncertainty sets for service times and no-shows, we introduce the Robust Appointment Scheduling Problem with Waiting Time Guarantees. We show that the problem is NP-hard in general and introduce a mixed-integer linear program that can be solved in reasonable computation time. For special cases, we prove that polynomial-time variants of the well-known Smallest-Variance-First sequencing rule and the Bailey-Welch scheduling rule are optimal. Furthermore, a case study with data from the radiology department of a large university hospital demonstrates that the approach not only guarantees acceptable waiting times but, compared to existing robust approaches, may simultaneously reduce costs incurred by idle time and overtime. This work suggests that limiting instead of minimizing customer waiting times is a win-win solution in the trade-off between customer satisfaction and cost minimization. Additionally, it provides an easy-to-implement and customizable appointment scheduling framework with waiting time guarantees.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12575",
        "category": "econ",
        "title": "Cross-Market Mergers with Common Customers: When (and Why) Do They Increase Negotiated Prices?",
        "abstract": "I examine the implications of cross-market mergers of suppliers to intermediaries that bundle products for consumers. These mergers are controversial. Some argue that suppliers' products will be substitutes for intermediaries, despite not being substitutes for consumers. Others contend that because bundling makes products complements for consumers, products must be complements for intermediaries. I contribute to this debate by showing that two products can be complements for consumers but substitutes for intermediaries when the products serve a similar role in attracting consumers to purchase the bundle. This result leads to new recommendations and helps explain why cross-market hospital mergers raise prices.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12583",
        "category": "econ",
        "title": "Non-linear Triple Changes Estimator for Targeted Policies",
        "abstract": "The renowned difference-in-differences (DiD) estimator relies on the assumption of 'parallel trends,' which does not hold in many practical applications. To address this issue, the econometrics literature has turned to the triple difference estimator. Both DiD and triple difference are limited to assessing average effects exclusively. An alternative avenue is offered by the changes-in-changes (CiC) estimator, which provides an estimate of the entire counterfactual distribution at the cost of relying on (stronger) distributional assumptions. In this work, we extend the triple difference estimator to accommodate the CiC framework, presenting the `triple changes estimator' and its identification assumptions, thereby expanding the scope of the CiC paradigm. Subsequently, we empirically evaluate the proposed framework and apply it to a study examining the impact of Medicaid expansion on children's preventive care.",
        "references": [
            {
                "arxivId": "2201.10743",
                "title": "Combining Experimental and Observational Data for Identification and Estimation of Long-Term Causal Effects",
                "abstract": "We consider the task of identifying and estimating the causal effect of a treatment variable on a long-term outcome variable using data from an observational domain and an experimental domain. The observational domain is subject to unobserved confounding. Furthermore, subjects in the experiment are only followed for a short period of time; hence, long-term effects of treatment are unobserved but short-term effects will be observed. Therefore, data from neither domain alone suffices for causal inference about the effect of the treatment on the long-term outcome, and must be pooled in a principled way, instead. Athey et al. (2020) proposed a method for systematically combining such data for identifying the downstream causal effect in view. Their approach is based on the assumptions of internal and external validity of the experimental data, and an extra novel assumption called latent unconfoundedness. In this paper, we first review their proposed approach, and then we propose three alternative approaches for data fusion for the purpose of identifying and estimating average treatment effect as well as the effect of treatment on the treated. Our first approach is based on assuming equi-confounding bias for the short-term and long-term outcomes. Our second approach is based on a relaxed version of the equi-confounding bias assumption, where we assume the existence of an observed confounder such that the short-term and long-term potential outcome variables have the same partial additive association with that confounder. Our third approach is based on the proximal causal inference framework, in which we assume the existence of an extra variable in the system which is a proxy of the latent confounder of the treatment-outcome relation. We propose influence function-based estimation strategies for each of our data fusion frameworks and study the robustness properties of the proposed estimators."
            },
            {
                "arxivId": "2010.04814",
                "title": "When Is Parallel Trends Sensitive to Functional Form?",
                "abstract": "This paper assesses when the validity of difference\u2010in\u2010differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger \u201cparallel trends\u201d\u2010type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form."
            },
            {
                "arxivId": "1809.00399",
                "title": "Flexible Sensitivity Analysis for Observational Studies Without Observable Implications",
                "abstract": "Abstract A fundamental challenge in observational causal inference is that assumptions about unconfoundedness are not testable from data. Assessing sensitivity to such assumptions is therefore important in practice. Unfortunately, some existing sensitivity analysis approaches inadvertently impose restrictions that are at odds with modern causal inference methods, which emphasize flexible models for observed data. To address this issue, we propose a framework that allows (1) flexible models for the observed data and (2) clean separation of the identified and unidentified parts of the sensitivity model. Our framework extends an approach from the missing data literature, known as Tukey\u2019s factorization, to the causal inference setting. Under this factorization, we can represent the distributions of unobserved potential outcomes in terms of unidentified selection functions that posit a relationship between treatment assignment and unobserved potential outcomes. The sensitivity parameters in this framework are easily interpreted, and we provide heuristics for calibrating these parameters against observable quantities. We demonstrate the flexibility of this approach in two examples, where we estimate both average treatment effects and quantile treatment effects using Bayesian nonparametric models for the observed data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12607",
        "category": "econ",
        "title": "Inference on LATEs with covariates",
        "abstract": "In theory, two-stage least squares (TSLS) identifies a weighted average of covariate-specific local average treatment effects (LATEs) from a saturated specification without making parametric assumptions on how available covariates enter the model. In practice, TSLS is severely biased when saturation leads to a number of control dummies that is of the same order of magnitude as the sample size, and the use of many, arguably weak, instruments. This paper derives asymptotically valid tests and confidence intervals for an estimand that identifies the weighted average of LATEs targeted by saturated TSLS, even when the number of control dummies and instrument interactions is large. The proposed inference procedure is robust against four key features of saturated economic data: treatment effect heterogeneity, covariates with rich support, weak identification strength, and conditional heteroskedasticity.",
        "references": [
            {
                "arxivId": "1507.02493",
                "title": "Inference in Linear Regression Models with Many Covariates and Heteroscedasticity",
                "abstract": "ABSTRACT The linear regression model is widely used in empirical work in economics, statistics, and many other disciplines. Researchers often include many covariates in their linear model specification in an attempt to control for confounders. We give inference methods that allow for many covariates and heteroscedasticity. Our results are obtained using high-dimensional approximations, where the number of included covariates is allowed to grow as fast as the sample size. We find that all of the usual versions of Eicker\u2013White heteroscedasticity consistent standard error estimators for linear models are inconsistent under this asymptotics. We then propose a new heteroscedasticity consistent standard error formula that is fully automatic and robust to both (conditional) heteroscedasticity of unknown form and the inclusion of possibly many covariates. We apply our findings to three settings: parametric linear models with many covariates, linear panel models with many fixed effects, and semiparametric semi-linear models with many technical regressors. Simulation evidence consistent with our theoretical results is provided, and the proposed methods are also illustrated with an empirical application. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.12859",
        "category": "econ",
        "title": "ATLAS: A Model of Short-term European Electricity Market Processes under Uncertainty - Balancing Modules",
        "abstract": "The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes. ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail. Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints--both regarding different production units and the different market constraints. This working paper describes the simulated balancing processes in detail and is the second part of the ATLAS documentation.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-21.json",
        "arxivId": "2402.13177",
        "category": "econ",
        "title": "The Ebb and Flow of Brand Loyalty: A 28-Year Bibliometric and Content Analysis",
        "abstract": "Business research is facing the challenge of scattered knowledge, particularly in the realm of brand loyalty (BL). Although literature reviews on BL exist, they predominantly concentrate on the pre-sent state, neglecting future trends. Therefore, a comprehensive review is imperative to ascertain emerging trends in BL This study employs a bibliometric approach, analyzing 1,468 papers from the Scopus database. Various tools including R software, VOS viewer software, and Publish or Perish are utilized. The aim is to portray the knowledge map, explore the publication years, identify the top authors and their co-occurrence, reliable documents, institutions, subjects, research hotspots, and pioneering countries and universities in the study of BL. The qualitative section of this research identifies gaps and emerging trends in BL through Word Cloud charts, word growth analysis, and a review of highly cited articles from the past four years. Results showed that highly cited articles mention topics such as brand love, consumer-brand identification, and social networks and the U.S. had the most productions in this field. Besides, most citations were related to Keller with 1,173 citations. Furthermore, in the qualitative section, social networks and brand experiences were found to be of interest to researchers in the field. Finally, by introducing the antecedents and consequences of BL, the gaps and emerging trends in BL were identified, so as to present the di-rection of future research in this area.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2104.02044",
        "category": "econ",
        "title": "Screening for breakthroughs: Omitted proofs",
        "abstract": "This document contains all proofs omitted from our working paper 'Screening for breakthroughs'; specifically, the February 2024 version of the paper (arXiv:2011.10090v8).",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2209.10137",
        "category": "econ",
        "title": "Rank-Preserving Multidimensional Mechanisms",
        "abstract": "We show that the mechanism-design problem for a monopolist selling multiple, heterogeneous objects to a buyer with ex ante symmetric and additive values is equivalent to the mechanism-design problem for a monopolist selling identical objects to a buyer with decreasing marginal values. Symmetric and incentive-compatible mechanisms for heterogeneous objects are rank preserving, i.e., higher-valued objects are assigned with a higher probability. In the identical-objects model, every mechanism is rank preserving. This facilitates the equivalence, which we use in three applications.",
        "references": [
            {
                "arxivId": "2210.17150",
                "title": "Monotonic Mechanisms for Selling Multiple Goods",
                "abstract": "Maximizing the revenue from selling two or more goods has been shown to require the use of nonmonotonic mechanisms, where a higher-valuation buyer may pay less than a lower-valuation one. Here we show that the restriction to monotonic mechanisms may not just lower the revenue, but may in fact yield only a negligible fraction of the maximal revenue; more precisely, the revenue from monotonic mechanisms is no more than k times the simple revenue obtainable by selling the goods separately, or bundled (where k is the number of goods), whereas the maximal revenue may be arbitrarily larger. We then study the class of monotonic mechanisms and its subclass of allocation-monotonic mechanisms, and obtain useful characterizations and revenue bounds."
            },
            {
                "arxivId": "2105.12304",
                "title": "Multi-Dimensional Screening: Buyer-Optimal Learning and Informational Robustness",
                "abstract": "What is the optimal mechanism that a monopolist should use to sell multiple goods to a single buyer? Despite being a classic economic problem, multi-dimensional screening is notoriously intractable. Even if the seller has just two goods and the buyer's values are additive, independent, and identically distributed, the optimal mechanism is hard to characterize generally. In this paper, we study a general version (with arbitrarily many goods and non-additive values) of this problem but with the novel feature of buyer learning. As it turns out, introducing this new feature makes the model tractable and in certain environments---including the one with independent and additive values---makes pure bundling an optimal mechanism."
            },
            {
                "arxivId": "2105.02828",
                "title": "Robustly-Optimal Mechanism for Selling Multiple Goods",
                "abstract": "We study robustly-optimal mechanisms for selling multiple items. The seller maximizes revenue against a worst-case distribution of a buyer's valuations within a set of distributions, called an \"ambiguity\" set. We identify the exact forms of robustly-optimal selling mechanisms and the worst-case distributions when the ambiguity set satisfies a variety of moment conditions on the values of subsets of goods. We also identify general properties of the ambiguity set that lead to the robust optimality of partial bundling which includes separate sales and pure bundling as special cases."
            },
            {
                "arxivId": "2009.11545",
                "title": "Selling Two Identical Objects",
                "abstract": null
            },
            {
                "arxivId": "1704.05027",
                "title": "Optimal Multi-Unit Mechanisms with Private Demands",
                "abstract": "We study a pricing problem that is motivated by the following examples. A cloud computing platform such as Amazon EC2 sells virtual machines to clients, each of who needs a different number of virtual machine hours. Similarly, cloud storage providers such as Dropbox have customers that require different amounts of storage. Software companies such as Microsoft sell software subscriptions that can have different levels of service. The levels could be the number of different documents you are allowed to create, or the number of hours you are allowed to use the software. Companies like Google and Microsoft sell API calls to artificial intelligence software such as face recognition, to other software developers. Video and mobile games are increasingly designed in such a way that one can pay for better access to certain features. Spotify and iTunes sell music subscription, and different people listen to different number of songs in a month. Cellphone service providers like AT&T and Verizon offer cellular phone call minutes and data. People have widely varying amounts of data consumption."
            },
            {
                "arxivId": "1204.1846",
                "title": "Approximate revenue maximization with multiple items",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2211.16298",
        "category": "econ",
        "title": "Double Robust Bayesian Inference on Average Treatment Effects",
        "abstract": "We propose a double robust Bayesian inference procedure on the average treatment effect (ATE) under unconfoundedness. Our robust Bayesian approach involves two important modifications: first, we adjust the prior distributions of the conditional mean function; second, we correct the posterior distribution of the resulting ATE. Both adjustments make use of pilot estimators motivated by the semiparametric influence function for ATE estimation. We prove asymptotic equivalence of our Bayesian procedure and efficient frequentist ATE estimators by establishing a new semiparametric Bernstein-von Mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. Consequently, the resulting Bayesian credible sets form confidence intervals with asymptotically exact coverage probability. In simulations, our double robust Bayesian procedure leads to significant bias reduction of point estimation over conventional Bayesian methods and more accurate coverage of confidence intervals compared to existing frequentist methods. We illustrate our method in an application to the National Supported Work Demonstration.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2304.06466",
        "category": "econ",
        "title": "Market-Based\"Actual\"Returns of Investors",
        "abstract": "We describe how the market-based average and volatility of the\"actual\"return, which the investors gain within their market sales, depend on the statistical moments, volatilities, and correlations of the current and past market trade values. We describe three successive approximations. First, we derive the dependence of the market-based average and volatility of a single sale return on market trade statistical moments determined by multiple purchases in the past. Then, we describe the dependence of average and volatility of return that a single investor gains during the\"trading day.\"Finally, we derive the market-based average and volatility of return of different investors during the\"trading day\"as a function of volatilities and correlations of market trade values. That highlights the distribution of the\"actual\"return of market trade and can serve as a benchmark for\"purchasing\"investors.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.09384",
        "category": "econ",
        "title": "Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions",
        "abstract": "A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Well-intentioned policies aiming to provide more information, such as keeping a\"human-in-the-loop\"or requiring maximal prediction accuracy, could strictly worsen decision quality compared to systems with no human or no algorithmic assistance. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.09697",
        "category": "econ",
        "title": "On Three-Layer Data Markets",
        "abstract": "We study a three-layer data market comprising users (data owners), platforms, and a data buyer. Each user benefits from platform services in exchange for data, incurring privacy loss when their data, albeit noisily, is shared with the buyer. The user chooses platforms to share data with, while platforms decide on data noise levels and pricing before selling to the buyer. The buyer selects platforms to purchase data from. We model these interactions via a multi-stage game, focusing on the subgame Nash equilibrium. We find that when the buyer places a high value on user data (and platforms can command high prices), all platforms offer services to the user who joins and shares data with every platform. Conversely, when the buyer's valuation of user data is low, only large platforms with low service costs can afford to serve users. In this scenario, users exclusively join and share data with these low-cost platforms. Interestingly, increased competition benefits the buyer, not the user: as the number of platforms increases, the user utility does not necessarily improve while the buyer utility improves. However, increasing the competition improves the overall utilitarian welfare. Building on our analysis, we then study regulations to improve the user utility. We discover that banning data sharing maximizes user utility only when all platforms are low-cost. In mixed markets of high- and low-cost platforms, users prefer a minimum noise mandate over a sharing ban. Imposing this mandate on high-cost platforms and banning data sharing for low-cost ones further enhances user utility.",
        "references": [
            {
                "arxivId": "2209.06340",
                "title": "Optimal Data Acquisition with Privacy-Aware Agents",
                "abstract": "We study the problem faced by a data analyst or platform that wishes to collect private data from privacy-aware agents. To incentivize participation, in exchange for this data, the platform provides a service to the agents in the form of a statistic computed using all agents' submitted data. The agents decide whether to join the platform (and truthfully reveal their data) or not participate by considering both the privacy costs of joining and the benefit they get from obtaining the statistic. The platform must ensure the statistic is computed differentially privately and chooses a central level of noise to add to the computation, but can also induce personalized privacy levels (or costs) by giving different weights to different agents in the computation as a function of their heterogeneous privacy preferences (which are known to the platform). We assume the platform aims to optimize the accuracy of the statistic, and must pick the privacy level of each agent to trade-off between i) incentivizing more participation and ii) adding less noise to the estimate. We provide a semi-closed form characterization of the optimal choice of agent weights for the platform in two variants of our model. In both of these models, we identify a common nontrivial structure in the platform's optimal solution: an instance-specific number of agents with the least stringent privacy requirements are pooled together and given the same weight, while the weights of the remaining agents decrease as a function of the strength of their privacy requirement. We also provide algorithmic results on how to find the optimal value of the noise parameter used by the platform and of the weights given to the agents."
            },
            {
                "arxivId": "2207.04557",
                "title": "Mechanisms that Incentivize Data Sharing in Federated Learning",
                "abstract": "Federated learning is typically considered a beneficial technology which allows 1 multiple agents to collaborate with each other, improve the accuracy of their models, 2 and solve problems which are otherwise too data-intensive / expensive to be solved 3 individually. However, under the expectation that other agents will share their 4 data, rational agents may be tempted to engage in detrimental behavior such as 5 free-riding where they contribute no data but still enjoy an improved model. In 6 this work, we propose a framework to analyze the behavior of such rational data 7 generators. We first show how a naive scheme leads to catastrophic levels of 8 free-riding where the benefits of data sharing are completely eroded. Then, using 9 ideas from contract theory, we introduce accuracy shaping based mechanisms to 10 maximize the amount of data generated by each agent. These provably prevent 11 free-riding without needing any payment mechanism. 12"
            },
            {
                "arxivId": "2201.03968",
                "title": "Optimal and Differentially Private Data Acquisition: Central and Local Mechanisms",
                "abstract": "We consider a platform's problem of collecting data from privacy sensitive users to estimate an underlying parameter of interest. We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her (verifiable) data in exchange for a monetary reward or services, but at the same time has a (private) heterogeneous privacy cost which we quantify using differential privacy. We consider two popular differential privacy settings for providing privacy guarantees for the users: central and local. In both settings, we establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Building on this characterization, we pose the mechanism design problem as the optimal selection of an estimator and payments that will elicit truthful reporting of users' privacy sensitivities. Under a regularity condition on the distribution of privacy sensitivities we develop efficient algorithmic mechanisms to solve this problem in both privacy settings. Our mechanism in the central setting can be implemented in time O (n log n) where n is the number of users and our mechanism in the local setting admits a Polynomial Time Approximation Scheme (PTAS). The full paper is available at: https://arxiv.org/abs/2201.03968"
            },
            {
                "arxivId": "2101.07304",
                "title": "Buying Data Over Time: Approximately Optimal Strategies for Dynamic Data-Driven Decisions",
                "abstract": "We consider a model where an agent has a repeated decision to make and wishes to maximize their total payoff. Payoffs are influenced by an action taken by the agent, but also an unknown state of the world that evolves over time. Before choosing an action each round, the agent can purchase noisy samples about the state of the world. The agent has a budget to spend on these samples, and has flexibility in deciding how to spread that budget across rounds. We investigate the problem of choosing a sampling algorithm that optimizes total expected payoff. For example: is it better to buy samples steadily over time, or to buy samples in batches? We solve for the optimal policy, and show that it is a natural instantiation of the latter. Under a more general model that includes per-round fixed costs, we prove that a variation on this batching policy is a 2-approximation."
            },
            {
                "arxivId": "1912.04774",
                "title": "Voluntary Disclosure and Personalized Pricing",
                "abstract": "A concern central to the economics of privacy is that firms may use consumer data to price discriminate. A common response is that consumers should have control over their data and the ability to choose how firms access it. Since firms draw inferences based on both the data seen as well as the consumer's disclosure choices, the strategic implications of this proposal are unclear. We investigate whether such measures improve consumer welfare in monopolistic and competitive environments. We find that consumer control can guarantee gains for every consumer type relative to both perfect price discrimination and no personalized pricing. This result is driven by two ideas. First, consumers can use disclosure to amplify competition between firms. Second, consumers can share information that induces a seller---even a monopolist---to make price concessions. Furthermore, whether consumer control improves consumer surplus depends on both the technology of disclosure and the competitiveness of the marketplace. In a competitive market, simple disclosure technologies such as \"track / do-not-track'' suffice for guaranteeing gains in consumer welfare. However, in a monopolistic market, welfare gains require richer forms of disclosure technology whereby consumers can decide how much information they would like to convey."
            },
            {
                "arxivId": "2004.03107",
                "title": "The Economics of Social Data",
                "abstract": "A data intermediary pays consumers for information about their preferences and sells the information so acquired to firms that use it to tailor their products and prices. The social dimension of the individual data---whereby an individual's data are predictive of the behavior of others---generates a data externality that reduces the intermediary's cost of acquiring information. We derive the intermediary's optimal data policy and show that it preserves the privacy of the consumers' identities while providing precise information about market demand to the firms. This enables the intermediary to capture the entire value of information as the number of consumers grows large."
            },
            {
                "arxivId": "1811.12655",
                "title": "Prior-free Data Acquisition for Accurate Statistical Estimation",
                "abstract": "We study a data analyst's problem of acquiring data from self-interested individuals to obtain an accurate estimation of some statistic of a population, subject to an expected budget constraint. Each data holder incurs a cost, which is unknown to the data analyst, to acquire and report his data. The cost can be arbitrarily correlated with the data. The data analyst has an expected budget that she can use to incentivize individuals to provide their data. The goal is to design a joint acquisition-estimation mechanism to optimize the performance of the produced estimator, without any prior information on the underlying distribution of cost and data. We investigate two types of estimations: unbiased point estimation and confidence interval estimation. Unbiased estimators: We design a truthful, individually rational, online mechanism to acquire data from individuals and output an unbiased estimator of the population mean when the data analyst has no prior information on the cost-data distribution and individuals arrive in a random order. The performance of this mechanism matches that of the optimal mechanism, which knows the true cost distribution, within a constant factor. The performance of an estimator is evaluated by its variance under the worst-case cost-data correlation. Confidence intervals: We characterize an approximately optimal (within a factor 2) mechanism for obtaining a confidence interval of the population mean when the data analyst knows the true cost distribution at the beginning. This mechanism is efficiently computable. We then design a truthful, individually rational, online algorithm that is only worse than the approximately optimal mechanism by a constant factor. The performance of an estimator is evaluated by its expected length under the worst-case cost-data correlation."
            },
            {
                "arxivId": "1202.4741",
                "title": "Take It or Leave It: Running a Survey When Privacy Comes at a Cost",
                "abstract": null
            },
            {
                "arxivId": "1011.1375",
                "title": "Selling privacy at auction",
                "abstract": "We initiate the study of markets for private data, through the lens of differential privacy. Although the purchase and sale of private data has already begun on a large scale, a theory of privacy as a commodity is missing. In this paper, we propose to build such a theory. Specifically, we consider a setting in which a data analyst wishes to buy information from a population from which he can estimate some statistic. The analyst wishes to obtain an accurate estimate cheaply, while the owners of the private data experience some cost for their loss of privacy, and must be compensated for this loss. Agents are selfish, and wish to maximize their profit, so our goal is to design truthful mechanisms. Our main result is that such problems can naturally be viewed and optimally solved as variants of multi-unit procurement auctions. Based on this result, we derive auctions which are optimal up to small constant factors for two natural settings: When the data analyst has a fixed accuracy goal, we show that an application of the classic Vickrey auction achieves the analyst's accuracy goal while minimizing his total payment. When the data analyst has a fixed budget, we give a mechanism which maximizes the accuracy of the resulting estimate while guaranteeing that the resulting sum payments do not exceed the analyst's budget.\n In both cases, our comparison class is the set of envy-free mechanisms, which correspond to the natural class of fixed-price mechanisms in our setting.\n In both of these results, we ignore the privacy cost due to possible correlations between an individual's private data and his valuation for privacy itself. We then show that generically, no individually rational mechanism can compensate individuals for the privacy loss incurred due to their reported valuations for privacy. This is nevertheless an important issue, and modeling it correctly is one of the many exciting directions for future work."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13278",
        "category": "econ",
        "title": "Determinantes do planejamento estrat\\'egico da rede de uma companhia a\\'erea",
        "abstract": "This work focuses on trying to understand how the construction of an airline's network is made. For this purpose, the case of Azul was studied, investigating which and how factors affect the decision of this airline to enter domestic routes, in addition to analyzing how the merger of Azul with the regional airline Trip affected the company's network planning. For this, an academic study was conducted using an econometric model to understand the airline's entry model. The results show that Azul's business model is based on connecting new destinations, not yet served by its competitors, to one of its hubs, and consistently avoiding routes or airports dominated by other airlines. Regarding the effects of the merger, the results suggest that Azul moved away from its original entry model, based on JetBlue, to a model more oriented towards regional aviation, entering shorter routes and regional airports.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13279",
        "category": "econ",
        "title": "Privatiza\\c{c}\\~ao de aeroportos: motiva\\c{c}\\~oes, regula\\c{c}\\~ao e efici\\^encia operacional",
        "abstract": "In this study, we will address some topics related to the privatization of airports in the scientific literature, in an attempt to provide an answer to the following question: does the privatization of airports bring positive results? Firstly, we turn our attention to the motivations leading to privatization, considering the two main parties involved, the government and the private sector. After all, the success of such a decision will be relative to the reasons that justify it. In a second moment, we will consider the regulatory issue, whose influence on the airport's financial performance is consolidated in the literature. In the third part, we address the main documented results of privatization, with special attention to productive efficiency, whose improvement is the most popular motivation for privatization.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13338",
        "category": "econ",
        "title": "Incentivized Exploration via Filtered Posterior Sampling",
        "abstract": "We study\"incentivized exploration\"(IE) in social learning problems where the principal (a recommendation algorithm) can leverage information asymmetry to incentivize sequentially-arriving agents to take exploratory actions. We identify posterior sampling, an algorithmic approach that is well known in the multi-armed bandits literature, as a general-purpose solution for IE. In particular, we expand the existing scope of IE in several practically-relevant dimensions, from private agent types to informative recommendations to correlated Bayesian priors. We obtain a general analysis of posterior sampling in IE which allows us to subsume these extended settings as corollaries, while also recovering existing results as special cases.",
        "references": [
            {
                "arxivId": "2306.01990",
                "title": "Incentivizing Exploration with Linear Contexts and Combinatorial Actions",
                "abstract": "We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection."
            },
            {
                "arxivId": "2206.00494",
                "title": "Incentivizing Combinatorial Bandit Exploration",
                "abstract": "Consider a bandit algorithm that recommends actions to self-interested users in a recommendation system. The users are free to choose other actions and need to be incentivized to follow the algorithm's recommendations. While the users prefer to exploit, the algorithm can incentivize them to explore by leveraging the information collected from the previous users. All published work on this problem, known as incentivized exploration, focuses on small, unstructured action sets and mainly targets the case when the users' beliefs are independent across actions. However, realistic exploration problems often feature large, structured action sets and highly correlated beliefs. We focus on a paradigmatic exploration problem with structure: combinatorial semi-bandits. We prove that Thompson Sampling, when applied to combinatorial semi-bandits, is incentive-compatible when initialized with a sufficient number of samples of each arm (where this number is determined in advance by the Bayesian prior). Moreover, we design incentive-compatible algorithms for collecting the initial samples."
            },
            {
                "arxivId": "2005.10624",
                "title": "Greedy Algorithm almost Dominates in Smoothed Contextual Bandits",
                "abstract": "Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future. While necessary in the worst case, explicit exploration has a number of disadvantages compared to the greedy algorithm that always \"exploits\" by choosing an action that currently looks optimal. We ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm in the linear contextual bandits model. We improve on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\\tilde O(T^{1/3})$."
            },
            {
                "arxivId": "2002.00558",
                "title": "The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity",
                "abstract": "We consider incentivized exploration: a version of multi-armed bandits where the choice of arms is controlled by self-interested agents, and the algorithm can only issue recommendations. The algorithm controls the flow of information, and the information asymmetry can incentivize the agents to explore. Prior work achieves optimal regret rates up to multiplicative factors that become arbitrarily large depending on the Bayesian priors, and scale exponentially in the number of arms. A more basic problem of sampling each arm once runs into similar factors. We focus on the price of incentives: the loss in performance, broadly construed, incurred for the sake of incentive-compatibility. We prove that Thompson Sampling, a standard bandit algorithm, is incentive-compatible if initialized with sufficiently many data points. The performance loss due to incentives is therefore limited to the initial rounds when these data points are collected. The problem is largely reduced to that of sample complexity: how many rounds are needed? We address this question, providing matching upper and lower bounds and instantiating them in various corollaries. Typically, the optimal sample complexity is polynomial in the number of arms and exponential in the \"strength of beliefs\"."
            },
            {
                "arxivId": "1904.07272",
                "title": "Introduction to Multi-Armed Bandits",
                "abstract": "Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments. The chapters are as follows: stochastic bandits, lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full Feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual Bandits; bandits and games; bandits with knapsacks; bandits and incentives."
            },
            {
                "arxivId": "1811.06026",
                "title": "Incentivizing Exploration with Selective Data Disclosure",
                "abstract": "We study the design of rating systems that incentivize (more) efficient social learning among self-interested agents. Agents arrive sequentially and are presented with a set of possible actions, each of which yields a positive reward with an unknown probability. A disclosure policy sends messages about the rewards of previously-chosen actions to arriving agents. These messages can alter agents' incentives towards exploration, taking potentially sub-optimal actions for the sake of learning more about their rewards. Prior work achieves much progress with disclosure policies that merely recommend an action to each user, without any other supporting information, and sometimes recommend exploratory actions. All this work relies heavily on standard, yet very strong rationality assumptions. However, these assumptions are quite problematic in the context of the motivating applications: recommendation systems such as Yelp, Amazon, or Netflix, and macthing markets such as AirBnB. It is very unclear whether users would know and understand a complicated disclosure policy announced by the principal, let alone trust the principal to faithfully implement it. (The principal may deviate from the announced policy either intentionally, or due to insufficient information about the users, or because of bugs in implementation.) Even if the users understand the policy and trust that it was implemented as claimed, they might not react to it rationally, particularly given the lack of supporting information and the possibility of being singled out for exploration. For example, users may find such disclosure policies unacceptable and leave the system. We study a particular class of disclosure policies that use messages, called unbiased subhistories, consisting of the actions and rewards from a subsequence of past agents. Each subsequence is chosen ahead of time, according to a predetermined partial order on the rounds. We posit a flexible model of frequentist agent response, which we argue is plausible for this class of \"order-based\" disclosure policies. We measure the performance of a policy by its regret, i.e., the difference in expected total reward between the best action and the policy. A disclosure policy that reveals full history in each round risks inducing herding behavior among the agents, and typically has regret linear in the time horizon T. Our main result is an order-based disclosure policy that obtains regret ~O (\u221aT). This regret is known to be optimal in the worst case over reward distributions, even absent incentives. We also exhibit simpler order-based policies with higher, but still sublinear, regret. These policies can be interpreted as dividing a sublinear number of agents into constant-sized focus groups, whose histories are then revealed to future agents. Helping market participants find whatever they are looking for, and coordinating their search and exploration behavior in a globally optimal way, is an essential part of market design. This paper continues the line of work on \"incentivized exploration\": essentially, exploration-exploitation learning in the presence of self-interested users whose incentives are skewed in favor of exploitation. Conceptually, we study the interplay of information design, social learning, and multi-armed bandit algorithms. To the best of our knowledge, this is the first paper in the literature on incentivized exploration (and possibly in the broader literature on \"learning and incentives\") which attempts to mitigate the limitations of standard economic assumptions. Full version: https://arxiv.org/abs/1811.06026."
            },
            {
                "arxivId": "1707.02038",
                "title": "A Tutorial on Thompson Sampling",
                "abstract": "Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms."
            },
            {
                "arxivId": "1704.09011",
                "title": "Mostly Exploration-Free Algorithms for Contextual Bandits",
                "abstract": "The contextual bandit literature has traditionally focused on algorithms that address the exploration\u2013exploitation tradeoff. In particular, greedy algorithms that exploit current estimates without any exploration may be suboptimal in general. However, exploration-free greedy algorithms are desirable in practical settings where exploration may be costly or unethical (e.g., clinical trials). Surprisingly, we find that a simple greedy algorithm can be rate optimal (achieves asymptotically optimal regret) if there is sufficient randomness in the observed contexts (covariates). We prove that this is always the case for a two-armed bandit under a general class of context distributions that satisfy a condition we term covariate diversity. Furthermore, even absent this condition, we show that a greedy algorithm can be rate optimal with positive probability. Thus, standard bandit algorithms may unnecessarily explore. Motivated by these results, we introduce Greedy-First, a new algorithm that uses only observed contexts and rewards to determine whether to follow a greedy algorithm or to explore. We prove that this algorithm is rate optimal without any additional assumptions on the context distribution or the number of arms. Extensive simulations demonstrate that Greedy-First successfully reduces exploration and outperforms existing (exploration-based) contextual bandit algorithms such as Thompson sampling or upper confidence bound. This paper was accepted by J. George Shanthikumar, big data analytics."
            },
            {
                "arxivId": "1507.07191",
                "title": "Economic Recommendation Systems",
                "abstract": "In the on-line Explore and Exploit literature, central to Machine Learning, a central planner is faced with a set of alternatives, each yielding some unknown reward. The planner's goal is to learn the optimal alternative as soon as possible, via experimentation. A typical assumption in this model is that the planner has full control over the experiment design and implementation. When experiments are implemented by a society of self-motivated agents the planner can only recommend experimentation but has no power to enforce it. Kremer et al (JPE, 2014) introduce the first study of explore and exploit schemes that account for agents' incentives. In their model it is implicitly assumed that agents do not see nor communicate with each other. Their main result is a characterization of an optimal explore and exploit scheme. In this work we extend Kremer et al (JPE, 2014) by adding a layer of a social network according to which agents can observe each other. It turns out that when observability is factored in the scheme proposed by Kremer et al (JPE, 2014) is no longer incentive compatible. In our main result we provide a tight bound on how many other agents can each agent observe and still have an incentive-compatible algorithm and asymptotically optimal outcome. More technically, for a setting with N agents where the number of nodes with degree greater than N^alpha is bounded by N^beta and 2*alpha+beta < 1 we construct incentive-compatible asymptotically optimal mechanism. The bound 2*alpha+beta < 1 is shown to be tight."
            },
            {
                "arxivId": "1502.04147",
                "title": "Bayesian Incentive-Compatible Bandit Exploration",
                "abstract": "Individual decision-makers consume information revealed by the previous decision makers, and produce information that may help in future decision makers. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as elsewhere, such as medical decisions. Each decision maker when required to select an action, would individually prefer to exploit, select the highest expected reward action conditional on her information. At the same time, each decision maker would prefer previous decision makers to explore, producing information about the rewards of various actions. A social planner, by means of carefully designed information disclosure, can incentivize the agents to balance the exploration and exploitation, and maximize social welfare. We formulate this problem as a multi-arm bandit problem (and various generalizations thereof) under incentive-compatibility constraints induced by agents' Bayesian priors. We design an incentive-compatible bandit algorithm for the social planner with asymptotically optimal regret. Further, we provide a black-box reduction from an arbitrary multi-arm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit settings, even ones that incorporate contexts and arbitrary partial feedback."
            },
            {
                "arxivId": "1403.5341",
                "title": "An Information-Theoretic Analysis of Thompson Sampling",
                "abstract": "We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance."
            },
            {
                "arxivId": "1301.2609",
                "title": "Learning to Optimize via Posterior Sampling",
                "abstract": "This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multiarmed bandit problems. The algorithm, also known as Thompson Sampling and as probability matching, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB..."
            },
            {
                "arxivId": "1110.2392",
                "title": "A Variant of Azuma's Inequality for Martingales with Subgaussian Tails",
                "abstract": "We provide a variant of Azuma's concentration inequality for martingales, in which the standard boundedness requirement is replaced by the milder requirement of a subgaussian tail."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13375",
        "category": "econ",
        "title": "A Strategic Model of Software Dependency Networks",
        "abstract": "Modern software development involves collaborative efforts and reuse of existing code, which reduces the cost of developing new software. However, reusing code from existing packages exposes coders to vulnerabilities in these dependencies. We study the formation of dependency networks among software packages and libraries, guided by a structural model of network formation with observable and unobservable heterogeneity. We estimate costs, benefits, and link externalities of the network of 696,790 directed dependencies between 35,473 repositories of the Rust programming language using a novel scalable algorithm. We find evidence of a positive externality exerted on other coders when coders create dependencies. Furthermore, we show that coders are likely to link to more popular packages of the same software type but less popular packages of other types. We adopt models for the spread of infectious diseases to measure a package's systemicness as the number of downstream packages a vulnerability would affect. Systemicness is highly skewed with the most systemic repository affecting almost 90% of all repositories only two steps away. Lastly, we show that protecting only the ten most important repositories reduces vulnerability contagion by nearly 40%.",
        "references": [
            {
                "arxivId": "2305.16458",
                "title": "Effective Vaccination Strategies in Network-based SIR Model",
                "abstract": "Controlling and understanding epidemic outbreaks has recently drawn great interest in a large spectrum of research communities. Vaccination is one of the most well-established and effective strategies in order to contain an epidemic. In the present study, we investigate a network-based virus-spreading model building on the popular SIR model. Furthermore, we examine the efficacy of various vaccination strategies in preventing the spread of infectious diseases and maximizing the survival ratio. The experimented strategies exploit a wide range of approaches such as relying on network structure centrality measures, focusing on disease-spreading parameters, and a combination of both. Our proposed hybrid algorithm, which combines network centrality and illness factors, is found to perform better than previous strategies in terms of lowering the final death ratio in the community on various real-world networks and synthetic graph models. Our findings particularly emphasize the significance of taking both network structure properties and disease characteristics into account when devising effective vaccination strategies."
            },
            {
                "arxivId": "2107.00375",
                "title": "A semiparametric Bayesian approach to epidemics, with application to the spread of the coronavirus MERS in South Korea in 2015",
                "abstract": "We consider incomplete observations of stochastic processes governing the spread of infectious diseases through finite populations by way of contact. We propose a flexible semiparametric modelling framework with at least three advantages. First, it enables researchers to study the structure of a population contact network and its impact on the spread of infectious diseases. Second, it can accommodate short- and long-tailed degree distributions and detect potential superspreaders, who represent an important public health concern. Third, it addresses the important issue of incomplete data. Starting from first principles, we show when the incomplete-data generating process is ignorable for the purpose of Bayesian inference for the parameters of the population model. We demonstrate the semiparametric modelling framework by simulations and an application to the partially observed MERS epidemic in South Korea in 2015. We conclude with an extended discussion of open questions and directions for future research."
            },
            {
                "arxivId": "2105.12704",
                "title": "A Structural Model of Business Card Exchange Networks",
                "abstract": "Social and professional networks affect labor market dynamics, knowledge diffusion and new business creation. To understand the determinants of how these networks are formed in the first place, we analyze a unique dataset of business card exchanges among a sample of over 240,000 users of the multi-platform contact management and professional social networking tool for individuals Eight. We develop a structural model of network formation with strategic interactions, and we estimate users\u2019 payoffs that depend on the composition of business relationships, as well as indirect business interactions. We allow heterogeneity of users in both observable and unobservable characteristics to affect how relationships form and are maintained. The model\u2019s stationary equilibrium delivers a likelihood that is a mixture of exponential random graph models that we can characterize in closed-form. We overcome several econometric and computational challenges in estimation, by exploiting a two-step estimation procedure, variational approximations and minorization-maximization methods. Our algorithm is scalable, highly parallelizable and makes efficient use of computer memory to allow estimation in massive networks. We show that users payoffs display homophily in several dimensions, e.g. location; furthermore, users unobservable characteristics also display homophily."
            },
            {
                "arxivId": "2012.07167",
                "title": "Pseudo-likelihood-based $M$-estimation of random graphs with dependent edges and parameter vectors of increasing dimension.",
                "abstract": "An important question in statistical network analysis is how to construct and estimate models of dependent network data without sacrificing computational scalability and statistical guarantees. We demonstrate that scalable estimation of random graph models with dependent edges is possible, by establishing the first consistency results and convergence rates for pseudo-likelihood-based $M$-estimators for parameter vectors of increasing dimension based on a single observation of dependent random variables. The main results cover models of dependent random variables with countable sample spaces, and may be of independent interest. To showcase consistency results and convergence rates, we introduce a novel class of generalized $\\beta$-models with dependent edges and parameter vectors of increasing dimension.We establish consistency results and convergence rates for pseudo-likelihood-based $M$-estimators of generalized $\\beta$-models with dependent edges, in dense- and sparse-graph settings."
            },
            {
                "arxivId": "1912.06346",
                "title": "Network Data",
                "abstract": "Many economic activities are embedded in networks: sets of agents and the (often) rivalrous relationships connecting them to one another. Input sourcing by firms, interbank lending, scientific research, and job search are four examples, among many, of networked economic activities. Motivated by the premise that networks' structures are consequential, this chapter describes econometric methods for analyzing them. I emphasize (i) dyadic regression analysis incorporating unobserved agent-specific heterogeneity and supporting causal inference, (ii) techniques for estimating, and conducting inference on, summary network parameters (e.g., the degree distribution or transitivity index); and (iii) empirical models of strategic network formation admitting interdependencies in preferences. Current research challenges and open questions are also discussed."
            },
            {
                "arxivId": "1902.09217",
                "title": "Small World with High Risks: A Study of Security Threats in the npm Ecosystem",
                "abstract": "The popularity of JavaScript has lead to a large ecosystem of third-party packages available via the npm software package registry. The open nature of npm has boosted its growth, providing over 800,000 free and reusable software packages. Unfortunately, this open nature also causes security risks, as evidenced by recent incidents of single packages that broke or attacked software running on millions of computers. This paper studies security risks for users of npm by systematically analyzing dependencies between packages, the maintainers responsible for these packages, and publicly reported security issues. Studying the potential for running vulnerable or malicious code due to third-party dependencies, we find that individual packages could impact large parts of the entire ecosystem. Moreover, a very small number of maintainer accounts could be used to inject malicious code into the majority of all packages, a problem that has been increasing over time. Studying the potential for accidentally using vulnerable code, we find that lack of maintenance causes many packages to depend on vulnerable code, even years after a vulnerability has become public. Our results provide evidence that npm suffers from single points of failure and that unmaintained packages threaten large code bases. We discuss several mitigation techniques, such as trusted maintainers and total first-party security, and analyze their potential effectiveness."
            },
            {
                "arxivId": "1710.04936",
                "title": "An empirical comparison of dependency network evolution in seven software packaging ecosystems",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13378",
        "category": "econ",
        "title": "Stable matching as transportation",
        "abstract": "We study matching markets with aligned preferences and establish a connection between common design objectives -- stability, efficiency, and fairness -- and the theory of optimal transport. Optimal transport gives new insights into the structural properties of matchings obtained from pursuing these objectives, and into the trade-offs between different objectives. Matching markets with aligned preferences provide a tractable stylized model capturing supply-demand imbalances in a range of settings such as partnership formation, school choice, organ donor exchange, and markets with transferable utility where bargaining over transfers happens after a match is formed.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13439",
        "category": "econ",
        "title": "Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada",
        "abstract": "This paper investigates the demand for lamb, beef, pork, and poultry in Canada, both at the national level and in disaggregated provinces, to identify meat consumption patterns in different provinces. Meat consumption plays a significant role in Canada's economy and is an important source of calories for the population. However, meat demand faces several consumption challenges due to logistic constraints, as a significant portion of the supply is imported from other countries. Therefore, there is a need for a better understanding of the causal relationships underlying lamb, beef, pork, and poultry consumption in Canada. Until recently, there have been no attempts to estimate meat consumption at the provincial level in Canada. Different Almost Ideal Demand System (AIDS) models have been applied for testing specifications to circumvent several econometric and theoretical problems. In particular, generalized AIDS and its Quadratic extension QUAIDS methods have been estimated across each province using the Iterative Linear Least Squares Estimator (ILLE) estimation Method. Weekly retail meat consumption price and quantity data from 2019 to 2022 have been used for Canada and for each province namely Quebec, Maritime provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario, total West (Yukon, Northwest Territory and Nunavut), Alberta, Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent coefficients and demand elasticities estimates reveal patterns of substitution and/or complementarity between the four categories of meat. Meat consumption patterns differ across each province. Results show that the demand for the four categories of meat is responsive to price changes. Overall, lamb expenditure was found to be elastic and thus considered a luxury good during the study period, while the other three categories are considered normal goods across Canada.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13789",
        "category": "econ",
        "title": "The seasonality of air ticket prices before and after the pandemic (preprint)",
        "abstract": "This study investigates price seasonality in the Brazilian air transport industry, emphasizing the impact of the COVID-19 pandemic on domestic airline pricing strategies. Given potential shifts in demand patterns following the global health crisis, this study explores possible long-term structural changes in the seasonality of Brazilian airfare. We analyze an open dataset of domestic city pairs from 2013 to 2023, employing an econometric model developed using Stata software. Our findings indicate alterations in seasonal patterns and long-term trends in the post-pandemic era. These changes underscore potential shifts in the composition of leisure and business travelers, along with the cost pressures faced by airlines.",
        "references": [
            {
                "arxivId": "2402.07124",
                "title": "Econometric analysis to estimate the impact of holidays on airfares",
                "abstract": "The number of air transportation passengers during the holidays in Brazil has grown notably since the late nineties. One of the reasons is greater competition in airfares made possible by economic liberalization. This paper presents an econometric model of airline pricing aiming at estimating the impacts of holiday periods on fares, with special emphasis on three-day holiday events. It makes use of a database with daily collected data from the internet between 2008 and 2010 for the major Brazilian city, Sao Paulo. The econometric panel data model employs a two-way error components \u201cwithin\u201d estimator, controlling for airline/airport-pair fixed effect along with quotation and departure months effects. The decomposition of time effects between quotation and departure month effects is the main methodological contribution of the paper. Results allow for a comparative analysis of the performance of Sao Paulo\u2019s downtown and international airports - respectively, Congonhas (CGH), and Guarulhos (GRU) airports. As a result, the price of tickets bought 60 days in advance for flights with two stops leaving from the downtown airport fell by most."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-22.json",
        "arxivId": "2402.13807",
        "category": "econ",
        "title": "Offshoring emissions through used vehicle exports",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2111.11963",
        "category": "econ",
        "title": "Affirmative Action's Cumulative Fractional Assignments",
        "abstract": "The Central Educational Institutions (Reservation in Teachers' Cadre) Act, 2019 provides for reserving teaching vacancies in India's central educational institutions for beneficiaries of its affirmative action policy. Reservation of teaching vacancies had been a contentious issue, and the act was introduced to resolve it after the Supreme Court's solution was met with protests from the Teachers' Union. Our paper demonstrates an impossibility result in the Supreme Court's solution and the act, which are flawed in reserving seats simultaneously at both the university and within its departments. To overcome this impossibility, we propose an alternative solution based on approximate implementation of fractional assignments, offering a promising middle-ground between the two disputed solutions practiced in India. This novel application demonstrates the practical relevance of the approximate implementation approach (Akbarpourand Nikzad(2020)) beyond the constraint structures examined in the literature.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2202.04706",
        "category": "econ",
        "title": "Stable allocations in discrete exchange economies",
        "abstract": "We study stable allocations in an exchange economy with indivisible goods. The problem is well-known to be challenging, and rich enough to encode fundamentally unstable economies, such as the roommate problem. Our approach stems from generalizing the original study of an exchange economy with unit demand and unit endowments, the \\emph{housing model}. Our first approach uses Scarf's theorem, and proposes sufficient conditions under which a ``convexify then round'' technique ensures that the core is nonempty. The upshot is that a core allocation exists in categorical economies with dichotomous preferences. Our second approach uses a generalization of the TTC: it works under general conditions, and finds a solution that is a version of the stable set.",
        "references": [
            {
                "arxivId": "2010.02618",
                "title": "A Faster Algorithm for Finding Tarski Fixed Points",
                "abstract": "Dang et al. have given an algorithm that can find a Tarski fixed point in a k-dimensional lattice of width n using O(log k n) queries [2]. Multiple authors have conjectured that this algorithm is optimal [2, 7], and indeed this has been proven for two-dimensional instances [7]. We show that these conjectures are false in dimension three or higher by giving an O(log2 n) query algorithm for the three-dimensional Tarski problem. We also give a new decomposition theorem for k-dimensional Tarski problems which, in combination with our new algorithm for three dimensions, gives an O(log2 \u2308k/3\u2309 n) query algorithm for the k-dimensional problem."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2301.12163",
        "category": "econ",
        "title": "Fair congested assignment problem",
        "abstract": "We propose a fair and efficient solution for assigning agents to m posts subject to congestion, when agents care about both their post and its congestion. Examples include assigning jobs to busy servers, students to crowded schools or crowded classes, commuters to congested routes, workers to crowded office spaces or to team projects etc... Congestion is anonymous (it only depends on the number n of agents in a given post). A canonical interpretation of ex ante fairness allows each agent to choose m post-specific caps on the congestion they tolerate: these requests are mutually feasible if and only if the sum of the caps is n. For ex post fairness we impose a competitive requirement close to envy freeness: taking the congestion profile as given each agent is assigned to one of her best posts. If a competitive assignment exists, it delivers unique congestion and welfare profiles and is also efficient and ex ante fair. In a fractional (randomised or time sharing) version of our model, a unique competitive congestion profile always exists. It is approximately implemented by a mixture of ex post deterministic assignments: with an approxination factor equal to the largest utility loss from one more unit of congestion, the latter deliver identical welfare profiles and are weakly efficient. Our approach to ex ante fairness generalises to the model where each agent's congestion is weighted. Now the caps on posts depend only upon own weight and total congestion, not on the number of other agents contributing to it. Remarkably in both models these caps are feasible if and only if they give to each agent the right to veto all but (1/m) of their feasible allocations.",
        "references": [
            {
                "arxivId": "2312.07431",
                "title": "Algorithms and Complexity for Congested Assignments",
                "abstract": "We study the congested assignment problem as introduced by Bogomolnaia and Moulin (2023). We show that deciding whether a competitive assignment exists can be done in polynomial time, while deciding whether an envy-free assignment exists is NP-complete."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2303.09680",
        "category": "econ",
        "title": "Bootstrap based asymptotic refinements for high-dimensional nonlinear models",
        "abstract": "We consider penalized extremum estimation of a high-dimensional, possibly nonlinear model that is sparse in the sense that most of its parameters are zero but some are not. We use the SCAD penalty function, which provides model selection consistent and oracle efficient estimates under suitable conditions. However, asymptotic approximations based on the oracle model can be inaccurate with the sample sizes found in many applications. This paper gives conditions under which the bootstrap, based on estimates obtained through SCAD penalization with thresholding, provides asymptotic refinements of size \\(O \\left( n^{- 2} \\right)\\) for the error in the rejection (coverage) probability of a symmetric hypothesis test (confidence interval) and \\(O \\left( n^{- 1} \\right)\\) for the error in the rejection (coverage) probability of a one-sided or equal tailed test (confidence interval). The results of Monte Carlo experiments show that the bootstrap can provide large reductions in errors in rejection and coverage probabilities. The bootstrap is consistent, though it does not necessarily provide asymptotic refinements, even if some parameters are close but not equal to zero. Random-coefficients logit and probit models and nonlinear moment models are examples of models to which the procedure applies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2305.00545",
        "category": "econ",
        "title": "Optimal multi-action treatment allocation: A two-phase field experiment to boost immigrant naturalization",
        "abstract": "Research underscores the role of naturalization in enhancing immigrants' socio-economic integration, yet application rates remain low. We estimate a policy rule for a letter-based information campaign encouraging newly eligible immigrants in Zurich, Switzerland, to naturalize. The policy rule assigns one out of three treatment letters to each individual, based on their observed characteristics. We field the policy rule to one-half of 1,717 immigrants, while sending random treatment letters to the other half. Despite only moderate treatment effect heterogeneity, the policy tree yields a larger, albeit insignificant, increase in application rates compared to assigning the same letter to everyone.",
        "references": [
            {
                "arxivId": "2012.04055",
                "title": "Who should get vaccinated? Individualized allocation of vaccines over SIR network",
                "abstract": null
            },
            {
                "arxivId": "1912.08726",
                "title": "Econometrics for Decision Making: Building Foundations Sketched by Haavelmo and Wald",
                "abstract": "Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes use of statistical decision theory to evaluate the performance of models in decision making. I consider the common practice of \n as\u2010if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as\u2010if optimization or any other model\u2010based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancing computation is the primary task to complete the foundations sketched by Haavelmo and Wald.\n"
            },
            {
                "arxivId": "1810.13237",
                "title": "Machine Learning Estimation of Heterogeneous Causal Effects: Empirical Monte Carlo Evidence",
                "abstract": "\n We investigate the finite-sample performance of causal machine learning estimators for heterogeneous causal effects at different aggregation levels. We employ an empirical Monte Carlo study that relies on arguably realistic data generation processes (DGPs) based on actual data in an observational setting. We consider 24 DGPs, eleven causal machine learning estimators, and three aggregation levels of the estimated effects. Four of the considered estimators perform consistently well across all DGPs and aggregation levels. These estimators have multiple steps to account for the selection into the treatment and the outcome process."
            },
            {
                "arxivId": "1810.04778",
                "title": "Offline Multi-Action Policy Learning: Generalization and Optimization",
                "abstract": "As a result of digitization of the economy, more and more decision makers from a wide range of domains have gained the ability to target products, services, and information provision based on individual characteristics. Examples include selecting offers, prices, advertisements, or emails to send to consumers, choosing a bid to submit in a contextual first-price auctions, and determining which medication to prescribe to a patient. The key to enabling this is to learn a treatment policy from historical observational data in a sample-efficient way, hence uncovering the best personalized treatment choice recommendation. In \u201cOffline Policy Learning: Generalization and Optimization,\u201d Z. Zhou, S. Athey, and S. Wager provide a sample-optimal policy learning algorithm that is computationally efficient and that learns a tree-based treatment policy from observational data. In our quest toward fully automated personalization, the work provides a theoretically sound and practically implementable approach."
            },
            {
                "arxivId": "1709.10279",
                "title": "Heterogeneous Employment Effects of Job Search Programs",
                "abstract": "We systematically investigate the effect heterogeneity of job search programs for unemployed workers. To investigate possibly heterogeneous employment effects, we combine nonexperimental causal empirical models with Lassotype estimators. The empirical analyses are based on rich administrative data from Swiss social security records. We find considerable heterogeneities during the first six months after the start of training. Consistent with previous results in the literature, unemployed persons with fewer employment opportunities profit more from participating in these programs. Finally, we show the potential of easy-to-implement program participation rules for improving average employment effects of these active labor market programs."
            },
            {
                "arxivId": "1706.03461",
                "title": "Metalearners for estimating heterogeneous treatment effects using machine learning",
                "abstract": "Significance Estimating and analyzing heterogeneous treatment effects is timely, yet challenging. We introduce a unifying framework for many conditional average treatment effect estimators, and we propose a metalearner, the X-learner, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect. We present its favorable properties, using theory and simulations. We apply it, using random forests, to two field experiments in political science, where it is shown to be easy to use and to produce results that are interpretable. There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms\u2014such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks\u2014to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods."
            },
            {
                "arxivId": "1610.01271",
                "title": "Generalized random forests",
                "abstract": "We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN."
            },
            {
                "arxivId": "1510.04342",
                "title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests",
                "abstract": "ABSTRACT Many scientific and engineering challenges\u2014ranging from personalized medicine to customized marketing recommendations\u2014require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman\u2019s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates."
            },
            {
                "arxivId": "1305.6099",
                "title": "Inference on Treatment Effects after Selection Amongst High-Dimensional Controls",
                "abstract": "In this supplementary appendix we provide additional results, omitted proofs and extensive simulations that complement the analysis of the main text (arXiv:1201.0224)."
            },
            {
                "arxivId": "1103.4601",
                "title": "Doubly Robust Policy Evaluation and Learning",
                "abstract": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. \nIn this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2312.15198",
        "category": "econ",
        "title": "Do LLM Agents Exhibit Social Behavior?",
        "abstract": "The advances of Large Language Models (LLMs) are expanding their utility in both academic research and practical applications. Recent social science research has explored the use of these ``black-box'' LLM agents for simulating complex social systems and potentially substituting human subjects in experiments. Our study delves into this emerging domain, investigating the extent to which LLMs exhibit key social interaction principles, such as social learning, social preference, and cooperative behavior (indirect reciprocity), in their interactions with humans and other agents. We develop a framework for our study, wherein classical laboratory experiments involving human subjects are adapted to use LLM agents. This approach involves step-by-step reasoning that mirrors human cognitive processes and zero-shot learning to assess the innate preferences of LLMs. Our analysis of LLM agents' behavior includes both the primary effects and an in-depth examination of the underlying mechanisms. Focusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a range of human-like social behaviors such as distributional and reciprocity preferences, responsiveness to group identity cues, engagement in indirect reciprocity, and social learning capabilities. However, our analysis also reveals notable differences: LLMs demonstrate a pronounced fairness preference, weaker positive reciprocity, and a more calculating approach in social learning compared to humans. These insights indicate that while LLMs hold great promise for applications in social science research, such as in laboratory experiments and agent-based modeling, the subtle behavioral differences between LLM agents and humans warrant further investigation. Careful examination and development of protocols in evaluating the social behaviors of LLMs are necessary before directly applying these models to emulate human behavior.",
        "references": [
            {
                "arxivId": "2311.04076",
                "title": "Do LLMs exhibit human-like response biases? A case study in survey design",
                "abstract": "As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of\"prompts\"have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey"
            },
            {
                "arxivId": "2305.16867",
                "title": "Playing repeated games with Large Language Models",
                "abstract": "Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM's cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4's behavior can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice. These results enrich our understanding of LLM's social behavior and pave the way for a behavioral game theory for machines."
            },
            {
                "arxivId": "2303.11504",
                "title": "Language Model Behavior: A Comprehensive Survey",
                "abstract": "Abstract Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models."
            },
            {
                "arxivId": "2303.06074",
                "title": "Susceptibility to Influence of Large Language Models",
                "abstract": "Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence."
            },
            {
                "arxivId": "2210.13966",
                "title": "The debate over understanding in AI\u2019s large language models",
                "abstract": "We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language\u2014and the physical and social situations language encodes\u2014in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition."
            },
            {
                "arxivId": "2209.14338",
                "title": "Who is GPT-3? An exploration of personality, values and demographics",
                "abstract": "Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa."
            },
            {
                "arxivId": "2209.06899",
                "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
                "abstract": "Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one such tool\u2014the GPT-3 language model\u2014is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create \u201csilicon samples\u201d by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."
            },
            {
                "arxivId": "2208.10264",
                "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."
            },
            {
                "arxivId": "2206.14576",
                "title": "Using cognitive psychology to understand GPT-3",
                "abstract": "Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations."
            },
            {
                "arxivId": "2206.02336",
                "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%)."
            },
            {
                "arxivId": "1910.03771",
                "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2401.11422",
        "category": "econ",
        "title": "Local Identification in Instrumental Variable Multivariate Quantile Regression Models",
        "abstract": "The instrumental variable (IV) quantile regression model introduced by Chernozhukov and Hansen (2005) is a useful tool for analyzing quantile treatment effects in the presence of endogeneity, but when outcome variables are multidimensional, it is silent on the joint distribution of different dimensions of each variable. To overcome this limitation, we propose an IV model built on the optimal-transport-based multivariate quantile that takes into account the correlation between the entries of the outcome variable. We then provide a local identification result for the model. Surprisingly, we find that the support size of the IV required for the identification is independent of the dimension of the outcome vector, as long as the IV is sufficiently informative. Our result follows from a general identification theorem that we establish, which has independent theoretical significance.",
        "references": [
            {
                "arxivId": "math/0601086",
                "title": "On the second boundary value problem for Monge-Amp\u00e8re type equations and optimal transportation",
                "abstract": "This paper is concerned with the existence of globally smooth so- lutions for the second boundary value problem for certain Monge-Amp` ere type equations and the application to regularity of potentials in optimal transportation. In particular we address the fundamental issue of determining conditions on costs and domains to ensure that optimal mappings are smooth diffeomorphisms. The cost functions satisfy a weak form of the condition (A3), which was introduced in a recent paper with Xi-nan Ma, in conjunction with interior regularity. Our condition is optimal and includes the quadratic cost function case of Caffarelli and Urbas as well as the various examples in our previous work. The approach is through the derivation of global estimates for second derivatives of solutions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.09721",
        "category": "econ",
        "title": "Persuading a Learning Agent",
        "abstract": "We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bounded by the agent's regret (swap-regret). If the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can do significantly better than the non-learning model. These conclusions hold not only for Bayesian persuasion, but also for any generalized principal-agent problem with complete information, including Stackelberg games and contract design.",
        "references": [
            {
                "arxivId": "2402.09549",
                "title": "Pareto-Optimal Algorithms for Learning in Games",
                "abstract": "We study the problem of characterizing optimal learning algorithms for playing repeated games against an adversary with unknown payoffs. In this problem, the first player (called the learner) commits to a learning algorithm against a second player (called the optimizer), and the optimizer best-responds by choosing the optimal dynamic strategy for their (unknown but well-defined) payoff. Classic learning algorithms (such as no-regret algorithms) provide some counterfactual guarantees for the learner, but might perform much more poorly than other learning algorithms against particular optimizer payoffs. In this paper, we introduce the notion of asymptotically Pareto-optimal learning algorithms. Intuitively, if a learning algorithm is Pareto-optimal, then there is no other algorithm which performs asymptotically at least as well against all optimizers and performs strictly better (by at least $\\Omega(T)$) against some optimizer. We show that well-known no-regret algorithms such as Multiplicative Weights and Follow The Regularized Leader are Pareto-dominated. However, while no-regret is not enough to ensure Pareto-optimality, we show that a strictly stronger property, no-swap-regret, is a sufficient condition for Pareto-optimality. Proving these results requires us to address various technical challenges specific to repeated play, including the fact that there is no simple characterization of how optimizers who are rational in the long-term best-respond against a learning algorithm over multiple rounds of play. To address this, we introduce the idea of the asymptotic menu of a learning algorithm: the convex closure of all correlated distributions over strategy profiles that are asymptotically implementable by an adversary. We show that all no-swap-regret algorithms share the same asymptotic menu, implying that all no-swap-regret algorithms are ``strategically equivalent''."
            },
            {
                "arxivId": "2307.04175",
                "title": "Selling to Multiple No-Regret Buyers",
                "abstract": "We consider the problem of repeatedly auctioning a single item to multiple i.i.d buyers who each use a no-regret learning algorithm to bid over time. In particular, we study the seller's optimal revenue, if they know that the buyers are no-regret learners (but only that their behavior satisfies some no-regret property -- they do not know the precise algorithm/heuristic used). Our main result designs an auction that extracts revenue equal to the full expected welfare whenever the buyers are\"mean-based\"(a property satisfied by standard no-regret learning algorithms such as Multiplicative Weights, Follow-the-Perturbed-Leader, etc.). This extends a main result of [BMSW18] which held only for a single buyer. Our other results consider the case when buyers are mean-based but never overbid. On this front, [BMSW18] provides a simple LP formulation for the revenue-maximizing auction for a single-buyer. We identify several formal barriers to extending this approach to multiple buyers."
            },
            {
                "arxivId": "2207.08253",
                "title": "Rationality-Robust Information Design: Bayesian Persuasion under Quantal Response",
                "abstract": "Classic mechanism/information design imposes the assumption that agents are fully rational, meaning each of them always selects the action that maximizes her expected utility. Yet many empirical evidence suggests that human decisions may deviate from this full rationality assumption. In this work, we attempt to relax the full rationality assumption with bounded rationality. Specifically, we formulate the bounded rationality of an agent by adopting the quantal response model (McKelvey and Palfrey, 1995). We develop a theory of rationality-robust information design in the canonical setting of Bayesian persuasion (Kamenica and Gentzkow, 2011) with binary receiver action. We first identify conditions under which the optimal signaling scheme structure for a fully rational receiver remains optimal or approximately optimal for a boundedly rational receiver. In practice, it might be costly for the designer to estimate the degree of the receiver's bounded rationality level. Motivated by this practical consideration, we then study the existence and construction of robust signaling schemes when there is uncertainty about the receiver's bounded rationality level."
            },
            {
                "arxivId": "2202.10678",
                "title": "Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning",
                "abstract": "In today's economy, it becomes important for Internet platforms to consider the sequential information design problem to align its long term interest with incentives of the gig service providers (e.g., drivers, hosts). This paper proposes a novel model of sequential information design, namely the Markov persuasion processes (MPPs), in which a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximize the sender's cumulative utilities in a finite horizon Markovian environment with varying prior and utility functions. Planning in MPPs thus faces the unique challenge in finding a signaling policy that is simultaneously persuasive to the myopic receivers and inducing the optimal long-term cumulative utilities of the sender. Nevertheless, in the population level where the model is known, it turns out that we can efficiently determine the optimal (resp. \u03b5-optimal) policy with finite (resp. infinite) states and outcomes, through a modified formulation of the Bellman equation that additionally takes persuasiveness into consideration. Our main technical contribution is to study the MPP under the online reinforcement learning (RL) setting, where the goal is to learn the optimal signaling policy by interacting with with the underlying MPP, without the knowledge of the sender's utility functions, prior distributions, and the Markov transition kernels. For such a problem, we design a provably efficient no-regret learning algorithm, the Optimism-Pessimism Principle for Persuasion Process (OP4), which features a novel combination of both optimism and pessimism principles. In particular, we obtain optimistic estimates of the value functions to encourage exploration under the unknown environment, and additionally robustify the signaling policy with respect to the uncertainty of prior estimation to prevent receiver's detrimental equilibrium behavior. Our algorithm enjoys sample efficiency by achieving a sublinear \u221aT-regret upper bound. Furthermore, both our algorithm and theory can be applied to MPPs with large space of outcomes and states via function approximation, and we showcase such a success under the linear setting."
            },
            {
                "arxivId": "2202.06135",
                "title": "Online Bayesian Recommendation with No Regret",
                "abstract": "We introduce and study the online Bayesian recommendation problem for a platform, who can observe a utility-relevant state of a product, repeatedly interacting with a population of myopic users through an online recommendation mechanism. This paradigm is common in a wide range of scenarios in the current Internet economy. For each user with her own private preference and belief, the platform commits to a recommendation strategy to utilize his information advantage on the product state to persuade the self-interested user to follow the recommendation. The platform does not know user's preferences and beliefs, and has to use an adaptive recommendation strategy to persuade with gradually learning user's preferences and beliefs in the process. We aim to design online learning policies with no Stackelberg regret for the platform, i.e., against the optimum policy in hindsight under the assumption that users will correspondingly adapt their behaviors to the benchmark policy. Our first result is an online policy that achieves double logarithm regret dependence on the number of rounds. We then present a hardness result showing that no adaptive online policy can achieve regret with better dependency on the number of rounds. Finally, by formulating the platform's problem as optimizing a linear program with membership oracle access, we present our second online policy that achieves regret with polynomial dependence on the number of states but logarithm dependence on the number of rounds."
            },
            {
                "arxivId": "2105.13870",
                "title": "Regret-Minimizing Bayesian Persuasion",
                "abstract": "We study a Bayesian persuasion setting with binary actions (adopt and reject) for Receiver. We examine the following question - how well can Sender perform, in terms of persuading Receiver to adopt, when ignorant of Receiver's utility? We take a robust (adversarial) approach to study this problem; that is, our goal is to design signaling schemes for Sender that perform well for all possible Receiver's utilities. We measure performance of signaling schemes via the notion of (additive) regret: the difference between Sender's hypothetically optimal utility had she known Receiver's utility function and her actual utility induced by the given scheme. On the negative side, we show that if Sender has no knowledge at all about Receiver's utility, then Sender has no signaling scheme that performs robustly well. On the positive side, we show that if Sender only knows Receiver's ordinal preferences of the states of nature - i.e., Receiver's utility upon adoption is monotonic as a function of the state - then Sender can guarantee a surprisingly low regret even when the number of states tends to infinity. In fact, we exactly pin down the minimum regret value that Sender can guarantee in this case, which turns out to be at most 1/e. We further show that such positive results are not possible under the alternative performance measure of a multiplicative approximation ratio by proving that no constant ratio can be guaranteed even for monotonic Receiver's utility; this may serve to demonstrate the merits of regret as a robust performance measure that is not too pessimistic. Finally, we analyze an intermediate setting in between the no-knowledge and the ordinal-knowledge settings."
            },
            {
                "arxivId": "2102.10156",
                "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
                "abstract": "We study a repeated persuasion setting between a sender and a receiver, where at each time t, the sender shares information about a payoff-relevant state with the receiver. The state at each time t is drawn independently and identically from an unknown distribution, and subsequent to receiving information about it, the receiver (myopically) chooses an action from a finite set. The sender seeks to persuade the receiver into choosing actions that are aligned with her preference by selectively sharing information about the state. In contrast to the standard persuasion setting, we focus on the case where neither the sender nor the receiver knows the distribution of the payoff relevant state. Instead, the sender learns this distribution over time by observing the state realizations. We adopt the assumption common in the literature on Bayesian persuasion that at each time period, prior to observing the realized state in that period, the sender commits to a signaling mechanism that maps each state to a possibly random action recommendation. Subsequent to the state observation, the sender recommends an action as per the chosen signaling mechanism."
            },
            {
                "arxivId": "2009.05518",
                "title": "Mechanisms for a No-Regret Agent: Beyond the Common Prior",
                "abstract": "A rich class of mechanism design problems can be understood as incomplete-information games between a principal who commits to a policy and an agent who responds, with payoffs determined by an unknown state of the world. Traditionally, these models require strong and often-impractical assumptions about beliefs (a common prior over the state). In this paper, we dispense with the common prior. Instead, we consider a repeated interaction where both the principal and the agent may learn over time from the state history. We reformulate mechanism design as a reinforcement learning problem and develop mechanisms that attain natural benchmarks without any assumptions on the state-generating process. Our results make use of novel behavioral assumptions for the agent - based on counterfactual internal regret - that capture the spirit of rationality without relying on beliefs. 11For the full version of this paper, see https://arxiv.org/abs/2009.05518."
            },
            {
                "arxivId": "1909.13861",
                "title": "Strategizing against No-regret Learners",
                "abstract": "How should a player who repeatedly plays a game against a no-regret learner strategize to maximize his utility? We study this question and show that under some mild assumptions, the player can always guarantee himself a utility of at least what he would get in a Stackelberg equilibrium. When the no-regret learner has only two actions, we show that the player cannot get any higher utility than the Stackelberg equilibrium utility. But when the no-regret learner has more than two actions and plays a mean-based no-regret strategy, we show that the player can get strictly higher than the Stackelberg equilibrium utility. We construct the optimal game-play for the player against a mean-based no-regret learner who has three actions. When the no-regret learner's strategy also guarantees him a no-swap regret, we show that the player cannot get anything higher than a Stackelberg equilibrium utility."
            },
            {
                "arxivId": "2208.03758",
                "title": "Persuading Risk-Conscious Agents: A Geometric Approach",
                "abstract": "A Convex Programming Framework for Information Design Under Realistic Human Behavior Platform markets and services typically have additional relevant information in comparison with their users. Information design studies how the sharing of this information can be leveraged by the platform to influence user behavior and obtain desirable outcomes. Previous research has studied information design assuming that the users act to maximize their expected utility, but this assumption does not always hold in reality. Instead, people often exhibit biases and deviations from expected utility maximization. In \u201cPersuading Risk-Conscious Agents: A Geometric Approach,\u201d Anunrojwong, Iyer, and Lingenbrink study information design with \u201crisk-conscious\u201d agents whose utility functions may depend nonlinearly on their beliefs. They provide a convex programming approach for solving for the optimal persuasion mechanism and establish their structural properties in different settings. They illustrate their approach in an application involving the sharing of waiting-time information in a queueing system. Overall, this work contributes to the study of information design under realistic models of human behavior."
            },
            {
                "arxivId": "1505.00720",
                "title": "Econometrics for Learning Agents",
                "abstract": "The main goal of this paper is to develop a theory of inference of player valuations from observed data in the generalized second price auction without relying on the Nash equilibrium assumption. Existing work in Economics on inferring agent values from data relies on the assumption that all participant strategies are best responses of the observed play of other players, i.e. they constitute a Nash equilibrium. In this paper, we show how to perform inference relying on a weaker assumption instead: assuming that players are using some form of no-regret learning. Learning outcomes emerged in recent years as an attractive alternative to Nash equilibrium in analyzing game outcomes, modeling players who haven't reached a stable equilibrium, but rather use algorithmic learning, aiming to learn the best way to play from previous observations. In this paper we show how to infer values of players who use algorithmic learning strategies. Such inference is an important first step before we move to testing any learning theoretic behavioral model on auction data. We apply our techniques to a dataset from Microsoft's sponsored search ad auction system."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.11209",
        "category": "econ",
        "title": "When Simple is Near-Optimal in Security Games",
        "abstract": "Fraudulent or illegal activities are ubiquitous across applications and involve users bypassing the rule of law, often with the strategic aim of obtaining some benefit that would otherwise be unattainable within the bounds of lawful conduct. However, user fraud is detrimental, as it may compromise safety or impose disproportionate negative externalities on particular population groups. To mitigate the potential harms of user fraud, we study the problem of policing such fraud as a security game between an administrator and users. In this game, an administrator deploys $R$ security resources (e.g., police officers) across $L$ locations and levies fines against users engaging in fraud at those locations. For this security game, we study both welfare and revenue maximization administrator objectives. In both settings, we show that computing the optimal administrator strategy is NP-hard and develop natural greedy algorithm variants for the respective settings that achieve at least half the welfare or revenue as the welfare-maximizing or revenue-maximizing solutions, respectively. We also establish a resource augmentation guarantee that our proposed greedy algorithms with one extra resource, i.e., $R+1$ resources, achieve at least the same welfare (revenue) as the welfare-maximizing (revenue-maximizing) outcome with $R$ resources. Finally, since the welfare and revenue-maximizing solutions can differ significantly, we present a framework inspired by contract theory, wherein a revenue-maximizing administrator is compensated through contracts for the welfare it contributes. Beyond extending our theoretical results in the welfare and revenue maximization settings to studying equilibrium strategies in the contract game, we also present numerical experiments highlighting the efficacy of contracts in bridging the gap between the revenue and welfare-maximizing administrator outcomes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.11372",
        "category": "econ",
        "title": "Low costs na aviacao: importancia e desdobramentos",
        "abstract": "This study aims to discuss the impacts of a low-cost airline on the air transport market and, especially, to present the most recent findings from specialized literature in the field. To this end, various works on this topic, published since 2015, were selected and analyzed. From this analysis, it was possible to categorize the main topics discussed in the papers into five groups: (i) the impacts of a low-cost airline on competing airlines; (ii) impacts on airports; (iii) general impacts on the demand for air transport; (iv) effects on passengers' choice process; and (v) general effects on a geographical region.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.14090",
        "category": "econ",
        "title": "Social Environment Design",
        "abstract": "Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.",
        "references": [
            {
                "arxivId": "2305.12162",
                "title": "A Scalable Neural Network for DSIC Affine Maximizer Auction Design",
                "abstract": "Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both contextual and non-contextual multi-item auctions, scales well to larger auctions, generalizes well to different settings, and identifies useful deterministic allocations. Overall, our proposed approach offers an effective solution to automated DSIC auction design, with improved scalability and strong revenue performance in various settings."
            },
            {
                "arxivId": "2303.09500",
                "title": "Enabling First-Order Gradient-Based Learning for Equilibrium Computation in Markets",
                "abstract": "Understanding and analyzing markets is crucial, yet analytical equilibrium solutions remain largely infeasible. Recent breakthroughs in equilibrium computation rely on zeroth-order policy gradient estimation. These approaches commonly suffer from high variance and are computationally expensive. The use of fully differentiable simulators would enable more efficient gradient estimation. However, the discrete allocation of goods in economic simulations is a non-differentiable operation. This renders the first-order Monte Carlo gradient estimator inapplicable and the learning feedback systematically misleading. We propose a novel smoothing technique that creates a surrogate market game, in which first-order methods can be applied. We provide theoretical bounds on the resulting bias which justifies solving the smoothed game instead. These bounds also allow choosing the smoothing strength a priori such that the resulting estimate has low variance. Furthermore, we validate our approach via numerous empirical experiments. Our method theoretically and empirically outperforms zeroth-order methods in approximation quality and computational efficiency."
            },
            {
                "arxivId": "2205.14953",
                "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem",
                "abstract": "Large sequence model (SM) such as GPT series and BERT has displayed outstanding performance and generalization capabilities on vision, language, and recently reinforcement learning tasks. A natural follow-up question is how to abstract multi-agent decision making into an SM problem and benefit from the prosperous development of SMs. In this paper, we introduce a novel architecture named Multi-Agent Transformer (MAT) that effectively casts cooperative multi-agent reinforcement learning (MARL) into SM problems wherein the task is to map agents' observation sequence to agents' optimal action sequence. Our goal is to build the bridge between MARL and SMs so that the modeling power of modern sequence models can be unleashed for MARL. Central to our MAT is an encoder-decoder architecture which leverages the multi-agent advantage decomposition theorem to transform the joint policy search problem into a sequential decision making process; this renders only linear time complexity for multi-agent problems and, most importantly, endows MAT with monotonic performance improvement guarantee. Unlike prior arts such as Decision Transformer fit only pre-collected offline data, MAT is trained by online trials and errors from the environment in an on-policy fashion. To validate MAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo, Dexterous Hands Manipulation, and Google Research Football benchmarks. Results demonstrate that MAT achieves superior performance and data efficiency compared to strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that MAT is an excellent few-short learner on unseen tasks regardless of changes in the number of agents. See our project page at https://sites.google.com/view/multi-agent-transformer."
            },
            {
                "arxivId": "2106.07877",
                "title": "Learning Revenue-Maximizing Auctions With Differentiable Matching",
                "abstract": "We propose a new architecture to approximately learn incentive compatible, revenue-maximizing auctions from sampled valuations. Our architecture uses the Sinkhorn algorithm to perform a differentiable bipartite matching which allows the network to learn strategyproof revenue-maximizing mechanisms in settings not learnable by the previous RegretNet architecture. In particular, our architecture is able to learn mechanisms in settings without free disposal where each bidder must be allocated exactly some number of items. In experiments, we show our approach successfully recovers multiple known optimal mechanisms and high-revenue, low-regret mechanisms in larger settings where the optimal mechanism is unknown."
            },
            {
                "arxivId": "2106.02195",
                "title": "Celebrating Diversity in Shared Multi-Agent Reinforcement Learning",
                "abstract": "Recently, deep multi-agent reinforcement learning (MARL) has shown the promise to solve complex cooperative tasks. Its success is partly because of parameter sharing among agents. However, such sharing may lead agents to behave similarly and limit their coordination capacity. In this paper, we aim to introduce diversity in both optimization and representation of shared multi-agent reinforcement learning. Specifically, we propose an information-theoretical regularization to maximize the mutual information between agents' identities and their trajectories, encouraging extensive exploration and diverse individualized behaviors. In representation, we incorporate agent-specific modules in the shared neural network architecture, which are regularized by L1-norm to promote learning sharing among agents while keeping necessary diversity. Empirical results show that our method achieves state-of-the-art performance on Google Research Football and super hard StarCraft II micromanagement tasks."
            },
            {
                "arxivId": "2010.01523",
                "title": "RODE: Learning Roles to Decompose Multi-Agent Tasks",
                "abstract": "Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy -- the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 10 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents. Demonstrative videos are available at this https URL ."
            },
            {
                "arxivId": "2006.04109",
                "title": "Incorporating Pragmatic Reasoning Communication into Emergent Language",
                "abstract": "Emergentism and pragmatics are two research fields that study the dynamics of linguistic communication along substantially different timescales and intelligence levels. From the perspective of multi-agent reinforcement learning, they correspond to stochastic games with reinforcement training and stage games with opponent awareness. Given that their combination has been explored in linguistics, we propose computational models that combine short-term mutual reasoning-based pragmatics with long-term language emergentism. We explore this for agent communication referential games as well as in Starcraft II, assessing the relative merits of different kinds of mutual reasoning pragmatics models both empirically and theoretically. Our results shed light on their importance for making inroads towards getting more natural, accurate, robust, fine-grained, and succinct utterances."
            },
            {
                "arxivId": "1910.05366",
                "title": "Learning Nearly Decomposable Value Functions Via Communication Minimization",
                "abstract": "Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between agents' action selection and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows us to cut off more than $80\\%$ of communication without sacrificing the performance. The videos of our experiments are available at this https URL."
            },
            {
                "arxivId": "1910.00091",
                "title": "Deep Coordination Graphs",
                "abstract": "This paper introduces the deep coordination graph (DCG) for collaborative multi-agent reinforcement learning. DCG strikes a flexible trade-off between representational capacity and generalization by factorizing the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks and parameter sharing improves generalization over the state-action space. We show that DCG can solve challenging predator-prey tasks that are vulnerable to the relative overgeneralization pathology and in which all other known value factorization approaches fail."
            },
            {
                "arxivId": "1905.05408",
                "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning",
                "abstract": "We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively."
            },
            {
                "arxivId": "1901.09216",
                "title": "Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning",
                "abstract": "Though limited in real-world decision making, most multi-agent reinforcement learning (MARL) models assume perfectly rational agents -- a property hardly met due to individual's cognitive limitation and/or the tractability of the decision problem. In this paper, we introduce generalized recursive reasoning (GR2) as a novel framework to model agents with different \\emph{hierarchical} levels of rationality; our framework enables agents to exhibit varying levels of ``thinking'' ability thereby allowing higher-level agents to best respond to various less sophisticated learners. We contribute both theoretically and empirically. On the theory side, we devise the hierarchical framework of GR2 through probabilistic graphical models and prove the existence of a perfect Bayesian equilibrium. Within the GR2, we propose a practical actor-critic solver, and demonstrate its convergent property to a stationary point in two-player games through Lyapunov analysis. On the empirical side, we validate our findings on a variety of MARL benchmarks. Precisely, we first illustrate the hierarchical thinking process on the Keynes Beauty Contest, and then demonstrate significant improvements compared to state-of-the-art opponent modeling baselines on the normal-form games and the cooperative navigation benchmark."
            },
            {
                "arxivId": "1810.00147",
                "title": "M^3RL: Mind-aware Multi-agent Management Reinforcement Learning",
                "abstract": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly controlling the agents to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not wish to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is maximizing the overall productivity as well as minimizing payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation."
            },
            {
                "arxivId": "1812.09755",
                "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks",
                "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability."
            },
            {
                "arxivId": "1805.03382",
                "title": "Automated Mechanism Design via Neural Networks",
                "abstract": "Using AI approaches to automatically design mechanisms has been a central research mission at the interface of AI and economics. Previous approaches that attempt to design revenue optimal auctions for the multi-dimensional settings fall short in at least one of the three aspects: 1) representation --- search in a space that probably does not even contain the optimal mechanism; 2) exactness --- finding a mechanism that is either not truthful or far from optimal; 3) domain dependence --- need a different design for different environment settings. To resolve the three difficulties, in this paper, we put forward a unified neural network based framework that automatically learns to design revenue optimal mechanisms. Our framework consists of a mechanism network that takes an input distribution for training and outputs a mechanism, as well as a buyer network that takes a mechanism as input and output an action. Such a separation in design mitigates the difficulty to impose incentive compatibility constraints on the mechanism, by making it a rational choice of the buyer. As a result, our framework easily overcomes the previously mentioned difficulty in incorporating IC constraints and always returns exactly incentive compatible mechanisms. We then applied our framework to a number of multi-item auction design settings, for a few of which the theoretically optimal mechanisms are unknown. We then go on to theoretically prove that the mechanisms found by our framework are indeed optimal."
            },
            {
                "arxivId": "1803.11485",
                "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
                "abstract": "In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods."
            },
            {
                "arxivId": "1707.06347",
                "title": "Proximal Policy Optimization Algorithms",
                "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
            },
            {
                "arxivId": "1707.06600",
                "title": "A multi-agent reinforcement learning model of common-pool resource appropriation",
                "abstract": "Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality."
            },
            {
                "arxivId": "1707.02174",
                "title": "Methods for finding leader-follower equilibria with multiple followers",
                "abstract": "The concept of leader--follower (or Stackelberg) equilibrium plays a central role in a number of real--world applications of game theory. While the case with a single follower has been thoroughly investigated, results with multiple followers are only sporadic and the problem of designing and evaluating computationally tractable equilibrium-finding algorithms is still largely open. In this work, we focus on the fundamental case where multiple followers play a Nash equilibrium once the leader has committed to a strategy---as we illustrate, the corresponding equilibrium finding problem can be easily shown to be $\\mathcal{FNP}$--hard and not in Poly--$\\mathcal{APX}$ unless $\\mathcal{P} = \\mathcal{NP}$ and therefore it is one among the hardest problems to solve and approximate. We propose nonconvex mathematical programming formulations and global optimization methods to find both exact and approximate equilibria, as well as a heuristic black box algorithm. All the methods and formulations that we introduce are thoroughly evaluated computationally."
            },
            {
                "arxivId": "1706.02275",
                "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
                "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies."
            },
            {
                "arxivId": "1705.08926",
                "title": "Counterfactual Multi-Agent Policy Gradients",
                "abstract": "\n \n Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.\n \n"
            },
            {
                "arxivId": "1506.02438",
                "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
                "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. \nOur approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.14189",
        "category": "econ",
        "title": "Optimal transmission expansion minimally reduces decarbonization costs of U.S. electricity",
        "abstract": "Solar and wind power are cost-competitive with fossil fuels, yet their intermittent nature presents challenges. Significant temporal and geographic differences in land, wind, and solar resources suggest that long-distance transmission could be particularly beneficial. Using a detailed, open-source model, we analyze optimal transmission expansion jointly with storage, generation, and hourly operations across the three primary interconnects in the United States. Transmission expansion offers far more benefits in a high-renewable system than in a system with mostly conventional generation. Yet while an optimal nationwide plan would have more than triple current interregional transmission, transmission decreases the cost of a 100% clean system by only 4% compared to a plan that relies solely on current transmission. Expanding capacity only within existing interconnects can achieve most of these savings. Adjustments to energy storage and generation mix can leverage the current interregional transmission infrastructure to build a clean power system at a reasonable cost.",
        "references": [
            {
                "arxivId": "1804.05481",
                "title": "Switch 2.0: A modern platform for planning high-renewable power systems",
                "abstract": null
            },
            {
                "arxivId": "1709.05716",
                "title": "Response to \u2018Burden of proof: A comprehensive review of the feasibility of 100% renewable-electricity systems\u2019",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.14206",
        "category": "econ",
        "title": "The impact of Facebook-Cambridge Analytica data scandal on the USA tech stock market: An event study based on clustering method",
        "abstract": "This study delves into the intra-industry effects following a firm-specific scandal, with a particular focus on the Facebook data leakage scandal and its associated events within the U.S. tech industry and two additional relevant groups. We employ various metrics including daily spread, volatility, volume-weighted return, and CAPM-beta for the pre-analysis clustering, and subsequently utilize CAR (Cumulative Abnormal Return) to evaluate the impact on firms grouped within these clusters. From a broader industry viewpoint, significant positive CAARs are observed across U.S. sample firms over the three days post-scandal announcement, indicating no adverse impact on the tech sector overall. Conversely, after Facebook's initial quarterly earnings report, it showed a notable negative effect despite reported positive performance. The clustering principle should aid in identifying directly related companies and thus reducing the influence of randomness. This was indeed achieved for the effect of the key event, namely\"The Effect of Congressional Hearing on Certain Clusters across U.S. Tech Stock Market,\"which was identified as delayed and significantly negative. Therefore, we recommend applying the clustering method when conducting such or similar event studies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.14269",
        "category": "econ",
        "title": "Optimal Mechanism in a Dynamic Stochastic Knapsack Environment",
        "abstract": "This study introduces an optimal mechanism in a dynamic stochastic knapsack environment. The model features a single seller who has a fixed quantity of a perfectly divisible item. Impatient buyers with a piece-wise linear utility function arrive randomly and they report the two-dimensional private information: marginal value and demanded quantity. We derive a revenue-maximizing dynamic mechanism in a finite discrete time framework that satisfies incentive compatibility, individual rationality, and feasibility conditions. This is achieved by characterizing buyers' utility and utilizing the Bellman equation. Moreover, we establish the essential penalty scheme for incentive compatibility, as well as the allocation and payment policies. Lastly, we propose algorithms to approximate the optimal policy, based on the Monte Carlo simulation-based regression method and reinforcement learning.",
        "references": [
            {
                "arxivId": "2209.11934",
                "title": "The Online Knapsack Problem with Departures",
                "abstract": "The online knapsack problem is a classic online resource allocation problem in networking and operations research. Its basic version studies how to pack online arriving items of different sizes and values into a capacity-limited knapsack. In this paper, we study a general version that includes item departures, while also considering multiple knapsacks and multi-dimensional item sizes. We design a threshold-based online algorithm and prove that the algorithm can achieve order-optimal competitive ratios. Beyond worst-case performance guarantees, we also aim to achieve near-optimal average performance under typical instances. Towards this goal, we propose a data-driven online algorithm that learns within a policy-class that guarantees a worst-case performance bound. In trace-driven experiments, we show that our data-driven algorithm outperforms other benchmark algorithms in an application of online knapsack to job scheduling for cloud computing."
            },
            {
                "arxivId": "1509.02971",
                "title": "Continuous control with deep reinforcement learning",
                "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."
            },
            {
                "arxivId": "1502.06934",
                "title": "An optimal bidimensional multi-armed bandit auction for multi-unit procurement",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-23.json",
        "arxivId": "2402.14506",
        "category": "econ",
        "title": "Enhancing Rolling Horizon Production Planning Through Stochastic Optimization Evaluated by Means of Simulation",
        "abstract": "Production planning must account for uncertainty in a production system, arising from fluctuating demand forecasts. Therefore, this article focuses on the integration of updated customer demand into the rolling horizon planning cycle. We use scenario-based stochastic programming to solve capacitated lot sizing problems under stochastic demand in a rolling horizon environment. This environment is replicated using a discrete event simulation-optimization framework, where the optimization problem is periodically solved, leveraging the latest demand information to continually adjust the production plan. We evaluate the stochastic optimization approach and compare its performance to solving a deterministic lot sizing model, using expected demand figures as input, as well as to standard Material Requirements Planning (MRP). In the simulation study, we analyze three different customer behaviors related to forecasting, along with four levels of shop load, within a multi-item and multi-stage production system. We test a range of significant parameter values for the three planning methods and compute the overall costs to benchmark them. The results show that the production plans obtained by MRP are outperformed by deterministic and stochastic optimization. Particularly, when facing tight resource restrictions and rising uncertainty in customer demand, the use of stochastic optimization becomes preferable compared to deterministic optimization.",
        "references": [
            {
                "arxivId": "1812.00773",
                "title": "Effects of forecast errors on optimal utilisation in aggregate production planning with stochastic customer demand",
                "abstract": "The hierarchical structure of production planning has the advantage of assigning different decision variables to their respective time horizons and therefore ensures their manageability. However, the restrictive structure of this top-down approach implying that upper level decisions are the constraints for lower level decisions also has its shortcomings. One problem that occurs is that deterministic mixed integer decision problems are often used for long-term planning, but the real production system faces a set of stochastic influences. Therefore, a planned utilisation factor has to be included into this deterministic aggregate planning problem. In practice, this decision is often based on past data and not consciously taken. In this paper, the effect of long-term forecast error on the optimal planned utilisation factor is evaluated for a production system facing stochastic demand and the benefit of exploiting this decision\u2019s potential is discussed. Overall costs including capacity, backorder and inventory costs, are determined with simulation for different multi-stage and multi-item production system structures. The results show that the planned utilisation factor used in the aggregate planning problem has a high influence on optimal costs. Additionally, the negative effect of forecast errors is evaluated and discussed in detail for different production system environments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-26.json",
        "arxivId": "2105.03405",
        "category": "econ",
        "title": "Dynamic tariffs-based demand response in retail electricity market under uncertainty",
        "abstract": "Demand response (DR) programs play a crucial role in improving system reliability and mitigating price volatility by altering the core profile of electricity consumption. This paper proposes a game-theoretical model that captures the dynamic interplay between retailers (leaders) and consumers (followers) in a tariffs-based electricity market under uncertainty. The proposed procedure offers theoretical and economic insights by analyzing demand flexibility within a hierarchical decision-making framework. In particular, two main market configurations are examined under uncertainty: i) there exists a retailer that exercises market power over consumers, and ii) the retailer and the consumers participate in a perfect competitive game. The former case is formulated as a mathematical program with equilibrium constraints (MPEC), whereas the latter case is recast as a mixed-integer linear program (MILP). These problems are solved by deriving equivalent tractable reformulations based on the Karush-Kuhn-Tucker (KKT) optimality conditions of each agent's problem. Numerical simulations based on real data from the European Energy Exchange platform are used to illustrate the performance of the proposed methodology. The results indicate that the proposed model effectively characterizes the interactions between retailers and flexible consumers in both perfect and imperfect market structures. Under perfect competition, the economic benefits extend not only to consumers but also to overall social welfare. Conversely, in an imperfect market, retailers leverage consumer flexibility to enhance their expected profits, transferring the risk of uncertainty to end-users. Additionally, the degree of consumer flexibility and their valuation of electricity consumption play significant roles in shaping market outcomes.",
        "references": [
            {
                "arxivId": "2201.09927",
                "title": "Contract design in electricity markets with high penetration of renewables: A two-stage approach",
                "abstract": null
            },
            {
                "arxivId": "2104.15062",
                "title": "Contracts in electricity markets under EU ETS: A stochastic programming approach",
                "abstract": null
            },
            {
                "arxivId": "1809.10448",
                "title": "Solving Linear Bilevel Problems Using Big-Ms: Not All That Glitters Is Gold",
                "abstract": "The most common procedure to solve a linear bilevel problem in the PES community is, by far, to transform it into an equivalent single-level problem by replacing the lower level with its KKT optimality conditions. Then, the complementarity conditions are reformulated using binary variables and large enough constants (big-Ms) to cast the single-level problem as a mixed-integer linear program that can be solved using optimization software. In most cases, such large constants are tuned by trial and error. We show, through a counterexample, that this widely used trial-and-error approach may lead to highly suboptimal solutions. Then, further research is required to properly select big-M values to solve linear bilevel problems."
            },
            {
                "arxivId": "1601.05678",
                "title": "Achieving an optimal trade-off between revenue and energy peak within a smart grid environment",
                "abstract": null
            },
            {
                "arxivId": "1508.01982",
                "title": "JuMP: A Modeling Language for Mathematical Optimization",
                "abstract": "JuMP is an open-source modeling language that allows users to express a wide range of optimization problems (linear, mixed-integer, quadratic, conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic syntax. JuMP takes advantage of advanced features of the Julia programming language to offer unique functionality while achieving performance on par with commercial modeling tools for standard tasks. In this work we will provide benchmarks, present the novel aspects of the implementation, and discuss how JuMP can be extended to new problem classes and composed with state-of-the-art tools for visualization and interactivity."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-26.json",
        "arxivId": "2301.11554",
        "category": "econ",
        "title": "Heat and worker health.",
        "abstract": null,
        "references": [
            {
                "arxivId": "1903.01690",
                "title": "Fast Poisson estimation with high-dimensional fixed effects",
                "abstract": "In this article, we present ppmlhdfe, a new command for estimation of (pseudo-)Poisson regression models with multiple high-dimensional fixed effects (HDFE). Estimation is implemented using a modified version of the iteratively reweighted least-squares algorithm that allows for fast estimation in the presence of HDFE. Because the code is built around the reghdfe package (Correia, 2014, Statistical Software Components S457874, Department of Economics, Boston College), it has similar syntax, supports many of the same functionalities, and benefits from reghdfe\u2018s fast convergence properties for computing high-dimensional leastsquares problems. Performance is further enhanced by some new techniques we introduce for accelerating HDFE iteratively reweighted least-squares estimation specifically. ppmlhdfe also implements a novel and more robust approach to check for the existence of (pseudo)maximum likelihood estimates."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-26.json",
        "arxivId": "2304.02989",
        "category": "econ",
        "title": "Covert learning and disclosure",
        "abstract": "I study a model of information acquisition and transmission in which the sender's ability to misreport her findings is limited. In equilibrium, the sender only influences the receiver by choosing to remain selectively ignorant, rather than by deceiving her about the discoveries. Although deception does not occur, I highlight how deception possibilities determine what information the sender chooses to acquire and transmit. I then turn to comparative statics, characterizing in which sense the sender benefits from her claims being more verifiable, showing this is akin to increasing her commitment power. Finally, I characterize sender- and receiver-optimal falsification environments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-26.json",
        "arxivId": "2307.06684",
        "category": "econ",
        "title": "The Heterogeneous Earnings Impact of Job Loss Across Workers, Establishments, and Markets",
        "abstract": "Using generalized random forests and rich Swedish administrative data, we show that the earnings effects of job displacement due to establishment closures are extremely heterogeneous across and within (observable) worker types, establishments, and markets. The decile with the largest predicted effects loses 50 percent of annual earnings the year after displacement and losses accumulate to 200 percent over 7 years. The least affected decile experiences only marginal losses of 6 percent in the year after displacement. Prior to displacement workers in the most affected decile were lower paid and had negative earnings trajectories. Workers with large predicted effects are more sensitive to adverse market conditions than other workers. When restricting attention to simple targeting rules, the subgroup consisting of older workers in routine-task intensive jobs has the highest predictable effects of displacement.",
        "references": [
            {
                "arxivId": "2111.07966",
                "title": "Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects",
                "abstract": "There are a number of available methods for selecting whom to prioritize for treatment, including ones based on treatment effect estimation, risk scoring, and hand-crafted rules. We propose rank-weighted average treatment effect (RATE) metrics as a simple and general family of metrics for comparing and testing the quality of treatment prioritization rules. RATE metrics are agnostic as to how the prioritization rules were derived, and only assess how well they identify individuals that benefit the most from treatment. We define a family of RATE estimators and prove a central limit theorem that enables asymptotically exact inference in a wide variety of randomized and observational study settings. RATE metrics subsume a number of existing metrics, including the Qini coefficient, and our analysis directly yields inference methods for these metrics. We showcase RATE in the context of a number of applications, including optimal targeting of aspirin to stroke patients."
            },
            {
                "arxivId": "1712.04912",
                "title": "Quasi-oracle estimation of heterogeneous treatment effects",
                "abstract": "\n Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines."
            },
            {
                "arxivId": "1610.01271",
                "title": "Generalized random forests",
                "abstract": "We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-26.json",
        "arxivId": "2402.15072",
        "category": "econ",
        "title": "Impacts of Extreme Heat on Labor Force Dynamics",
        "abstract": "We use daily longitudinal data and a within-worker identification approach to examine the impacts of heat on labor force dynamics in Australia. High temperatures during 2001-2019 significantly reduced work attendance and hours worked, which were not compensated for in subsequent days and weeks. The largest reductions occurred in cooler regions and recent years, and were not solely concentrated amongst outdoor-based workers. Financial and Insurance Services was the most strongly affected industry, with temperatures above 38{\\deg}C (100{\\deg}F) increasing absenteeism by 15 percent. Adverse heat effects during the work commute and during outdoor work hours are shown to be key mechanisms.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-26.json",
        "arxivId": "2402.15418",
        "category": "econ",
        "title": "Reputational Algorithm Aversion",
        "abstract": "People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called\"algorithm aversion\". This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "1807.05737",
        "category": "econ",
        "title": "Consumption Smoothing in the Working-Class Households of Interwar Japan",
        "abstract": "I analyze factory worker households in the early 1920s in Osaka to examine idiosyncratic income shocks and consumption. Using the household-level monthly panel dataset, I find that while households could not fully cope with idiosyncratic income shocks at that time, they mitigated fluctuations in indispensable consumption during economic hardship. In terms of risk-coping mechanisms, I find suggestive evidence that savings institutions helped mitigate vulnerabilities and that both using borrowing institutions and adjusting labor supply served as risk-coping strategies among households with less savings.",
        "references": [
            {
                "arxivId": "1905.04419",
                "title": "The role of pawnshops in risk coping in early twentieth-century Japan",
                "abstract": "This study examines the role of pawnshops as a risk-coping device in Japan in the early twentieth century, when the poor were very vulnerable to unexpected shocks such as illness. In contrast to European countries, Japanese pawnshops were the primary financial institution for low-income people up to the 1920s. Using data on pawnshop loans for more than 250 municipalities and exploiting the 1918\u201320 influenza pandemic as a natural experiment, we find that the adverse health shock increased the total amount of loans from pawnshops. This is because those who regularly relied on pawnshops borrowed more money from them than usual, and not because the number of people who used pawnshops increased. Our estimation results indicate that pawnshop loan amounts increased by approximately 7\u201310 percent due to the pandemic. These findings suggest that pawnshop loans were widely used as a risk-coping strategy."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "1902.07260",
        "category": "econ",
        "title": "The preference lattice.",
        "abstract": "Most comparisons of preferences have the structure of single-crossing dominance. We examine the lattice structure of single-crossing dominance, proving characterisation, existence and uniqueness results for minimum upper bounds of arbitrary sets of preferences. We apply these theorems to monotone comparative statics, ambiguity- and risk-aversion, social choice, and politically correct discourse.",
        "references": [
            {
                "arxivId": "1010.5311",
                "title": "Diamond-free families",
                "abstract": null
            },
            {
                "arxivId": "math/0005267",
                "title": "Stochastic monotonicity and realizable monotonicity",
                "abstract": "We explore and relate two notions of monotonicity, stochastic and realizable, for a system of probability measures on a common finite partially ordered set (poset) f when the measures are indexed by another poset A. We give counterexamples to show that the two notions are not always equivalent, but for various large classes of f we also present conditions on the poset A that are necessary and sufficient for equivalence. When A = f, the condition that the cover graph of f have no cycles is necessary and sufficient for equivalence. This case arises in comparing applicability of the perfect sampling algorithms of Propp and Wilson and the first author of the present paper."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "1908.08600",
        "category": "econ",
        "title": "Online Causal Inference for Advertising in Real-Time Bidding Auctions",
        "abstract": "Real-time bidding (RTB) systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. Assessing the effectiveness of such advertising remains a challenge in research and practice. This paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. Leveraging the economic structure of first- and second-price auctions, we first show that the effects of advertising are identified by the optimal bids. Hence, since these optimal bids are the only objects that need to be recovered, we introduce an adapted Thompson sampling (TS) algorithm to solve a multi-armed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising while minimizing the costs of experimentation. We derive a regret bound for our algorithm which is order optimal and use data from RTB auctions to show that it outperforms commonly used methods that estimate the effects of advertising.",
        "references": [
            {
                "arxivId": "2003.09795",
                "title": "Optimal No-regret Learning in Repeated First-price Auctions",
                "abstract": "We study online learning in repeated first-price auctions with censored feedback, where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid in order to maximize her cumulative payoff. To achieve this goal, the bidder faces a challenging dilemma: if she wins the bid--the only way to achieve positive payoffs--then she is not able to observe the highest bid of the other bidders, which we assume is iid drawn from an unknown distribution. This dilemma, despite being reminiscent of the exploration-exploitation trade-off in contextual bandits, cannot directly be addressed by the existing UCB or Thompson sampling algorithms in that literature, mainly because contrary to the standard bandits setting, when a positive reward is obtained here, nothing about the environment can be learned. \nIn this paper, by exploiting the structural properties of first-price auctions, we develop the first learning algorithm that achieves $O(\\sqrt{T}\\log^2 T)$ regret bound when the bidder's private values are stochastically generated. We do so by providing an algorithm on a general class of problems, which we call monotone group contextual bandits, where the same regret bound is established under stochastically generated contexts. Further, by a novel lower bound argument, we characterize an $\\Omega(T^{2/3})$ lower bound for the case where the contexts are adversarially generated, thus highlighting the impact of the contexts generation mechanism on the fundamental learning limit. Despite this, we further exploit the structure of first-price auctions and develop a learning algorithm that operates sample-efficiently (and computationally efficiently) in the presence of adversarially generated private values. We establish an $O(\\sqrt{T}\\log^3 T)$ regret bound for this algorithm, hence providing a complete characterization of optimal learning guarantees for this problem."
            },
            {
                "arxivId": "1911.02768",
                "title": "Confidence intervals for policy evaluation in adaptive experiments",
                "abstract": "Significance Randomized controlled trials are central to the scientific process, but they can be costly. For example, a clinical trial may assign patients to treatments that are detrimental to them. Adaptive experimental designs, such as multiarmed bandit algorithms, reduce costs by increasing the probability of assigning promising treatments over the course of the experiment. However, because observations collected by these methods are dependent and their distribution is nonstationary, statistical inference can be challenging. We propose a treatment-effect estimator that has an asymptotically unbiased and normal test statistic under straightforward, relatively weak conditions on the adaptive design. This estimator generalizes for a variety of parameters of interest. Adaptive experimental designs can dramatically improve efficiency in randomized trials. But with adaptively collected data, common estimators based on sample means and inverse propensity-weighted means can be biased or heavy-tailed. This poses statistical challenges, in particular when the experimenter would like to test hypotheses about parameters that were not targeted by the data-collection mechanism. In this paper, we present a class of test statistics that can handle these challenges. Our approach is to adaptively reweight the terms of an augmented inverse propensity-weighting estimator to control the contribution of each term to the estimator\u2019s variance. This scheme reduces overall variance and yields an asymptotically normal test statistic. We validate the accuracy of the resulting estimates and their CIs in numerical experiments and show that our methods compare favorably to existing alternatives in terms of mean squared error, coverage, and CI size."
            },
            {
                "arxivId": "1810.04088",
                "title": "Bridging the gap between regret minimization and best arm identification, with application to A/B tests",
                "abstract": "State of the art online learning procedures focus either on selecting the best alternative (\"best arm identification\") or on minimizing the cost (the \"regret\"). We merge these two objectives by providing the theoretical analysis of cost minimizing algorithms that are also delta-PAC (with a proven guaranteed bound on the decision time), hence fulfilling at the same time regret minimization and best arm identification. This analysis sheds light on the common observation that ill-callibrated UCB-algorithms minimize regret while still identifying quickly the best arm. \nWe also extend these results to the non-iid case faced by many practitioners. This provides a technique to make cost versus decision time compromise when doing adaptive tests with applications ranging from website A/B testing to clinical trials."
            },
            {
                "arxivId": "1707.02038",
                "title": "A Tutorial on Thompson Sampling",
                "abstract": "Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms."
            },
            {
                "arxivId": "1705.07377",
                "title": "Instrument-Armed Bandits",
                "abstract": "We extend the classic multi-armed bandit (MAB) model to the setting of noncompliance, where the arm pull is a mere instrument and the treatment applied may differ from it, which gives rise to the instrument-armed bandit (IAB) problem. The IAB setting is relevant whenever the experimental units are human since free will, ethics, and the law may prohibit unrestricted or forced application of treatment. In particular, the setting is relevant in bandit models of dynamic clinical trials and other controlled trials on human interventions. Nonetheless, the setting has not been fully investigate in the bandit literature. We show that there are various and divergent notions of regret in this setting, all of which coincide only in the classic MAB setting. We characterize the behavior of these regrets and analyze standard MAB algorithms. We argue for a particular kind of regret that captures the causal effect of treatments but show that standard MAB algorithms cannot achieve sublinear control on this regret. Instead, we develop new algorithms for the IAB problem, prove new regret bounds for them, and compare them to standard MAB algorithms in numerical examples."
            },
            {
                "arxivId": "1701.02490",
                "title": "Real-Time Bidding by Reinforcement Learning in Display Advertising",
                "abstract": "The majority of online display ads are served through real-time bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods."
            },
            {
                "arxivId": "1606.03203",
                "title": "Causal Bandits: Learning Good Interventions via Causal Inference",
                "abstract": "We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information."
            },
            {
                "arxivId": "1605.08988",
                "title": "On Explore-Then-Commit strategies",
                "abstract": "We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case."
            },
            {
                "arxivId": "1512.04922",
                "title": "Always Valid Inference: Bringing Sequential Analysis to A/B Testing",
                "abstract": "A/B tests are typically analyzed via frequentist p-values and confidence intervals; but these inferences are wholly unreliable if users endogenously choose samples sizes by *continuously monitoring* their tests. We define *always valid* p-values and confidence intervals that let users try to take advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. Always valid inference can be interpreted as a natural interface for a sequential hypothesis test, which empowers users to implement a modified test tailored to them. In particular, we show in an appropriate sense that the measures we develop tradeoff sample size and power efficiently, despite a lack of prior knowledge of the user's relative preference between these two goals. We also use always valid p-values to obtain multiple hypothesis testing control in the sequential context. Our methodology has been implemented in a large scale commercial A/B testing platform to analyze hundreds of thousands of experiments to date."
            },
            {
                "arxivId": "1507.08025",
                "title": "Multi-armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges.",
                "abstract": "Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice."
            },
            {
                "arxivId": "1407.7073",
                "title": "Real-Time Bidding Benchmarking with iPinYou Dataset",
                "abstract": "Being an emerging paradigm for display advertising, Real-Time Bidding (RTB) drives the focus of the bidding strategy from context to users' interest by computing a bid for each impression in real time. The data mining work and particularly the bidding strategy development becomes crucial in this performance-driven business. However, researchers in computational advertising area have been suffering from lack of publicly available benchmark datasets, which are essential to compare different algorithms and systems. Fortunately, a leading Chinese advertising technology company iPinYou decided to release the dataset used in its global RTB algorithm competition in 2013. The dataset includes logs of ad auctions, bids, impressions, clicks, and final conversions. These logs reflect the market environment as well as form a complete path of users' responses from advertisers' perspective. This dataset directly supports the experiments of some important research problems such as bid optimisation and CTR estimation. To the best of our knowledge, this is the first publicly available dataset on RTB display advertising. Thus, they are valuable for reproducible research and understanding the whole RTB ecosystem. In this paper, we first provide the detailed statistical analysis of this dataset. Then we introduce the research problem of bid optimisation in RTB and the simple yet comprehensive evaluation protocol. Besides, a series of benchmark experiments are also conducted, including both click-through rate (CTR) estimation and bid optimisation."
            },
            {
                "arxivId": "1311.0466",
                "title": "Thompson Sampling for Complex Online Problems",
                "abstract": "We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward perarm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. Using particle filters for computing posterior distributions which lack an explicit closed-form, we present numerical results for the performance of Thompson sampling for subset-selection and job scheduling problems."
            },
            {
                "arxivId": "1310.1404",
                "title": "Sequential Monte Carlo Bandits",
                "abstract": "In this paper we propose a flexible and efficient framework for handling multi-armed bandits, combining sequential Monte Carlo algorithms with hierarchical Bayesian modeling techniques. The framework naturally encompasses restless bandits, contextual bandits, and other bandit variants under a single inferential model. Despite the model's generality, we propose efficient Monte Carlo algorithms to make inference scalable, based on recent developments in sequential Monte Carlo methods. Through two simulation studies, the framework is shown to outperform other empirical methods, while also naturally scaling to more complex problems for which existing approaches can not cope. Additionally, we successfully apply our framework to online video-based advertising recommendation, and show its increased efficacy as compared to current state of the art bandit algorithms."
            },
            {
                "arxivId": "1304.5758",
                "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling",
                "abstract": "We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit than the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. We first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14\u221anK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 over 20\u221anK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2010.01249",
        "category": "econ",
        "title": "Optimal Echo Chambers",
        "abstract": "This paper studies some benefits of ignoring those who disagree with you. We model a decision maker who draws a signal about the (real-valued) state of the world from a collection of unbiased sources of heterogeneous quality. Exclusively sampling signals close to the prior expectation can be beneficial, as they are more likely high quality. Since echo chambers are a rational response to uncertain information quality, eliminating them can backfire. Signals close to the prior expectation can move beliefs further than more contrary views; limiting the ability to ignore opposing views can make beliefs less accurate and reduce the extent to which signals are heeded.",
        "references": [
            {
                "arxivId": "1812.06967",
                "title": "Optimal Dynamic Allocation of Attention",
                "abstract": "We consider a decision maker (DM) who, before taking an action, seeks information by allocating her limited attention dynamically over different news sources that are biased toward alternative actions. Endogenous choice of information generates rich dynamics: the chosen news source either reinforces or weakens the prior, shaping subsequent attention choices, belief updating, and the final action. The DM adopts a learning strategy biased toward the current belief when the belief is extreme and against that belief when it is moderate. Applied to consumption of news media, observed behavior exhibits an \u201c echo-chamber\u201d effect for partisan voters and a novel \u201c anti-echo-chamber\u201d effect for moderates. (JEL D72, D83, D91, L82)"
            },
            {
                "arxivId": "1801.01665",
                "title": "Political Discourse on Social Media: Echo Chambers, Gatekeepers, and the Price of Bipartisanship",
                "abstract": "Echo chambers, i.e., situations where one is exposed only to opinions that agree with their own, are an increasing concern for the political discourse in many democratic countries. This paper studies the phenomenon of political echo chambers on social media. We identify the two components in the phenomenon: the opinion that is shared, and the \u00bbchamber\u00bb (i.e., the social network) that allows the opinion to \u00bbecho\u00bb (i.e., be re-shared in the network) -- and examine closely at how these two components interact. We define a production and consumption measure for social-media users, which captures the political leaning of the content shared and received by them. By comparing the two, we find that Twitter users are, to a large degree, exposed to political opinions that agree with their own. We also find that users who try to bridge the echo chambers, by sharing content with diverse leaning, have to pay a \u00bbprice of bipartisanship\u00bb in terms of their network centrality and content appreciation. In addition, we study the role of \u00bbgatekeepers,\u00bb users who consume content with diverse leaning but produce partisan content (with a single-sided leaning), in the formation of echo chambers. Finally, we apply these findings to the task of predicting partisans and gatekeepers from social and content features. While partisan users turn out relatively easy to identify, gatekeepers prove to be more challenging."
            },
            {
                "arxivId": "2104.04703",
                "title": "Echo Chambers: Voter-to-Voter Communication and Political Competition",
                "abstract": "I investigate, in a model of informative campaign advertising, how the ability of voters to strategically communicate with each other shapes the advertising strategies of two competing parties. Two main results are put forward. First, information does not travel among voters biased toward different parties even if they are ideologically close \u00e2 \u00e2echo chambers\u00e2 arise endogenously. Second, whenever the probability of interaction among like-minded voters is low (low homophily), parties tailor their advertising on their opponent\u00e2s supporters rather than on swing or core states voters."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2106.14077",
        "category": "econ",
        "title": "The Role of Contextual Information in Best Arm Identification",
        "abstract": "We study the best-arm identification problem with fixed confidence when contextual (covariate) information is available in stochastic bandits. Although we can use contextual information in each round, we are interested in the marginalized mean reward over the contextual distribution. Our goal is to identify the best arm with a minimal number of samplings under a given value of the error rate. We show the instance-specific sample complexity lower bounds for the problem. Then, we propose a context-aware version of the\"Track-and-Stop\"strategy, wherein the proportion of the arm draws tracks the set of optimal allocations and prove that the expected number of arm draws matches the lower bound asymptotically. We demonstrate that contextual information can be used to improve the efficiency of the identification of the best marginalized mean reward compared with the results of Garivier&Kaufmann (2016). We experimentally confirm that context information contributes to faster best-arm identification.",
        "references": [
            {
                "arxivId": "1801.01750",
                "title": "Nonparametric Stochastic Contextual Bandits",
                "abstract": "\n \n We analyze the K-armed bandit problem where the reward for each arm is a noisy realization based on an observed context under mild nonparametric assumptions.We attain tight results for top-arm identification and a sublinear regret of \u00d5(T1+D/(2+D), where D is the context dimension, for a modified UCB algorithm that is simple to implement. We then give global intrinsic dimension dependent and ambient dimension independent regret bounds. We also discuss recovering topological structures within the context space based on expected bandit performance and provide an extension to infinite-armed contextual bandits. Finally, we experimentally show the improvement of our algorithm over existing approaches for both simulated tasks and MNIST image classification.\n \n"
            },
            {
                "arxivId": "1502.01418",
                "title": "RELEAF: An Algorithm for Learning and Exploiting Relevance",
                "abstract": "Recommender systems, medical diagnosis, network security, etc., require on-going learning and decision-making in real time. These-and many others-represent perfect examples of the opportunities and difficulties presented by Big Data: the available information often arrives from a variety of sources and has diverse features so that learning from all the sources may be valuable but integrating what is learned is subject to the curse of dimensionality. This paper develops and analyzes algorithms that allow efficient learning and decision-making while avoiding the curse of dimensionality. We formalize the information available to the learner/decision-maker at a particular time as a context vector which the learner should consider when taking actions. In general the context vector is very high dimensional, but in many settings, the most relevant information is embedded into only a few relevant dimensions. If these relevant dimensions were known in advance, the problem would be simple-but they are not. Moreover, the relevant dimensions may be different for different actions. Our algorithm learns the relevant dimensions for each action, and makes decisions based in what it has learned. Formally, we build on the structure of a contextual multi-armed bandit by adding and exploiting a relevance relation. We prove a general regret bound for our algorithm whose time order depends only on the maximum number of relevant dimensions among all the actions, which in the special case where the relevance relation is single-valued (a function), reduces to \\mathtildeO(T2(\u221a2-1)); in the absence of a relevance relation, the best known contextual bandit algorithms achieve regret \\mathtildeO(T(D+1)/(D+2)), where D is the full dimension of the context vector. Our algorithm alternates between exploring and exploiting and does not require observing outcomes during exploitation (so allows for active learning). Moreover, during exploitation, suboptimal actions are chosen with arbitrarily low probability. Our algorithm is tested on datasets arising from network security and online news article recommendations."
            },
            {
                "arxivId": "1003.5956",
                "title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms",
                "abstract": "Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. Offline evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their \"partial-label\" nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a replay methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method."
            },
            {
                "arxivId": "0802.2655",
                "title": "Pure exploration in finitely-armed and continuous-armed bandits",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2202.09323",
        "category": "econ",
        "title": "Market-Based Price Autocorrelation",
        "abstract": "This paper assumes that the randomness of market trade values and volumes determines the properties of stochastic market prices. We derive the direct dependence of the first two price statistical moments and price volatility on statistical moments, volatilities, and correlations of market trade values and volumes. That helps describe the dependence of market-based price autocorrelation between times t and t-{\\tau} on statistical moments and correlations between trade values and volumes. That highlights the impact of the randomness of the size of market deals on price statistical moments and autocorrelation. Statistical moments and correlations of market trade values and volumes are assessed by conventional frequency-based probabilities. The distinctions between market-based price autocorrelation and autocorrelation that are assessed by the frequency-based probability analysis of price time series reveal the different approaches to the definitions of price probabilities. To forecast market-based price autocorrelation, one should predict the statistical moments and correlations of trade values and volumes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2304.06961",
        "category": "econ",
        "title": "Social Welfare Functions with Voters Qualifications: Impossibility Results",
        "abstract": "We consider the social welfare function a la Arrow, where some voters are not qualified to evaluate some alternatives. Thus, the inputs of the social welfare function are the preferences of voters on the alternatives that they are qualified to evaluate only. Our model is a generalization of the peer rating model, where each voter evaluates the other voters (except for himself/herself). We demonstrate the following three impossibility results. First, if a transitive valued social welfare function satisfies independence of irrelevant alternatives and the Pareto principle, then a dictator who is qualified to evaluate all alternatives exists. Second, a transitive valued function satisfying the Pareto principle exists if and only if at least one voter is qualified to evaluate all alternatives. Finally, if no voter is qualified to evaluate all alternatives, then under a transitive valued social welfare function satisfying the weak Pareto principle and independence of irrelevant alternatives, all alternatives are indifferent for any preference profile of voters.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2304.13985",
        "category": "econ",
        "title": "The Effects of High-Frequency Anticipatory Trading: Small Informed Trader vs. Round-Tripper",
        "abstract": "In an extended Kyle's model, the interactions between a large informed trader and a high-frequency trader (HFT) who can anticipate the former's incoming order are studied. We find that, in equilibrium, HFT may play the role of Small-IT or Round-Tripper: both of them trade in the same direction as IT in advance, but when IT's order arrives, Small-IT continues to take liquidity away, while Round-Tripper supplies liquidity back. So Small-IT always harms IT, while Round-Tripper may benefit her. What's more, with an anticipatory HFT, normal-speed small uninformed traders suffer less and price discovery is accelerated.",
        "references": [
            {
                "arxivId": "2211.06046",
                "title": "Are Large Traders Harmed by Front-running HFTs?",
                "abstract": null
            },
            {
                "arxivId": "1808.05169",
                "title": "Inventory Management for High-Frequency Trading With Imperfect Competition",
                "abstract": "We study Nash equilibria for inventory-averse high-frequency traders (HFTs), who trade to exploit information about future price changes. For discrete trading rounds, the HFTs' optimal trading strategies and their equilibrium price impact are described by a system of nonlinear equations; explicit solutions obtain around the continuous-time limit. Unlike in the risk-neutral case, the optimal inventories become mean-reverting and vanish as the number of trading rounds becomes large. In contrast, the HFTs' risk-adjusted profits and the equilibrium price impact converge to their risk-neutral counterparts. Compared to a social-planner solution for cooperative HFTs, Nash competition leads to excess trading, so that marginal transaction taxes in fact decrease market liquidity."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2306.16591",
        "category": "econ",
        "title": "Nonparametric Causal Decomposition of Group Disparities",
        "abstract": "We propose a nonparametric framework that decomposes the causal contributions of a treatment variable to an outcome disparity between two groups. We decompose the causal contributions of treatment into group differences in 1) treatment prevalence, 2) average treatment effects, and 3) selection into treatment based on individual-level treatment effects. Our framework reformulates the classic Kitagawa-Blinder-Oaxaca decomposition nonparametrically in causal terms, complements causal mediation analysis by explaining group disparities instead of group effects, and distinguishes more mechanisms than recent random equalization decomposition. In contrast to all prior approaches, our framework isolates the causal contribution of differential selection into treatment as a novel mechanism for explaining and ameliorating group disparities. We develop nonparametric estimators based on efficient influence functions that are $\\sqrt{n}$-consistent, asymptotically normal, semiparametrically efficient, and multiply robust to misspecification. We apply our framework to decompose the causal contributions of education to the disparity in adult income between parental income groups (intergenerational income persistence). We find that both differential prevalence of, and differential selection into, college graduation significantly contribute to intergenerational income persistence.",
        "references": [
            {
                "arxivId": "2307.05122",
                "title": "Synthetic Decomposition for Counterfactual Predictions",
                "abstract": "Counterfactual predictions are challenging when the policy variable goes beyond its pre-policy support. However, in many cases, information about the policy of interest is available from different (\"source\") regions where a similar policy has already been implemented. In this paper, we propose a novel method of using such data from source regions to predict a new policy in a target region. Instead of relying on extrapolation of a structural relationship using a parametric specification, we formulate a transferability condition and construct a synthetic outcome-policy relationship such that it is as close as possible to meeting the condition. The synthetic relationship weighs both the similarity in distributions of observables and in structural relationships. We develop a general procedure to construct asymptotic confidence intervals for counterfactual predictions and prove its asymptotic validity. We then apply our proposal to predict average teenage employment in Texas following a counterfactual increase in the minimum wage."
            },
            {
                "arxivId": "2205.13127",
                "title": "Sensitivity analysis for causal decomposition analysis: Assessing robustness toward omitted variable bias",
                "abstract": "Abstract A key objective of decomposition analysis is to identify a factor (the \u201cmediator\u201d) contributing to disparities in an outcome between social groups. In decomposition analysis, a scholarly interest often centers on estimating how much the disparity (e.g., health disparities between Black women and White men) would be reduced/remain if we set the mediator (e.g., education) distribution of one social group equal to another. However, causally identifying disparity reduction and remaining depends on the no omitted mediator\u2013outcome confounding assumption, which is not empirically testable. Therefore, we propose a set of sensitivity analyses to assess the robustness of disparity reduction to possible unobserved confounding. We derived general bias formulas for disparity reduction, which can be used beyond a particular statistical model and do not require any functional assumptions. Moreover, the same bias formulas apply with unobserved confounding measured before and after the group status. On the basis of the formulas, we provide sensitivity analysis techniques based on regression coefficients and R 2 {R}^{2} values by extending the existing approaches. The R 2 {R}^{2} -based sensitivity analysis offers a straightforward interpretation of sensitivity parameters and a standard way to report the robustness of research findings. Although we introduce sensitivity analysis techniques in the context of decomposition analysis, they can be utilized in any mediation setting based on interventional indirect effects when the exposure is randomized (or conditionally ignorable given covariates)."
            },
            {
                "arxivId": "1912.09936",
                "title": "Nonparametric efficient causal mediation with intermediate confounders",
                "abstract": "\n Interventional effects for mediation analysis were proposed as a solution to the lack of identifiability of natural (in)direct effects in the presence of a mediator-outcome confounder affected by exposure. We present a theoretical and computational study of the properties of the interventional (in)direct effect estimands based on the efficient influence function in the nonparametric statistical model. We use the efficient influence function to develop two asymptotically optimal nonparametric estimators that leverage data-adaptive regression for the estimation of nuisance parameters: a one-step estimator and a targeted minimum loss estimator. We further present results establishing the conditions under which these estimators are consistent, multiply robust, $n^{1/2}$-consistent and efficient. We illustrate the finite-sample performance of the estimators and corroborate our theoretical results in a simulation study. We also demonstrate the use of the estimators in our motivating application to elucidate the mechanisms behind the unintended harmful effects that a housing intervention had on risky behaviour in adolescent girls."
            },
            {
                "arxivId": "1908.04427",
                "title": "A groupwise approach for inferring heterogeneous treatment effects in causal inference",
                "abstract": "\n Recently, there has been great interest in estimating the conditional average treatment effect using flexible machine learning methods. However, in practice, investigators often have working hypotheses about effect heterogeneity across pre-defined subgroups of study units, which we call the groupwise approach. The paper compares two modern ways to estimate groupwise treatment effects, a non-parametric approach and a semi-parametric approach, with the goal of better informing practice. Specifically, we compare (a) the underlying assumptions, (b) efficiency and adaption to the underlying data generating models, and (c) a way to combine the two approaches. We also discuss how to test a key assumption concerning the semi-parametric estimator and to obtain cluster-robust standard errors if study units in the same subgroups are correlated. We demonstrate our findings by conducting simulation studies and reanalysing the Early Childhood Longitudinal Study."
            },
            {
                "arxivId": "1905.05389",
                "title": "Experimental Evaluation of Individualized Treatment Rules",
                "abstract": "Abstract The increasing availability of individual-level data has led to numerous applications of individualized (or personalized) treatment rules (ITRs). Policy makers often wish to empirically evaluate ITRs and compare their relative performance before implementing them in a target population. We propose a new evaluation metric, the population average prescriptive effect (PAPE). The PAPE compares the performance of ITR with that of non-individualized treatment rule, which randomly treats the same proportion of units. Averaging the PAPE over a range of budget constraints yields our second evaluation metric, the area under the prescriptive effect curve (AUPEC). The AUPEC represents an overall performance measure for evaluation, like the area under the receiver and operating characteristic curve (AUROC) does for classification, and is a generalization of the QINI coefficient used in uplift modeling. We use Neyman\u2019s repeated sampling framework to estimate the PAPE and AUPEC and derive their exact finite-sample variances based on random sampling of units and random assignment of treatment. We extend our methodology to a common setting, in which the same experimental data are used to both estimate and evaluate ITRs. In this case, our variance calculation incorporates the additional uncertainty due to random splits of data used for cross-validation. The proposed evaluation metrics can be estimated without requiring modeling assumptions, asymptotic approximation, or resampling methods. As a result, it is applicable to any ITR including those based on complex machine learning algorithms. The open-source software package is available for implementing the proposed methodology. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1810.03260",
                "title": "Visually Communicating and Teaching Intuition for Influence Functions",
                "abstract": "Abstract Estimators based on influence functions (IFs) have been shown to be effective in many settings, especially when combined with machine learning techniques. By focusing on estimating a specific target of interest (e.g., the average effect of a treatment), rather than on estimating the full underlying data generating distribution, IF-based estimators are often able to achieve asymptotically optimal mean-squared error. Still, many researchers find IF-based estimators to be opaque or overly technical, which makes their use less prevalent and their benefits less available. To help foster understanding and trust in IF-based estimators, we present tangible, visual illustrations of when and how IF-based estimators can outperform standard \u201cplug-in\u201d estimators. The figures we show are based on connections between IFs, gradients, linear approximations, and Newton\u2013Raphson."
            },
            {
                "arxivId": "1703.05899",
                "title": "Decomposition Analysis to Identify Intervention Targets for Reducing Disparities",
                "abstract": "There has been considerable interest in using decomposition methods in epidemiology (mediation analysis) and economics (Oaxaca\u2013Blinder decomposition) to understand how health disparities arise and how they might change upon intervention. It has not been clear when estimates from the Oaxaca\u2013Blinder decomposition can be interpreted causally because its implementation does not explicitly address potential confounding of target variables. While mediation analysis does explicitly adjust for confounders of target variables, it typically does so in a way that effectively entails equalizing confounders across racial groups, which may not reflect the intended intervention. Revisiting prior analyses in the National Longitudinal Survey of Youth on disparities in wages, unemployment, incarceration, and overall health with test scores, taken as a proxy for educational attainment, as a target intervention, we propose and demonstrate a novel decomposition that controls for confounders of test scores (e.g., measures of childhood socioeconomic status [SES]) while leaving their association with race intact. We compare this decomposition with others that use standardization (to equalize childhood SES [the confounders] alone), mediation analysis (to equalize test scores within levels of childhood SES), and one that equalizes both childhood SES and test scores. We also show how these decompositions, including our novel proposals, are equivalent to implementations of the Oaxaca\u2013Blinder decomposition but provide a more formal causal interpretation for these decompositions."
            },
            {
                "arxivId": "1512.05635",
                "title": "The sorted effects method: discovering heterogeneous effects beyond their averages",
                "abstract": "The partial (ceteris paribus) e?ects of interest in nonlinear and interactive linear models are heterogeneous as they can vary dramatically with the underlying observed or unobserved covariates. Despite the apparent importance of heterogeneity, a common practice in modern empirical work is to largely ignore it by reporting average partial e?ects (or, at best, average e?ects for some groups, see e.g. Angrist and Pischke (2008)). While average e?ects provide very convenient scalar summaries of typical e?ects, by de?nition they fail to re?ect the entire variety of the heterogenous e?ects. In order to discover these e?ects much more fully, we propose to estimate and report sorted e?ects \u2013 a collection of estimated partial e?ects sorted in increasing order and indexed by percentiles. By construction the sorted e?ect curves completely represent and help visualize all of the heterogeneous e?ects in one plot. They are as convenient and easy to report in practice as the conventional average partial e?ects. We also provide a quanti?cation of uncertainty (standard errors and con?dence bands) for the estimated sorted e?ects. We apply the sorted e?ects method to demonstrate several striking patterns of gender-based discrimination in wages, and of race-based discrimination in mortgage lending. Using di?erential geometry and functional delta methods, we establish that the estimated sorted e?ects are consistent for the true sorted e?ects, and derive asymptotic normality and bootstrap approximation results, enabling construction of pointwise con?dence bands (point-wise with respect to percentile indices). We also derive functional central limit theorems and bootstrap approximation results, enabling construction of simultaneous con?dence bands (simultaneous with respect to percentile indices). The derived statistical results in turn rely on establishing Hadamard di?erentiability of the multivariate sorting operator, a result of independent mathematical interest."
            },
            {
                "arxivId": "1508.01378",
                "title": "The influence function of semiparametric estimators",
                "abstract": "There are many economic parameters that depend on nonparametric first steps. Examples include games, dynamic discrete choice, average exact consumer surplus, and treatment effects. Often estimators of these parameters are asymptotically equivalent to a sample average of an object referred to as the influence function. The influence function is useful in local policy analysis, in evaluating local sensitivity of estimators, and constructing debiased machine learning estimators. We show that the influence function is a Gateaux derivative with respect to a smooth deviation evaluated at a point mass. This result generalizes the classic Von Mises (1947) and Hampel (1974) calculation to estimators that depend on smooth nonparametric first steps. We give explicit influence functions for first steps that satisfy exogenous or endogenous orthogonality conditions. We use these results to generalize the omitted variable bias formula for regression to policy analysis for and sensitivity to structural changes. We apply this analysis and find no sensitivity to endogeneity of average equivalent variation estimates in a gasoline demand application."
            },
            {
                "arxivId": "1206.6840",
                "title": "Direct and Indirect Effects of Sequential Treatments",
                "abstract": "In this paper we review the notion of direct and indirect causal effect as introduced by Pearl (2001). We show how it can be formulated without counterfactuals, using regime indicators instead. This allows to consider the natural (in)direct effect as a special case of sequential treatments discussed by Dawid & Didelez (2005) which immediately yields conditions for identifiability as well as a graphical way of checking identifiability."
            },
            {
                "arxivId": "1301.2300",
                "title": "Direct and Indirect Effects",
                "abstract": "The direct effect of one event on another can be defined and measured by holding constant all intermediate variables between the two. Indirect effects present conceptual and practical difficulties (in nonlinear models), because they cannot be isolated by holding certain variables constant. This paper presents a new way of defining the effect transmitted through a restricted set of paths, without controlling variables on the remaining paths. This permits the assessment of a more natural type of direct and indirect effects, one that is applicable in both linear and nonlinear models and that has broader policy-related interpretations. The paper establishes conditions under which such assessments can be estimated consistently from experimental and nonexperimental data, and thus extends path-analytic techniques to nonlinear and nonparametric models."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2310.16281",
        "category": "econ",
        "title": "Improving Robust Decisions with Data",
        "abstract": "A decision-maker faces uncertainty governed by a data-generating process (DGP), which is only known to belong to a set of sequences of independent but possibly non-identical distributions. A robust decision maximizes the expected payoff against the worst possible DGP in this set. This paper characterizes when and how such robust decisions can be improved with data, measured by the expected payoff under the true DGP, no matter which possible DGP is the truth. It further develops novel and simple inference methods to achieve it, as common methods (e.g., maximum likelihood or Bayesian) often fail to deliver such an improvement.",
        "references": [
            {
                "arxivId": "2204.11748",
                "title": "Optimal Decision Rules when Payoffs are Partially Identified",
                "abstract": "We derive optimal statistical decision rules for discrete choice problems when payoffs depend on a partially-identified parameter $\\theta$ and the decision maker can use a point-identified parameter $P$ to deduce restrictions on $\\theta$. Leading examples include optimal treatment choice under partial identification and optimal pricing with rich unobserved heterogeneity. Our optimal decision rules minimize the maximum risk or regret over the identified set of payoffs conditional on $P$ and use the data efficiently to learn about $P$. We discuss implementation of optimal decision rules via the bootstrap and Bayesian methods, in both parametric and semiparametric models. We provide detailed applications to treatment choice and optimal pricing. Using a limits of experiments framework, we show that our optimal decision rules can dominate seemingly natural alternatives. Our asymptotic approach is well suited for realistic empirical settings in which the derivation of finite-sample optimal rules is intractable."
            },
            {
                "arxivId": "2004.11751",
                "title": "Microeconometrics with Partial Identi\ufb01cation",
                "abstract": "This chapter reviews the microeconometrics literature on partial identification, focusing on the developments of the last thirty years. The topics presented illustrate that the available data combined with credible maintained assumptions may yield much information about a parameter of interest, even if they do not reveal it exactly. Special attention is devoted to discussing the challenges associated with, and some of the solutions put forward to, (1) obtain a tractable characterization of the values for the parameters of interest which are observationally equivalent, given the available data and maintained assumptions; (2) estimate this set of values; (3) conduct test of hypotheses and make confidence statements. The chapter reviews advances in partial identification analysis both as applied to learning (functionals of) probability distributions that are well-defined in the absence of models, as well as to learning parameters that are well-defined only in the context of particular models. A simple organizing principle is highlighted: the source of the identification problem can often be traced to a collection of random variables that are consistent with the available data and maintained assumptions. This collection may be part of the observed data or be a model implication. In either case, it can be formalized as a random set. Random set theory is then used as a mathematical framework to unify a number of special results and produce a general methodology to carry out partial identification analysis."
            },
            {
                "arxivId": "1912.08726",
                "title": "Econometrics for Decision Making: Building Foundations Sketched by Haavelmo and Wald",
                "abstract": "Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes use of statistical decision theory to evaluate the performance of models in decision making. I consider the common practice of \n as\u2010if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as\u2010if optimization or any other model\u2010based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancing computation is the primary task to complete the foundations sketched by Haavelmo and Wald.\n"
            },
            {
                "arxivId": "1402.6118",
                "title": "Approximate Models and Robust Decisions",
                "abstract": "Decisions based partly or solely on predictions from probabilistic models may be sensitive to model misspecification. Statisticians are taught from an early stage that \"all models are wrong\", but little formal guidance exists on how to assess the impact of model approximation on decision making, or how to proceed when optimal actions appear sensitive to model fidelity. This article presents an overview of recent developments across different disciplines to address this. We review diagnostic techniques, including graphical approaches and summary statistics, to help highlight decisions made through minimised expected loss that are sensitive to model misspecification. We then consider formal methods for decision making under model misspecification by quantifying stability of optimal actions to perturbations to the model within a neighbourhood of model space. This neighbourhood is defined in either one of two ways. Firstly, in a strong sense via an information (Kullback-Leibler) divergence around the approximating model. Or using a nonparametric model extension, again centred at the approximating model, in order to `average out' over possible misspecifications. This is presented in the context of recent work in the robust control, macroeconomics and financial mathematics literature. We adopt a Bayesian approach throughout although the methods are agnostic to this position."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2311.14320",
        "category": "econ",
        "title": "Consumption Smoothing in Metropolis: Evidence from the Working-class Households in Prewar Tokyo",
        "abstract": "I analyze the risk-coping behaviors among factory worker households in early 20th-century Tokyo. I digitize a unique daily longitudinal survey dataset on household budgets to examine the extent to which consumption is affected by idiosyncratic shocks. I find that while the households were so vulnerable that the shocks impacted their consumption levels, the income elasticity for food consumption is relatively low in the short-run perspective. The result from mechanism analysis suggests that credit purchases with local retailers played a role in smoothing short-run food consumption. The event-study analysis using the adverse health shock as the idiosyncratic income shock confirms the robustness of the results. I also find evidence that the misassignment of payday in data aggregation results in a systematic attenuation bias due to measurement error in the standard consumption smoothing regressions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2312.12612",
        "category": "econ",
        "title": "Stochastic Control Barrier Functions for Economics",
        "abstract": "Control barrier functions (CBFs) and safety-critical control have seen a rapid increase in popularity in recent years, predominantly applied to systems in aerospace, robotics and neural network controllers. Control barrier functions can provide a computationally efficient method to monitor arbitrary primary controllers and enforce state constraints to ensure overall system safety. One area that has yet to take advantage of the benefits offered by CBFs is the field of finance and economics. This manuscript re-introduces three applications of traditional control to economics, and develops and implements CBFs for such problems. We consider the problem of optimal advertising for the deterministic and stochastic case and Merton's portfolio optimization problem. Numerical simulations are used to demonstrate the effectiveness of using traditional control solutions in tandem with CBFs and stochastic CBFs to solve such problems in the presence of state constraints.",
        "references": [
            {
                "arxivId": "2003.03498",
                "title": "Control barrier functions for stochastic systems",
                "abstract": null
            },
            {
                "arxivId": "1709.07523",
                "title": "Hamilton-Jacobi reachability: A brief overview and recent advances",
                "abstract": "Hamilton-Jacobi (HJ) reachability analysis is an important formal verification method for guaranteeing performance and safety properties of dynamical systems; it has been applied to many small-scale systems in the past decade. Its advantages include compatibility with general nonlinear system dynamics, formal treatment of bounded disturbances, and the availability of well-developed numerical tools. The main challenge is addressing its exponential computational complexity with respect to the number of state variables. In this tutorial, we present an overview of basic HJ reachability theory and provide instructions for using the most recent numerical tools, including an efficient GPU-parallelized implementation of a Level Set Toolbox for computing reachable sets. In addition, we review some of the current work in high-dimensional HJ reachability to show how the dimensionality challenge can be alleviated via various general theoretical and application-specific insights."
            },
            {
                "arxivId": "1609.06408",
                "title": "Control Barrier Function Based Quadratic Programs for Safety Critical Systems",
                "abstract": "Safety critical systems involve the tight coupling between potentially conflicting control objectives and safety constraints. As a means of creating a formal framework for controlling systems of this form, and with a view toward automotive applications, this paper develops a methodology that allows safety conditions\u2014expressed as  control barrier functions\u2014to be unified with performance objectives\u2014expressed as control Lyapunov functions\u2014in the context of real-time optimization-based controllers. Safety conditions are specified in terms of forward invariance of a set, and are verified via two novel generalizations of barrier functions; in each case, the existence of a barrier function satisfying Lyapunov-like conditions implies forward invariance of the set, and the relationship between these two classes of barrier functions is characterized. In addition, each of these formulations yields a notion of control barrier function (CBF), providing inequality constraints in the control input that, when satisfied, again imply forward invariance of the set. Through these constructions, CBFs can naturally be unified with control Lyapunov functions (CLFs) in the context of a quadratic program (QP); this allows for the achievement of control objectives (represented by CLFs) subject to conditions on the admissible states of the system (represented by CBFs). The mediation of safety and performance through a QP is demonstrated on adaptive cruise control and lane keeping, two automotive control problems that present both safety and performance considerations coupled with actuator bounds."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2401.05657",
        "category": "econ",
        "title": "An impossibility theorem concerning positive involvement in voting",
        "abstract": null,
        "references": [
            {
                "arxivId": "2106.11502",
                "title": "Measuring Violations of Positive Involvement in Voting",
                "abstract": "In the context of computational social choice, we study voting methods that assign a set of winners to each profile of voter preferences. A voting method satisfies the property of positive involvement (PI) if for any election in which a candidate x would be among the winners, adding another voter to the election who ranks x first does not cause x to lose. Surprisingly, a number of standard voting methods violate this natural property. In this paper, we investigate different ways of measuring the extent to which a voting method violates PI, using computer simulations. We consider the probability (under different probability models for preferences) of PI violations in randomly drawn profiles vs. profile-coalition pairs (involving coalitions of different sizes). We argue that in order to choose between a voting method that satisfies PI and one that does not, we should consider the probability of PI violation conditional on the voting methods choosing different winners. We should also relativize the probability of PI violation to what we call voter potency, the probability that a voter causes a candidate to lose. Although absolute frequencies of PI violations may be low, after this conditioning and relativization, we see that under certain voting methods that violate PI, much of a voter's potency is turned against them - in particular, against their desire to see their favorite candidate elected."
            },
            {
                "arxivId": "2009.02979",
                "title": "An Analysis of Random Elections with Large Numbers of Voters",
                "abstract": null
            },
            {
                "arxivId": "2004.02350",
                "title": "Split Cycle: a new Condorcet-consistent voting method independent of clones and immune to spoilers",
                "abstract": null
            },
            {
                "arxivId": "1602.08063",
                "title": "Optimal Bounds for the No-Show Paradox via SAT Solving",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2401.11229",
        "category": "econ",
        "title": "Estimation with pairwise observations",
        "abstract": "The paper introduces a new estimation method for the standard linear regression model. The procedure is not driven by the optimisation of any objective function rather, it is a simple weighted average of slopes from observation pairs. The paper shows that such estimator is consistent for carefully selected weights. Other properties, such as asymptotic distributions, have also been derived to facilitate valid statistical inference. Unlike traditional methods, such as Least Squares and Maximum Likelihood, among others, the estimated residual of this estimator is not by construction orthogonal to the explanatory variables of the model. This property allows a wide range of practical applications, such as the testing of endogeneity, i.e., the correlation between the explanatory variables and the disturbance terms.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.15620",
        "category": "econ",
        "title": "Comparison of sectoral structures between China and Japan: A network perspective",
        "abstract": "Economic structure comparisons between China and Japan have long captivated development economists. To delve deeper into their sectoral differences from 1995 to 2018, we used the annual input-output tables (IOTs) of both nations to construct weighted and directed input-output networks (IONs). This facilitated deeper network analyses. Strength distributions underscored variations in inter-sector economic interactions. Weighted, directed assortativity coefficients encapsulated the homophily among connecting sectors' features. By adjusting emphasis in PageRank centrality, key sectors were identified. Community detection revealed their clustering tendencies among the sectors. As anticipated, the analysis pinpointed manufacturing as China's central sector, while Japan favored services. Yet, at a finer level of the specific sectors, both nations exhibited varied structural evolutions. Contrastingly, sectoral communities in both China and Japan demonstrated commendable stability over the examined duration.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.15904",
        "category": "econ",
        "title": "Optimal Budget Aggregation with Single-Peaked Preferences",
        "abstract": "We study the problem of aggregating distributions, such as budget proposals, into a collective distribution. An ideal aggregation mechanism would be Pareto efficient, strategyproof, and fair. Most previous work assumes that agents evaluate budgets according to the $\\ell_1$ distance to their ideal budget. We investigate and compare different models from the larger class of star-shaped utility functions - a multi-dimensional generalization of single-peaked preferences. For the case of two alternatives, we extend existing results by proving that under very general assumptions, the uniform phantom mechanism is the only strategyproof mechanism that satisfies proportionality - a minimal notion of fairness introduced by Freeman et al. (2021). Moving to the case of more than two alternatives, we establish sweeping impossibilities for $\\ell_1$ and $\\ell_\\infty$ disutilities: no mechanism satisfies efficiency, strategyproofness, and proportionality. We then propose a new kind of star-shaped utilities based on evaluating budgets by the ratios of shares between a given budget and an ideal budget. For these utilities, efficiency, strategyproofness, and fairness become compatible. In particular, we prove that the mechanism that maximizes the Nash product of individual utilities is characterized by group-strategyproofness and a core-based fairness condition.",
        "references": [
            {
                "arxivId": "2309.02613",
                "title": "Project-Fair and Truthful Mechanisms for Budget Aggregation",
                "abstract": "We study the budget aggregation problem in which a set of strategic voters must split a finite divisible resource (such as money or time) among a set of competing projects. Our goal is twofold: We seek truthful mechanisms that provide fairness guarantees to the projects. For the first objective, we focus on the class of moving phantom mechanisms, which are -- to this day -- essentially the only known truthful mechanisms in this setting. For project fairness, we consider the mean division as a fair baseline, and bound the maximum difference between the funding received by any project and this baseline. We propose a novel and simple moving phantom mechanism that provides optimal project fairness guarantees. As a corollary of our results, we show that our new mechanism minimizes the L1 distance to the mean for three projects and gives the first non-trivial bounds on this quantity for more than three projects."
            },
            {
                "arxivId": "2307.15586",
                "title": "Settling the Score: Portioning with Cardinal Preferences",
                "abstract": "We study a portioning setting in which a public resource such as time or money is to be divided among a given set of candidates, and each agent proposes a division of the resource. We consider two families of aggregation rules for this setting - those based on coordinate-wise aggregation and those that optimize some notion of welfare - as well as the recently proposed Independent Markets mechanism. We provide a detailed analysis of these rules from an axiomatic perspective, both for classic axioms, such as strategyproofness and Pareto optimality, and for novel axioms, which aim to capture proportionality in this setting. Our results indicate that a simple rule that computes the average of all proposals satisfies many of our axioms, including some that are violated by more sophisticated rules."
            },
            {
                "arxivId": "2305.10286",
                "title": "Balanced Donor Coordination",
                "abstract": "Charity is typically done either by individual donors, who donate money to the charities that they support, or by centralized organizations such as governments or municipalities, which collect the individual contributions and distribute them among a set of charities. On the one hand, individual charity respects the will of the donors but may be inefficient due to a lack of coordination. On the other hand, centralized charity is potentially more efficient but may ignore the will of individual donors."
            },
            {
                "arxivId": "2111.01566",
                "title": "Strategyproof and Proportionally Fair Facility Location",
                "abstract": "We focus on a simple, one-dimensional collective decision problem (often referred to as the facility location problem) and explore issues of strategyproofness and proportionality-based fairness. We introduce and analyze a hierarchy of proportionality-based fairness axioms of varying strength: Individual Fair Share (IFS), Unanimous Fair Share (UFS), Proportionality (as in Freeman et al, 2021), and Proportional Fairness (PF). For each axiom, we characterize the family of mechanisms that satisfy the axiom and strategyproofness. We show that imposing strategyproofness renders many of the axioms to be equivalent: the family of mechanisms that satisfy proportionality, unanimity, and strategyproofness is equivalent to the family of mechanisms that satisfy UFS and strategyproofness, which, in turn, is equivalent to the family of mechanisms that satisfy PF and strategyproofness. Furthermore, there is a unique such mechanism: the Uniform Phantom mechanism, which is studied in Freeman et al. (2021). We also characterize the outcomes of the Uniform Phantom mechanism as the unique (pure) equilibrium outcome for any mechanism that satisfies continuity, strict monotonicity, and UFS. Finally, we analyze the approximation guarantees, in terms of optimal social welfare and minimum total cost, obtained by mechanisms that are strategyproof and satisfy each proportionality-based fairness axiom. We show that the Uniform Phantom mechanism provides the best approximation of the optimal social welfare (and also minimum total cost) among all mechanisms that satisfy UFS."
            },
            {
                "arxivId": "2003.00606",
                "title": "Participatory Budgeting: Models and Approaches",
                "abstract": null
            },
            {
                "arxivId": "2009.06856",
                "title": "Knapsack Voting for Participatory Budgeting",
                "abstract": "We address the question of aggregating the preferences of voters in the context of participatory budgeting. We scrutinize the voting method currently used in practice, underline its drawbacks, and introduce a novel scheme tailored to this setting, which we call \u201cKnapsack Voting.\u201d We study its strategic properties\u2014we show that it is strategy-proof under a natural model of utility (a dis-utility given by the \u21131 distance between the outcome and the true preference of the voter) and \u201cpartially\u201d strategy-proof under general additive utilities. We extend Knapsack Voting to more general settings with revenues, deficits, or surpluses and prove a similar strategy-proofness result. To further demonstrate the applicability of our scheme, we discuss its implementation on the digital voting platform that we have deployed in partnership with the local government bodies in many cities across the nation. From voting data thus collected, we present empirical evidence that Knapsack Voting works well in practice."
            },
            {
                "arxivId": "1905.00457",
                "title": "Truthful Aggregation of Budget Proposals",
                "abstract": null
            },
            {
                "arxivId": "1807.05293",
                "title": "Markets Beyond Nash Welfare for Leontief Utilities",
                "abstract": "We study the allocation of divisible goods to competing agents via a market mechanism, focusing on agents with Leontief utilities. The majority of the economics and mechanism design literature has focused on \\emph{linear} prices, meaning that the cost of a good is proportional to the quantity purchased. Equilibria for linear prices are known to be exactly the maximum Nash welfare allocations. \n\\emph{Price curves} allow the cost of a good to be any (increasing) function of the quantity purchased. We show that price curve equilibria are not limited to maximum Nash welfare allocations with two main results. First, we show that an allocation can be supported by strictly increasing price curves if and only if it is \\emph{group-domination-free}. A similarly characterization holds for weakly increasing price curves. We use this to show that given any allocation, we can compute strictly (or weakly) increasing price curves that support it (or show that none exist) in polynomial time. These results involve a connection to the \\emph{agent-order matrix} of an allocation, which may have other applications. Second, we use duality to show that in the bandwidth allocation setting, any allocation maximizing a CES welfare function can be supported by price curves."
            },
            {
                "arxivId": "1712.02542",
                "title": "Fair Mixing: the Case of Dichotomous Preferences",
                "abstract": "We consider a setting in which agents vote to choose a fair mixture of public outcomes. The agents have dichotomous preferences: each outcome is liked or disliked by an agent. We discuss three outstanding voting rules. The Conditional Utilitarian rule, a variant of the random dictator, is strategyproof and guarantees to any group of like-minded agents an influence proportional to its size. It is easier to compute and more efficient than the familiar Random Priority rule. Its worst case (resp. average) inefficiency is provably (resp. in numerical experiments) low if the number of agents is low. The efficient Egalitarian rule protects individual agents but not coalitions. It is excludable strategyproof: I do not want to lie if I cannot consume outcomes I claim to dislike. The efficient Nash Max Product rule offers the strongest welfare guarantees to coalitions, who can force any outcome with a probability proportional to their size. But it even fails the excludable form of strategyproofness."
            },
            {
                "arxivId": "1610.03474",
                "title": "The Core of the Participatory Budgeting Problem",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.15965",
        "category": "econ",
        "title": "Evolving E-commerce Logistics Planning- Integrating Embedded Technology and Ant Colony Algorithm for Enhanced Efficiency",
        "abstract": "Amidst the era of networking, the e-commerce sector has undergone notable expansion, notably with the advent of Cross-border E-commerce (CBEC) in recent times. This growth trend persists, necessitating robust logistical frameworks to sustainably support operations. However, the current e-commerce logistics paradigm faces challenges in meeting evolving user demands, prompting a quest for innovative solutions. This research endeavors to address these complexities by undertaking a comprehensive analysis of CBEC logistics models and integrating embedded technology into logistical frameworks, resulting in the development of an advanced logistics tracking system. Moreover, employing the ant colony algorithm, the study conducts experimental investigations into optimizing logistics package distribution route planning. Noteworthy enhancements are observed in key metrics such as average delivery time, signaling the efficacy of this approach. In essence, this research offers a promising pathway towards optimizing logistics package distribution routes and bolstering package transportation efficiency within the CBEC domain.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.16322",
        "category": "econ",
        "title": "Estimating Stochastic Block Models in the Presence of Covariates",
        "abstract": "In the standard stochastic block model for networks, the probability of a connection between two nodes, often referred to as the edge probability, depends on the unobserved communities each of these nodes belongs to. We consider a flexible framework in which each edge probability, together with the probability of community assignment, are also impacted by observed covariates. We propose a computationally tractable two-step procedure to estimate the conditional edge probabilities as well as the community assignment probabilities. The first step relies on a spectral clustering algorithm applied to a localized adjacency matrix of the network. In the second step, k-nearest neighbor regression estimates are computed on the extracted communities. We study the statistical properties of these estimators by providing non-asymptotic bounds.",
        "references": [
            {
                "arxivId": "1312.1733",
                "title": "Impact of regularization on spectral clustering",
                "abstract": "Summary form only given. Clustering in networks/graphs is an important problem with applications in the analysis of gene-gene interactions, social networks, text mining, to name a few. Spectral clustering is one of the more popular techniques for such purposes, chiefly due to its computational advantage and generality of application. The algorithm's generality arises from the fact that it is not tied to any modeling assumptions on the data, but is rooted in intuitive measures of community structure such as sparsest cut based measures (Hagen and Kahng (1992), Shi and Malik (2000), Ng. et. al (2002)).Here, we attempt to understand regularized form of spectral clustering. Our motivation for this work was empirical results in Amini et. al (2013) that showed that the performance of spectral clustering can greatly be improved via regularization. Here regularization entails adding a constant matrix to the adjacency matrix and calculating the corresponding Laplacian matrix. The value of the constant is called the regularization parameter. Our analysis is carried out under the stochastic block model (SBM) framework. Under the (SBM) (and its extensions). Previous results on spectral clustering (McSherry (2001), Dasgupta et. al. (2004), Rohe et. al (2011)) also assumed the SBM and relied on the minimum degree of the graph being sufficiently large to prove its good performance. By analyzing the spectrum of the Laplacian of an SBM as a function of the regularization parameter, we provide bounds for the perturbation of the regularized eigenvectors, which, in some situations, does not depend on the minimum degree. For example, in the two block SBM, our bounds depend inversely on the maximum degree, as opposed to the minimum degree. More importantly, we show the usefulness of regularization in the important practical situation where not all nodes can be clustered accurately. In such situations, in the absence of regularization, the top eigenvectors need not discriminate between the nodes which do belong to well-defined clusters. With a proper choice of regularization parameter, we demonstrate that top eigenvectors indeed discriminate between the well-defined clusters. A crucial ingredient in the above is the analysis of the spectrum of the Laplacian as a function of the regularization parameter. Assuming that there are K clusters, an adequate gap between the top K eigenvalues and the remaining eigenvalues, ensures that these clusters can be estimated well. Such a gap is commonly referred to as the eigen gap. In the situation considered in above paragraph, an adequate eigen gap may not exist for the unregularized Laplacian. We show that regularization works by creating a gap, allowing us to recover the clusters. As an important application of our bounds, we propose a data-driven technique DK-est (standing for estimated Davis-Kahn bounds) for choosing the regularization parameter. DK-est is shown to perform very well for simulated and real data sets."
            },
            {
                "arxivId": "1306.1433",
                "title": "Tight Lower Bound on the Probability of a Binomial Exceeding its Expectation",
                "abstract": null
            },
            {
                "arxivId": "1211.0373",
                "title": "MINIMAX SPARSE PRINCIPAL SUBSPACE ESTIMATION IN HIGH DIMENSIONS",
                "abstract": "We study sparse principal components analysis in high dimensions, where p (the number of variables) can be much larger than n (the number of observations), and analyze the problem of estimating the subspace spanned by the principal eigenvectors of the population covariance matrix. We prove optimal, non-asymptotic lower and upper bounds on the minimax subspace estimation error under two different, but related notions of lq subspace sparsity for 0 \ufffd q \ufffd 1. Our upper bounds apply to general classes of covariance matrices, and they show that lq constrained estimates can achieve optimal minimax rates without restrictive spiked covariance conditions. 1. Introduction. Principal components analysis (PCA) was introduced in the early 20th century (Pearson, 1901; Hotelling, 1933) and is arguably the most well known and widely used technique for dimension reduction. It is part of the mainstream statistical repertoire and is routinely used in numerous and diverse areas of application. However, contemporary applications often involve much higher-dimensional data than envisioned by the early developers of PCA. In such high-dimensional situations, where the number of variables p is of the same order or much larger than the number of observations n, serious difficulties emerge: standard PCA can produce inconsistent estimates of the principal directions of variation and lead to unreliable conclusions (Johnstone and Lu, 2009; Paul, 2007; Nadler, 2008). The principal directions of variation correspond to the eigenvectors of the covariance matrix, and in high-dimensions consistent estimation of the eigenvectors is generally not possible without additional assumptions about the covariance matrix or its eigenstructure. Much of the recent development in PCA has focused on methodology that applies the concept of sparsity to eigenvector estimation (some examples include Jolliffe, Trendafilov and Uddin,"
            },
            {
                "arxivId": "1202.5101",
                "title": "The method of moments and degree distributions for network models",
                "abstract": "Probability models on graphs are becoming increasingly important in many applications, but statistical tools for fitting such models are not yet well developed. Here we propose a general method of moments approach that can be used to fit a large class of probability models through empirical counts of certain patterns in a graph. We establish some general asymptotic properties of empirical graph moments and prove consistency of the estimates as the graph size grows for all ranges of the average degree including $\\Omega(1)$. Additional results are obtained for the important special case of degree distributions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.16401",
        "category": "econ",
        "title": "A Stationary Equilibrium Model of Green Technology Adoption with Endogenous Carbon Price",
        "abstract": "This paper proposes and analyzes a stationary equilibrium model for a competitive industry which endogenously determines the carbon price necessary to achieve a given emission target. In the model, firms are identified by their level of technology and make production, entry, and abatement decisions. Polluting firms are subject to a carbon price and abatement is formulated as an irreversible investment, which entails a sunk cost and results in the firms switching to a carbon neutral technology. In equilibrium, we identify a carbon price and a stationary distribution of incumbent, polluting firms, that guarantee the compliance with a certain emission target. Our general theoretical framework is complemented with a case study with Brownian technology shocks, in which we discuss some implications of our model. We observe that a carbon pricing system alongside installation subsidies and tax benefits for green firms trigger earlier investment, while higher income taxes for polluting firms may be distorting. Moreover, we discuss the role of a welfare maximizing regulator, who, by optimally setting the emission target, may mitigate or revert some parameters' effects observed in the model with fixed limit.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.16538",
        "category": "econ",
        "title": "Learning to Maximize (Expected) Utility",
        "abstract": "We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind. We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles. In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems. Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences. A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory. Finally, in addition to finding that choice consistency is positively correlated with cognitive ability, we document that subjects who learned to maximize utility were more cognitively able than those who did not. We discuss potential implications of our analysis.",
        "references": [
            {
                "arxivId": "2307.09411",
                "title": "Risk Preference Types, Limited Consideration, and Welfare",
                "abstract": "Abstract We provide sufficient conditions for semi-nonparametric point identification of a mixture model of decision making under risk, when agents make choices in multiple lines of insurance coverage (contexts) by purchasing a bundle. As a first departure from the related literature, the model allows for two preference types. In the first one, agents behave according to standard expected utility theory with CARA Bernoulli utility function, with an agent-specific coefficient of absolute risk aversion whose distribution is left completely unspecified. In the other, agents behave according to the dual theory of choice under risk combined with a one-parameter family distortion function, where the parameter is agent-specific and is drawn from a distribution that is left completely unspecified. Within each preference type, the model allows for unobserved heterogeneity in consideration sets, where the latter form at the bundle level\u2014a second departure from the related literature. Our point identification result rests on observing sufficient variation in covariates across contexts, without requiring any independent variation across alternatives within a single context. We estimate the model on data on households\u2019 deductible choices in two lines of property insurance, and use the results to assess the welfare implications of a hypothetical market intervention where the two lines of insurance are combined into a single one. We study the role of limited consideration in mediating the welfare effects of such intervention."
            },
            {
                "arxivId": "2212.03931",
                "title": "A Better Test of Choice Overload",
                "abstract": "Choice overload - by which larger choice sets are detrimental to a chooser's wellbeing - is potentially of great importance to the design of economic policy. Yet the current evidence on its prevalence is inconclusive. We argue that existing tests are likely to be underpowered and hence that choice overload may occur more often than the literature suggests. We propose more powerful tests based on richer data and characterization theorems for the Random Utility Model. These new approaches come with significant econometric challenges, which we show how to address. We apply our tests to new experimental data and find strong evidence of choice overload that would likely be missed using current approaches."
            },
            {
                "arxivId": "1902.06629",
                "title": "Discrete Choice under Risk with Limited Consideration",
                "abstract": "This paper is concerned with learning decision-makers\u2019 preferences using data on observed choices from a finite set of risky alternatives. We propose a discrete choice model with unobserved heterogeneity in consideration sets and in standard risk aversion. We obtain sufficient conditions for the model\u2019s semi-nonparametric point identification, including in cases where consideration depends on preferences and on some of the exogenous variables. Our method yields an estimator that is easy to compute and is applicable in markets with large choice sets. We illustrate its properties using a dataset on property insurance purchases. (JEL D81, D83, D91, G22, G52)"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-27.json",
        "arxivId": "2402.16771",
        "category": "econ",
        "title": "Wisdom and Foolishness of Noisy Matching Markets",
        "abstract": "We consider a many-to-one matching market where colleges share true preferences over students but make decisions using only independent noisy rankings. Each student has a true value $v$, but each college $c$ ranks the student according to an independently drawn estimated value $v + X_c$ for $X_c\\sim \\mathcal{D}.$ We ask a basic question about the resulting stable matching: How noisy is the set of matched students? Two striking effects can occur in large markets (i.e., with a continuum of students and a large number of colleges). When $\\mathcal{D}$ is light-tailed, noise is fully attenuated: only the highest-value students are matched. When $\\mathcal{D}$ is long-tailed, noise is fully amplified: students are matched uniformly at random. These results hold for any distribution of student preferences over colleges, and extend to when only subsets of colleges agree on true student valuations instead of the entire market. More broadly, our framework provides a tractable approach to analyze implications of imperfect preference formation in large markets.",
        "references": [
            {
                "arxivId": "2312.09841",
                "title": "Monoculture in Matching Markets",
                "abstract": "Algorithmic monoculture arises when many decision-makers rely on the same algorithm to evaluate applicants. An emerging body of work investigates possible harms of this kind of homogeneity, but has been limited by the challenge of incorporating market effects in which the preferences and behavior of many applicants and decision-makers jointly interact to determine outcomes. Addressing this challenge, we introduce a tractable theoretical model of algorithmic monoculture in a two-sided matching market with many participants. We use the model to analyze outcomes under monoculture (when decision-makers all evaluate applicants using a common algorithm) and under polyculture (when decision-makers evaluate applicants independently). All else equal, monoculture (1) selects less-preferred applicants when noise is well-behaved, (2) matches more applicants to their top choice, though individual applicants may be worse off depending on their value to decision-makers and risk tolerance, and (3) is more robust to disparities in the number of applications submitted."
            },
            {
                "arxivId": "2109.07835",
                "title": "Incentives in Two-sided Matching Markets with Prediction-enhanced Preference-formation",
                "abstract": "Two-sided matching markets have long existed to pair agents in the absence of regulated exchanges. A common example is school choice, where a matching mechanism uses student and school preferences to assign students to schools. In such settings, forming preferences is both difficult and critical. Prior work has suggested various prediction mechanisms that help agents make decisions about their preferences. Although often deployed together, these matching and prediction mechanisms are almost always analyzed separately. The present work shows that at the intersection of the two lies a previously unexplored type of strategic behavior: agents returning to the market (e.g., schools) can attack future predictions by interacting short-term non-optimally with their matches. Here, we first introduce this type of strategic behavior, which we call an `adversarial interaction attack'. Next, we construct a formal economic model that captures the feedback loop between prediction mechanisms designed to assist agents and the matching mechanism used to pair them. This economic model allows us to analyze adversarial interaction attacks. Finally, using school choice as an example, we build a simulation to show that, as the trust in and accuracy of predictions increases, schools gain progressively more by initiating an adversarial interaction attack. We also show that this attack increases inequality in the student population."
            },
            {
                "arxivId": "2108.08843",
                "title": "Learning Equilibria in Matching Markets from Bandit Feedback",
                "abstract": "Large-scale, two-sided matching platforms must find market outcomes that align with user preferences while simultaneously learning these preferences from data. Classical notions of stability (Gale and Shapley, 1962; Shapley and Shubik, 1971) are unfortunately of limited value in the learning setting, given that preferences are inherently uncertain and destabilizing while they are being learned. To bridge this gap, we develop a framework and algorithms for learning stable market outcomes under uncertainty. Our primary setting is matching with transferable utilities, where the platform both matches agents and sets monetary transfers between them. We design an incentive-aware learning objective that captures the distance of a market outcome from equilibrium. Using this objective, we analyze the complexity of learning as a function of preference structure, casting learning as a stochastic multi-armed bandit problem. Algorithmically, we show that\"optimism in the face of uncertainty,\"the principle underlying many bandit algorithms, applies to a primal-dual formulation of matching with transfers and leads to near-optimal regret bounds. Our work takes a first step toward elucidating when and how stable matchings arise in large, data-driven marketplaces."
            },
            {
                "arxivId": "2106.06706",
                "title": "Decentralized Matching in a Probabilistic Environment",
                "abstract": "We consider a model for repeated stochastic matching where compatibility is probabilistic, is realized the first time agents are matched, and persists in the future. Such a model has applications in the gig economy, kidney exchange, and mentorship matching. We ask whether adecentralized matching process can approximate the optimal online algorithm. In particular, we consider a decentralizedstable matching process where agents match with the most compatible partner who does not prefer matching with someone else, and known compatible pairs continue matching in all future rounds. We demonstrate that the above process provides a 0.316-approximation to the optimal online algorithm for matching on general graphs. We also provide a 1/7-approximation for many-to-one bipartite matching, a 1/11-approximation for capacitated matching on general graphs, and a 1/2k-approximation for forming teams of up to k agents. Our results rely on a novel coupling argument that decomposes the successful edges of the optimal online algorithm in terms of their round-by-round comparison with stable matching."
            },
            {
                "arxivId": "2101.05853",
                "title": "Algorithmic monoculture and social welfare",
                "abstract": "Significance Algorithmic monoculture is a growing concern in the use of algorithms for high-stakes screening decisions in areas such as employment and lending. If many firms use the same algorithm, even if it is more accurate than the alternatives, the resulting \u201cmonoculture\u201d may be susceptible to correlated failures, much as a monocultural system is in biological settings. To investigate this concern, we develop a model of selection under monoculture. We find that even without any assumption of shocks or correlated failures\u2014i.e., under \u201cnormal operations\u201d\u2014the quality of decisions may decrease when multiple firms use the same algorithm. Thus, the introduction of a more accurate algorithm may decrease social welfare\u2014a kind of \u201cBraess\u2019 paradox\u201d for algorithmic decision-making. As algorithms are increasingly applied to screen applicants for high-stakes decisions in employment, lending, and other domains, concerns have been raised about the effects of algorithmic monoculture, in which many decision-makers all rely on the same algorithm. This concern invokes analogies to agriculture, where a monocultural system runs the risk of severe harm from unexpected shocks. Here, we show that the dangers of algorithmic monoculture run much deeper, in that monocultural convergence on a single algorithm by a group of decision-making agents, even when the algorithm is more accurate for any one agent in isolation, can reduce the overall quality of the decisions being made by the full collection of agents. Unexpected shocks are therefore not needed to expose the risks of monoculture; it can hurt accuracy even under \u201cnormal\u201d operations and even for algorithms that are more accurate when used by only a single decision-maker. Our results rely on minimal assumptions and involve the development of a probabilistic framework for analyzing systems that use multiple noisy estimates of a set of alternatives."
            },
            {
                "arxivId": "2011.00159",
                "title": "Learning Strategies in Decentralized Matching Markets under Uncertain Preferences",
                "abstract": "We study two-sided decentralized matching markets in which participants have uncertain preferences. We present a statistical model to learn the preferences. The model incorporates uncertain state and the participants' competition on one side of the market. We derive an optimal strategy that maximizes the agent's expected payoff and calibrate the uncertain state by taking the opportunity costs into account. We discuss the sense in which the matching derived from the proposed strategy has a stability property. We also prove a fairness property that asserts that there exists no justified envy according to the proposed strategy. We provide numerical results to demonstrate the improved payoff, stability and fairness, compared to alternative methods."
            },
            {
                "arxivId": "1906.05363",
                "title": "Competing Bandits in Matching Markets",
                "abstract": "Stable matching, a classical model for two-sided markets, has long been studied with little consideration for how each side's preferences are learned. With the advent of massive online markets powered by data-driven matching platforms, it has become necessary to better understand the interplay between learning and market objectives. We propose a statistical learning model in which one side of the market does not have a priori knowledge about its preferences for the other side and is required to learn these from stochastic rewards. Our model extends the standard multi-armed bandits framework to multiple players, with the added feature that arms have preferences over players. We study both centralized and decentralized approaches to this problem and show surprising exploration-exploitation trade-offs compared to the single player multi-armed bandits setting."
            },
            {
                "arxivId": "1207.7209",
                "title": "Concentration inequalities for order statistics",
                "abstract": "This note describes non-asymptotic variance and tail bounds for order statistics of samples\u00a0of independent identically distributed random variables. When the sampling distribution belongs to a maximum domain of attraction, these bounds are checked to be asymptotically tight. When the sampling distribution has a non decreasing hazard rate, we derive an exponential Efron-Stein inequality for order statistics, that is\u00a0 an inequality connecting the logarithmic moment generating function of order statistics with exponential moments of Efron-Stein (jackknife) estimates of variance. This connection is used to derive variance and tail bounds for order statistics of Gaussian samples that are not within the scope of the Gaussian concentration inequality. Proofs are elementary and combine Renyi's representation\u00a0of order statistics with the entropy approach to concentration of measure popularized by M. Ledoux."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2103.09603",
        "category": "econ",
        "title": "DoubleML - An Object-Oriented Implementation of Double Machine Learning in R",
        "abstract": "The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods.",
        "references": [
            {
                "arxivId": "2104.03220",
                "title": "DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python",
                "abstract": "DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org."
            },
            {
                "arxivId": "2101.04025",
                "title": "Distributed Double Machine Learning with a Serverless Architecture",
                "abstract": "This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation DoubleML-Serverless for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs."
            },
            {
                "arxivId": "2012.00745",
                "title": "Double machine learning for sample selection models+",
                "abstract": "This paper considers treatment evaluation when outcomes are only observed for a subpopulation due to sample selection or outcome attrition/non-response. For identification, we combine a selection-on-observables assumption for treatment assignment with either selection-on-observables or instrumental variable assumptions concerning the outcome attrition/sample selection process. To control in a data-driven way for potentially high dimensional pre-treatment covariates that motivate the selection-on-observables assumptions, we adapt the double machine learning framework to sample selection problems. That is, we make use of (a) Neyman-orthogonal and doubly robust score functions, which imply the robustness of treatment effect estimation to moderate regularization biases in the machine learning-based estimation of the outcome, treatment, or sample selection models and (b) sample splitting (or cross-fitting) to prevent overfitting bias. We demonstrate that the proposed estimators are asymptotically normal and root-n consistent under specific regularity conditions concerning the machine learners. The estimator is available in the causalweight package for the statistical software R."
            },
            {
                "arxivId": "2003.03191",
                "title": "Double Machine Learning Based Program Evaluation under Unconfoundedness",
                "abstract": "\n This paper reviews, applies and extends recently proposed methods based on Double Machine Learning (DML) with a focus on program evaluation under unconfoundedness. DML based methods leverage flexible prediction models to adjust for confounding variables in the estimation of (i) standard average effects, (ii) different forms of heterogeneous effects, and (iii) optimal treatment assignment rules. An evaluation of multiple programs of the Swiss Active Labour Market Policy illustrates how DML based methods enable a comprehensive program evaluation. Motivated by extreme individualised treatment effect estimates of the DR-learner, we propose the normalised DR-learner (NDR-learner) to address this issue. The NDR-learner acknowledges that individualised effect estimates can be stabilised by an individualised normalisation of inverse probability weights."
            },
            {
                "arxivId": "2002.12710",
                "title": "Causal mediation analysis with double machine learning",
                "abstract": "\n This paper combines causal mediation analysis with double machine learning for a data-driven control of observed confounders in a high-dimensional setting. The average indirect effect of a binary treatment and the unmediated direct effect are estimated based on efficient score functions, which are robust w.r.t. misspecifications of the outcome, mediator, and treatment models. This property is key for selecting these models by double machine learning, which is combined with data splitting to prevent overfitting. We demonstrate that the effect estimators are asymptotically normal and n\u22121/2-consistent under specific regularity conditions and investigate the finite sample properties of the suggested methods in a simulation study when considering lasso as machine learner. We also provide an empirical application to the U.S. National Longitudinal Survey of Youth, assessing the indirect effect of health insurance coverage on general health operating via routine checkups as mediator, as well as the direct effect."
            },
            {
                "arxivId": "1610.01271",
                "title": "Generalized random forests",
                "abstract": "We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN."
            },
            {
                "arxivId": "1503.06426",
                "title": "High-dimensional inference in misspecified linear models",
                "abstract": "We consider high-dimensional inference when the assumed linear model is misspecified. We describe some correct interpretations and corresponding sufficient assumptions for valid asymptotic inference of the model parameters, which still have a useful meaning when the model is misspecified. We largely focus on the de-sparsified Lasso procedure but we also indicate some implications for (multiple) sample splitting techniques. In view of available methods and software, our results contribute to robustness considerations with respect to model misspecification."
            },
            {
                "arxivId": "1305.6099",
                "title": "Inference on Treatment Effects after Selection Amongst High-Dimensional Controls",
                "abstract": "In this supplementary appendix we provide additional results, omitted proofs and extensive simulations that complement the analysis of the main text (arXiv:1201.0224)."
            },
            {
                "arxivId": "1110.2563",
                "title": "Confidence intervals for low dimensional parameters in high dimensional linear models",
                "abstract": "The purpose of this paper is to propose methodologies for statistical inference of low dimensional parameters with high dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broader context. The theoretical results that are presented provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite dimensional covariance matrices. These sufficient conditions allow the number of variables to exceed the sample size and the presence of many small non\u2010zero coefficients. Our methods and theory apply to interval estimation of a preconceived regression coefficient or contrast as well as simultaneous interval estimation of many regression coefficients. Moreover, the method proposed turns the regression data into an approximate Gaussian sequence of point estimators of individual regression coefficients, which can be used to select variables after proper thresholding. The simulation results that are presented demonstrate the accuracy of the coverage probability of the confidence intervals proposed as well as other desirable properties, strongly supporting the theoretical results."
            },
            {
                "arxivId": "1105.1475",
                "title": "Pivotal estimation via square-root Lasso in nonparametric regression",
                "abstract": "We propose a self-tuning $\\sqrt{\\mathrm {Lasso}} $ method that simultaneously resolves three important practical problems in high-dimensional regression analysis, namely it handles the unknown scale, heteroscedasticity and (drastic) non-Gaussianity of the noise. In addition, our analysis allows for badly behaved designs, for example, perfectly collinear regressors, and generates sharp bounds even in extreme cases, such as the infinite variance case and the noiseless case, in contrast to Lasso. We establish various nonasymptotic bounds for $\\sqrt{\\mathrm {Lasso}} $ including prediction norm rate and sparsity. Our analysis is based on new impact factors that are tailored for bounding prediction norm. In order to cover heteroscedastic non-Gaussian noise, we rely on moderate deviation theory for self-normalized sums to achieve Gaussian-like results under weak conditions. Moreover, we derive bounds on the performance of ordinary least square (ols) applied to the model selected by $\\sqrt{\\mathrm {Lasso}} $ accounting for possible misspecification of the selected model. Under mild conditions, the rate of convergence of ols post $\\sqrt{\\mathrm {Lasso}} $ is as good as $\\sqrt{\\mathrm {Lasso}} $\u2019s rate. As an application, we consider the use of $\\sqrt{\\mathrm {Lasso}} $ and ols post $\\sqrt{\\mathrm {Lasso}} $ as estimators of nuisance parameters in a generic semiparametric problem (nonlinear moment condition or $Z$-problem), resulting in a construction of $\\sqrt{n}$-consistent and asymptotically normal estimators of the main parameters."
            },
            {
                "arxivId": "1201.0490",
                "title": "Scikit-learn: Machine Learning in Python",
                "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2208.01594",
        "category": "econ",
        "title": "The character of non-manipulable collective choices between two alternatives",
        "abstract": "We consider classes of non-manipulable social choice functions with range of cardinality at most two within a set of at least two alternatives. We provide the functional form for each of the classes we consider. This functional form is a characterization that explicitly describes how a social choice function of that particular class selects the collective choice corresponding to a profile. We provide a unified formulation of these characterizations using the new concept of\"character\". The choice of the character, depending on the class of social choice functions, gives the functional form of all social choice functions of the class.",
        "references": [
            {
                "arxivId": "2104.10205",
                "title": "On the relation between preference reversal and strategy-proofness",
                "abstract": null
            },
            {
                "arxivId": "2008.02041",
                "title": "Geometry of anonymous binary social choices that are strategy-proof",
                "abstract": null
            },
            {
                "arxivId": "2007.01552",
                "title": "Anonymous, non-manipulable binary social choice",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2306.03816",
        "category": "econ",
        "title": "Parametrization, Prior Independence, and the Semiparametric Bernstein-von Mises Theorem for the Partially Linear Model",
        "abstract": "I prove a semiparametric Bernstein-von Mises theorem for a partially linear regression model with independent priors for the low-dimensional parameter of interest and the infinite-dimensional nuisance parameters. My result avoids a prior invariance condition that arises from a loss of information in not knowing the nuisance parameter. The key idea is a feasible reparametrization of the regression function that mimics the Gaussian profile likelihood. This allows a researcher to assume independent priors for the model parameters while automatically accounting for the loss of information associated with not knowing the nuisance parameter. As these prior stability conditions often impose strong restrictions on the underlying data-generating process, my results provide a more robust asymptotic normality theorem than the original parametrization of the partially linear model.",
        "references": [
            {
                "arxivId": "2211.16298",
                "title": "Double Robust Bayesian Inference on Average Treatment Effects",
                "abstract": "We propose a double robust Bayesian inference procedure on the average treatment effect (ATE) under unconfoundedness. Our robust Bayesian approach involves two important modifications: first, we adjust the prior distributions of the conditional mean function; second, we correct the posterior distribution of the resulting ATE. Both adjustments make use of pilot estimators motivated by the semiparametric influence function for ATE estimation. We prove asymptotic equivalence of our Bayesian procedure and efficient frequentist ATE estimators by establishing a new semiparametric Bernstein-von Mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. Consequently, the resulting Bayesian credible sets form confidence intervals with asymptotically exact coverage probability. In simulations, our double robust Bayesian procedure leads to significant bias reduction of point estimation over conventional Bayesian methods and more accurate coverage of confidence intervals compared to existing frequentist methods. We illustrate our method in an application to the National Supported Work Demonstration."
            },
            {
                "arxivId": "1909.12078",
                "title": "Debiased Bayesian inference for average treatment effects",
                "abstract": "Bayesian approaches have become increasingly popular in causal inference problems due to their conceptual simplicity, excellent performance and in-built uncertainty quantification ('posterior credible sets'). We investigate Bayesian inference for average treatment effects from observational data, which is a challenging problem due to the missing counterfactuals and selection bias. Working in the standard potential outcomes framework, we propose a data-driven modification to an arbitrary (nonparametric) prior based on the propensity score that corrects for the first-order posterior bias, thereby improving performance. We illustrate our method for Gaussian process (GP) priors using (semi-)synthetic data. Our experiments demonstrate significant improvement in both estimation accuracy and uncertainty quantification compared to the unmodified GP, rendering our approach highly competitive with the state-of-the-art."
            },
            {
                "arxivId": "1704.02646",
                "title": "Posterior asymptotic normality for an individual coordinate in high-dimensional linear regression",
                "abstract": "We consider the sparse high-dimensional linear regression model $Y=Xb+\\epsilon$ where $b$ is a sparse vector. For the Bayesian approach to this problem, many authors have considered the behavior of the posterior distribution when, in truth, $Y=X\\beta+\\epsilon$ for some given $\\beta$. There have been numerous results about the rate at which the posterior distribution concentrates around $\\beta$, but few results about the shape of that posterior distribution. We propose a prior distribution for $b$ such that the marginal posterior distribution of an individual coordinate $b_i$ is asymptotically normal centered around an asymptotically efficient estimator, under the truth. Such a result gives Bayesian credible intervals that match with the confidence intervals obtained from an asymptotically efficient estimator for $b_i$. We also discuss ways of obtaining such asymptotically efficient estimators on individual coordinates. We compare the two-step procedure proposed by Zhang and Zhang (2014) and a one-step modified penalization method."
            },
            {
                "arxivId": "1607.04367",
                "title": "The semi-parametric Bernstein-von Mises theorem for regression models with symmetric errors",
                "abstract": "In a smooth semi-parametric model, the marginal posterior distribution of a finite-dimensional parameter of interest is expected to be asymptotically equivalent to the sampling distribution of any efficient point estimator. This assertion leads to asymptotic equivalence of the credible and confidence sets of the parameter of interest, and is known as the semi-parametric Bernstein-von Mises theorem. In recent years, this theorem has received much attention and has been widely applied. Here, we consider models in which errors with symmetric densities play a role. Specifically, we show that the marginal posterior distributions of the regression coefficients in linear regression and linear mixed-effect models satisfy the semi-parametric Bernstein-von Mises assertion. As a result, Bayes estimators in these models achieve frequentist inferential optimality, as expressed, for example, in Hajek\u2019s convolution and asymptotic minimax theorems. For the prior on the space of error densities, we provide two well-known examples, namely, the Dirichlet process mixture of normal densities and random series priors. The results provide efficient estimates of the regression coefficients in the linear mixed-effect model, for which no efficient point estimators currently exist."
            },
            {
                "arxivId": "1605.00499",
                "title": "Monte Carlo Confidence Sets for Identified Sets",
                "abstract": "In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows."
            },
            {
                "arxivId": "1602.02176",
                "title": "Regularization and Confounding in Linear Regression for Treatment Effect Estimation",
                "abstract": "This paper investigates the use of regularization priors in the context of treatment effect estimation using observational data where the number of control variables is large relative to the number of observations. A reparameterized simultaneous regression model is presented which permits prior specifications designed to overcome certain pitfalls of the more conventional direct parametrization. The new approach is illustrated on synthetic and empirical data."
            },
            {
                "arxivId": "1503.04493",
                "title": "Semiparametric Bernstein-von Mises Theorem: Second Order Studies",
                "abstract": "The major goal of this paper is to study the second order frequentist properties of the marginal posterior distribution of the parametric component in semiparametric Bayesian models, in particular, a second order semiparametric Bernstein-von Mises (BvM) Theorem. Our first contribution is to discover an interesting interference phenomenon between Bayesian estimation and frequentist inferential accuracy: more accurate Bayesian estimation on the nuisance function leads to higher frequentist inferential accuracy on the parametric component. As the second contribution, we propose a new class of dependent priors under which Bayesian inference procedures for the parametric component are not only efficient but also adaptive (w.r.t. the smoothness of nonparametric component) up to the second order frequentist validity. However, commonly used independent priors may even fail to produce a desirable root-n contraction rate for the parametric component in this adaptive case unless some stringent assumption is imposed. Three important classes of semiparametric models are examined, and extensive simulations are also provided."
            },
            {
                "arxivId": "1007.0179",
                "title": "The semiparametric Bernstein-von Mises theorem",
                "abstract": "In a smooth semiparametric estimation problem, the marginal posterior for the parameter of interest is expected to be asymptotically normal and satisfy frequentist criteria of optimality if the model is endowed with a suitable prior. It is shown that, under certain straightforward and interpretable conditions, the assertion of Le Cam\u2019s acclaimed, but strictly parametric, Bernstein-von Mises theorem [Univ. California Publ. Statist. 1 (1953) 277-329] holds in the semiparametric situation as well. As a consequence, Bayesian point-estimators achieve efficiency, for example, in the sense of Hajek\u2019s convolution theorem [Z. Wahrsch. Verw. Gebiete 14 (1970) 323-330]. The model is required to satisfy differentiability and metric entropy conditions, while the nuisance prior must assign nonzero mass to certain Kullback-Leibler neighborhoods [Ghosal, Ghosh and van der Vaart Ann. Statist. 28 (2000) 500-531]. In addition, the marginal posterior is required to converge at parametric rate, which appears to be the most stringent condition in examples. The results are applied to estimation of the linear coefficient in partial linear regression, with a Gaussian prior on a smoothness class for the nuisance."
            },
            {
                "arxivId": "0908.4167",
                "title": "BERNSTEIN-VON MISES THEOREM FOR LINEAR FUNCTIONALS OF THE DENSITY",
                "abstract": "In this paper, we study the asymptotic posterior distribution of linear functionals of the density by deriving general conditions to obtain a semi-parametric version of the Bernstein\u2013von Mises theorem. The special case of the cumulative distributive function, evaluated at a specific point, is widely considered. In particular, we show that for infinite-dimensional exponential families, under quite general assumptions, the asymptotic posterior distribution of the functional can be either Gaussian or a mixture of Gaussian distributions with different centering points. This illustrates the positive, but also the negative, phenomena that can occur in the study of Bernstein\u2013von Mises results."
            },
            {
                "arxivId": "0807.2734",
                "title": "Lower bounds for posterior rates with Gaussian process priors",
                "abstract": "Upper bounds for rates of convergence of posterior distributions associated to Gaussian process priors are obtained by van der Vaart and van Zanten in [14] and expressed in terms of a concentration function involving the Reproducing Kernel Hilbert Space of the Gaussian prior. Here lower-bound counterparts are obtained. As a corollary, we obtain the precise rate of convergence of posteriors for Gaussian priors in various settings. Additionally, we extend the upper-bound results of [14] about Riemann-Liouville priors to a continuous family of parameters."
            },
            {
                "arxivId": "0806.3024",
                "title": "Rates of contraction of posterior distributions based on Gaussian process priors",
                "abstract": "We derive rates of contraction of posterior distributions on nonparametric or semiparametric models based on Gaussian processes. The rate of contraction is shown to depend on the position of the true parameter relative to the reproducing kernel Hilbert space of the Gaussian process and the small ball probabilities of the Gaussian process. We determine these quantities for a range of examples of Gaussian priors and in several statistical settings. For instance, we consider the rate of contraction of the posterior distribution based on sampling from a smooth density model when the prior models the log density as a (fractionally integrated) Brownian motion. We also consider regression with Gaussian errors and smooth classification under a logistic or probit link function combined with various priors."
            },
            {
                "arxivId": "0805.3252",
                "title": "Reproducing kernel Hilbert spaces of Gaussian priors",
                "abstract": "We review definitions and properties of reproducing kernel Hilbert spaces attached to Gaussian variables and processes, with a view to applications in nonparametric Bayesian statistics using Gaussian priors. The rate of contraction of posterior distributions based on Gaussian priors can be described through a concentration function that is expressed in the reproducing Hilbert space. Absolute continuity of Gaussian measures and concentration inequalities play an important role in understanding and deriving this result. Series expansions of Gaussian variables and transformations of their reproducing kernel Hilbert spaces under linear maps are useful tools to compute the concentration function."
            },
            {
                "arxivId": "0708.0491",
                "title": "Convergence rates of posterior distributions for non-i.i.d. observations",
                "abstract": "We consider the asymptotic behavior of posterior distributions and Bayes estimators based on observations which are required to be neither independent nor identically distributed. We give general results on the rate of convergence of the posterior measure relative to distances derived from a testing criterion. We then specialize our results to independent, nonidentically distributed observations, Markov processes, stationary Gaussian time series and the white noise model. We apply our general results to several examples of infinite-dimensional statistical models including nonparametric regression with normal errors, binary regression, Poisson regression, an interval censoring model, Whittle estimation of the spectral density of a time series and a nonlinear autoregressive model. \u00a9 Institute of Mathematical Statistics, 2007."
            },
            {
                "arxivId": "math/0611230",
                "title": "The Bernstein\u2013von Mises theorem for the proportional hazard model",
                "abstract": "We study large sample properties of Bayesian analysis of the proportional hazard model with neutral to the right process priors on the baseline hazard function. We show that the posterior distribution of the baseline cumulative hazard function and regression coefficients centered at the maximum likelihood estimator is jointly asymptotically equivalent to the sampling distribution of the maximum likelihood estimator."
            },
            {
                "arxivId": "math/0607023",
                "title": "Misspecification in infinite-dimensional Bayesian statistics",
                "abstract": "We consider the asymptotic behavior of posterior distributions if the model is misspecified. Given a prior distribution and a random sample from a distribution P 0 , which may not be in the support of the prior, we show that the posterior concentrates its mass near the points in the support of the prior that minimize the Kullback-Leibler divergence with respect to P 0 . An entropy condition and a prior-mass condition determine the rate of convergence. The method is applied to several examples, with special interest for infinite-dimensional models. These include Gaussian mixtures, nonparametric regression and parametric models."
            },
            {
                "arxivId": "2301.07782",
                "title": "An MCMC Approach to Classical Estimation",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2310.18504",
        "category": "econ",
        "title": "Doubly Robust Identification of Causal Effects of a Continuous Treatment using Discrete Instruments",
        "abstract": "Many empirical applications estimate causal effects of a continuous endogenous variable (treatment) using a binary instrument. Estimation is typically done through linear 2SLS. This approach requires a mean treatment change and causal interpretation requires the LATE-type monotonicity in the first stage. An alternative approach is to explore distributional changes in the treatment, where the first-stage restriction is treatment rank similarity. We propose causal estimands that are doubly robust in that they are valid under either of these two restrictions. We apply the doubly robust estimation to estimate the impacts of sleep on well-being. Our new estimates corroborate the usual 2SLS estimates.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2402.17042",
        "category": "econ",
        "title": "Towards Generalizing Inferences from Trials to Target Populations",
        "abstract": "Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop's discourse; and identify persistent hurdles while suggesting avenues for future research. By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods.",
        "references": [
            {
                "arxivId": "2401.14512",
                "title": "Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population",
                "abstract": "Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial -- investigating the effectiveness of medication for opioid use disorder -- to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations."
            },
            {
                "arxivId": "2311.09388",
                "title": "Synthesis estimators for positivity violations with a continuous covariate",
                "abstract": "Studies intended to estimate the effect of a treatment, like randomized trials, often consist of a biased sample of the desired target population. To correct for this bias, estimates can be transported to the desired target population. Methods for transporting between populations are often premised on a positivity assumption, such that all relevant covariate patterns in one population are also present in the other. However, eligibility criteria, particularly in the case of trials, can result in violations of positivity. To address nonpositivity, a synthesis of statistical and mathematical models can be considered. This approach integrates multiple data sources (e.g. trials, observational, pharmacokinetic studies) to estimate treatment effects, leveraging mathematical models to handle positivity violations. This approach was previously demonstrated for positivity violations by a single binary covariate. Here, we extend the synthesis approach for positivity violations with a continuous covariate. For estimation, two novel augmented inverse probability weighting estimators are proposed. Both estimators are contrasted with other common approaches for addressing nonpositivity. Empirical performance is compared via Monte Carlo simulation. Finally, the competing approaches are illustrated with an example in the context of two-drug versus one-drug antiretroviral therapy on CD4 T cell counts among women with HIV."
            },
            {
                "arxivId": "2310.18500",
                "title": "Designing Randomized Experiments to Predict Unit-Specific Treatment Effects",
                "abstract": "Typically, a randomized experiment is designed to test a hypothesis about the average treatment effect and sometimes hypotheses about treatment effect variation. The results of such a study may then be used to inform policy and practice for units not in the study. In this paper, we argue that given this use, randomized experiments should instead be designed to predict unit-specific treatment effects in a well-defined population. We then consider how different sampling processes and models affect the bias, variance, and mean squared prediction error of these predictions. The results indicate, for example, that problems of generalizability (differences between samples and populations) can greatly affect bias both in predictive models and in measures of error in these models. We also examine when the average treatment effect estimate outperforms unit-specific treatment effect predictive models and implications of this for planning studies."
            },
            {
                "arxivId": "2309.03693",
                "title": "A Two-Stage Method for Extending Inferences from a Collection of Trials",
                "abstract": "When considering the effect a treatment will cause in a population of interest, we often look to evidence from randomized controlled trials. In settings where multiple trials on a treatment are available, we may wish to synthesize the trials' participant data to obtain causally interpretable estimates of the average treatment effect in a specific target population. Traditional meta-analytic approaches to synthesizing data from multiple studies estimate the average effect among the studies. The resulting estimate is often not causally interpretable in any population, much less a particular population of interest, due to heterogeneity in the effect of treatment across studies. Inspired by traditional two-stage meta-analytic methods, as well as methods for extending inferences from a single study, we propose a two-stage approach to extending inferences from a collection of randomized controlled trials that can be used to obtain causally interpretable estimates of treatment effects in a target population when there is between-study heterogeneity in conditional average treatment effects. We first introduce a collection of assumptions under which the target population's average treatment effect is identifiable when conditional average treatment effects are heterogeneous across studies. We then introduce an estimator that utilizes weighting in two stages, taking a weighted average of study-specific estimates of the treatment effect in the target population. The performance of our proposed approach is assessed through simulation studies and an application to a collection of trials studying an online therapy treatment for symptoms of pediatric traumatic brain injury."
            },
            {
                "arxivId": "2307.04304",
                "title": "Enhancing Treatment Effect Estimation: A Model Robust Approach Integrating Randomized Experiments and External Controls using the Double Penalty Integration Estimator",
                "abstract": "Randomized experiments (REs) are the cornerstone for treatment effect evaluation. However, due to practical considerations, REs may encounter difficulty recruiting sufficient patients. External controls (ECs) can supplement REs to boost estimation efficiency. Yet, there may be incomparability between ECs and concurrent controls (CCs), resulting in misleading treatment effect evaluation. We introduce a novel bias function to measure the difference in the outcome mean functions between ECs and CCs. We show that the ANCOVA model augmented by the bias function for ECs renders a consistent estimator of the average treatment effect, regardless of whether or not the ANCOVA model is correct. To accommodate possibly different structures of the ANCOVA model and the bias function, we propose a double penalty integration estimator (DPIE) with different penalization terms for the two functions. With an appropriate choice of penalty parameters, our DPIE ensures consistency, oracle property, and asymptotic normality even in the presence of model misspecification. DPIE is more efficient than the estimator derived from REs alone, validated through theoretical and experimental results."
            },
            {
                "arxivId": "2307.01449",
                "title": "A Double Machine Learning Approach to Combining Experimental and Observational Data",
                "abstract": "Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one of these assumptions is violated, we provide semiparametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. Through comparative analyses, we show our framework's superiority over existing data fusion methods. The practical utility of our approach is further exemplified by three real-world case studies, underscoring its potential for widespread application in empirical research."
            },
            {
                "arxivId": "2302.13428",
                "title": "Methods for Integrating Trials and Non-experimental Data to Examine Treatment Effect Heterogeneity.",
                "abstract": "Estimating treatment effects conditional on observed covariates can improve the ability to tailor treatments to particular individuals. Doing so effectively requires dealing with potential confounding, and also enough data to adequately estimate effect moderation. A recent influx of work has looked into estimating treatment effect heterogeneity using data from multiple randomized controlled trials and/or observational datasets. With many new methods available for assessing treatment effect heterogeneity using multiple studies, it is important to understand which methods are best used in which setting, how the methods compare to one another, and what needs to be done to continue progress in this field. This paper reviews these methods broken down by data setting: aggregate-level data, federated learning, and individual participant-level data. We define the conditional average treatment effect and discuss differences between parametric and nonparametric estimators, and we list key assumptions, both those that are required within a single study and those that are necessary for data combination. After describing existing approaches, we compare and contrast them and reveal open areas for future research. This review demonstrates that there are many possible approaches for estimating treatment effect heterogeneity through the combination of datasets, but that there is substantial work to be done to compare these methods through case studies and simulations, extend them to different settings, and refine them to account for various challenges present in real data."
            },
            {
                "arxivId": "2211.04876",
                "title": "Generalizing and transporting inferences about the effects of treatment assignment subject to non-adherence",
                "abstract": "We discuss the identifiability of causal estimands for generalizability and transportability analyses, both under perfect and imperfect adherence to treatment assignment. We consider a setting where the trial data contain information on baseline covariates, assignment at baseline, intervention at baseline (point treatment), and outcomes; and where the data from non-randomized individuals only contain information on baseline covariates. In this setting, we review identification results under perfect adherence and study two examples in which non-adherence severely limits the ability to transport inferences about the effects of treatment assignment to the target population. In the first example, trial participation has a direct effect on treatment receipt and, through treatment receipt, on the outcome (a\"trial engagement effect\"via adherence). In the second example, participation in the trial has unmeasured common causes with treatment receipt. In both examples, the effect of assignment on the outcome in the target population is not identifiable. In the first example, however, the effect of joint interventions to scale-up trial activities that affect adherence and assign treatment is identifiable. We conclude that generalizability and transportability analyses should consider trial engagement effects via adherence and selection for participation on the basis of unmeasured factors that influence adherence."
            },
            {
                "arxivId": "2208.05543",
                "title": "Heterogeneity assessment in causal data fusion problems",
                "abstract": "Previous works have formalized the conditions under which findings from a source population could be reasonably extrapolated to another target population, the so-called\"transportability\"problem. While most of these works focus on a setting with two populations, many recent works have also provided the identifiability of a causal parameter when multiple data sources are available, under certain homogeneity assumptions. However, we know of little work examining transportability when data sources are possibly heterogeneous, e.g. in the distribution of mediators of the exposure-outcome relation. The presence of such heterogeneity generally invalidates the transportability assumption required in most of the literature. In this paper, we will propose a general approach for heterogeneity assessment when estimating the average exposure effect in a target population, with mediator and outcome data obtained from multiple external sources. To account for heterogeneity, we define different effect estimands when the mediator and outcome information is transported from different sources. We discuss the causal assumptions to identify these estimands, then propose efficient semi-parametric estimation strategies that allow the use of flexible data-adaptive machine learning methods to estimate the nuisance parameters. We also propose two new methods to investigate sources of heterogeneity in the transported estimates. These methods will inform users about how much of the observed statistical heterogeneity in the transported effects is due to the differences across data sources in: 1) conditional distribution of mediator variables, and/or 2) conditional distribution of the outcome. We illustrate the proposed methods using four sites that were part of the Moving to Opportunity Study, which was an experiment that randomized housing voucher receipt to participating families living in public housing."
            },
            {
                "arxivId": "2202.03408",
                "title": "Sensitivity Analysis in the Generalization of Experimental Results",
                "abstract": "Randomized controlled trials (RCT\u2019s) allow researchers to estimate causal e\ufb00ects in an experimental sample with minimal identifying assumptions. However, to generalize or transport a causal e\ufb00ect from an RCT to a target population, researchers must adjust for a set of treatment e\ufb00ect moderators. In practice, it is impossible to know whether the set of moderators has been properly accounted for. In the following paper, I propose a three parameter sensitivity analysis for generalizing or transporting experimental results using weighted estimators, with several advantages over existing methods. First, the framework does not require assumptions on the underlying data generating process for either the experimental sample selection mechanism or treatment e\ufb00ect heterogeneity. Second, I show that the sensitivity parameters are guaranteed to be bounded and propose several tools researchers can use to perform sensitivity analysis: (1) graphical and numerical summaries for researchers to assess how robust a point estimate is to killer confounders; (2) an extreme scenario analysis; and (3) a formal benchmarking approach for researchers to estimate potential sensitivity parameter values using existing data. Finally, I demonstrate that the proposed framework can be easily extended to the class of doubly robust, augmented weighted estimators. The sensitivity analysis framework is applied to a set of Jobs Training Program experiments."
            },
            {
                "arxivId": "2110.00107",
                "title": "Regression-based estimation of heterogeneous treatment effects when extending inferences from a randomized trial to a target population",
                "abstract": null
            },
            {
                "arxivId": "2103.04907",
                "title": "Generalizing trial evidence to target populations in non\u2010nested designs: Applications\u00a0to AIDS clinical trials",
                "abstract": "Comparative effectiveness evidence from randomized trials may not be directly generalizable to a target population of substantive interest when, as in most cases, trial participants are not randomly sampled from the target population. Motivated by the need to generalize evidence from two trials conducted in the AIDS Clinical Trials Group (ACTG), we consider weighting, regression and doubly robust estimators to estimate the causal effects of HIV interventions in a specified population of people living with HIV in the USA. We focus on a non\u2010nested trial design and discuss strategies for both point and variance estimation of the target population average treatment effect. Specifically in the generalizability context, we demonstrate both analytically and empirically that estimating the known propensity score in trials does not increase the variance for each of the weighting, regression and doubly robust estimators. We apply these methods to generalize the average treatment effects from two ACTG trials to specified target populations and operationalize key practical considerations. Finally, we report on a simulation study that investigates the finite\u2010sample operating characteristics of the generalizability estimators and their sandwich variance estimators."
            },
            {
                "arxivId": "2102.11904",
                "title": "A Review of Generalizability and Transportability",
                "abstract": "When assessing causal effects, determining the target population to which the results are intended to generalize is a critical decision. Randomized and observational studies each have strengths and limitations for estimating causal effects in a target population. Estimates from randomized data may have internal validity but are often not representative of the target population. Observational data may better reflect the target population, and hence be more likely to have external validity, but are subject to potential bias due to unmeasured confounding. While much of the causal inference literature has focused on addressing internal validity bias, both internal and external validity are necessary for unbiased estimates in a target population. This article presents a framework for addressing external validity bias, including a synthesis of approaches for generalizability and transportability, and the assumptions they require, as well as tests for the heterogeneity of treatment effects and differences between study and target populations. Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 10 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates."
            },
            {
                "arxivId": "2011.08047",
                "title": "Causal Inference Methods for Combining Randomized Trials and Observational Studies: A Review",
                "abstract": "With increasing data availability, treatment causal effects can be evaluated across different dataset, both randomized trials and observational studies. Randomized trials isolate the effect of the treatment from that of unwanted (confounding) co-occuring effects. But they may be applied to limited populations, and thus lack external validity. On the opposite large observational samples are often more representative of the target population but can conflate confounding effects with the treatment of interest. In this paper, we review the growing literature on methods for causal inference on combined randomized trial and observational studies, striving for the best of both worlds. We first discuss identification and estimation methods that improve generalizability of randomized controlled trials (RCTs) using the representativeness of observational data. Classical estimators include weighting, difference between conditional outcome models, and double robust estimators. We then discuss methods that combining RCTs and observational data to improve the (conditional) average treatment effect estimation, handling possible unmeasured confounding in the observational data. We also connect and contrast works developed in both the potential outcomes framework and the structural causal models framework. Finally, we compare the main methods using a simulation study and real world data to analyse the effect of tranexamic acid on the mortality rate in major trauma patients. Code to implement many of the methods is provided."
            },
            {
                "arxivId": "1911.02685",
                "title": "A Comprehensive Survey on Transfer Learning",
                "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice."
            },
            {
                "arxivId": "1908.09230",
                "title": "Efficient and robust methods for causally interpretable meta\u2010analysis: Transporting inferences from multiple randomized trials to a target population",
                "abstract": "We present methods for causally interpretable meta\u2010analyses that combine information from multiple randomized trials to draw causal inferences for a target population of substantive interest. We consider identifiability conditions, derive implications of the conditions for the law of the observed data, and obtain identification results for transporting causal inferences from a collection of independent randomized trials to a new target population in which experimental data may not be available. We propose an estimator for the potential outcome mean in the target population under each treatment studied in the trials. The estimator uses covariate, treatment, and outcome data from the collection of trials, but only covariate data from the target population sample. We show that it is doubly robust in the sense that it is consistent and asymptotically normal when at least one of the models it relies on is correctly specified. We study the finite sample properties of the estimator in simulation studies and demonstrate its implementation using data from a multicenter randomized trial."
            },
            {
                "arxivId": "1905.10943",
                "title": "Distributionally Robust Optimization and Generalization in Kernel Methods",
                "abstract": "Distributionally robust optimization (DRO) has attracted attention in machine learning due to its connections to regularization, generalization, and robustness. Existing work has considered uncertainty sets based on phi-divergences and Wasserstein distances, each of which have drawbacks. In this paper, we study DRO with uncertainty sets measured via maximum mean discrepancy (MMD). We show that MMD DRO is roughly equivalent to regularization by the Hilbert norm and, as a byproduct, reveal deep connections to classic results in statistical learning. In particular, we obtain an alternative proof of a generalization bound for Gaussian kernel ridge regression via a DRO lense. The proof also suggests a new regularizer. Our results apply beyond kernel methods: we derive a generically applicable approximation of MMD DRO, and show that it generalizes recent work on variance-based regularization."
            },
            {
                "arxivId": "1812.11806",
                "title": "An introduction to domain adaptation and transfer learning",
                "abstract": "In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical."
            },
            {
                "arxivId": "1810.08750",
                "title": "Learning Models with Uniform Performance via Distributionally Robust Optimization",
                "abstract": "A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts, or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition, and providing good tail performance, the distributionally robust approach often exhibits improved performance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2402.17103",
        "category": "econ",
        "title": "Causal Orthogonalization: Multicollinearity, Economic Interpretability, and the Gram-Schmidt Process",
        "abstract": "This paper considers the problem of interpreting orthogonalization model coefficients. We derive a causal economic interpretation of the Gram-Schmidt orthogonalization process and provide the conditions for its equivalence to total effects from a recursive Directed Acyclic Graph. We extend the Gram-Schmidt process to groups of simultaneous regressors common in economic data sets and derive its finite sample properties, finding its coefficients to be unbiased, stable, and more efficient than those from Ordinary Least Squares. Finally, we apply the estimator to childhood reading comprehension scores, controlling for such highly collinear characteristics as race, education, and income. The model expands Bohren et al.'s decomposition of systemic discrimination into channel-specific effects and improves its coefficient significance levels.",
        "references": [
            {
                "arxivId": "1907.07271",
                "title": "Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics",
                "abstract": "In this essay I discuss potential outcome and graphical approaches to causality, and their relevance for empirical work in economics. I review some of the work on directed acyclic graphs, including the recent The Book of Why (Pearl and Mackenzie 2018). I also discuss the potential outcome framework developed by Rubin and coauthors (e.g., Rubin 2006), building on work by Neyman (1990 [1923]). I then discuss the relative merits of these approaches for empirical work in economics, focusing on the questions each framework answers well, and why much of the the work in economics is closer in spirit to the potential outcome perspective. (JEL C31, C36, I26)"
            },
            {
                "arxivId": "1011.1079",
                "title": "Identification, Inference and Sensitivity Analysis for Causal Mediation Effects",
                "abstract": "Causal mediation analysis is routinely conducted by applied researchers in a variety of disciplines. The goal of such an analysis is to investigate alternative causal mechanisms by examining the roles of intermediate variables that lie in the causal paths between the treat- ment and outcome variables. In this paper we first prove that under a particular version of sequential ignorability assumption, the aver- age causal mediation effect (ACME) is nonparametrically identified. We compare our identification assumption with those proposed in the literature. Some practical implications of our identification result are also discussed. In particular, the popular estimator based on the linear structural equation model (LSEM) can be interpreted as an ACME estimator once additional parametric assumptions are made. We show that these assumptions can easily be relaxed within and outside of the LSEM framework and propose simple nonparametric estimation strate- gies. Second, and perhaps most importantly, we propose a new sensi- tivity analysis that can be easily implemented by applied researchers within the LSEM framework. Like the existing identifying assumptions, the proposed sequential ignorability assumption may be too strong in many applied settings. Thus, sensitivity analysis is essential in order to examine the robustness of empirical findings to the possible existence of an unmeasured confounder. Finally, we apply the proposed methods to a randomized experiment from political psychology. We also make easy-to-use software available to implement the proposed methods."
            },
            {
                "arxivId": "1301.2300",
                "title": "Direct and Indirect Effects",
                "abstract": "The direct effect of one event on another can be defined and measured by holding constant all intermediate variables between the two. Indirect effects present conceptual and practical difficulties (in nonlinear models), because they cannot be isolated by holding certain variables constant. This paper presents a new way of defining the effect transmitted through a restricted set of paths, without controlling variables on the remaining paths. This permits the assessment of a more natural type of direct and indirect effects, one that is applicable in both linear and nonlinear models and that has broader policy-related interpretations. The paper establishes conditions under which such assessments can be estimated consistently from experimental and nonexperimental data, and thus extends path-analytic techniques to nonlinear and nonparametric models."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2402.17142",
        "category": "econ",
        "title": "Distributions of Posterior Quantiles via Matching",
        "abstract": "We offer a simple analysis of the problem of choosing a statistical experiment to optimize the induced distribution of posterior medians, or more generally $q$-quantiles for any $q \\in (0,1)$. We show that all implementable distributions of the posterior $q$-quantile are implemented by a single experiment, the $q$-quantile matching experiment, which pools pairs of states across the $q$-quantile of the prior in a positively assortative manner, with weight $q$ on the lower state in each pair. A dense subset of implementable distributions of posterior $q$-quantiles can be uniquely implemented by perturbing the $q$-quantile matching experiment. A linear functional is optimized over distributions of posterior $q$-quantiles by taking the optimal selection from each set of $q$-quantiles induced by the $q$-quantile matching experiment. The $q$-quantile matching experiment is the only experiment that simultaneously implements all implementable distributions of the posterior $q$-quantile.",
        "references": [
            {
                "arxivId": "2311.02889",
                "title": "Persuasion and Matching: Optimal Productive Transport",
                "abstract": "We consider general Bayesian persuasion problems where the receiver's utility is single-peaked in a one-dimensional action. We show that a signal that pools at most two states in each realization is always optimal, and that such pairwise signals are the only solutions under a non-singularity condition (the twist condition). Our core results provide conditions under which riskier prospects induce higher or lower actions, so that the induced action is single-dipped or single-peaked on each set of nested prospects. We also provide conditions for the optimality of either full disclosure or negative assortative disclosure, where all prospects are nested. Methodologically, our results rely on novel duality and complementary slackness theorems. Our analysis extends to a general problem of assigning one-dimensional inputs to productive units, which we call optimal productive transport. This problem covers additional applications including club economies (assigning workers to firms, or students to schools), robust option pricing (assigning future asset prices to price distributions), and partisan gerrymandering (assigning voters to districts)."
            },
            {
                "arxivId": "2304.09381",
                "title": "The Economics of Partisan Gerrymandering",
                "abstract": "In the United States, the boundaries of congressional districts are often drawn by political partisans. In the resulting partisan gerrymandering problem, a designer partitions voters into equal-sized districts with the goal of winning as many districts as possible. When the designer can perfectly predict how each individual will vote, the solution is to pack unfavorable voters into homogeneous districts and crack favorable voters across districts that each contain a bare majority of favorable voters. We study the more realistic case where the designer faces both aggregate and individual-level uncertainty, provide conditions under which appropriate generalizations of the pack and crack solution remain optimal, and analyze comparative statics. All districting plans that we find to be optimal are equivalent to special cases of segregate-pair districting, a generalization of pack and crack where all sufficiently unfavorable voter types are segregated in homogeneous districts, and the remaining types are matched in a negatively assortative pattern. Methodologically, we exploit a mathematical connection between gerrymandering\u2014partitioning voters into districts\u2014and information design\u2014partitioning states of the world into signals."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2402.17201",
        "category": "econ",
        "title": "A Decentralized Market Mechanism for Energy Communities under Operating Envelopes",
        "abstract": "We propose an operating envelopes (OEs) aware energy community market mechanism that dynamically charges/rewards its members based on two-part pricing. The OEs are imposed exogenously by a regulated distribution system operator (DSO) on the energy community's revenue meter that is subject to a generalized net energy metering (NEM) tariff design. By formulating the interaction of the community manager and its members as a Stackelberg game, we show that the proposed two-part pricing achieves a Nash equilibrium and maximizes the community's social welfare in a decentralized fashion while ensuring that the community's operation abides by the OEs. The market mechanism conforms with the cost-causation principle and guarantees community members a surplus level no less than their maximum surplus when they autonomously face the DSO. The dynamic and uniform community price is a monotonically decreasing function of the community's aggregate renewable generation. We also analyze the impact of exogenous parameters such as NEM rates and OEs on the value of joining the community. Lastly, through numerical studies, we showcase the community's welfare, pricing, and compare its members' surplus to customers under the DSO and other OEs arrangements.",
        "references": [
            {
                "arxivId": "2310.07157",
                "title": "Operating-Envelopes-Aware Decentralized Welfare Maximization for Energy Communities",
                "abstract": "We propose an operating-envelope-aware, prosumer-centric, and efficient energy community that aggregates individual and shared community distributed energy resources and transacts with a regulated distribution system operator (DSO) under a generalized net energy metering tariff design. To ensure safe network operation, the DSO imposes dynamic export and import limits, known as dynamic operating envelopes, on end-users\u2019 revenue meters. Given the operating envelopes, we propose an incentive-aligned community pricing mechanism under which the decentralized optimization of community members\u2019 benefit implies the optimization of overall community welfare. The proposed pricing mechanism satisfies the cost-causation principle and ensures the stability of the energy community in a coalition game setting. Numerical examples provide insights into the characteristics of the proposed pricing mechanism and quantitative measures of its performance."
            },
            {
                "arxivId": "2012.06947",
                "title": "Network-Cognizant Time-Coupled Aggregate Flexibility of Distribution Systems Under Uncertainties",
                "abstract": "Increasing integration of distributed energy resources (DERs) within distribution feeders provides unprecedented flexibility at the distribution-transmission interconnection. To exploit this flexibility and to use the capacity potential of aggregate DERs, feasible substation power injection trajectories need to be efficiently characterized. This letter provides an ellipsoidal inner approximation of the set of feasible power injection trajectories at the substation such that for any point in the set, there exists a feasible disaggregation strategy of DERs for any load uncertainty realization. The problem is formulated as one of finding the robust maximum volume ellipsoid inside the flexibility region under uncertainty. Though the problem is NP-hard even in the deterministic case, this letter derives novel approximations of the resulting adaptive robust optimization problem based on optimal second-stage policies. The proposed approach yields less conservative flexibility characterization than existing flexibility region approximation formulations. The efficacy of the proposed method is demonstrated on a realistic distribution feeder."
            },
            {
                "arxivId": "2012.02152",
                "title": "Strategies for Network-Safe Load Control With a Third-Party Aggregator and a Distribution Operator",
                "abstract": "When providing bulk power system services, a third-party aggregator could inadvertently cause operational issues at the distribution level. We propose a coordination architecture in which an aggregator and distribution operator coordinate to avoid distribution network constraint violations, while preserving private information. The aggregator controls thermostatic loads to provide frequency regulation, while the distribution operator overrides the aggregator's control actions when necessary to ensure safe network operation. Using this architecture, we propose two control strategies, which differ in terms of measurement and communication requirements, as well as model complexity and scalability. The first uses an aggregate model and blocking controller, while the second uses individual load models and a mode-count controller. Both outperform a benchmark strategy in terms of tracking accuracy. Furthermore, the second strategy performs better than the first, with only 0.10% average RMS error (compared to 0.70%). The second is also able to maintain safe operation of the distribution network while overriding less than 1% of the aggregator's control actions (compared to approximately 15% by the first strategy). However, the second strategy has significantly more measurement, communication, and computational requirements, and therefore would be more complex and expensive to implement than the first strategy."
            },
            {
                "arxivId": "2010.15455",
                "title": "Optimal Sharing and Fair Cost Allocation of Community Energy Storage",
                "abstract": "This paper studies an energy storage (ES) sharing model which is cooperatively invested by multiple buildings for harnessing on-site renewable utilization and grid price arbitrage. To maximize the economic benefits, we jointly consider the ES sizing, operation, and cost allocation via a coalition game formulation. Particularly, we study a fair <italic>ex-post</italic> cost allocation based on <italic>nucleolus</italic> which addresses fairness by minimizing the minimal dissatisfaction of all the players. To overcome the exponential computation burden caused by the implicit characteristic function, we employ a constraint generation technique to gradually approach the unique <italic>nucleolus</italic> by leveraging the sparse problem structure. We demonstrate both the fairness and computational efficiency of the method through case studies, which are not provided by the existing <italic>Shapley approach</italic> or <italic>proportional method</italic>. Particularly, only a small fraction of characteristic function (less than 1% for 20 buildings) is required to achieve the cost allocation versus the exponential information required by <italic>Shapley approach</italic>. Though there exists a minor increase of computation over the <italic>proportional method</italic>, the proposed method can ensure fairness while the latter fails in some cases. Further, we demonstrate both the building-wise and community-wise economic benefits are enhanced with the ES sharing model over the individual ES (IES) model. Accordingly, the overall <italic>value</italic><xref ref-type=\"fn\" rid=\"fn1\"><sup>1</sup></xref> of ES is considerably improved (about 1.83 times).<fn id=\"fn1\"><label><sup>1</sup></label><p>The proportion of total electricity bill reduction relative to the ES capital cost. </p></fn>"
            },
            {
                "arxivId": "1803.00818",
                "title": "Convex Restriction of Power Flow Feasibility Sets",
                "abstract": "The convex restriction of power flow feasibility sets identifies the convex subset of power injections where a solution for power flow equations is guaranteed to exist and satisfy the operational constraints. In contrast to convex relaxations, the convex restriction provides a sufficient condition for power flow feasibility under variations in power generation and demand. In this article, we present a general framework to construct convex restrictions of an algebraic set defined by equality and inequality constraints and apply this framework to the power flow feasibility problem. The procedure results in convex quadratic constraints that provide a sufficiently large region for practical operation of the grid."
            },
            {
                "arxivId": "1709.06071",
                "title": "Managing Price Uncertainty in Prosumer-Centric Energy Trading: A Prospect-Theoretic Stackelberg Game Approach",
                "abstract": "In this paper, the problem of energy trading between smart grid prosumers, who can simultaneously consume and produce energy, and a grid power company is studied. The problem is formulated as a single-leader, multiple-follower Stackelberg game between the power company and multiple prosumers. In this game, the power company acts as a leader who determines the pricing strategy that maximizes its profits, while the prosumers act as followers who react by choosing the amount of energy to buy or sell so as to optimize their current and future profits. The proposed game accounts for each prosumer\u2019s subjective decision when faced with the uncertainty of profits, induced by the random future price. In particular, the framing effect, from the framework of prospect theory (PT), is used to account for each prosumer\u2019s valuation of its gains and losses with respect to an individual utility reference point. The reference point changes between prosumers and stems from their past experience and future aspirations of profits. The followers\u2019 noncooperative game is shown to admit a unique pure-strategy Nash equilibrium (NE) under classical game theory which is obtained using a fully distributed algorithm. The results are extended to account for the case of PT using algorithmic solutions that can achieve an NE under certain conditions. Simulation results show that the total grid load varies significantly with the prosumers\u2019 reference point and their loss-aversion level. In addition, it is shown that the power company\u2019s profits considerably decrease when it fails to account for the prosumers\u2019 subjective perceptions under PT."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2402.17374",
        "category": "econ",
        "title": "Quasi-Bayesian Estimation and Inference with Control Functions",
        "abstract": "We consider a quasi-Bayesian method that combines a frequentist estimation in the first stage and a Bayesian estimation/inference approach in the second stage. The study is motivated by structural discrete choice models that use the control function methodology to correct for endogeneity bias. In this scenario, the first stage estimates the control function using some frequentist parametric or nonparametric approach. The structural equation in the second stage, associated with certain complicated likelihood functions, can be more conveniently dealt with using a Bayesian approach. This paper studies the asymptotic properties of the quasi-posterior distributions obtained from the second stage. We prove that the corresponding quasi-Bayesian credible set does not have the desired coverage in large samples. Nonetheless, the quasi-Bayesian point estimator remains consistent and is asymptotically equivalent to a frequentist two-stage estimator. We show that one can obtain valid inference by bootstrapping the quasi-posterior that takes into account the first-stage estimation uncertainty.",
        "references": [
            {
                "arxivId": "math/0612191",
                "title": "General frequentist properties of the posterior profile distribution",
                "abstract": "In this paper, inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution is thoroughly investigated from the frequentist viewpoint. The higher-order validity of the profile sampler obtained in Cheng and Kosorok [Ann. Statist. 36 (2008)] is extended to semiparametric models in which the infinite dimensional nuisance parameter may not have a root-n convergence rate. This is a nontrivial extension because it requires a delicate analysis of the entropy of the semiparametric models involved. We find that the accuracy of inferences based on the profile sampler improves as the convergence rate of the nuisance parameter increases. Simulation studies are used to verify this theoretical result. We also establish that an exact frequentist confidence interval obtained by inverting the profile log-likelihood ratio can be estimated with higher-order accuracy by the credible set of the same type obtained from the posterior profile distribution. Our theory is verified for several specific examples."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-02-28.json",
        "arxivId": "2402.17526",
        "category": "econ",
        "title": "Political Pandering and Bureaucratic Influence",
        "abstract": "This paper examines the impact of bureaucracy on policy implementation in environments where electoral incentives generate pandering. A two-period model is developed to analyze the interactions between politicians and bureaucrats, who are categorized as either aligned -- sharing the voters' preferences over policies -- or intent on enacting policies that favor elite groups. The findings reveal equilibria in which aligned politicians resort to pandering, whereas aligned bureaucrats either support or oppose such behavior. The analysis further indicates that, depending on parameters, any level of bureaucratic influence can maximize the voters' welfare, ranging from scenarios with an all-powerful to a toothless bureaucracy.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "1903.11198",
        "category": "econ",
        "title": "Parallel Experimentation and Competitive Interference on Online Advertising Platforms",
        "abstract": "This paper studies the measurement of advertising effects on online platforms when parallel experimentation occurs, that is, when multiple advertisers experiment concurrently. It provides a framework that makes precise how parallel experimentation affects the experiment's value: while ignoring parallel experimentation yields an estimate of the average effect of advertising in-place, which has limited value in decision-making in an environment with variable advertising competition, accounting for parallel experimentation captures the actual uncertainty advertisers face due to competitive actions. It then implements an experimental design that enables the estimation of these effects on JD.com, a large e-commerce platform that is also a publisher of digital ads. Using traditional and kernel-based estimators, it shows that not accounting for competitive actions can result in the advertiser inaccurately estimating the advertising lift by a factor of two or higher, which can be consequential for decision-making.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2202.08370",
        "category": "econ",
        "title": "CAREER: A Foundation Model for Labor Sequence Data",
        "abstract": "Labor economists regularly analyze employment data by fitting predictive models to small, carefully constructed longitudinal survey datasets. Although machine learning methods offer promise for such problems, these survey datasets are too small to take advantage of them. In recent years large datasets of online resumes have also become available, providing data about the career trajectories of millions of individuals. However, standard econometric models cannot take advantage of their scale or incorporate them into the analysis of survey data. To this end we develop CAREER, a foundation model for job sequences. CAREER is first fit to large, passively-collected resume data and then fine-tuned to smaller, better-curated datasets for economic inferences. We fit CAREER to a dataset of 24 million job sequences from resumes, and adjust it on small longitudinal survey datasets. We find that CAREER forms accurate predictions of job sequences, outperforming econometric baselines on three widely-used economics datasets. We further find that CAREER can be used to form good predictions of other downstream variables. For example, incorporating CAREER into a wage model provides better predictions than the econometric models currently in use.",
        "references": [
            {
                "arxivId": "2307.09288",
                "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
            },
            {
                "arxivId": "2306.05284",
                "title": "Simple and Controllable Music Generation",
                "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft."
            },
            {
                "arxivId": "2305.06161",
                "title": "StarCoder: may the source be with you!",
                "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
            },
            {
                "arxivId": "2203.15556",
                "title": "Training Compute-Optimal Large Language Models",
                "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher."
            },
            {
                "arxivId": "2109.07276",
                "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models",
                "abstract": "Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence."
            },
            {
                "arxivId": "2109.06387",
                "title": "Rationales for Sequential Predictions",
                "abstract": "Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales."
            },
            {
                "arxivId": "2108.07258",
                "title": "On the Opportunities and Risks of Foundation Models",
                "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature."
            },
            {
                "arxivId": "2102.12092",
                "title": "Zero-Shot Text-to-Image Generation",
                "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion."
            },
            {
                "arxivId": "2010.11929",
                "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
            },
            {
                "arxivId": "2004.07667",
                "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
                "abstract": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."
            },
            {
                "arxivId": "2009.07429",
                "title": "Job2Vec: Job Title Benchmarking with Collective Multi-View Representation Learning",
                "abstract": "Job Title Benchmarking (JTB) aims at matching job titles with similar expertise levels across various companies. JTB could provide precise guidance and considerable convenience for both talent recruitment and job seekers for position and salary calibration/prediction. Traditional JTB approaches mainly rely on manual market surveys, which is expensive and labor intensive. Recently, the rapid development of Online Professional graph has accumulated a large number of talent career records, which provides a promising trend for data-driven solutions. However, it is still a challenging task since (1) the job title and job transition (job-hopping) data is messy which contains a lot of subjective and non-standard naming conventions for a same position (\\eg,Programmer, Software Development Engineer, SDE, Implementation Engineer ), (2) there is a large amount of missing title/transition information, and (3) one talent only seeks limited numbers of jobs which brings the incompleteness and randomness for modeling job transition patterns. To overcome these challenges, we aggregate all the records to construct a large-scale Job Title Benchmarking Graph (Job-Graph), where nodes denote job titles affiliated with specific companies and links denote the correlations between jobs. We reformulate the JTB as the task of link prediction over the Job-Graph that matched job titles should have links. Along this line, we propose a collective multi-view representation learning method (Job2Vec) by examining the Job-Graph jointly in (1) graph topology view (the structure of relationships among job titles), (2) semantic view (semantic meaning of job descriptions), (3) job transition balance view (the numbers of bidirectional transitions between two similar-level jobs are close), and (4) job transition duration view (the shorter the average duration of transitions is, the more similar the job titles are). We fuse the multi-view representations in the encode-decode paradigm to obtain an unified optimal representations for the task of link prediction. Finally, we conduct extensive experiments to validate the effectiveness of our proposed method."
            },
            {
                "arxivId": "1908.10063",
                "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models",
                "abstract": "Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods."
            },
            {
                "arxivId": "1711.03560",
                "title": "SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements",
                "abstract": "We develop SHOPPER, a sequential probabilistic model of market baskets. SHOPPER uses interpretable components to model the forces that drive how a customer chooses products; in particular, we designed SHOPPER to capture how items interact with other items. We develop an efficient posterior inference algorithm to estimate these forces from large-scale data, and we analyze a large dataset from a major chain grocery store. We are interested in answering counterfactual queries about changes in prices. We found that SHOPPER provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products."
            },
            {
                "arxivId": "1607.06450",
                "title": "Layer Normalization",
                "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques."
            },
            {
                "arxivId": "1606.08415",
                "title": "Gaussian Error Linear Units (GELUs)",
                "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2204.07672",
        "category": "econ",
        "title": "Abadie's Kappa and Weighting Estimators of the Local Average Treatment Effect",
        "abstract": "Recent research has demonstrated the importance of flexibly controlling for covariates in instrumental variables estimation. In this paper we study the finite sample and asymptotic properties of various weighting estimators of the local average treatment effect (LATE), motivated by Abadie's (2003) kappa theorem and offering the requisite flexibility relative to standard practice. We argue that two of the estimators under consideration, which are weight normalized, are generally preferable. Several other estimators, which are unnormalized, do not satisfy the properties of scale invariance with respect to the natural logarithm and translation invariance, thereby exhibiting sensitivity to the units of measurement when estimating the LATE in logs and the centering of the outcome variable more generally. We also demonstrate that, when noncompliance is one sided, certain weighting estimators have the advantage of being based on a denominator that is strictly greater than zero by construction. This is the case for only one of the two normalized estimators, and we recommend this estimator for wider use. We illustrate our findings with a simulation study and three empirical applications, which clearly document the sensitivity of unnormalized estimators to how the outcome variable is coded. We implement the proposed estimators in the Stata package kappalate.",
        "references": [
            {
                "arxivId": "2208.01300",
                "title": "Doubly Robust Estimation of Local Average Treatment Effects Using Inverse Probability Weighted Regression Adjustment",
                "abstract": ": We revisit the problem of estimating the local average treatment e\ufb00ect (LATE) and the local average treatment e\ufb00ect on the treated (LATT) when control variables are available, either to render the instrumental variable (IV) suitably exogenous or to improve precision. Unlike previous approaches, our doubly robust (DR) estimation procedures use quasi-likelihood methods weighted by the inverse of the IV propensity score \u2013 so-called inverse probability weighted regression adjustment (IPWRA) estimators. By properly choosing models for the propensity score and outcome models, \ufb01tted values are ensured to be in the logical range determined by the response variable, producing DR estimators of LATE and LATT with appealing small sample properties. Inference is relatively straightforward both analytically and using the nonparametric bootstrap. Our DR LATE and DR LATT estimators work well in simulations. We also propose a DR version of the Hausman test that can be used to assess the unconfoundedness assumption through a comparison of di\ufb00erent estimates of the average treatment e\ufb00ect on the treated (ATT) under one-sided noncompliance. Unlike the usual test that compares OLS and IV estimates, this procedure is robust to treatment e\ufb00ect heterogeneity."
            },
            {
                "arxivId": "2109.06150",
                "title": "Nonparametric Estimation of Truncated Conditional Expectation Functions",
                "abstract": "Truncated conditional expectation functions are objects of interest in a wide range of economic applications, including income inequality measurement, financial risk management, and impact evaluation. They typically involve truncating the outcome variable above or below certain quantiles of its conditional distribution. In this paper, based on local linear methods, a novel, two-stage, nonparametric estimator of such functions is proposed. In this estimation problem, the conditional quantile function is a nuisance parameter that has to be estimated in the first stage. The proposed estimator is insensitive to the first-stage estimation error owing to the use of a Neyman-orthogonal moment in the second stage. This construction ensures that inference methods developed for the standard nonparametric regression can be readily adapted to conduct inference on truncated conditional expectations. As an extension, estimation with an estimated truncation quantile level is considered. The proposed estimator is applied in two empirical settings: sharp regression discontinuity designs with a manipulated running variable and randomized experiments with sample selection. \u2217I am grateful to my Ph.D. advisor Christoph Rothe for his invaluable support. I thank Fran\u00e7ois Gerard for kindly running my estimation routine on a restricted-use dataset. I thank Timo Dimitriadis, Claudia Noack, Yoshiyasu Rai, and participants of IAAE Conference 2021, EEA-ESEM Virtual 2021, Econometrics Seminar in Mannheim, HKMetrics-Workshop, and Bonn-Mannheim PhD Workshop for their helpful comments. I gratefully acknowledge funding by the German Research Foundation (DFG) through CRC TR 224 (Project A04) and by the European Research Council (ERC) through grant SH1-77202. Address: University of Mannheim, Department of Economics, L7, 3\u20135; 68161 Mannheim, Germany. Email: tomasz.j.olma@gmail.com. Website: tomaszolma.github.io. 1 ar X iv :2 10 9. 06 15 0v 1 [ ec on .E M ] 1 3 Se p 20 21"
            },
            {
                "arxivId": "2011.06695",
                "title": "When Should We (Not) Interpret Linear IV Estimands as Late?",
                "abstract": "In this paper I revisit the interpretation of the linear instrumental variables (IV) estimand as a weighted average of conditional local average treatment effects (LATEs). I focus on a practically relevant situation in which additional covariates are required for identification but the reduced-form and first-stage regressions are possibly misspecified as a result of neglected heterogeneity in the effects of the instrument. If we also allow for conditional monotonicity, i.e. the existence of compliers but no defiers at some covariate values and the existence of defiers but no compliers elsewhere, then the weights on some conditional LATEs are negative and the IV estimand is no longer interpretable as a causal effect. Even if monotonicity holds unconditionally, the IV estimand is not interpretable as the unconditional LATE parameter unless the groups that are encouraged and not encouraged to get treated are roughly equal sized."
            },
            {
                "arxivId": "2007.04346",
                "title": "Efficient Covariate Balancing for the Local Average Treatment Effect",
                "abstract": "Abstract This article develops an empirical balancing approach for the estimation of treatment effects under two-sided noncompliance using a binary instrumental variable. The method weighs both treatment and outcome information with inverse probabilities to impose exact finite sample balance across instrument level groups. It is free of functional form assumptions on the outcome or the treatment selection step. By tailoring the loss function for the instrument propensity scores, the resulting treatment effect estimates are automatically weight normalized and exhibit both low bias and reduced variance in finite samples compared to conventional inverse probability weighting methods. We provide conditions for asymptotic normality and semiparametric efficiency and demonstrate how to use additional information about the treatment selection step for bias reduction in finite samples. A doubly robust extension is proposed as well. Monte Carlo simulations suggest that the theoretical advantages translate well to finite samples. The method is illustrated in an empirical example."
            },
            {
                "arxivId": "1812.01723",
                "title": "Doubly Robust Difference-in-Differences Estimators",
                "abstract": "Abstract This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying particular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available."
            },
            {
                "arxivId": "1810.01370",
                "title": "Covariate Distribution Balance via Propensity Scores",
                "abstract": "This paper proposes new estimators for the propensity score that aim to maximize the covariate distribution balance among different treatment groups. Heuristically, our proposed procedure attempts to estimate a propensity score model by making the underlying covariate distribution of different treatment groups as close to each other as possible. Our estimators are data-driven, do not rely on tuning parameters such as bandwidths, admit an asymptotic linear representation, and can be used to estimate different treatment effect parameters under different identifying assumptions, including unconfoundedness and local treatment effects. We derive the asymptotic properties of inverse probability weighted estimators for the average, distributional, and quantile treatment effects based on the proposed propensity score estimator and illustrate their finite sample performance via Monte Carlo simulations and two empirical applications."
            },
            {
                "arxivId": "1803.09015",
                "title": "Difference-in-Differences with Multiple Time Periods",
                "abstract": "Difference-in-Differences (DID) is one of the most important and popular designs for evaluating causal effects of policy changes. In its standard format, there are two time periods and two groups: in the first period no one is treated, and in the second period a \"treatment group\" becomes treated, whereas a \"control group\" remains untreated. However, many em- pirical applications of the DID design have more than two periods and variation in treatment timing. In this article, we consider identification and estimation of treatment effect parameters using DID with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the \"parallel trends assumption\" holds potentially only after conditioning on observed covariates. We propose a simple two-step estimation strategy, establish the asymptotic prop- erties of the proposed estimators, and prove the validity of a computationally convenient bootstrap procedure. Furthermore we propose a semiparametric data-driven testing procedure to assess the credibility of the DID design in our context. Finally, we analyze the effect of the minimum wage on teen employment from 2001-2007. By using our proposed methods we confront the challenges related to variation in the timing of the state-level minimum wage policy changes. Open-source software is available for implementing the proposed methods."
            },
            {
                "arxivId": "1311.2645",
                "title": "Program evaluation and causal inference with high-dimensional data",
                "abstract": "In this paper, we provide efficient estimators and honest con fidence bands for a variety of treatment eff ects including local average (LATE) and local quantile treatment eff ects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment e ffects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces ecient estimators and honest bands for (functional) average treatment eff ects (ATE) and quantile treatment eff ects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the eff ect of 401(k) eligibility and participation on accumulated assets. The results on program evaluation are obtained as a consequence of more general results on honest inference in a general moment condition framework, which arises from structural equation models in econometrics. Here too the crucial ingredient is the use of orthogonal moment conditions, which can be constructed from the initial moment conditions. We provide results on honest inference for (function-valued) parameters within this general framework where any high-quality, modern machine learning methods can be used to learn the nonparametric/high-dimensional components of the model. These include a number of supporting auxilliary results that are of major independent interest: namely, we (1) prove uniform validity of a multiplier bootstrap, (2) o er a uniformly valid functional delta method, and (3) provide results for sparsity-based estimation of regression functions for function-valued outcomes."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2207.04856",
        "category": "econ",
        "title": "Research Joint Ventures: The Role of Financial Constraints",
        "abstract": "This paper provides a novel theory of research joint ventures for financially constrained firms. When firms choose R&D portfolios, an RJV can help to coordinate research efforts, reducing investments in duplicate projects. This can free up resources, increase the variety of pursued projects and thereby increase the probability of discovering the innovation. RJVs improve innovation outcomes when market competition is weak and external financing conditions are bad. An RJV may increase the innovation probability and nevertheless lower total R&D costs. RJVs that increase innovation also increase consumer surplus and tend to be profitable, but innovation-reducing RJVs also exist. Finally, we compare RJVs to innovation-enhancing mergers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2303.07008",
        "category": "econ",
        "title": "Status substitution and conspicuous consumption",
        "abstract": "This paper adapts ideas from social identity theory to set out a new framework for modelling conspicuous consumption. Agents derive status from their own conspicuous consumption and from belonging to an identity group with high conspicuous consumption. Importantly, these two sources of status are substitutes. Agents also feel pressure to conform with their neighbours in a network. This framework can rationalise a set of seemingly conflicting stylised facts about conspicuous consumption that are currently explained by different families of models. In addition, our model delivers new testable predictions regarding the effect of network structure and income inequality on conspicuous consumption.",
        "references": [
            {
                "arxivId": "2203.10305",
                "title": "Keeping Up with \u201cThe Joneses\u201d: Reference-Dependent Choice with Social Comparisons",
                "abstract": "Keeping up with \u201cThe Joneses\u201d matters. This paper examines a model of reference-dependent choice where reference points are determined by social comparisons. An increase in the strength of social comparisons, even by only a few agents, increases consumption and decreases welfare for everyone. Strikingly, a higher marginal cost of consumption can increase welfare. In a labor market, social comparisons with coworkers create a big fish in a small pond effect, inducing incomplete labor market sorting. Further, it is the skilled workers with the weakest social networks who are induced to give up income to become the big fish. (JEL D85, J22, J24, J61)"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2303.14947",
        "category": "econ",
        "title": "Measuring Self-Preferencing on Digital Platforms",
        "abstract": "Digital platforms use recommendations to facilitate exchanges between platform actors, such as trade between buyers and sellers. Aiming to protect consumers and guarantee fair competition on platforms, legislators increasingly require that recommendations on market-dominating platforms be free from self-preferencing. That is, platforms that also act as sellers (e.g., Amazon) or information providers (e.g., Google) must not prefer their own offers over comparable third-party offers. Yet, successful enforcement of self-preferencing bans -- to the potential benefit of consumers and third-party actors -- requires defining and measuring self-preferencing across a platform. In the context of recommendations through search results, this research contributes by i) conceptualizing a\"recommendation\"as an offer's level of search engine visibility across an entire platform (instead of its position in specific search queries, as in previous research); ii) discussing two tests for self-preferencing, and iii) implementing them in two empirical studies across three international Amazon marketplaces. Contrary to consumer expectations and emerging literature, our analysis finds almost no evidence for self-preferencing. A survey reveals that even if Amazon were proven to engage in self-preferencing, most consumers would not change their shopping behavior on the platform -- highlighting Amazon's significant market power and suggesting the need for robust protections for sellers and consumers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2304.05805",
        "category": "econ",
        "title": "GDP nowcasting with artificial neural networks: How much does long-term memory matter?",
        "abstract": "In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), longer training sequences do not help the models' predictive performance; instead, they seem to weaken their generalization capability. Our results show that 1D CNN, with the same parameters, generates accurate nowcasts in both of our evaluation periods. Consequently, first in the literature, we propose the use of this specific neural network architecture for economic nowcasting.",
        "references": [
            {
                "arxivId": "1905.03554",
                "title": "1D Convolutional Neural Networks and Applications: A Survey",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2309.04020",
        "category": "econ",
        "title": "Local Priority Mechanisms",
        "abstract": "We introduce a novel family of mechanisms for constrained allocation problems which we call local priority mechanisms. These mechanisms are parameterized by a function which assigns a set of agents, the local compromisers, to every infeasible allocation. The mechanism then greedily attempts to match agents with their top choices. Whenever it reaches an infeasible allocation, the local compromisers move to their next favorite alternative. Local priority mechanisms exist for any constraint, so this provides a method of constructing new designs for any constrained allocation problem. We give axioms which characterize local priority mechanisms. Since constrained allocation includes many canonical problems as special constraints, we apply this characterization to show that several well-known mechanisms, including deferred acceptance for school choice, top trading cycles for house allocation, and serial dictatorship can be understood as instances of local priority mechanisms. Other mechanisms, including the Boston mechanism, are not local priority mechanisms. We give sufficient conditions for a local priority mechanism to be group strategy-proof. We also provide conditions which enable welfare comparisons across local priority mechanisms.",
        "references": [
            {
                "arxivId": "2301.13037",
                "title": "Royal Processions: Incentives, Efficiency and Fairness in Two-Sided Matching",
                "abstract": "We study two-sided matching as introduced in Gale and Shapley [1962]. In contrast to much of the literature, we ignore stability and characterize all mechanisms which are group strategy-proof, efficient and treat the two sides symmetrically in the sense that the mechanism is invariant with respect to a reflection between the two sides. We find that all group strategy-proof, efficient, and \"gender-neutral\" mechanisms can be implemented by an algorithm which operates in a sequence of rounds. In each round, two agents are selected, one from each side and their matching is determined before moving on to the next round. We refer to these agents as the \"royals.\" The royals are given their most-preferred available matches whenever possible. However, when their preferences conflict one of two \"regimes\" is used. The royals are either \"matched-by-default\" or \"unmatched-by-default.\" In the former case, either of the royals can unilaterally force the other to match with them while in the latter case, they may only match together if both agree. In either case, if this pair of agents is not matched together, each gets their top choices among the set of remaining agents, excluding one another. We call the set of all such mechanisms \"royalty mechanisms.\""
            },
            {
                "arxivId": "2203.06353",
                "title": "Efficiency in Random Resource Allocation and Social Choice",
                "abstract": "We study efficiency in general collective choice problems where agents have ordinal preferences and randomization is allowed. We explore the structure of preference profiles where ex-ante and ex-post efficiency coincide, offer a unifying perspective on the known results, and give several new characterizations. The results have implications for well-studied mechanisms including random serial dictatorship and a number of specific environments, including the dichotomous, single-peaked, and social choice domains."
            },
            {
                "arxivId": "2006.06776",
                "title": "Incentives and Efficiency in Constrained Allocation Mechanisms",
                "abstract": "We study private-good allocation mechanisms where an arbitrary constraint delimits the set of feasible joint allocations. This generality provides a unified perspective over several prominent examples that can be parameterized as constraints in this model, including house allocation, roommate assignment, and social choice. We first characterize the set of two-agent strategy-proof and Pareto efficient mechanisms, showing that every mechanism is a \"local dictatorship.\" For more than two agents, we leverage this result to provide a new characterization of group strategy-proofness. In particular, an N-agent mechanism is group strategy-proof if and only if all its two-agent marginal mechanisms (defined by holding fixed all but two agents' preferences) are individually strategy-proof and Pareto efficient. To illustrate their usefulness, we apply these results to the roommates problem to discover the novel finding that all group strategy-proof and Pareto efficient mechanisms are generalized serial dictatorships, a new class of mechanisms. Our results also yield a simple new proof of the Gibbard-Satterthwaite Theorem."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2402.18713",
        "category": "econ",
        "title": "Identifying Assumptions and Research Dynamics",
        "abstract": "A representative researcher pursuing a question has repeated opportunities for empirical research. To process findings, she must impose an identifying assumption, which ensures that repeated observation would provide a definitive answer to her question. Research designs vary in quality and are implemented only when the assumption is plausible enough according to a KL-divergence-based criterion, and then beliefs are Bayes-updated as if the assumption were perfectly valid. We study the dynamics of this learning process and its induced long-run beliefs. The rate of research cannot uniformly accelerate over time. We characterize environments in which it is stationary. Long-run beliefs can exhibit history-dependence. We apply the model to stylized examples of empirical methodologies: experiments, causal-inference techniques, and (in an extension) ``structural'' identification methods such as ``calibration'' and ``Heckman selection.''",
        "references": [
            {
                "arxivId": "2106.05957",
                "title": "Subjective Causality in Choice",
                "abstract": "An agent makes a stochastic choice from a set of lotteries. She infers the outcomes of each available lottery using a subjective causal model represented by a directed acyclic graph, and consequently may misinterpret correlation as causation. Her choices affect her inferences, which in turn affect her choices, so the two together must form a personal equilibrium. We show how an analyst can identify the agent\u2019s subjective causal model from her random choice rule. Her choices reveal the chains of causal reasoning that she undergoes, and these chains pin down her subjective causal model. In addition, we provide necessary and sufficient conditions that allow an analyst to test whether the agent\u2019s behavior is compatible with the model."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2402.18908",
        "category": "econ",
        "title": "Facility Location Games with Scaling Effects",
        "abstract": "We take the classic facility location problem and consider a variation, in which each agent's individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement. In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios. We focus on the objectives of total and maximum cost, describing the computation of the optimal solution. We then move to the approximate mechanism design setting, observing that the agents' preferences may no longer be single-peaked. Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences. Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous mechanisms.",
        "references": [
            {
                "arxivId": "2111.01566",
                "title": "Strategyproof and Proportionally Fair Facility Location",
                "abstract": "We focus on a simple, one-dimensional collective decision problem (often referred to as the facility location problem) and explore issues of strategyproofness and proportionality-based fairness. We introduce and analyze a hierarchy of proportionality-based fairness axioms of varying strength: Individual Fair Share (IFS), Unanimous Fair Share (UFS), Proportionality (as in Freeman et al, 2021), and Proportional Fairness (PF). For each axiom, we characterize the family of mechanisms that satisfy the axiom and strategyproofness. We show that imposing strategyproofness renders many of the axioms to be equivalent: the family of mechanisms that satisfy proportionality, unanimity, and strategyproofness is equivalent to the family of mechanisms that satisfy UFS and strategyproofness, which, in turn, is equivalent to the family of mechanisms that satisfy PF and strategyproofness. Furthermore, there is a unique such mechanism: the Uniform Phantom mechanism, which is studied in Freeman et al. (2021). We also characterize the outcomes of the Uniform Phantom mechanism as the unique (pure) equilibrium outcome for any mechanism that satisfies continuity, strict monotonicity, and UFS. Finally, we analyze the approximation guarantees, in terms of optimal social welfare and minimum total cost, obtained by mechanisms that are strategyproof and satisfy each proportionality-based fairness axiom. We show that the Uniform Phantom mechanism provides the best approximation of the optimal social welfare (and also minimum total cost) among all mechanisms that satisfy UFS."
            },
            {
                "arxivId": "2102.11686",
                "title": "New Characterizations of Strategy-Proofness under Single-Peakedness",
                "abstract": null
            },
            {
                "arxivId": "1905.00457",
                "title": "Truthful Aggregation of Budget Proposals",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2402.19421",
        "category": "econ",
        "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines",
        "abstract": "In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their\"cognitive\"processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines.",
        "references": [
            {
                "arxivId": "2305.12763",
                "title": "The emergence of economic rationality of GPT.",
                "abstract": "As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory. We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms."
            },
            {
                "arxivId": "2302.06590",
                "title": "The Impact of AI on Developer Productivity: Evidence from GitHub Copilot",
                "abstract": "Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers."
            },
            {
                "arxivId": "2212.04037",
                "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "abstract": "Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance."
            },
            {
                "arxivId": "2204.07705",
                "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\u2014training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models."
            },
            {
                "arxivId": "2203.02155",
                "title": "Training language models to follow instructions with human feedback",
                "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
            },
            {
                "arxivId": "2112.09332",
                "title": "WebGPT: Browser-assisted question-answering with human feedback",
                "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit."
            },
            {
                "arxivId": "2005.11401",
                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-01.json",
        "arxivId": "2402.19425",
        "category": "econ",
        "title": "Testing Information Ordering for Strategic Agents",
        "abstract": "A key primitive of a strategic environment is the information available to players. Specifying a priori an information structure is often difficult for empirical researchers. We develop a test of information ordering that allows researchers to examine if the true information structure is at least as informative as a proposed baseline. We construct a computationally tractable test statistic by utilizing the notion of Bayes Correlated Equilibrium (BCE) to translate the ordering of information structures into an ordering of functions. We apply our test to examine whether hubs provide informational advantages to certain airlines in addition to market power.",
        "references": [
            {
                "arxivId": "1911.04529",
                "title": "Identification in discrete choice models with imperfect information",
                "abstract": "We study identification of preferences in static single-agent discrete choice models where decision makers may be imperfectly informed about the state of the world. We leverage the notion of one-player Bayes Correlated Equilibrium by Bergemann and Morris (2016) to provide a tractable characterization of the sharp identified set. We develop a procedure to practically construct the sharp identified set following a sieve approach, and provide sharp bounds on counterfactual outcomes of interest. We use our methodology and data on the 2017 UK general election to estimate a spatial voting model under weak assumptions on agents' information about the returns to voting. Counterfactual exercises quantify the consequences of imperfect information on the well-being of voters and parties."
            },
            {
                "arxivId": "1710.09707",
                "title": "Calibrated Projection in MATLAB: Users' Manual",
                "abstract": "We present the calibrated-projection MATLAB package implementing the method to construct confidence intervals proposed by Kaido, Molinari and Stoye (2017). This manual provides details on how to use the package for inference on projections of partially identified parameters. It also explains how to use the MATLAB functions we developed to compute confidence intervals on solutions of nonlinear optimization problems with estimated constraints."
            },
            {
                "arxivId": "1710.03830",
                "title": "Inference on Auctions with Weak Assumptions on Information",
                "abstract": "Given a sample of bids from independent auctions, this paper examines the question of inference on auction fundamentals (e.g. valuation distributions, welfare measures) under weak assumptions on information structure. The question is important as it allows us to learn about the valuation distribution in a robust way, i.e., without assuming that a particular information structure holds across observations. We leverage the recent contributions of \\cite{Bergemann2013} in the robust mechanism design literature that exploit the link between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in incomplete information games to construct an econometrics framework for learning about auction fundamentals using observed data on bids. We showcase our construction of identified sets in private value and common value auctions. Our approach for constructing these sets inherits the computational simplicity of solving for correlated equilibria: checking whether a particular valuation distribution belongs to the identified set is as simple as determining whether a {\\it linear} program is feasible. A similar linear program can be used to construct the identified set on various welfare measures and counterfactual objects. For inference and to summarize statistical uncertainty, we propose novel finite sample methods using tail inequalities that are used to construct confidence regions on sets. We also highlight methods based on Bayesian bootstrap and subsampling. A set of Monte Carlo experiments show adequate finite sample properties of our inference procedures. We illustrate our methods using data from OCS auctions."
            },
            {
                "arxivId": "2102.12249",
                "title": "Set Identification in Models with Multiple Equilibria",
                "abstract": "We propose a computationally feasible way of deriving the identified features of models with multiple equilibria in pure or mixed strategies. It is shown that in the case of Shapley regular normal form games, the identified set is characterized by the inclusion of the true data distribution within the core of a Choquet capacity, which is interpreted as the generalized likelihood of the model. In turn, this inclusion is characterized by a finite set of inequalities and efficient and easily implementable combinatorial methods are described to check them. In all normal form games, the identified set is characterized in terms of the value of a submodular or convex optimization program. Efficient algorithms are then given and compared to check inclusion of a parameter in this identified set. The latter are illustrated with family bargaining games and oligopoly entry games. Copyright 2011, Oxford University Press."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "1910.14023",
        "category": "econ",
        "title": "Firm Entry and Exit with Unbounded Productivity Growth",
        "abstract": "In Hopenhayn's (1992) entry-exit model productivity is bounded, implying that the predicted firm size distribution cannot match the power law tail observable in the data. In this paper we remove the boundedness assumption and, in this more general setting, provide an exact characterization of existence of stationary equilibria, as well as a novel sufficient condition for existence based on treating production as a Lyapunov function. We also provide new representations of the rate of entry and aggregate supply. Finally, we prove that the firm size distribution has a power law tail under a very broad set of productivity growth specifications.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2107.08112",
        "category": "econ",
        "title": "Hamiltonian Monte Carlo for Regression with High-Dimensional Categorical Data",
        "abstract": "Latent variable models are increasingly used in economics for high-dimensional categorical data like text and surveys. We demonstrate the effectiveness of Hamiltonian Monte Carlo (HMC) with parallelized automatic differentiation for analyzing such data in a computationally efficient and methodologically sound manner. Our new model, Supervised Topic Model with Covariates, shows that carefully modeling this type of data can have significant implications on conclusions compared to a simpler, frequently used, yet methodologically problematic, two-step approach. A simulation study and revisiting Bandiera et al. (2020)'s study of executive time use demonstrate these results. The approach accommodates thousands of parameters and doesn't require custom algorithms specific to each model, making it accessible for applied researchers",
        "references": [
            {
                "arxivId": "1912.11554",
                "title": "Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro",
                "abstract": "NumPyro is a lightweight library that provides an alternate NumPy backend to the Pyro probabilistic programming language with the same modeling interface, language primitives and effect handling abstractions. Effect handlers allow Pyro's modeling API to be extended to NumPyro despite its being built atop a fundamentally different JAX-based functional backend. In this work, we demonstrate the power of composing Pyro's effect handlers with the program transformations that enable hardware acceleration, automatic differentiation, and vectorization in JAX. In particular, NumPyro provides an iterative formulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation that is much faster than existing alternatives in both the small and large dataset regimes."
            },
            {
                "arxivId": "1802.02163",
                "title": "How to make causal inferences using texts",
                "abstract": "Text as data techniques offer a great promise: the ability to inductively discover measures that are useful for testing social science theories with large collections of text. Nearly all text-based causal inferences depend on a latent representation of the text, but we show that estimating this latent representation from the data creates underacknowledged risks: we may introduce an identification problem or overfit. To address these risks, we introduce a split-sample workflow for making rigorous causal inferences with discovered measures as treatments or outcomes. We then apply it to estimate causal effects from an experiment on immigration attitudes and a study on bureaucratic responsiveness."
            },
            {
                "arxivId": "1708.00955",
                "title": "Hamiltonian Monte Carlo with Energy Conserving Subsampling",
                "abstract": "Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large datasets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that utilize gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients."
            },
            {
                "arxivId": "1206.7051",
                "title": "Stochastic variational inference",
                "abstract": "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2203.05593",
        "category": "econ",
        "title": "Labor Demand on a Tight Leash",
        "abstract": "We develop a labor demand model that encompasses pre-match hiring cost arising from tight labor markets. Through the lens of the model, we study the effect of labor market tightness on firms' labor demand by applying novel shift-share instruments to the universe of German firms. In line with theory, we find that a doubling in tightness reduces firms' employment by 5 percent. Taking into account the resulting search externalities, the wage elasticity of firms' labor demand reduces from -0.7 to -0.5 through reallocation effects. In light of our results, pre-match hiring cost amount to 40 percent of annual wage payments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2210.01282",
        "category": "econ",
        "title": "Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees",
        "abstract": "We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm converges to a stationary solution with a finite-time guarantee. Further, if the reward is parameterized linearly, we show that the algorithm approximates the maximum likelihood estimator sublinearly. Finally, by using robotics control problems in MuJoCo and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks.",
        "references": [
            {
                "arxivId": "2106.12142",
                "title": "IQ-Learn: Inverse soft-Q Learning for Imitation",
                "abstract": "In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x."
            },
            {
                "arxivId": "2103.02863",
                "title": "Inverse Reinforcement Learning with Explicit Policy Estimates",
                "abstract": "Various methods for solving the inverse reinforcement learning (IRL) problem have been developed independently in machine learning and economics. In particular, the method of Maximum Causal Entropy IRL is based on the perspective of entropy maximization, while related advances in the field of economics instead assume the existence of unobserved action shocks to explain expert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability method, Nested Pseudo-Likelihood Algorithm). In this work, we make previously unknown connections between these related methods from both fields. We achieve this by showing that they all belong to a class of optimization problems, characterized by a common form of the objective, the associated policy and the objective gradient. We demonstrate key computational and algorithmic differences which arise between the methods due to an approximation of the optimal soft value function, and describe how this leads to more efficient algorithms. Using insights which emerge from our study of this class of optimization problems, we identify various problem scenarios and investigate each method's suitability for these problems."
            },
            {
                "arxivId": "2011.04709",
                "title": "f-IRL: Inverse Reinforcement Learning via State Marginal Matching",
                "abstract": "Imitation learning is well-suited for robotic tasks where it is difficult to directly program the behavior or specify a cost for optimal control. In this work, we propose a method for learning the reward function (and the corresponding policy) to match the expert state density. Our main result is the analytic gradient of any f-divergence between the agent and expert state distribution w.r.t. reward parameters. Based on the derived gradient, we present an algorithm, f-IRL, that recovers a stationary reward function from the expert density by gradient descent. We show that f-IRL can learn behaviors from a hand-designed target state density or implicitly through expert observations. Our method outperforms adversarial imitation learning methods in terms of sample efficiency and the required number of expert trajectories on IRL benchmarks. Moreover, we show that the recovered reward function can be used to quickly solve downstream tasks, and empirically demonstrate its utility on hard-to-explore tasks and for behavior transfer across changes in dynamics."
            },
            {
                "arxivId": "2008.07820",
                "title": "A Relation Analysis of Markov Decision Process Frameworks",
                "abstract": "We study the relation between different Markov Decision Process (MDP) frameworks in the machine learning and econometrics literatures, including the standard MDP, the entropy and general regularized MDP, and stochastic MDP, where the latter is based on the assumption that the reward function is stochastic and follows a given distribution. We show that the entropy-regularized MDP is equivalent to a stochastic MDP model, and is strictly subsumed by the general regularized MDP. Moreover, we propose a distributional stochastic MDP framework by assuming that the distribution of the reward function is ambiguous. We further show that the distributional stochastic MDP is equivalent to the regularized MDP, in the sense that they always yield the same optimal policies. We also provide a connection between stochastic/regularized MDP and constrained MDP. Our work gives a unified view on several important MDP frameworks, which would lead new ways to interpret the (entropy/general) regularized MDP frameworks through the lens of stochastic rewards and vice-versa. Given the recent popularity of regularized MDP in (deep) reinforcement learning, our work brings new understandings of how such algorithmic schemes work and suggest ideas to develop new ones."
            },
            {
                "arxivId": "2006.13506",
                "title": "When Will Generative Adversarial Imitation Learning Algorithms Attain Global Convergence",
                "abstract": "Generative adversarial imitation learning (GAIL) is a popular inverse reinforcement learning approach for jointly optimizing policy and reward from expert trajectories. A primary question about GAIL is whether applying a certain policy gradient algorithm to GAIL attains a global minimizer (i.e., yields the expert policy), for which existing understanding is very limited. Such global convergence has been shown only for the linear (or linear-type) MDP and linear (or linearizable) reward. In this paper, we study GAIL under general MDP and for nonlinear reward function classes (as long as the objective function is strongly concave with respect to the reward parameter). We characterize the global convergence with a sublinear rate for a broad range of commonly used policy gradient algorithms, all of which are implemented in an alternating manner with stochastic gradient ascent for reward update, including projected policy gradient (PPG)-GAIL, Frank-Wolfe policy gradient (FWPG)-GAIL, trust region policy optimization (TRPO)-GAIL and natural policy gradient (NPG)-GAIL. This is the first systematic theoretical study of GAIL for global convergence."
            },
            {
                "arxivId": "2001.02792",
                "title": "On Computation and Generalization of Generative Adversarial Imitation Learning",
                "abstract": "Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (e.g., human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax computational formulation of GAIL without convex-concave structure. To bridge such a gap between theory and practice, this paper investigates the theoretical properties of GAIL. Specifically, we show: (1) For GAIL with general reward parameterization, the generalization can be guaranteed as long as the class of the reward functions is properly controlled; (2) For GAIL, where the reward is parameterized as a reproducing kernel function, GAIL can be efficiently solved by stochastic first order optimization algorithms, which attain sublinear convergence to a stationary solution. To the best of our knowledge, these are the first results on statistical and computational guarantees of imitation learning with reward/policy function ap- proximation. Numerical experiments are provided to support our analysis."
            },
            {
                "arxivId": "1902.02234",
                "title": "Finite-Sample Analysis for SARSA with Linear Function Approximation",
                "abstract": "SARSA is an on-policy algorithm to learn a Markov decision process policy in reinforcement learning. We investigate the SARSA algorithm with linear function approximation under the non-i.i.d.\\ setting, where a single sample trajectory is available. With a Lipschitz continuous policy improvement operator that is smooth enough, SARSA has been shown to converge asymptotically. However, its non-asymptotic analysis is challenging and remains unsolved due to the non-i.i.d. samples, and the fact that the behavior policy changes dynamically with time. In this paper, we develop a novel technique to explicitly characterize the stochastic bias of a type of stochastic approximation procedures with time-varying Markov transition kernels. Our approach enables non-asymptotic convergence analyses of this type of stochastic approximation algorithms, which may be of independent interest. Using our bias characterization technique and a gradient descent type of analysis, we further provide the finite-sample analysis on the mean square error of the SARSA algorithm. In the end, we present a fitted SARSA algorithm, which includes the original SARSA algorithm and its variant as special cases. This fitted SARSA algorithm provides a framework for \\textit{iterative} on-policy fitted policy iteration, which is more memory and computationally efficient. For this fitted SARSA algorithm, we also present its finite-sample analysis."
            },
            {
                "arxivId": "1810.02054",
                "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
                "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. \nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods."
            },
            {
                "arxivId": "1806.02450",
                "title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation",
                "abstract": "Temporal difference learning (TD) is a simple iterative algorithm widely used for policy evaluation in Markov reward processes. Bhandari et al. prove finite time convergence rates for TD learning with linear function approximation. The analysis follows using a key insight that establishes rigorous connections between TD updates and those of online gradient descent. In a model where observations are corrupted by i.i.d. noise, convergence results for TD follow by essentially mirroring the analysis for online gradient descent. Using an information-theoretic technique, the authors also provide results for the case when TD is applied to a single Markovian data stream where the algorithm\u2019s updates can be severely biased. Their analysis seamlessly extends to the study of TD learning with eligibility traces and Q-learning for high-dimensional optimal stopping problems."
            },
            {
                "arxivId": "1801.01290",
                "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
            },
            {
                "arxivId": "1710.11248",
                "title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
                "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings."
            },
            {
                "arxivId": "1702.08165",
                "title": "Reinforcement Learning with Deep Energy-Based Policies",
                "abstract": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model."
            },
            {
                "arxivId": "1608.00033",
                "title": "Locally Robust Semiparametric Estimation",
                "abstract": "Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where first steps have no effect, locally, on average moment functions. Using these orthogonal moments reduces model selection and regularization bias, as is important in many applications, especially for machine learning first steps. Also, associated standard errors are robust to misspecification when there is the same number of moment functions as parameters of interest.\n We use these orthogonal moments and cross\u2010fitting to construct debiased machine learning estimators of functions of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that additional first steps needed for the orthogonal moment functions have no effect, globally, on average orthogonal moment functions. We give a general approach to estimating those additional first steps. We characterize double robustness and give a variety of new doubly robust moment functions. We give general and simple regularity conditions for asymptotic theory."
            },
            {
                "arxivId": "1606.03476",
                "title": "Generative Adversarial Imitation Learning",
                "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2306.08559",
        "category": "econ",
        "title": "Inference in IV models with clustered dependence, many instruments and weak identification",
        "abstract": "Data clustering reduces the effective sample size from the number of observations towards the number of clusters. For instrumental variable models I show that this reduced effective sample size makes the instruments more likely to be weak, in the sense that they contain little information about the endogenous regressor, and many, in the sense that their number is large compared to the sample size. Clustered data therefore increases the need for many and weak instrument robust tests. However, none of the previously developed many and weak instrument robust tests can be applied to this type of data as they all require independent observations. I therefore adapt two types of such tests to clustered data. First, I derive cluster jackknife Anderson-Rubin and score tests by removing clusters rather than individual observations from the statistics. Second, I propose a cluster many instrument Anderson-Rubin test which improves on the first type of tests by using a more optimal, but more complex, weighting matrix. I show that if the clusters satisfy an invariance assumption the higher complexity poses no problems. By revisiting a study on the effect of queenly reign on war I show the empirical relevance of the new tests.",
        "references": [
            {
                "arxivId": "2309.01637",
                "title": "The Robust F-Statistic as a Test for Weak Instruments",
                "abstract": "Montiel Olea and Pflueger (2013) proposed the effective F-statistic as a test for weak instruments in terms of the Nagar bias of the two-stage least squares (2SLS) estimator relative to a benchmark worst-case bias. We show that their methodology applies to a class of linear generalized method of moments (GMM) estimators with an associated class of generalized effective F-statistics. The standard nonhomoskedasticity robust F-statistic is a member of this class. The associated GMMf estimator, with the extension f for first-stage, is a novel and unusual estimator as the weight matrix is based on the first-stage residuals. As the robust F-statistic can also be used as a test for underidentification, expressions for the calculation of the weak-instruments critical values in terms of the Nagar bias of the GMMf estimator relative to the benchmark simplify and no simulation methods or Patnaik (1949) distributional approximations are needed. In the grouped-data IV designs of Andrews (2018), where the robust F-statistic is large but the effective F-statistic is small, the GMMf estimator is shown to behave much better in terms of bias than the 2SLS estimator, as expected by the weak-instruments test results."
            },
            {
                "arxivId": "2303.07822",
                "title": "Identification- and many instrument-robust inference via invariant moment conditions",
                "abstract": "Identification-robust hypothesis tests are commonly based on the continuous updating objective function or its score. When the number of moment conditions grows proportionally with the sample size, the large-dimensional weighting matrix prohibits the use of conventional asymptotic approximations and the behavior of these tests remains unknown. We show that the structure of the weighting matrix opens up an alternative route to asymptotic results when, under the null hypothesis, the distribution of the moment conditions is reflection invariant. In a heteroskedastic linear instrumental variables model, we then establish asymptotic normality of conventional tests statistics under many instrument sequences. A key result is that the additional terms that appear in the variance are negative. Revisiting a study on the elasticity of substitution between immigrant and native workers where the number of instruments is over a quarter of the sample size, the many instrument-robust approximation indeed leads to substantially narrower confidence intervals."
            },
            {
                "arxivId": "2209.03259",
                "title": "A Ridge-Regularised Jackknifed Anderson-Rubin Test",
                "abstract": "We consider hypothesis testing in instrumental variable regression models with few included exogenous covariates but many instruments -- possibly more than the number of observations. We show that a ridge-regularised version of the jackknifed Anderson Rubin (1949, henceforth AR) test controls asymptotic size in the presence of heteroskedasticity, and when the instruments may be arbitrarily weak. Asymptotic size control is established under weaker assumptions than those imposed for recently proposed jackknifed AR tests in the literature. Furthermore, ridge-regularisation extends the scope of jackknifed AR tests to situations in which there are more instruments than observations. Monte-Carlo simulations indicate that our method has favourable finite-sample size and power properties compared to recently proposed alternative approaches in the literature. An empirical application on the elasticity of substitution between immigrants and natives in the US illustrates the usefulness of the proposed method for practitioners."
            },
            {
                "arxivId": "2207.11137",
                "title": "A conditional linear combination test with many weak instruments",
                "abstract": null
            },
            {
                "arxivId": "2205.03288",
                "title": "Leverage, influence, and the jackknife in clustered regression models: Reliable inference using summclust",
                "abstract": "We introduce a new command, summclust, that summarizes the cluster structure of the dataset for linear regression models with clustered disturbances. The key unit of observation for such a model is the cluster. We therefore propose cluster-level measures of leverage, partial leverage, and influence and show how to compute them quickly in most cases. The measures of leverage and partial leverage can be used as diagnostic tools to identify datasets and regression designs in which cluster\u2013robust inference is likely to be challenging. The measures of influence can provide valuable information about how the results depend on the data in the various clusters. We also show how to calculate two jackknife variance matrix estimators efficiently as a by-product of our other computations. These estimators, which are already available in Stata, are generally more conservative than conventional variance matrix estimators. The summclust command computes all the quantities that we discuss."
            },
            {
                "arxivId": "2205.03285",
                "title": "Cluster-robust inference: A guide to empirical practice",
                "abstract": null
            },
            {
                "arxivId": "2004.12445",
                "title": "Inference with Many Weak Instruments",
                "abstract": "We develop a concept of weak identification in linear IV models in which the number of instruments can grow at the same rate or slower than the sample size. We propose a jackknifed version of the classical weak identification-robust Anderson-Rubin (AR) test statistic. Large-sample inference based on the jackknifed AR is valid under heteroscedasticity and weak identification. The feasible version of this statistic uses a novel variance estimator. The test has uniformly correct size and good power properties. We also develop a pre-test for weak identification that is related to the size property of a Wald test based on the Jackknife Instrumental Variable Estimator (JIVE). This new pre-test is valid under heteroscedasticity and with many instruments."
            },
            {
                "arxivId": "1806.01494",
                "title": "Leave-Out Estimation of Variance Components",
                "abstract": "We propose leave-out estimators of quadratic forms designed for the study of linear models with unrestricted heteroscedasticity. Applications include analysis of variance and tests of linear restrictions in models with many regressors. An approximation algorithm is provided that enables accurate computation of the estimator in very large datasets. We study the large sample properties of our estimator allowing the number of regressors to grow in proportion to the number of observations. Consistency is established in a variety of settings where plug-in methods and estimators predicated on homoscedasticity exhibit first-order biases. For quadratic forms of increasing rank, the limiting distribution can be represented by a linear combination of normal and non-central $\\chi^2$ random variables, with normality ensuing under strong identification. Standard error estimators are proposed that enable tests of linear restrictions and the construction of uniformly valid confidence intervals for quadratic forms of interest. We find in Italian social security records that leave-out estimates of a variance decomposition in a two-way fixed effects model of wage determination yield substantially different conclusions regarding the relative contribution of workers, firms, and worker-firm sorting to wage inequality than conventional methods. Monte Carlo exercises corroborate the accuracy of our asymptotic approximations, with clear evidence of non-normality emerging when worker mobility between blocks of firms is limited."
            },
            {
                "arxivId": "1801.09138",
                "title": "Cross-fitting and fast remainder rates for semiparametric estimation",
                "abstract": "There are many interesting and widely used estimators of a functional with ?nite semi-parametric variance bound that depend on nonparametric estimators of nuisance func-tions. We use cross-?tting to construct such estimators with fast remainder rates. We give cross-?t doubly robust estimators that use separate subsamples to estimate di?erent nuisance functions. We show that a cross-?t doubly robust spline regression estimator of the expected conditional covariance is semiparametric e?cient under minimal conditions. Corresponding estimators of other average linear functionals of a conditional expectation are shown to have the fastest known remainder rates under certain smoothness conditions. The cross-?t plug-in estimator shares some of these properties but has a remainder term that is larger than the cross-?t doubly robust estimator. As speci?c examples we consider the expected conditional covariance, mean with randomly missing data, and a weighted average derivative."
            },
            {
                "arxivId": "1409.6337",
                "title": "Conditional Inference with a Functional Nuisance Parameter",
                "abstract": "This paper shows that the problem of testing hypotheses in moment condition models without any assumptions about identification may be considered as a problem of testing with an infinite\u2010dimensional nuisance parameter. We introduce a sufficient statistic for this nuisance parameter in a Gaussian problem and propose conditional tests. These conditional tests have uniformly correct asymptotic size for a large class of models and test statistics. We apply our approach to construct tests based on quasi\u2010likelihood ratio statistics, which we show are efficient in strongly identified models and perform well relative to existing alternatives in two examples."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2402.08223",
        "category": "econ",
        "title": "The Limits of Price Discrimination Under Privacy Constraints",
        "abstract": "We consider a producer's problem of selling a product to a continuum of privacy-conscious consumers, where the producer can implement third-degree price discrimination, offering different prices to different market segments. In the absence of privacy constraints, Bergemann, Brooks, and Morris [2015] characterize the set of all possible consumer-producer utilities, showing that it is a triangle. We consider a privacy mechanism that provides a degree of protection by probabilistically masking each market segment, and we establish that the resultant set of all consumer-producer utilities forms a convex polygon, characterized explicitly as a linear mapping of a certain high-dimensional convex polytope into $\\mathbb{R}^2$. This characterization enables us to investigate the impact of the privacy mechanism on both producer and consumer utilities. In particular, we establish that the privacy constraint always hurts the producer by reducing both the maximum and minimum utility achievable. From the consumer's perspective, although the privacy mechanism ensures an increase in the minimum utility compared to the non-private scenario, interestingly, it may reduce the maximum utility. Finally, we demonstrate that increasing the privacy level does not necessarily intensify these effects. For instance, the maximum utility for the producer or the minimum utility for the consumer may exhibit nonmonotonic behavior in response to an increase of the privacy level.",
        "references": [
            {
                "arxivId": "1912.04774",
                "title": "Voluntary Disclosure and Personalized Pricing",
                "abstract": "A concern central to the economics of privacy is that firms may use consumer data to price discriminate. A common response is that consumers should have control over their data and the ability to choose how firms access it. Since firms draw inferences based on both the data seen as well as the consumer's disclosure choices, the strategic implications of this proposal are unclear. We investigate whether such measures improve consumer welfare in monopolistic and competitive environments. We find that consumer control can guarantee gains for every consumer type relative to both perfect price discrimination and no personalized pricing. This result is driven by two ideas. First, consumers can use disclosure to amplify competition between firms. Second, consumers can share information that induces a seller---even a monopolist---to make price concessions. Furthermore, whether consumer control improves consumer surplus depends on both the technology of disclosure and the competitiveness of the marketplace. In a competitive market, simple disclosure technologies such as \"track / do-not-track'' suffice for guaranteeing gains in consumer welfare. However, in a monopolistic market, welfare gains require richer forms of disclosure technology whereby consumers can decide how much information they would like to convey."
            },
            {
                "arxivId": "2004.03107",
                "title": "The Economics of Social Data",
                "abstract": "A data intermediary pays consumers for information about their preferences and sells the information so acquired to firms that use it to tailor their products and prices. The social dimension of the individual data---whereby an individual's data are predictive of the behavior of others---generates a data externality that reduces the intermediary's cost of acquiring information. We derive the intermediary's optimal data policy and show that it preserves the privacy of the consumers' identities while providing precise information about market demand to the firms. This enables the intermediary to capture the entire value of information as the number of consumers grows large."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2403.00422",
        "category": "econ",
        "title": "Inference for Interval-Identified Parameters Selected from an Estimated Set",
        "abstract": "Interval identification of parameters such as average treatment effects, average partial effects and welfare is particularly common when using observational data and experimental data with imperfect compliance due to the endogeneity of individuals' treatment uptake. In this setting, a treatment or policy will typically become an object of interest to the researcher when it is either selected from the estimated set of best-performers or arises from a data-dependent selection rule. In this paper, we develop new inference tools for interval-identified parameters chosen via these forms of selection. We develop three types of confidence intervals for data-dependent and interval-identified parameters, discuss how they apply to several examples of interest and prove their uniform asymptotic validity under weak assumptions.",
        "references": [
            {
                "arxivId": "2311.15878",
                "title": "Policy Learning with Distributional Welfare",
                "abstract": "In this paper, we explore optimal treatment allocation policies that target distributional welfare. Most literature on treatment choice has considered utilitarian welfare based on the conditional average treatment effect (ATE). While average welfare is intuitive, it may yield undesirable allocations especially when individuals are heterogeneous (e.g., with outliers) - the very reason individualized treatments were introduced in the first place. This observation motivates us to propose an optimal policy that allocates the treatment based on the conditional quantile of individual treatment effects (QoTE). Depending on the choice of the quantile probability, this criterion can accommodate a policymaker who is either prudent or negligent. The challenge of identifying the QoTE lies in its requirement for knowledge of the joint distribution of the counterfactual outcomes, which is generally hard to recover even with experimental data. Therefore, we introduce minimax policies that are robust to model uncertainty. A range of identifying assumptions can be used to yield more informative policies. For both stochastic and deterministic policies, we establish the asymptotic bound on the regret of implementing the proposed policies. In simulations and two empirical applications, we compare optimal decisions based on the QoTE with decisions based on other criteria. The framework can be generalized to any setting where welfare is defined as a functional of the joint distribution of the potential outcomes."
            },
            {
                "arxivId": "2204.11748",
                "title": "Optimal Decision Rules when Payoffs are Partially Identified",
                "abstract": "We derive optimal statistical decision rules for discrete choice problems when payoffs depend on a partially-identified parameter $\\theta$ and the decision maker can use a point-identified parameter $P$ to deduce restrictions on $\\theta$. Leading examples include optimal treatment choice under partial identification and optimal pricing with rich unobserved heterogeneity. Our optimal decision rules minimize the maximum risk or regret over the identified set of payoffs conditional on $P$ and use the data efficiently to learn about $P$. We discuss implementation of optimal decision rules via the bootstrap and Bayesian methods, in both parametric and semiparametric models. We provide detailed applications to treatment choice and optimal pricing. Using a limits of experiments framework, we show that our optimal decision rules can dominate seemingly natural alternatives. Our asymptotic approach is well suited for realistic empirical settings in which the derivation of finite-sample optimal rules is intractable."
            },
            {
                "arxivId": "2111.10904",
                "title": "Orthogonal Policy Learning Under Ambiguity",
                "abstract": "This paper studies the problem of estimating individualized treatment rules when treatment e\ufb00ects are partially identi\ufb01ed, as it is often the case with observational data. By drawing connections between the treatment assignment problem and classical decision theory, we characterize several notions of optimal treatment policies in the presence of partial identi\ufb01cation. The proposed framework allows to incorporate user-de\ufb01ned constraints on the policies, such as restrictions for transparency or interpretability, while also ensuring computational feasibility. We show that partial identi\ufb01cation leads to a novel statistical learning problem with risk directionally \u2013 but not fully \u2013 di\ufb00erentiable with respect to an in\ufb01nite-dimensional nuisance component. We propose an estimation procedure that ensures Neyman-orthogonality with respect to the nuisance component and provide statistical guarantees that depend on the amount of concentration around the points of non-di\ufb00erentiability in the data-generating process. The proposed method is illustrated using data from the Job Partnership Training Act study."
            },
            {
                "arxivId": "2111.04926",
                "title": "Optimal Decision Rules Under Partial Identification",
                "abstract": "I consider a class of statistical decision problems in which the policy maker must decide between two alternative policies to maximize social welfare based on a finite sample. The central assumption is that the underlying, possibly infinite-dimensional parameter, lies in a known convex set, potentially leading to partial identification of the welfare effect. An example of such restrictions is the smoothness of counterfactual outcome functions. As the main theoretical result, I derive a finite-sample, exact minimax regret decision rule within the class of all decision rules under normal errors with known variance. When the error distribution is unknown, I obtain a feasible decision rule that is asymptotically minimax regret. I apply my results to the problem of whether to change a policy eligibility cutoff in a regression discontinuity setup, and illustrate them in an empirical application to a school construction program in Burkina Faso."
            },
            {
                "arxivId": "2110.00864",
                "title": "Probabilistic Prediction for Binary Treatment Choice: with Focus on Personalized Medicine",
                "abstract": "This paper carries further my research applying statistical decision theory to treatment choice with sample data, using maximum regret to evaluate the performance of treatment rules. The specific new contribution is to study as-if optimization using estimates of illness probabilities, when choosing between surveillance and aggressive treatment. With this motivation, I introduce a new form of analysis of kernel estimation. Beyond its specifics, the paper sends a broad message. Biostatisticians and computer scientists have addressed medical risk assessment in alternative indirect ways, the former applying classical statistical theory and the latter measuring ex-post prediction accuracy in test samples. Neither approach is satisfactory. Statistical decision theory provides a coherent, generally applicable methodology. I am grateful for comments from Michael Gmeiner, Valentyn Litvin, and Filip Obradovic. I am grateful to Litvin and Gmeiner for programming the computations in Sections 4.1 and 4.2 respectively."
            },
            {
                "arxivId": "2011.12873",
                "title": "Hybrid Confidence Intervals for Informative Uniform Asymptotic Inference After Model Selection",
                "abstract": "\n I propose a new type of confidence interval for correct asymptotic inference after using data to select a model of interest without assuming any model is correctly specified. This hybrid confidence interval is constructed by combining techniques from the selective inference and post-selection inference literatures to yield a short confidence interval across a wide range of data realizations. I show that hybrid confidence intervals have correct asymptotic coverage, uniformly over a large class of probability distributions that do not bound scaled model parameters. I illustrate the use of these confidence intervals in the problem of inference after using the LASSO objective function to select a regression model of interest and provide evidence of their desirable length and coverage properties in small samples via a set of Monte Carlo experiments that entail a variety of different data distributions as well as an empirical application to the predictors of diabetes disease progression."
            },
            {
                "arxivId": "2009.13861",
                "title": "A computational approach to identification of treatment effects for policy evaluation",
                "abstract": null
            },
            {
                "arxivId": "2002.02579",
                "title": "Estimating optimal treatment rules with an instrumental variable: A partial identification learning approach",
                "abstract": "Individualized treatment rules (ITRs) are considered a promising recipe to deliver better policy interventions. One key ingredient in optimal ITR estimation problems is to estimate the average treatment effect conditional on a subject\u2019s covariate information, which is often challenging in observational studies due to the universal concern of unmeasured confounding. Instrumental variables (IVs) are widely used tools to infer the treatment effect when there is unmeasured confounding between the treatment and outcome. In this work, we propose a general framework of approaching the optimal ITR estimation problem when a valid IV is allowed to only partially identify the treatment effect. We introduce a novel notion of optimality called \u2018IV\u2010optimality\u2019. A treatment rule is said to be IV\u2010optimal if it minimizes the maximum risk with respect to the putative IV and the set of IV identification assumptions. We derive a bound on the risk of an IV\u2010optimal rule that illuminates when an IV\u2010optimal rule has favourable generalization performance. We propose a classification\u2010based statistical learning method that estimates such an IV\u2010optimal rule, design computationally efficient algorithms, and prove theoretical guarantees. We contrast our proposed method to the popular outcome weighted learning (OWL) approach via extensive simulations, and apply our method to study which mothers would benefit from travelling to deliver their premature babies at hospitals with high\u2010level neonatal intensive care units. R package ivitr implements the proposed method."
            },
            {
                "arxivId": "1912.10014",
                "title": "Optimal Dynamic Treatment Regimes and Partial Welfare Ordering",
                "abstract": "Dynamic treatment regimes are treatment allocations tailored to heterogeneous individuals. The optimal dynamic treatment regime is a regime that maximizes counterfactual welfare. This paper investigates the possibility of identification of optimal dynamic regimes when data are generated from sequential (natural) experiments. We propose a framework in which we can partially learn the optimal dynamic regime and ordering of welfares, relaxing sequential randomization assumptions commonly employed in the literature. We establish the sharp partial ordering of counterfactual welfares with respect to dynamic regimes by using a series of linear programs. A distinct feature of our approach is that, instead of solving a large number of large-scale linear programs, we provide simple analytical conditions for the ordering. The identified set of the optimal regime is then characterized as the set of maximal elements of the partial order. We also propose topological sorts of the partial order as a policy menu. We show how policymaking can be further guided by imposing assumptions such as monotonicity/uniformity of different stringency, agent's learning, Markovian structure, and stationarity."
            },
            {
                "arxivId": "1912.08726",
                "title": "Econometrics for Decision Making: Building Foundations Sketched by Haavelmo and Wald",
                "abstract": "Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes use of statistical decision theory to evaluate the performance of models in decision making. I consider the common practice of \n as\u2010if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as\u2010if optimization or any other model\u2010based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancing computation is the primary task to complete the foundations sketched by Haavelmo and Wald.\n"
            },
            {
                "arxivId": "1909.10062",
                "title": "Inference for Linear Conditional Moment Inequalities",
                "abstract": "\n We show that moment inequalities in a wide variety of economic applications have a particular linear conditional structure. We use this structure to construct uniformly valid confidence sets that remain computationally tractable even in settings with nuisance parameters. We first introduce least favorable critical values which deliver non-conservative tests if all moments are binding. Next, we introduce a novel conditional inference approach which ensures a strong form of insensitivity to slack moments. Our recommended approach is a hybrid technique which combines desirable aspects of the least favorable and conditional methods. The hybrid approach performs well in simulations calibrated to Wollmann (2018), with favorable power and computational time comparisons relative to existing alternatives."
            },
            {
                "arxivId": "1311.6238",
                "title": "Exact post-selection inference, with application to the lasso",
                "abstract": "We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2403.00458",
        "category": "econ",
        "title": "Prices and preferences in the electric vehicle market",
        "abstract": "Although electric vehicles are less polluting than gasoline powered vehicles, adoption is challenged by higher procurement prices. Existing discourse emphasizes EV battery costs as being principally responsible for this price differential and widespread adoption is routinely conditioned upon battery costs declining. We scrutinize such reasoning by sourcing data on EV attributes and market conditions between 2011 and 2023. Our findings are fourfold. First, EV prices are influenced principally by the number of amenities, additional features, and dealer-installed accessories sold as standard on an EV, and to a lesser extent, by EV horsepower. Second, EV range is negatively correlated with EV price implying that range anxiety concerns may be less consequential than existing discourse suggests. Third, battery capacity is positively correlated with EV price, due to more capacity being synonymous with the delivery of more horsepower. Collectively, this suggests that higher procurement prices for EVs reflects consumer preference for vehicles that are feature dense and more powerful. Fourth and finally, accommodating these preferences have produced vehicles with lower fuel economy, a shift that reduces envisioned lifecycle emissions benefits by at least 3.26 percent, subject to the battery pack chemistry leveraged and the carbon intensity of the electrical grid. These findings warrant attention as decarbonization efforts increasingly emphasize electrification as a pathway for complying with domestic and international climate agreements.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2403.00471",
        "category": "econ",
        "title": "Idiosyncratic Risk, Government Debt and Inflation",
        "abstract": "How does public debt matter for price stability? If it is useful for the private sector to insure idiosyncratic risk, government debt expansions can increase the natural rate of interest and create inflation. As I demonstrate using a tractable model, this holds in the presence of an active Taylor rule and does not require the absence of future fiscal consolidation. Further analysis using a full-blown 2-asset HANK model reveals the quantitative magnitude of the mechanism to crucially depend on the structure of the asset market: under standard assumptions, the effect of public debt on the natural rate is either overly strong or overly weak. Employing a parsimonious way to overcome this issue, my framework suggests relevant effects of public debt on inflation under active monetary policy: In particular, persistently elevated public debt may make it harder to go the last\"mile of disinflation\"unless central banks explicitly take its effect on the neutral rate into account.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-04.json",
        "arxivId": "2403.00653",
        "category": "econ",
        "title": "Modelling Global Fossil CO2 Emissions with a Lognormal Distribution: A Climate Policy Tool",
        "abstract": "Carbon dioxide (CO2) emissions have emerged as a critical issue with profound impacts on the environment, human health, and the global economy. The steady increase in atmospheric CO2 levels, largely due to human activities such as burning fossil fuels and deforestation, has become a major contributor to climate change and its associated catastrophic effects. To tackle this pressing challenge, a coordinated global effort is needed, which necessitates a deep understanding of emissions patterns and trends. In this paper, we explore the use of statistical modelling, specifically the lognormal distribution, as a framework for comprehending and predicting CO2 emissions. We build on prior research that suggests a complex distribution of emissions and seek to test the hypothesis that a simpler distribution can still offer meaningful insights for policy-makers. We utilize data from three comprehensive databases and analyse six candidate distributions (exponential, Fisk, gamma, lognormal, Lomax, Weibull) to identify a suitable model for global fossil CO2 emissions. Our findings highlight the adequacy of the lognormal distribution in characterizing emissions across all countries and years studied. Furthermore, to provide additional support for this distribution, we provide statistical evidence supporting the applicability of Gibrat's law to those CO2 emissions. Finally, we employ the lognormal model to predict emission parameters for the coming years and propose two policies for reducing total fossil CO2 emissions. Our research aims to provide policy-makers with accurate and detailed information to support effective climate change mitigation strategies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "1805.10869",
        "category": "econ",
        "title": "Tilting Approximate Models",
        "abstract": "Model approximations are common practice when estimating structural or quasi-structural models. The paper considers the econometric properties of estimators that utilize projections to reimpose information about the exact model in the form of conditional moments. The resulting estimator efficiently combines the information provided by the approximate law of motion and the moment conditions. The paper develops the corresponding asymptotic theory and provides simulation evidence that tilting substantially reduces the mean squared error for parameter estimates. It applies the methodology to pricing long-run risks in aggregate consumption in the US, whereas the model is solved using the Campbell and Shiller (1988) approximation. Tilting improves empirical fit and results suggest that approximation error is a source of upward bias in estimates of risk aversion and downward bias in the elasticity of intertemporal substitution.",
        "references": [
            {
                "arxivId": "2007.06169",
                "title": "An Adversarial Approach to Structural Estimation",
                "abstract": "We propose a new simulation\u2010based estimation method, adversarial estimation, for structural models. The estimator is formulated as the solution to a minimax problem between a generator (which generates simulated observations using the structural model) and a discriminator (which classifies whether an observation is simulated). The discriminator maximizes the accuracy of its classification while the generator minimizes it. We show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. We advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence."
            },
            {
                "arxivId": "1605.00499",
                "title": "Monte Carlo Confidence Sets for Identified Sets",
                "abstract": "In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows."
            },
            {
                "arxivId": "1408.0705",
                "title": "Using Invalid Instruments on Purpose: Focused Moment Selection and Averaging for GMM, Second Version",
                "abstract": "In finite samples, the use of a slightly endogenous but highly relevant instrument can reduce mean-squared error (MSE). Building on this observation, I propose a novel moment selection procedure for GMM\u2013the Focused Moment Selection Criterion (FMSC)\u2013in which moment conditions are chosen not based on their validity but on the MSE of their associated estimator of a user-specified target parameter. The FMSC mimics the situation faced by an applied researcher who begins with a set of relatively mild \u201cbaseline\u201d assumptions and must decide whether to impose any of a collection of stronger but more controversial \u201csuspect\u201d assumptions. When the (correctly specified) baseline moment conditions identify the model, the FMSC provides an asymptotically unbiased estimator of asymptotic MSE, allowing us to select over the suspect moment conditions. I go on to show how the framework used to derive the FMSC can address the problem of inference post-moment selection. Treating post-selection estimators as a special case of moment-averaging, in which estimators based on different moment sets are given data-dependent weights, I propose simulation-based procedures for inference that can be applied to a variety of formal and informal moment-selection and averaging procedures. Both the FMSC and confidence interval procedures perform well in simulations. I conclude with an empirical example examining the effect of instrument selection on the estimated relationship between malaria and income per capita."
            },
            {
                "arxivId": "1102.2650",
                "title": "Estimating and understanding exponential random graph models",
                "abstract": "We introduce a method for the theoretical analysis of exponential random graph models. The method is based on a large-deviations approximation to the normalizing constant shown to be consistent using theory developed by Chatterjee and Varadhan [European J. Combin. 32 (2011) 1000-1017]. The theory explains a host of difficulties encountered by applied workers: many distinct models have essentially the same MLE, rendering the problems ``practically'' ill-posed. We give the first rigorous proofs of ``degeneracy'' observed in these models. Here, almost all graphs have essentially no edges or are essentially complete. We supplement recent work of Bhamidi, Bresler and Sly [2008 IEEE 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS) (2008) 803-812 IEEE] showing that for many models, the extra sufficient statistics are useless: most realizations look like the results of a simple Erd\\H{o}s-R\\'{e}nyi model. We also find classes of models where the limiting graphs differ from Erd\\H{o}s-R\\'{e}nyi graphs. A limitation of our approach, inherited from the limitation of graph limit theory, is that it works only for dense graphs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2202.10030",
        "category": "econ",
        "title": "Multivariate Tie-breaker Designs",
        "abstract": "In a tie-breaker design (TBD), subjects with high values of a running variable are given some (usually desirable) treatment, subjects with low values are not, and subjects in the middle are randomized. TBDs are intermediate between regression discontinuity designs (RDDs) and randomized controlled trials (RCTs) by allowing a tradeoff between the resource allocation efficiency of an RDD and the statistical efficiency of an RCT. We study a model where the expected response is one multivariate regression for treated subjects and another for control subjects. For given covariates, we show how to use convex optimization to choose treatment probabilities that optimize a D-optimality criterion. We can incorporate a variety of constraints motivated by economic and ethical considerations. In our model, D-optimality for the treatment effect coincides with D-optimality for the whole regression, and without economic constraints, an RCT is globally optimal. We show that a monotonicity constraint favoring more deserving subjects induces sparsity in the number of distinct treatment probabilities and this is different from preexisting sparsity results for constrained designs. We also study a prospective D-optimality, analogous to Bayesian optimal design, to understand design tradeoffs without reference to a specific data set. We apply the convex optimization solution to a semi-synthetic example involving triage data from the MIMIC-IV-ED database.",
        "references": [
            {
                "arxivId": "2108.09400",
                "title": "Regression Discontinuity Designs",
                "abstract": "The regression discontinuity (RD) design is one of the most widely used nonexperimental methods for causal inference and program evaluation. Over the last two decades, statistical and econometric methods for RD analysis have expanded and matured, and there is now a large number of methodological results for RD identification, estimation, inference, and validation. We offer a curated review of this methodological literature organized around the two most popular frameworks for the analysis and interpretation of RD designs: the continuity framework and the local randomization framework. For each framework, we discuss three main topics: ( a) designs and parameters, focusing on different types of RD settings and treatment effects of interest; ( b) estimation and inference, presenting the most popular methods based on local polynomial regression and methods for the analysis of experiments, as well as refinements, extensions, and alternatives; and ( c) validation and falsification, summarizing an array of mostly empirical approaches to support the validity of RD designs in practice."
            },
            {
                "arxivId": "2101.09605",
                "title": "Kernel regression analysis of tie-breaker designs",
                "abstract": ": Tie-breaker experimental designs are hybrids of Randomized Controlled Trials (RCTs) and Regression Discontinuity Designs (RDDs) in which subjects with moderate scores are placed in an RCT while sub- jects with extreme scores are deterministically assigned to the treatment or control group. In settings where it is unfair or uneconomical to deny the treatment to the more deserving recipients, the tie-breaker design (TBD) trades o\ufb00 the practical advantages of the RDD with the statistical advan- tages of the RCT. The practical costs of the randomization in TBDs can be hard to quantify in generality, while the statistical bene\ufb01ts conferred by randomization in TBDs have only been studied under linear and quadratic models. In this paper, we discuss and quantify the statistical bene\ufb01ts of TBDs without using parametric modelling assumptions. If the goal is estimation of the average treatment e\ufb00ect or the treatment e\ufb00ect at more than one score value, the statistical bene\ufb01ts of using a TBD over an RDD are apparent. If the goal is nonparametric estimation of the mean treatment e\ufb00ect at merely one score value, we prove that about 2.8 times more subjects are needed for an RDD in order to achieve the same asymptotic mean squared error. We further demonstrate using both theoretical results and simulations from the Angrist and Lavy (1999) classroom size dataset, that larger experimental radii choices for the TBD lead to greater statistical e\ufb03ciency."
            },
            {
                "arxivId": "2011.08047",
                "title": "Causal Inference Methods for Combining Randomized Trials and Observational Studies: A Review",
                "abstract": "With increasing data availability, treatment causal effects can be evaluated across different dataset, both randomized trials and observational studies. Randomized trials isolate the effect of the treatment from that of unwanted (confounding) co-occuring effects. But they may be applied to limited populations, and thus lack external validity. On the opposite large observational samples are often more representative of the target population but can conflate confounding effects with the treatment of interest. In this paper, we review the growing literature on methods for causal inference on combined randomized trial and observational studies, striving for the best of both worlds. We first discuss identification and estimation methods that improve generalizability of randomized controlled trials (RCTs) using the representativeness of observational data. Classical estimators include weighting, difference between conditional outcome models, and double robust estimators. We then discuss methods that combining RCTs and observational data to improve the (conditional) average treatment effect estimation, handling possible unmeasured confounding in the observational data. We also connect and contrast works developed in both the potential outcomes framework and the structural causal models framework. Finally, we compare the main methods using a simulation study and real world data to analyse the effect of tranexamic acid on the mortality rate in major trauma patients. Code to implement many of the methods is provided."
            },
            {
                "arxivId": "2002.06708",
                "title": "Combining observational and experimental datasets using shrinkage estimators",
                "abstract": "We consider the problem of combining data from observational and experimental sources to draw causal conclusions. To derive combined estimators with desirable properties, we extend results from the Stein shrinkage literature. Our contributions are threefold. First, we propose a generic procedure for deriving shrinkage estimators in this setting, making use of a generalized unbiased risk estimate. Second, we develop two new estimators, prove finite sample conditions under which they have lower risk than an estimator using only experimental data, and show that each achieves a notion of asymptotic optimality. Third, we draw connections between our approach and results in sensitivity analysis, including proposing a method for evaluating the feasibility of our estimators."
            },
            {
                "arxivId": "1904.07272",
                "title": "Introduction to Multi-Armed Bandits",
                "abstract": "Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments. The chapters are as follows: stochastic bandits, lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full Feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual Bandits; bandits and games; bandits with knapsacks; bandits and incentives."
            },
            {
                "arxivId": "1804.07863",
                "title": "Propensity score methods for merging observational and experimental datasets",
                "abstract": "We consider how to merge a limited amount of data from a randomized controlled trial (RCT) into a much larger set of data from an observational data base (ODB), to estimate an average causal treatment effect. Our methods are based on stratification. The strata are defined in terms of effect moderators as well as propensity scores estimated in the ODB. Data from the RCT are placed into the strata they would have occupied, had they been in the ODB instead. We assume that treatment differences are comparable in the two data sources. Our first \u201cspiked\u2010in\u201d method simply inserts the RCT data into their corresponding ODB strata. We also consider a data\u2010driven convex combination of the ODB and RCT treatment effect estimates within each stratum. Using the delta method and simulations, we identify a bias problem with the spiked\u2010in estimator that is ameliorated by the convex combination estimator. We apply our methods to data from the Women's Health Initiative, a study of thousands of postmenopausal women which has both observational and experimental data on hormone therapy (HT). Using half of the RCT to define a gold standard, we find that a version of the spiked\u2010in estimator yields lower\u2010MSE estimates of the causal impact of HT on coronary heart disease than would be achieved using either a small RCT or the observational component on its own."
            },
            {
                "arxivId": "1711.07582",
                "title": "CVXR: An R Package for Disciplined Convex Optimization",
                "abstract": "CVXR is an R package that provides an object-oriented modeling language for convex optimization, similar to CVX, CVXPY, YALMIP, and Convex.jl. It allows the user to formulate convex optimization problems in a natural mathematical syntax rather than the restrictive form required by most solvers. The user specifies an objective and set of constraints by combining constants, variables, and parameters using a library of functions with known mathematical properties. CVXR then applies signed disciplined convex programming (DCP) to verify the problem's convexity. Once verified, the problem is converted into standard conic form using graph implementations and passed to a cone solver such as ECOS or SCS. We demonstrate CVXR's modeling framework with several applications."
            },
            {
                "arxivId": "1307.4953",
                "title": "Computing exact D-optimal designs by mixed integer second-order cone programming",
                "abstract": "i=1 wiAiA T i , where Ai,i = 1,...,s are known matrices with m rows. In this paper, we show that the criterion of D-optimality is secondorder cone representable. As a result, the method of second-order cone programming can be used to compute an approximate D-optimal design with any system of linear constraints on the vector of weights. More importantly, the proposed characterization allows us to compute an exact D-optimal design, which is possible thanks to highquality branch-and-cut solvers specialized to solve mixed integer second-order cone programming problems. Our results extend to the case of the criterion of DK-optimality, which measures the quality of w for the estimation of a linear parameter subsystem defined by a full-rank coefficient matrixK. We prove that some other widely used criteria are also secondorder cone representable, for instance, the criteria of A-, AK-, Gand I-optimality. We present several numerical examples demonstrating the efficiency and general applicability of the proposed method. We show that in many cases the mixed integer second-order cone programming approach allows us to find a provably optimal exact design, while the standard heuristics systematically miss the optimum."
            },
            {
                "arxivId": "2106.01946",
                "title": "Convex optimization",
                "abstract": "This textbook is based on lectures given by the authors at MIPT (Moscow), HSE (Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all, the authors focused on the program of a two-semester course of lectures on convex optimization, which is given to students of MIPT. The first chapter of this book contains the materials of the first semester (\"Fundamentals of convex analysis and optimization\"), the second and third chapters contain the materials of the second semester (\"Numerical methods of convex optimization\"). The textbook has a number of features. First, in contrast to the classic manuals, this book does not provide proofs of all the theorems mentioned. This allowed, on one side, to describe more themes, but on the other side, made the presentation less self-sufficient. The second important point is that part of the material is advanced and is published in the Russian educational literature, apparently for the first time. Third, the accents that are given do not always coincide with the generally accepted accents in the textbooks that are now popular. First of all, we talk about a sufficiently advanced presentation of conic optimization, including robust optimization, as a vivid demonstration of the capabilities of modern convex analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2203.03243",
        "category": "econ",
        "title": "Choice and Attention across Time",
        "abstract": "I study how past and future choices are linked in the framework of attention. Attention cannot be observed but past choices are necessarily considered in future decisions. This link connects two types of rationality violations, counterfactual and realized, where the former results from inattention and the latter fully pins down preferences. Results show that the necessary traces of limited attention lie within choice sequences because they enable and compel a decision maker to correct their\"mistakes\". The framework accommodates different attention structures and extends to framing, introducing choice sequences as an important channel to formulate, identify, and scrutinize limited attention.",
        "references": [
            {
                "arxivId": "2106.13350",
                "title": "Reference dependence and random attention",
                "abstract": null
            },
            {
                "arxivId": "1702.07826",
                "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations",
                "abstract": "We introduce \\em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2206.09883",
        "category": "econ",
        "title": "Policy Learning under Endogeneity Using Instrumental Variables",
        "abstract": "This paper studies the identification and estimation of individualized intervention policies in observational data settings characterized by endogenous treatment selection and the availability of instrumental variables. We introduce encouragement rules that manipulate an instrument. Incorporating the marginal treatment effects (MTE) as policy invariant structural parameters, we establish the identification of the social welfare criterion for the optimal encouragement rule. Focusing on binary encouragement rules, we propose to estimate the optimal policy via the Empirical Welfare Maximization (EWM) method and derive convergence rates of the regret (welfare loss). We consider extensions to accommodate multiple instruments and budget constraints. Using data from the Indonesian Family Life Survey, we apply the EWM encouragement rule to advise on the optimal tuition subsidy assignment. Our framework offers interpretability regarding why a certain subpopulation is targeted.",
        "references": [
            {
                "arxivId": "2103.15298",
                "title": "Empirical Welfare Maximization with Constraints",
                "abstract": "When designing eligibility criteria for welfare programs, policymakers naturally want to target the individuals who will benefit the most. This paper proposes two new econometric approaches to selecting an optimal eligibility criterion when individuals\u2019 costs to the program are unknown and need to be estimated. One is designed to achieve the highest benefit possible while satisfying a budget constraint with high probability. The other is designed to optimally trade off the benefit and the cost from violating the budget constraint. The setting I consider extends the previous literature on Empirical Welfare Maximization by allowing for uncertainty in estimating the budget needed to implement the criterion, in addition to its benefit. Consequently, my approaches improve the existing approach as they can be applied to settings with imperfect take-up or varying program needs. I illustrate my approaches empirically by deriving an optimal budget-constrained Medicaid expansion in the US."
            },
            {
                "arxivId": "1911.09260",
                "title": "A Semiparametric Instrumental Variable Approach to Optimal Treatment Regimes Under Endogeneity",
                "abstract": "Abstract There is a fast-growing literature on estimating optimal treatment regimes based on randomized trials or observational studies under a key identifying condition of no unmeasured confounding. Because confounding by unmeasured factors cannot generally be ruled out with certainty in observational studies or randomized trials subject to noncompliance, we propose a general instrumental variable (IV) approach to learning optimal treatment regimes under endogeneity. Specifically, we establish identification of both value function for a given regime and optimal regimes with the aid of a binary IV, when no unmeasured confounding fails to hold. We also construct novel multiply robust classification-based estimators. Furthermore, we propose to identify and estimate optimal treatment regimes among those who would comply to the assigned treatment under a monotonicity assumption. In this latter case, we establish the somewhat surprising result that complier optimal regimes can be consistently estimated without directly collecting compliance information and therefore without the complier average treatment effect itself being identified. Our approach is illustrated via extensive simulation studies and a data application on the effect of child rearing on labor participation. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1807.04183",
                "title": "Optimization over Continuous and Multi-dimensional Decisions with Observational Data",
                "abstract": "We consider the optimization of an uncertain objective over continuous and multi-dimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets."
            },
            {
                "arxivId": "1611.09925",
                "title": "Bounded, efficient and multiply robust estimation of average treatment effects using instrumental variables",
                "abstract": "Instrumental variables are widely used for estimating causal effects in the presence of unmeasured confounding. Under the standard instrumental variable model, however, the average treatment effect is only partially identifiable. To address this, we propose novel assumptions that enable identification of the average treatment effect. Our identification assumptions are clearly separated from model assumptions that are needed for estimation, so researchers are not required to commit to a specific observed data model in establishing identification. We then construct multiple estimators that are consistent under three different observed data models, and multiply robust estimators that are consistent in the union of these observed data models. We pay special attention to the case of binary outcomes, for which we obtain bounded estimators of the average treatment effect that are guaranteed to lie between \u22121 and 1. Our approaches are illustrated with simulations and a data analysis evaluating the causal effect of education on earnings."
            },
            {
                "arxivId": "1609.03167",
                "title": "Model Selection for Treatment Choice: Penalized Welfare Maximization",
                "abstract": "This paper studies a new statistical decision rule for the treatment assignment problem. Consider a utilitarian policy maker who must use sample data to allocate one of two treatments to members of a population, based on their observable characteristics. In practice, it is often the case that policy makers do not have full discretion on how these covariates can be used, for legal, ethical or political reasons. We treat this constrained problem as a statistical decision problem, where we evaluate the performance of decision rules by their maximum regret. We focus on settings in which the policy maker may want to select amongst a collection of such constrained classes: examples we consider include choosing the number of covariates over which to perform best-subset selection, and model selection when approximating a complicated class via a sieve. We adapt and extend results from statistical learning to develop a decision rule which we call the Penalized Welfare Maximization (PWM) rule. We establish an oracle inequality for the regret of the PWM rule which shows that it is able to perform model selection over the collection of available classes. We then use this oracle inequality to derive relevant bounds on maximum regret for PWM. We illustrate the model-selection capabilities of our method with a small simulation exercise, and conclude by applying our rule to data from the Job Training Partnership Act (JTPA) study."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2206.15098",
        "category": "econ",
        "title": "Talent Hoarding in Organizations",
        "abstract": "Most organizations rely on managers to identify talented workers. However, managers who are evaluated on team performance have an incentive to hoard workers. This study provides the first empirical evidence of talent hoarding using personnel records and survey evidence from a large manufacturing firm. Talent hoarding is reported by three-fourths of managers, is detectable in managerial decisions, and occurs more frequently when hoarding incentives are stronger. Using quasi-random variation in exposure to talent hoarding, I demonstrate that hoarding deters workers from applying to new positions, inhibiting worker career progression and altering the allocation of talent in the firm.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2210.00362",
        "category": "econ",
        "title": "Yurinskii's Coupling for Martingales",
        "abstract": "Yurinskii's coupling is a popular theoretical tool for non-asymptotic distributional analysis in mathematical statistics and applied probability, offering a Gaussian strong approximation with an explicit error bound under easily verified conditions. Originally stated in $\\ell^2$-norm for sums of independent random vectors, it has recently been extended both to the $\\ell^p$-norm, for $1 \\leq p \\leq \\infty$, and to vector-valued martingales in $\\ell^2$-norm, under some strong conditions. We present as our main result a Yurinskii coupling for approximate martingales in $\\ell^p$-norm, under substantially weaker conditions than those previously imposed. Our formulation further allows for the coupling variable to follow a more general Gaussian mixture distribution, and we provide a novel third-order coupling method which gives tighter approximations in certain settings. We specialize our main result to mixingales, martingales, and independent data, and derive uniform Gaussian mixture strong approximations for martingale empirical processes. Applications to nonparametric partitioning-based and local polynomial regression procedures are provided.",
        "references": [
            {
                "arxivId": "1904.02130",
                "title": "Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic Rates of Martingale CLT",
                "abstract": "We provide non-asymptotic convergence rates of the Polyak-Ruppert averaged stochastic gradient descent (SGD) to a normal random vector for a class of twice-differentiable test functions. A crucial intermediate step is proving a non-asymptotic martingale central limit theorem (CLT), i.e., establishing the rates of convergence of a multivariate martingale difference sequence to a normal random vector, which might be of independent interest. We obtain the explicit rates for the multivariate martingale CLT using a combination of Stein's method and Lindeberg's argument, which is then used in conjunction with a non-asymptotic analysis of averaged SGD proposed in [PJ92]. Our results have potentially interesting consequences for computing confidence intervals for parameter estimation with SGD and constructing hypothesis tests with SGD that are valid in a non-asymptotic sense."
            },
            {
                "arxivId": "1202.4777",
                "title": "Bernstein inequality and moderate deviations under strong mixing conditions",
                "abstract": "In this paper we obtain a Bernstein type inequality for a class of weakly dependent random variables. The proofs lead to a moderate deviation principle for sums of bounded random variables with exponential decay of the strong mixing coefficients that complements the large deviation result obtained by Bryc and Dembo (1998) under superexponential mixing rates."
            },
            {
                "arxivId": "1105.6154",
                "title": "Conditional Quantile Processes Based on Series or Many Regressors",
                "abstract": "Quantile regression (QR) is a principal regression method for analyzing the impact of covariates on outcomes. The impact is described by the conditional quantile function and its functionals. In this paper we develop the nonparametric QR-series framework covering many regressors as a special case, for performing inference on the entire conditional quantile function and its linear functionals. In this framework, we approximate the entire conditional quantile function by a linear combination of series terms with quantile-speci fic coefficients and estimate the function-valued coefficients from the data. We develop large sample theory for the QR-series coefficient process, namely we obtain uniform strong approximations to the QR-series coefficient process by conditionally pivotal and Gaussian processes. Based on these two strong approximations, or couplings, we develop four resampling methods (pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be used for inference on the entire QR-series coefficient function. We apply these results to obtain estimation and inference methods for linear functionals of the conditional quantile function, such as the conditional quantile function itself, its partial derivatives, average partial derivatives, and conditional average partial derivatives. Speci fically, we obtain uniform rates of convergence and show how to use the four resampling methods mentioned above for inference on the functionals. All of the above results are for function-valued parameters, holding uniformly in both the quantile index and the covariate value, and covering the pointwise case as a by-product. We demonstrate the practical utility of these results with an empirical example, where we estimate the price elasticity function and test the Slutsky condition of the individual demand for gasoline, as indexed by the individual unobserved propensity for gasoline consumption."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2309.08982",
        "category": "econ",
        "title": "Least squares estimation in nonstationary nonlinear cohort panels with learning from experience",
        "abstract": "We discuss techniques of estimation and inference for nonstationary nonlinear cohort panels with learning from experience, showing, inter alia, the consistency and asymptotic normality of the nonlinear least squares estimator used in empirical practice. Potential pitfalls for hypothesis testing are identified and solutions proposed. Monte Carlo simulations verify the properties of the estimator and corresponding test statistics in finite samples, while an application to a panel of survey expectations demonstrates the usefulness of the theory developed.",
        "references": [
            {
                "arxivId": "2205.03285",
                "title": "Cluster-robust inference: A guide to empirical practice",
                "abstract": null
            },
            {
                "arxivId": "1906.11293",
                "title": "Empirical process results for exchangeable arrays",
                "abstract": "Exchangeable arrays are natural tools to model common forms of dependence between units of a sample. Jointly exchangeable arrays are well suited to dyadic data, where observed random variables are indexed by two units from the same population. Examples include trade flows between countries or relationships in a network. Separately exchangeable arrays are well suited to multiway clustering, where units sharing the same cluster (e.g. geographical areas or sectors of activity when considering individual wages) may be dependent in an unrestricted way. We prove uniform laws of large numbers and central limit theorems for such exchangeable arrays. We obtain these results under the same moment restrictions and conditions on the class of functions as those typically assumed with i.i.d. data. We also show the convergence of bootstrap processes adapted to such arrays."
            },
            {
                "arxivId": "math/0702692",
                "title": "Quasi-maximum-likelihood estimation in conditionally heteroscedastic time series: A stochastic recurrence equations approach",
                "abstract": "This paper studies the quasi-maximum-likelihood estimator (QMLE) in a general conditionally heteroscedastic time series model of multiplicative form Xt = \u03c3t Zt , where the unobservable volatility \u03c3t is a parametric function of (Xt\u22121 ,...,X t\u2212p ,\u03c3 t\u22121 ,...,\u03c3 t\u2212q ) for some p, q \u2265 0, and (Zt ) is standardized i.i.d. noise. We assume that these models are solutions to stochastic recurrence equations which satisfy a contraction (random Lipschitz coefficient) property. These assumptions are satisfied for the popular GARCH, asymmetric GARCH and exponential GARCH processes. Exploiting the contraction property, we give conditions for the existence and uniqueness of a strictly stationary solution (Xt ) to the stochastic recurrence equation and establish consistency and asymptotic normality of the QMLE. We also discuss the problem of invertibility of such time series models. 1. Introduction. Gaussian quasi-maximum-likelihood estimation, that is, likelihood estimation under the hypothesis of Gaussian innovations, is a popular method which is widely used for inference in time series models. However, it is often a nontrivial task to establish the consistency and asymptotic normality of the quasi-maximum-likelihood estimator (QMLE) applied to specific models and, therefore, an in-depth analysis of the probabilistic structure generated by the model is called for. A classical example of this kind is the seminal paper by Hannan [18] on estimation in linear ARMA time series. In this paper we study the QMLE for a general class of conditionally heteroscedastic time series models, which includes GARCH, asymmetric GARCH and exponential GARCH. Recall that a GARCH(p, q) [generalized autoregressive conditionally heteroscedastic of order (p, q)] process [4 ]i s def ined by"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2312.08799",
        "category": "econ",
        "title": "Refined Characterizations of Approval-based Committee Scoring Rules",
        "abstract": "In approval-based committee (ABC) elections, the goal is to select a fixed-size subset of the candidates, a so-called committee, based on the voters' approval ballots over the candidates. One of the most popular classes of ABC voting rules are ABC scoring rules, for which voters give points to each committee and the committees with maximal total points are chosen. While the set of ABC scoring rules has recently been characterized in a model where the output is a ranking of all committees, no full characterization of these rules exists in the standard model where a set of winning committees is returned. We address this issue by characterizing two important subclasses of ABC scoring rules in the standard ABC election model, thereby both extending the result for ABC ranking rules to the standard setting and refining it to subclasses. In more detail, by relying on a consistency axiom for variable electorates, we characterize (i) the prominent class of Thiele rules and (ii) a new class of ABC voting rules called ballot size weighted approval voting. Based on these theorems, we also infer characterizations of three well-known ABC voting rules, namely multi-winner approval voting, proportional approval voting, and satisfaction approval voting.",
        "references": [
            {
                "arxivId": "2211.13567",
                "title": "Strategyproofness and Proportionality in Party-Approval Multiwinner Elections",
                "abstract": "In party-approval multiwinner elections the goal is to allocate the seats of a fixed-size committee to parties based on the approval ballots of the voters over the parties. In particular, each voter can approve multiple parties and each party can be assigned multiple seats. Two central requirements in this setting are proportional representation and strategyproofness. Intuitively, proportional representation requires that every sufficiently large group of voters with similar preferences is represented in the committee. Strategyproofness demands that no voter can benefit by misreporting her true preferences. We show that these two axioms are incompatible for anonymous party-approval multiwinner voting rules, thus proving a far-reaching impossibility theorem. The proof of this result is obtained by formulating the problem in propositional logic and then letting a SAT solver show that the formula is unsatisfiable. Additionally, we demonstrate how to circumvent this impossibility by considering a weakening of strategyproofness which requires that only voters who do not approve any elected party cannot manipulate. While most common voting rules fail even this weak notion of strategyproofness, we characterize Chamberlin-Courant approval voting within the class of Thiele rules based on this strategyproofness notion."
            },
            {
                "arxivId": "2112.10407",
                "title": "Axiomatic characterizations of consistent approval-based committee choice rules",
                "abstract": "We prove axiomatic characterizations of several important multiwinner rules within the class of approval-based committee choice rules. These are voting rules that return a set of (fixed-size) committees. In particular, we provide axiomatic characterizations of Proportional Approval Voting, the Chamberlin--Courant rule, and other Thiele methods. These rules share the important property that they satisfy an axiom called consistency, which is crucial in our characterizations."
            },
            {
                "arxivId": "2007.01795",
                "title": "Multi-Winner Voting with Approval Preferences",
                "abstract": null
            },
            {
                "arxivId": "1911.11747",
                "title": "Proportionality and the Limits of Welfarism",
                "abstract": "We study two influential voting rules proposed in the 1890s by Phragmen and Thiele, which elect a committee of k candidates which proportionally represents the voters. Voters provide their preferences by approving an arbitrary number of candidates. Previous work has proposed proportionality axioms satisfied by Thiele's rule (now known as Proportional Approval Voting, PAV) but not by Phragmen's rule. By proposing two new proportionality axioms (laminar proportionality and priceability) satisfied by Phragmen but not Thiele, we show that the two rules achieve two distinct forms of proportional representation. Phragmen's rule ensures that all voters have a similar amount of influence on the committee, and Thiele's rule ensures a fair utility distribution. Thiele's rule is a welfarist voting rule (one that maximizes a function of voter utilities). We show that no welfarist rule can satisfy our new axioms, and we prove that no such rule can satisfy the core. Conversely, some welfarist fairness properties cannot be guaranteed by Phragmen-type rules. This formalizes the difference between the two types of proportionality. We then introduce an attractive committee rule which satisfies a property intermediate between the core and extended justified representation (EJR). It satisfies laminar proportionality, priceability, and is computable in polynomial time. We show that our new rule provides a logarithmic approximation to the core. On the other hand, PAV provides a factor-2 approximation to the core, and this factor is optimal for rules that are fair in the sense of the Pigou--Dalton principle. The full version of the paper is available at http://arxiv.org/pdf/1911.11747.pdf."
            },
            {
                "arxivId": "1905.00640",
                "title": "Tight approximation bounds for maximum multi-coverage",
                "abstract": null
            },
            {
                "arxivId": "1710.04246",
                "title": "Monotonicity axioms in approval-based multi-winner voting rules",
                "abstract": "In this paper we study several monotonicity axioms in approval-based multi-winner voting rules. We consider monotonicity with respect to the support received by the winners and also monotonicity in the size of the committee. Monotonicity with respect to the support is studied when the set of voters does not change and when new voters enter the election. For each of these two cases we consider a strong and a weak version of the axiom. We observe certain incompatibilities between the monotonicity axioms and well-known representation axioms (extended/proportional justified representation) for the voting rules that we analyze and provide formal proofs of incompatibility between some of these axioms and perfect representation."
            },
            {
                "arxivId": "1704.02453",
                "title": "Consistent Approval-Based Multi-Winner Rules",
                "abstract": "This paper is an axiomatic study of consistent approval-based multi-winner rules, i.e., voting rules that select a fixed-size group of candidates based on approval ballots. We introduce the class of counting rules, provide an axiomatic characterization of this class and, in particular, show that counting rules are consistent. Building upon this result, we axiomatically characterize three important consistent multi-winner rules: Proportional Approval Voting, Multi-Winner Approval Voting and the Approval Chamberlin--Courant rule. Our results demonstrate the variety of multi-winner rules and illustrate three different, orthogonal principles that multi-winner voting rules may represent: individual excellence, diversity, and proportionality."
            },
            {
                "arxivId": "1611.08826",
                "title": "Phragm\u00e9n's and Thiele's election methods",
                "abstract": "The election methods introduced in 1894--1895 by Phragmen and Thiele, and their somewhat later versions for ordered (ranked) ballots, are discussed in detail. The paper includes definitions and examples and discussion of whether the methods satisfy some properties, including monotonicity, consistency and various proportionality criteria. The relation with STV is also discussed. The paper also contains historical information on the methods."
            },
            {
                "arxivId": "1611.08691",
                "title": "Multiwinner approval rules as apportionment methods",
                "abstract": "We establish a link between multiwinner elections and apportionment problems by showing how approval-based multiwinner election rules can be interpreted as methods of apportionment. We consider several multiwinner rules and observe that some, but not all, of them induce apportionment methods that are well-established in the literature and in the actual practice of representation, be it proportional or non-proportional. For instance, we show that proportional approval voting induces the D\u2019Hondt method and that Monroe\u2019s rule induces the largest remainder method. Our approach also yields apportionment methods implementing degressive proportionality. Furthermore, we consider properties of apportionment methods and exhibit multiwinner rules that induce apportionment methods satisfying these properties."
            },
            {
                "arxivId": "1505.00341",
                "title": "Structure in Dichotomous Preferences",
                "abstract": "Many hard computational social choice problems are known to become tractable when voters' preferences belong to a restricted domain, such as those of single-peaked or single-crossing preferences. However, to date, all algorithmic results of this type have been obtained for the setting where each voter's preference list is a total order of candidates. The goal of this paper is to extend this line of research to the setting where voters' preferences are dichotomous, i.e., each voter approves a subset of candidates and disapproves the remaining candidates. We propose several analogues of the notions of single-peaked and single-crossing preferences for dichotomous profiles and investigate the relationships among them. We then demonstrate that for some of these notions the respective restricted domains admit efficient algorithms for computationally hard approval-based multi-winner rules."
            },
            {
                "arxivId": "1407.8269",
                "title": "Justified representation in approval-based committee voting",
                "abstract": null
            },
            {
                "arxivId": "1407.3247",
                "title": "Computational Aspects of Multi-Winner Approval Voting",
                "abstract": "We study computational aspects of three prominent voting rules that use approval ballots to select multiple winners. These rules are proportional approval voting, reweighted approval voting, and satisfaction approval voting. Each rule is designed with the intention to compute a representative winning set. We show that computing the winner for proportional approval voting is NP-hard, closing an open problem (Kilgour, 2010). As none of the rules we examine are strategy-proof, we study various strategic aspects of the rules. In particular, we examine the computational complexity of computing a best response for both a single agent and a group of agents. In many settings, we show that it is NP-hard for an agent or agents to compute how best to vote given a fixed set of approval ballots of the other agents."
            },
            {
                "arxivId": "1506.02891",
                "title": "Properties of multiwinner voting rules",
                "abstract": null
            },
            {
                "arxivId": "1402.3044",
                "title": "Finding a collective set of items: From proportional multirepresentation to group recommendation",
                "abstract": null
            },
            {
                "arxivId": "1401.4602",
                "title": "Cloning in Elections: Finding the Possible Winners",
                "abstract": "We consider the problem of manipulating elections by cloning candidates. In our model, a manipulator can replace each candidate c by several clones, i.e., new candidates that are so similar to c that each voter simply replaces c in his vote with a block of these new candidates, ranked consecutively. The outcome of the resulting election may then depend on the number of clones as well as on how each voter orders the clones within the block. We formalize what it means for a cloning manipulation to be successful (which turns out to be a surprisingly delicate issue), and, for a number of common voting rules, characterize the preference profiles for which a successful cloning manipulation exists. We also consider the model where there is a cost associated with producing each clone, and study the complexity of finding a minimum-cost cloning manipulation. Finally, we compare cloning with two related problems: the problem of control by adding candidates and the problem of possible (co)winners when new alternatives can join."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2402.19268",
        "category": "econ",
        "title": "Extremal quantiles of intermediate orders under two-way clustering",
        "abstract": "This paper investigates extremal quantiles under two-way cluster dependence. We demonstrate that the limiting distribution of the unconditional intermediate order quantiles in the tails converges to a Gaussian distribution. This is remarkable as two-way cluster dependence entails potential non-Gaussianity in general, but extremal quantiles do not suffer from this issue. Building upon this result, we extend our analysis to extremal quantile regressions of intermediate order.",
        "references": [
            {
                "arxivId": "1907.13630",
                "title": "Kernel density estimation for undirected dyadic data",
                "abstract": "We study nonparametric estimation of density functions for undirected dyadic random variables (i.e., random variables defined for all n\\overset{def}{\\equiv}\\tbinom{N}{2} unordered pairs of agents/nodes in a weighted network of order N). These random variables satisfy a local dependence property: any random variables in the network that share one or two indices may be dependent, while those sharing no indices in common are independent. In this setting, we show that density functions may be estimated by an application of the kernel estimation method of Rosenblatt (1956) and Parzen (1962). We suggest an estimate of their asymptotic variances inspired by a combination of (i) Newey's (1994) method of variance estimation for kernel estimators in the \"monadic\" setting and (ii) a variance estimator for the (estimated) density of a simple network first suggested by Holland and Leinhardt (1976). More unusual are the rates of convergence and asymptotic (normal) distributions of our dyadic density estimates. Specifically, we show that they converge at the same rate as the (unconditional) dyadic sample mean: the square root of the number, N, of nodes. This differs from the results for nonparametric estimation of densities and regression functions for monadic data, which generally have a slower rate of convergence than their corresponding sample mean."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01318",
        "category": "econ",
        "title": "High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media",
        "abstract": "Motivated by the empirical power law of the distributions of credits (e.g., the number of\"likes\") of viral posts in social media, we introduce the high-dimensional tail index regression and methods of estimation and inference for its parameters. We propose a regularized estimator, establish its consistency, and derive its convergence rate. To conduct inference, we propose to debias the regularized estimate, and establish the asymptotic normality of the debiased estimator. Simulation studies support our theory. These methods are applied to text analyses of viral posts in X (formerly Twitter) concerning LGBTQ+.",
        "references": [
            {
                "arxivId": "1704.08095",
                "title": "Converting High-Dimensional Regression to High-Dimensional Conditional Density Estimation",
                "abstract": "There is a growing demand for nonparametric conditional density estimators (CDEs) in fields such as astronomy and economics. In astronomy, for example, one can dramatically improve estimates of the parameters that dictate the evolution of the Universe by working with full conditional densities instead of regression (i.e., conditional mean) estimates. More generally, standard regression falls short in any prediction problem where the distribution of the response is more complex with multi-modality, asymmetry or heteroscedastic noise. Nevertheless, much of the work on high-dimensional inference concerns regression and classification only, whereas research on density estimation has lagged behind. Here we propose FlexCode, a fully nonparametric approach to conditional density estimation that reformulates CDE as a non-parametric orthogonal series problem where the expansion coefficients are estimated by regression. By taking such an approach, one can efficiently estimate conditional densities and not just expectations in high dimensions by drawing upon the success in high-dimensional regression. Depending on the choice of regression procedure, our method can adapt to a variety of challenging high-dimensional settings with different structures in the data (e.g., a large number of irrelevant components and nonlinear manifold structure) as well as different data types (e.g., functional data, mixed data types and sample sets). We study the theoretical and empirical performance of our proposed method, and we compare our approach with traditional conditional density estimators on simulated as well as real-world data, such as photometric galaxy data, Twitter data, and line-of-sight velocities in a galaxy cluster."
            },
            {
                "arxivId": "1604.00540",
                "title": "Nonparametric Conditional Density Estimation in a High-Dimensional Regression Setting",
                "abstract": "In some applications (e.g., in cosmology and economics), the regression is not adequate to represent the association between a predictor x and a response Z because of multi-modality and asymmetry of f(z|x); using the full density instead of a single-point estimate can then lead to less bias in subsequent analysis. As of now, there are no effective ways of estimating f(z|x) when x represents high-dimensional, complex data. In this article, we propose a new nonparametric estimator of f(z|x) that adapts to sparse (low-dimensional) structure in x. By directly expanding f(z|x) in the eigenfunctions of a kernel-based operator, we avoid tensor products in high dimensions as well as ratios of estimated densities. Our basis functions are orthogonal with respect to the underlying data distribution, allowing fast implementation and tuning of parameters. We derive rates of convergence and show that the method adapts to the intrinsic dimension of the data. We also demonstrate the effectiveness of the series method on images, spectra, and an application to photometric redshift estimation of galaxies. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1512.07619",
                "title": "UNIFORMLY VALID POST-REGULARIZATION CONFIDENCE REGIONS FOR MANY FUNCTIONAL PARAMETERS IN Z-ESTIMATION FRAMEWORK.",
                "abstract": "In this paper, we develop procedures to construct simultaneous confidence bands for p \u02dc potentially infinite-dimensional parameters after model selection for general moment condition models where p \u02dc is potentially much larger than the sample size of available data, n. This allows us to cover settings with functional response data where each of the p \u02dc parameters is a function. The procedure is based on the construction of score functions that satisfy Neyman orthogonality condition approximately. The proposed simultaneous confidence bands rely on uniform central limit theorems for high-dimensional vectors (and not on Donsker arguments as we allow for p \u02dc \u226b n ). To construct the bands, we employ a multiplier bootstrap procedure which is computationally efficient as it only involves resampling the estimated score functions (and does not require resolving the high-dimensional optimization problems). We formally apply the general theory to inference on regression coefficient process in the distribution regression model with a logistic link, where two implementations are analyzed in detail. Simulations and an application to real data are provided to help illustrate the applicability of the results."
            },
            {
                "arxivId": "1306.3171",
                "title": "Confidence intervals and hypothesis testing for high-dimensional regression",
                "abstract": "Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p- values for these models. \n \nWe consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. \n \nOur approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a high-throughput genomic data set about riboflavin production rate, made publicly available by Buhlmann et al. (2014)."
            },
            {
                "arxivId": "1303.0518",
                "title": "On asymptotically optimal confidence regions and tests for high-dimensional models",
                "abstract": "We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs."
            },
            {
                "arxivId": "1112.6363",
                "title": "Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications",
                "abstract": "The \u21131-penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted \u21131-penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted \u21131-penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results."
            },
            {
                "arxivId": "1110.2563",
                "title": "Confidence intervals for low dimensional parameters in high dimensional linear models",
                "abstract": "The purpose of this paper is to propose methodologies for statistical inference of low dimensional parameters with high dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broader context. The theoretical results that are presented provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite dimensional covariance matrices. These sufficient conditions allow the number of variables to exceed the sample size and the presence of many small non\u2010zero coefficients. Our methods and theory apply to interval estimation of a preconceived regression coefficient or contrast as well as simultaneous interval estimation of many regression coefficients. Moreover, the method proposed turns the regression data into an approximate Gaussian sequence of point estimators of individual regression coefficients, which can be used to select variables after proper thresholding. The simulation results that are presented demonstrate the accuracy of the coverage probability of the confidence intervals proposed as well as other desirable properties, strongly supporting the theoretical results."
            },
            {
                "arxivId": "1012.2098",
                "title": "Multinomial Inverse Regression for Text Analysis",
                "abstract": "Text data, including speeches, stories, and other document forms, are often connected to sentiment variables that are of interest for research in marketing, economics, and elsewhere. It is also very high dimensional and difficult to incorporate into statistical analyses. This article introduces a straightforward framework of sentiment-sufficient dimension reduction for text data. Multinomial inverse regression is introduced as a general tool for simplifying predictor sets that can be represented as draws from a multinomial distribution, and we show that logistic regression of phrase counts onto document annotations can be used to obtain low-dimensional document representations that are rich in sentiment information. To facilitate this modeling, a novel estimation technique is developed for multinomial logistic regression with very high-dimensional response. In particular, independent Laplace priors with unknown variance are assigned to each regression coefficient, and we detail an efficient routine for maximization of the joint posterior over coefficients and their prior scale. This \u201cgamma-lasso\u201d scheme yields stable and effective estimation for general high-dimensional logistic regression, and we argue that it will be superior to current methods in many settings. Guidelines for prior specification are provided, algorithm convergence is detailed, and estimator properties are outlined from the perspective of the literature on nonconcave likelihood penalization. Related work on sentiment analysis from statistics, econometrics, and machine learning is surveyed and connected. Finally, the methods are applied in two detailed examples and we provide out-of-sample prediction studies to illustrate their effectiveness."
            },
            {
                "arxivId": "1010.2731",
                "title": "A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers",
                "abstract": "High-dimensional statistical inference deals with models in which the the number of parameters p is comparable to or larger than the sample size n. Since it is usually impossible to obtain consistent procedures unless p/n \u2192 0, a line of recent work has studied models with various types of structure (e.g., sparse vectors; block-structured matrices; low-rank matrices; Markov assumptions). In such settings, a general approach to estimation is to solve a regularized convex program (known as a regularized M-estimator) which combines a loss function (measuring how well the model fits the data) with some regularization function that encourages the assumed structure. The goal of this paper is to provide a unified framework for establishing consistency and convergence rates for such regularized M-estimators under high-dimensional scaling. We state one main theorem and show how it can be used to re-derive several existing results, and also to obtain several new results on consistency and convergence rates. Our analysis also identifies two key properties of loss and regularization functions, referred to as restricted strong convexity and decomposability, that ensure the corresponding regularized M-estimators have fast convergence rates."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01360",
        "category": "econ",
        "title": "\"Digitwashing\": The Gap between Words and Deeds in Digital Transformation and Stock Price Crash Risk",
        "abstract": "The contrast between companies'\"fleshy\"promises and the\"skeletal\"performance in digital transformation may lead to a higher risk of stock price crash. This paper selects a sample of Shanghai and Shenzhen A-share listed companies from 2010 to 2021, empirically analyses the specific impact of the gap between words and deeds in digital transformation (GDT) on the stock price crash risk, and explores the possible causes of GDT. We found that GDT significantly increases the stock price crash risk, and this finding is still valid after a series of robustness tests. In a further study, a deeper examination of the causes of GDT reveals that firms' perceptions of economic policy uncertainty significantly increase GDT, and the effect is more pronounced in the sample of loss-making firms. At the same time, the results of the heterogeneity test suggest that investors are more tolerant of state-owned enterprises when they are in the GDT situation. Taken together, we provide a concrete bridge between the two measures of digital transformation - digital text frequency and digital technology share - and offer new insights to enhance capital market stability.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01361",
        "category": "econ",
        "title": "Bandit Profit-maximization for Targeted Marketing",
        "abstract": "We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs. Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only noisy evaluations of chosen points on the demand curves. We prove a regret upper bound of $\\widetilde{\\mathcal{O}}\\big(nT^{3/4}\\big)$ and a lower bound of $\\Omega\\big((nT)^{3/4}\\big)$ for monotonic demand curves, and a regret bound of $\\widetilde{\\Theta}\\big(nT^{2/3}\\big)$ for demands curves that are monotonic in price and concave in the ancillary variables.",
        "references": [
            {
                "arxivId": "cs/0408007",
                "title": "Online convex optimization in the bandit setting: gradient descent without a gradient",
                "abstract": "We study a general online convex optimization problem. We have a convex set <i>S</i> and an unknown sequence of cost functions <i>c</i><inf>1</inf>, <i>c</i><inf>2</inf>,..., and in each period, we choose a feasible point <i>x<inf>t</inf></i> in <i>S</i>, and learn the cost <i>c<inf>t</inf></i>(<i>x<inf>t</inf></i>). If the function <i>c<inf>t</inf></i> is also revealed after each period then, as Zinkevich shows in [25], gradient descent can be used on these functions to get regret bounds of <i>O</i>(\u221a<i>n</i>). That is, after <i>n</i> rounds, the total cost incurred will be <i>O</i>(\u221a<i>n</i>) more than the cost of the best single feasible decision chosen with the benefit of hindsight, min<inf><i>x</i></inf> \u03a3 <i>ct</i>(<i>x</i>).We extend this to the \"bandit\" setting, where, in each period, only the cost <i>c<inf>t</inf></i>(<i>x<inf>t</inf></i>) is revealed, and bound the expected regret as <i>O</i>(<i>n</i><sup>3/4</sup>).Our approach uses a simple approximation of the gradient that is computed from evaluating <i>c<inf>t</inf></i> at a single (random) point. We show that this biased estimate is sufficient to approximate gradient descent on the sequence of functions. In other words, it is possible to use gradient descent without seeing anything more than the value of the functions at a single point. The guarantees hold even in the most general case: online against an adaptive adversary.For the online linear optimization problem [15], algorithms with low regrets in the bandit setting have recently been given against oblivious [1] and adaptive adversaries [19]. In contrast to these algorithms, which distinguish between explicit <i>explore</i> and <i>exploit</i> periods, our algorithm can be interpreted as doing a small amount of exploration in each period."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01386",
        "category": "econ",
        "title": "Minimax-Regret Sample Selection in Randomized Experiments (preprint)",
        "abstract": "Randomized controlled trials (RCTs) are often run in settings with many subpopulations that may have differential benefits from the treatment being evaluated. We consider the problem of sample selection, i.e., whom to enroll in an RCT, such as to optimize welfare in a heterogeneous population. We formalize this problem within the minimax-regret framework, and derive optimal sample-selection schemes under a variety of conditions. We also highlight how different objectives and decisions can lead to notably different guidance regarding optimal sample allocation through a synthetic experiment leveraging historical COVID-19 trial data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01421",
        "category": "econ",
        "title": "Predicting the Unpredictable under Subjective Expected Utility",
        "abstract": "We consider a decision maker who is unaware of objects to be sampled and thus cannot form beliefs about the occurrence of particular objects. Ex ante she can form beliefs about the occurrence of novelty and the frequencies of yet to be known objects. Conditional on any sampled objects, she can also form beliefs about the next object being novel or being one of the previously sampled objects. We characterize behaviorally such beliefs under subjective expected utility. In doing so, we relate\"reverse\"Bayesianism, a central property in the literature on decision making under growing awareness, with exchangeable random partitions, the central property in the literature on the discovery of species problem and mutations in statistics, combinatorial probability theory, and population genetics. Partition exchangeable beliefs do not necessarily satisfy\"reverse\"Bayesianism. Yet, the most prominent models of exchangeable random partitions, the model by De Morgan (1838), the one parameter model of Ewens (1972), and the two parameter model of Pitman (1995) and Zabell (1997), do satisfy\"reverse\"Bayesianism.",
        "references": [
            {
                "arxivId": "1907.07019",
                "title": "Unforeseen Evidence",
                "abstract": null
            },
            {
                "arxivId": "2109.05145",
                "title": "Discovery and Equilibrium in Games with Unawareness",
                "abstract": "Equilibrium notions for games with unawareness in the literature cannot be interpreted as steady-states of a learning process because players may discover novel actions during play. In this sense, many games with unawareness are ``self-destroying'' as a player's representation of the game may change after playing it once. We define discovery processes where at each state there is an extensive-form game with unawareness that together with the players' play determines the transition to possibly another extensive-form game with unawareness in which players are now aware of actions that they have discovered. A discovery process is rationalizable if players play extensive-form rationalizable strategies in each game with unawareness. We show that for any game with unawareness there is a rationalizable discovery process that leads to a self-confirming game that possesses a self-confirming equilibrium in extensive-form rationalizable strategies. This notion of equilibrium can be interpreted as steady-state of both a discovery and learning process."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01442",
        "category": "econ",
        "title": "Optimistic and pessimistic approaches for cooperative games",
        "abstract": "Cooperative game theory aims to study how to divide a joint value created by a set of players. These games are often studied through the characteristic function form with transferable utility, which represents the value obtainable by each coalition. In the presence of externalities, there are many ways to define this value. Various models that account for different levels of player cooperation and the influence of external players on coalition value have been studied. Although there are different approaches, typically, the optimistic and pessimistic approaches provide sufficient insights into strategic interactions. This paper clarifies the interpretation of these approaches by providing a unified framework. We show that making sure that no coalition receives more than their (optimistic) upper bounds is always at least as difficult as guaranteeing their (pessimistic) lower bounds. We also show that if externalities are negative, providing these guarantees is always feasible. Then, we explore applications and show how our findings can be applied to derive results from the existing literature.",
        "references": [
            {
                "arxivId": "2207.07190",
                "title": "Queueing games with an endogenous number of machines",
                "abstract": "This paper studies queueing problems with an endogenous number of machines with and without an initial queue, the novelty being that coalitions not only choose how to queue, but also on how many machines. For a given problem, agents can (de)activate as many machines as they want, at a cost. After minimizing the total cost (processing costs and machine costs), we use a game theoretical approach to share to proceeds of this cooperation, and study the existence of stable allocations. First, we study queueing problems with an endogenous number of machines, and examine how to share the total cost. We provide an upper bound and a lower bound on the cost of a machine to guarantee the non-emptiness of the core (the set of stable allocations). Next, we study requeueing problems with an endogenous number of machines, where there is an existing queue. We examine how to share the cost savings compared to the initial situation, when optimally requeueing/changing the number of machines. Although, in general, stable allocation may not exist, we guarantee the existence of stable allocations when all machines are considered public goods, and we start with an initial schedule that might not have the optimal number of machines, but in which agents with large waiting costs are processed first."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01585",
        "category": "econ",
        "title": "Calibrating doubly-robust estimators with unbalanced treatment assignment",
        "abstract": "Machine learning methods, particularly the double machine learning (DML) estimator (Chernozhukov et al., 2018), are increasingly popular for the estimation of the average treatment effect (ATE). However, datasets often exhibit unbalanced treatment assignments where only a few observations are treated, leading to unstable propensity score estimations. We propose a simple extension of the DML estimator which undersamples data for propensity score modeling and calibrates scores to match the original distribution. The paper provides theoretical results showing that the estimator retains the DML estimator's asymptotic properties. A simulation study illustrates the finite sample performance of the estimator.",
        "references": [
            {
                "arxivId": "2402.04674",
                "title": "Hyperparameter Tuning for Causal Inference with Double Machine Learning: A Simulation Study",
                "abstract": "Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks. While there is an extensive literature on tuning ML learners for prediction, there is only little guidance available on tuning ML learners for causal machine learning and how to select among different ML learners. In this paper, we empirically assess the relationship between the predictive performance of ML methods and the resulting causal estimation based on the Double Machine Learning (DML) approach by Chernozhukov et al. (2018). DML relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge. We provide empirical insights on the role of hyperparameter tuning and other practical decisions for causal estimation with DML. First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning. Second, we investigate how the choice of ML methods and hyperparameters, including recent AutoML frameworks, impacts the estimation performance for a causal parameter of interest. Third, we assess to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics."
            },
            {
                "arxivId": "1908.08779",
                "title": "Nonparametric estimation of causal heterogeneity under high-dimensional confounding",
                "abstract": "This paper considers the practically important case of nonparametrically estimating heterogeneous average treatment effects that vary with a limited number of discrete and continuous covariates in a selection-on-observables framework where the number of possible confounders is very large. We propose a two-step estimator for which the first step is estimated by machine learning. We show that this estimator has desirable statistical properties like consistency, asymptotic normality and rate double robustness. In particular, we derive the coupled convergence conditions between the nonparametric and the machine learning steps. We also show that estimating population average treatment effects by averaging the estimated heterogeneous effects is semi-parametrically efficient. The new estimator is an empirical example of the effects of mothers' smoking during pregnancy on the resulting birth weight."
            },
            {
                "arxivId": "1908.02399",
                "title": "Estimation of Conditional Average Treatment Effects With High-Dimensional Data",
                "abstract": "Abstract Given the unconfoundedness assumption, we propose new nonparametric estimators for the reduced dimensional conditional average treatment effect (CATE) function. In the first stage, the nuisance functions necessary for identifying CATE are estimated by machine learning methods, allowing the number of covariates to be comparable to or larger than the sample size. The second stage consists of a low-dimensional local linear regression, reducing CATE to a function of the covariate(s) of interest. We consider two variants of the estimator depending on whether the nuisance functions are estimated over the full sample or over a hold-out sample. Building on Belloni at al. and Chernozhukov et al., we derive functional limit theory for the estimators and provide an easy-to-implement procedure for uniform inference based on the multiplier bootstrap. The empirical application revisits the effect of maternal smoking on a baby\u2019s birth weight as a function of the mother\u2019s age."
            },
            {
                "arxivId": "1712.04912",
                "title": "Quasi-oracle estimation of heterogeneous treatment effects",
                "abstract": "\n Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines."
            },
            {
                "arxivId": "1309.4686",
                "title": "Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations",
                "abstract": null
            },
            {
                "arxivId": "1201.0490",
                "title": "Scikit-learn: Machine Learning in Python",
                "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01593",
        "category": "econ",
        "title": "A Continuous-Time Stochastic Model of the Fiscal Theory of the Price Level and Consistency of Its Critique",
        "abstract": "The paper tests the validity of the critique of the fiscal theory of the price level. A stochastic general equilibrium model with continuous time is constructed. An active fiscal policy and a passive monetary policy have been set. Monetary policy manages the interest rate through the Taylor rule. The stochastic default factor in the special form is introduced. A complete definite system of equations is obtained for the detection of equilibrium. It is asserted that the peculiarities of the approach to modeling are of critical importance for verifying the presence of certain hypotheses and formulating conclusions. The results of this work are in support of the fiscal theory of the price level.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01745",
        "category": "econ",
        "title": "Uncovering the Sino-US dynamic risk spillovers effects: Evidence from agricultural futures markets",
        "abstract": "Agricultural products play a critical role in human development. With economic globalization and the financialization of agricultural products continuing to advance, the interconnections between different agricultural futures have become closer. We utilize a TVP-VAR-DY model combined with the quantile method to measure the risk spillover between 11 agricultural futures on the futures exchanges of US and China from July 9,2014, to December 31,2022. This study yielded several significant findings. Firstly, CBOT corn, soybean, and wheat were identified as the primary risk transmitters, with DCE corn and soybean as the main risk receivers. Secondly, sudden events or increased economic uncertainty can increase the overall risk spillovers. Thirdly, there is an aggregation of risk spillovers amongst agricultural futures based on the dynamic directional spillover results. Lastly, the central agricultural futures under the conditional mean are CBOT corn and soybean, while CZCE hard wheat and long-grained rice are the two risk spillover centers in extreme cases, as per the results of the spillover network and minimum spanning tree. Based on these results, decision-makers are advised to safeguard against the price risk of agricultural futures under sudden economic events, and investors can utilize the results to construct a superior investment portfolio by taking different agricultural product futures as risk-leading indicators according to various situations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01770",
        "category": "econ",
        "title": "Experimenting with Generative AI: Does ChatGPT Really Increase Everyone's Productivity?",
        "abstract": "Generative AI technologies such as ChatGPT, Gemini, and MidJourney have made remarkable progress in recent years. Recent literature has documented ChatGPT's positive impact on productivity in areas where it has strong expertise, attributable to extensive training datasets, such as the English language and Python/SQL programming. However, there is still limited literature regarding ChatGPT's performance in areas where its capabilities could still be further enhanced. This paper aims to fill this gap. We conducted an experiment in which economics students were asked to perform writing analysis tasks in a non-English language (specifically, Thai) and math&data analysis tasks using a less frequently used programming package (specifically, Stata). The findings suggest that, on average, participants performed better using ChatGPT in terms of scores and time taken to complete the tasks. However, a detailed examination reveals that 34% of participants saw no improvement in writing analysis tasks, and 42% did not improve in math&data analysis tasks when employing ChatGPT. Further investigation indicated that higher-ability students, as proxied by their econometrics grades, were the ones who performed worse in writing analysis tasks when using ChatGPT. We also found evidence that students with better digital skills performed better with ChatGPT. This research provides insights on the impact of generative AI. Thus, stakeholders can make informed decisions to implement appropriate policy frameworks or redesign educational systems. It also highlights the critical role of human skills in addressing and complementing the limitations of technology.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.01964",
        "category": "econ",
        "title": "The Heterogeneous Productivity Effects of Generative AI",
        "abstract": "We analyse the individual productivity effects of Italy's ban on ChatGPT, a generative pretrained transformer chatbot. We compile data on the daily coding output quantity and quality of over 36,000 GitHub users in Italy and other European countries and combine these data with the sudden announcement of the ban in a difference-in-differences framework. Among the affected users in Italy, we find a short-term increase in output quantity and quality for less experienced users and a decrease in productivity on more routine tasks for experienced users.",
        "references": [
            {
                "arxivId": "2401.16754",
                "title": "AI Oversight and Human Mistakes: Evidence from Centre Court",
                "abstract": "Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because of these costs, umpires cared twice as much about Type II errors under AI oversight."
            },
            {
                "arxivId": "2308.02312",
                "title": "Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of ChatGPT Answers to Stack Overflow Questions",
                "abstract": "Q&A platforms have been crucial for the online help-seeking behavior of programmers. However, the recent popularity of ChatGPT is altering this trend. Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT's answers to programming questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT answers to 517 programming questions on Stack Overflow and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT answers. Furthermore, we conducted a large-scale linguistic analysis, as well as a user study, to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52% of ChatGPT answers contain incorrect information and 77% are verbose. Nonetheless, our user study participants still preferred ChatGPT answers 35% of the time due to their comprehensiveness and well-articulated language style. However, they also overlooked the misinformation in the ChatGPT answers 39% of the time. This implies the need to counter misinformation in ChatGPT answers to programming questions and raise awareness of the risks associated with seemingly correct answers."
            },
            {
                "arxivId": "2307.07367",
                "title": "Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow",
                "abstract": "Large language models like ChatGPT efficiently provide users with information about various topics, presenting a potential substitute for searching the web and asking people for help online. But since users interact privately with the model, these models may drastically reduce the amount of publicly available human-generated data and knowledge resources. This substitution can present a significant problem in securing training data for future models. In this work, we investigate how the release of ChatGPT changed human-generated open data on the web by analyzing the activity on Stack Overflow, the leading online Q\\&A platform for computer programming. We find that relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable, activity on Stack Overflow significantly decreased. A difference-in-differences model estimates a 16\\% decrease in weekly posts on Stack Overflow. This effect increases in magnitude over time, and is larger for posts related to the most widely used programming languages. Posts made after ChatGPT get similar voting scores than before, suggesting that ChatGPT is not merely displacing duplicate or low-quality content. These results suggest that more users are adopting large language models to answer questions and they are better substitutes for Stack Overflow for languages for which they have more training data. Using models like ChatGPT may be more efficient for solving certain programming problems, but its widespread adoption and the resulting shift away from public exchange on the web will limit the open data people and models can learn from in the future."
            },
            {
                "arxivId": "2302.06590",
                "title": "The Impact of AI on Developer Productivity: Evidence from GitHub Copilot",
                "abstract": "Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-05.json",
        "arxivId": "2403.02144",
        "category": "econ",
        "title": "Improved tests for mediation",
        "abstract": "Testing for a mediation effect is important in many disciplines, but is made difficult - even asymptotically - by the influence of nuisance parameters. Classical tests such as likelihood ratio (LR) and Wald (Sobel) tests have very poor power properties in parts of the parameter space, and many attempts have been made to produce improved tests, with limited success. In this paper we show that augmenting the critical region of the LR test can produce a test with much improved behavior everywhere. In fact, we first show that there exists a test of this type that is (asymptotically) exact for certain test levels $\\alpha $, including the common choices $\\alpha =.01,.05,.10.$ The critical region of this exact test has some undesirable properties. We go on to show that there is a very simple class of augmented LR critical regions which provides tests that are nearly exact, and avoid the issues inherent in the exact test. We suggest an optimal and coherent member of this class, provide the table needed to implement the test and to report p-values if desired. Simulation confirms validity with non-Gaussian disturbances, under heteroskedasticity, and in a nonlinear (logit) model. A short application of the method to an entrepreneurial attitudes study is included for illustration.",
        "references": [
            {
                "arxivId": "2012.11342",
                "title": "A Nearly Similar Powerful Test for Mediation",
                "abstract": "This paper derives a new powerful test for mediation that is easy to use. Testing for mediation is empirically very important in psychology, sociology, medicine, economics and business, generating over 100,000 citations to a single key paper. The no-mediation hypothesis H0 : \u03b81\u03b82 = 0 also poses a theoretically interesting statistical problem since it defines a manifold that is non-regular in the origin where rejection probabilities of standard tests are extremely low. We prove that a similar test for mediation only exists if the size is the reciprocal of an integer. It is unique, but has objectionable properties. We propose a new test that is nearly similar with power close to the envelope without these abject properties and is easy to use in practice. Construction uses the general varying g-method that we propose. We illustrate the results in an educational setting with gender role beliefs and in a trade union sentiment application."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2107.13737",
        "category": "econ",
        "title": "Design-Robust Two-Way-Fixed-Effects Regression For Panel Data",
        "abstract": "We propose a new estimator for average causal effects of a binary treatment with panel data in settings with general treatment patterns. Our approach augments the popular two-way-fixed-effects specification with unit-specific weights that arise from a model for the assignment mechanism. We show how to construct these weights in various settings, including the staggered adoption setting, where units opt into the treatment sequentially but permanently. The resulting estimator converges to an average (over units and time) treatment effect under the correct specification of the assignment model, even if the fixed effect model is misspecified. We show that our estimator is more robust than the conventional two-way estimator: it remains consistent if either the assignment mechanism or the two-way regression model is correctly specified. In addition, the proposed estimator performs better than the two-way-fixed-effect estimator if the outcome model and assignment mechanism are locally misspecified. This strong double robustness property underlines and quantifies the benefits of modeling the assignment process and motivates using our estimator in practice. We also discuss an extension of our estimator to handle dynamic treatment effects.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2203.01483",
        "category": "econ",
        "title": "Constitutional Implementation of Affirmative Action Policies in India",
        "abstract": "India is home to a comprehensive affirmative action program that reserves a fraction of positions at governmental institutions for various disadvantaged groups. While there is a Supreme Court-endorsed mechanism to implement these reservation policies when all positions are identical, courts have refrained from endorsing explicit mechanisms when positions are heterogeneous. This lacunae has resulted in widespread adoption of unconstitutional mechanisms, countless lawsuits, and inconsistent court rulings. Formulating mandates in the landmark Supreme Court judgment Saurav Yadav (2020) as technical axioms, we show that the 2SMH-DA mechanism is uniquely suited to overcome these challenges.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2204.05062",
        "category": "econ",
        "title": "On Locally Rationalizable Social Choice Functions",
        "abstract": "We consider a notion of rationalizability, where the rationalizing relation may depend on the set of feasible alternatives. More precisely, we say that a choice function is locally rationalizable if it is rationalized by a family of rationalizing relations such that a strict preference between two alternatives in some feasible set is preserved when removing other alternatives. Tyson (2008) has shown that a choice function is locally rationalizable if and only if it satisfies Sen's $\\gamma$. We expand the theory of local rationalizability by proposing a natural strengthening of $\\gamma$ that precisely characterizes local rationalizability via PIP-transitive relations and by introducing the $\\gamma$-hull of a choice function as its finest coarsening that satisfies $\\gamma$. Local rationalizability permits a unified perspective on social choice functions that satisfy $\\gamma$, including classic ones such as the top cycle and the uncovered set as well as new ones such as two-stage majoritarian choice and split cycle. We give simple axiomatic characterizations of some of these using local rationalizability and propose systematic procedures to define social choice functions that satisfy $\\gamma$.",
        "references": [
            {
                "arxivId": "2008.08451",
                "title": "Axioms for defeat in democratic elections",
                "abstract": "We propose six axioms concerning when one candidate should defeat another in a democratic election involving two or more candidates. Five of the axioms are widely satisfied by known voting procedures. The sixth axiom is a weakening of Kenneth Arrow\u2019s famous condition of the Independence of Irrelevant Alternatives (IIA). We call this weakening Coherent IIA. We prove that the five axioms plus Coherent IIA single out a method of determining defeats studied in our recent work: Split Cycle. In particular, Split Cycle provides the most resolute definition of defeat among any satisfying the six axioms for democratic defeat. In addition, we analyze how Split Cycle escapes Arrow\u2019s impossibility theorem and related impossibility results."
            },
            {
                "arxivId": "2004.02350",
                "title": "Split Cycle: a new Condorcet-consistent voting method independent of clones and immune to spoilers",
                "abstract": null
            },
            {
                "arxivId": "0803.2138",
                "title": "Minimal stable sets in tournaments",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2211.01116",
        "category": "econ",
        "title": "Medical Bill Shock and Imperfect Moral Hazard",
        "abstract": "Consumers are sensitive to medical prices when consuming care, but delays in price information may distort moral hazard. We study how medical bills affect household spillover spending following utilization, leveraging variation in insurer claim processing times. Households increase spending by 22\\% after a scheduled service, but then reduce spending by 11\\% after the bill arrives. Observed bill effects are consistent with resolving price uncertainty; bill effects are strongest when pricing information is particularly salient. A model of demand for healthcare with delayed pricing information suggests households misperceive pricing signals prior to bills, and that correcting these perceptions reduce average (median) spending by 16\\% (7\\%) annually.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2302.13426",
        "category": "econ",
        "title": "Arrow-Debreu Meets Kyle: Price Discovery for Derivatives",
        "abstract": "We analyze price discovery in a model where an agent has general private information regarding state probabilities and trades state-contingent claims. Our model unifies the elements of Arrow and Debreu (1954) and Kyle (1985). In the equivalent options formulation, the informed agent has general information regarding an underlying asset's payoff distribution and trades option portfolios. We characterize the informed demand, the price impact, and the information efficiency of prices. Our informed demand formula prescribes option strategies for trading on any given aspect of the underlying payoff distribution and explains those used in practice -- e.g., for volatility trading.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2305.05822",
        "category": "econ",
        "title": "Robust Regulation of Firms' Access to Consumer Data",
        "abstract": "In this paper I study how to regulate firms' access to consumer data when the latter is used for price discrimination and the regulator faces non-Bayesian uncertainty about the correlation structure between data and willingness to pay, and hence about the way the monopolist will use the consumers' information to segment the market. I fully characterize all policies that are worst-case optimal when the regulator maximizes consumer surplus: the regulator allows the monopolist to access data if the monopolist cannot use the database to identify a small group of consumers. Furthermore, from the set of policies that achieve the largest worst-case consumer surplus, I identify the ones that are undominated; i.e., there is no alternative policy that never yields lower consumer surplus, and sometimes strictly higher consumer surplus.",
        "references": [
            {
                "arxivId": "2205.03495",
                "title": "Credible Persuasion",
                "abstract": "We propose a new notion of credibility for Bayesian persuasion problems. A disclosure policy is credible if the Sender cannot profit from tampering with her messages while keeping the message distribution unchanged. Using optimal transport theory, we show that for every profile of Sender's disclosure policy and Receiver's strategy, the credibility of the profile is equivalent to a cyclical monotonicity condition on its induced distribution over states and actions. We provide conditions on when credibility considerations completely shut down informative communication, as well as settings where the Sender is guaranteed to benefit from credible persuasion. We apply our results to a classic setting of asymmetric information-the market for lemons. It is well-known that market outcome in such a setting may be inefficient due to adverse selection (Akerlof, 1970): despite common knowledge of gain from trade, some cars may not be traded. If the seller can commit to a disclosure policy to persuade the buyers, she can completely solve the market inefficiency by perfectly revealing 0 to the buyers. However, we show that if the buyers can only observe the message distribution of the seller's disclosure policy, but not exactly how these messages are generated, then the seller cannot credibly disclose any useful information to the buyer. Our paper offers foundations for studying Bayesian persuasion in a number of settings. One example is when the Sender's payoff is state-independent: in these cases, our results imply that all disclosure policies are credible, so the full-commitment assumption in the Bayesian persuasion approach is nonessential as long as the message distribution is observable. Another example is when the Sender's payoff is supermodular, in which case all monotone disclosure policies are credible."
            },
            {
                "arxivId": "1912.04774",
                "title": "Voluntary Disclosure and Personalized Pricing",
                "abstract": "A concern central to the economics of privacy is that firms may use consumer data to price discriminate. A common response is that consumers should have control over their data and the ability to choose how firms access it. Since firms draw inferences based on both the data seen as well as the consumer's disclosure choices, the strategic implications of this proposal are unclear. We investigate whether such measures improve consumer welfare in monopolistic and competitive environments. We find that consumer control can guarantee gains for every consumer type relative to both perfect price discrimination and no personalized pricing. This result is driven by two ideas. First, consumers can use disclosure to amplify competition between firms. Second, consumers can share information that induces a seller---even a monopolist---to make price concessions. Furthermore, whether consumer control improves consumer surplus depends on both the technology of disclosure and the competitiveness of the marketplace. In a competitive market, simple disclosure technologies such as \"track / do-not-track'' suffice for guaranteeing gains in consumer welfare. However, in a monopolistic market, welfare gains require richer forms of disclosure technology whereby consumers can decide how much information they would like to convey."
            },
            {
                "arxivId": "1910.04260",
                "title": "Robust Monopoly Regulation",
                "abstract": "We study the regulation of a monopolistic firm using a robust-design approach. We solve for the policy that minimizes the regulator's worst-case regret, where the regret is the difference between his complete-information payoff minus his realized payoff. When the regulator's payoff is consumers' surplus, it is optimal to impose a price cap. The optimal cap balances the benefit from more surplus for consumers and the loss from underproduction. When his payoff is consumers' surplus plus the firm's profit, he offers a piece-rate subsidy in order to mitigate underproduction, but caps the total subsidy so as not to incentivize severe overproduction."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2401.14435",
        "category": "econ",
        "title": "Islamic Law, Western European Law and the Roots of Middle East\u2019s Long Divergence: a Comparative Empirical Investigation (800-1600)",
        "abstract": "We examine the contribution of Islamic legal institutions to the comparative economic decline of the Middle East behind Latin Europe, which can be observed since the late Middle Ages. To this end, we explore whether the sacralization of Islamic law and its focus on the Sharia as supreme, sacred and unchangeable legal text, which reached its culmination in the 13th century had an impact on economic development. We use the population size of 145 cities in Islamic countries and 648 European cities for the period 800-1800 as proxies for the level of economic development, and construct novel estimates of the number of law schools (i.e. madaris) and estimate their contribution to the pre-industrial economic development. Our triple-differences estimates show that a higher density of madrasas before the sacralization of Islamic law predicts a more vibrant urban economy characterized by higher urban growth. After the consolidation of the sharia sacralization of law in the 13th century, greater density of law schools is associated with stagnating population size. We show that the economic decline of the Middle East can be partly explained by the absence of legal innovations or substitutes of them, which paved the way for the economic rise of Latin Europe, where ground-breaking legal reforms introduced a series of legal innovations conducive for economic growth. We find that the number of learned lawyers trained in universities with law schools is highly and positively correlated with the western European city population. Our counterfactual estimates show that almost all Islamic cities under consideration would have had much larger size by the year 1700 if legal innovations comparable to those in Western Europe were introduced. By making use of a series of synthetic control and difference-in-differences estimators our findings are robust against a large number of model specification checks.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2403.02467",
        "category": "econ",
        "title": "Applied Causal Inference Powered by ML and AI",
        "abstract": "An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2403.02560",
        "category": "econ",
        "title": "Impact of COVID-19 on Exchange rate volatility of Bangladesh: Evidence through GARCH model (preprint)",
        "abstract": "This study uses the GARCH (1,1) model to examine the impact of COVID-19 cases (log value) on the volatility of the Exchange rate return of Bangladeshi taka (BDT) over the US dollar (USD), Japanese Yen (JPY), and Swedish Krona (SEK). The result shows that an increase in the number of COVID-19-affected cases in Bangladesh has a significant and positive impact on the volatility of exchange rates BDT/USD, BDT/JPY, and BDT/SEK.",
        "references": [
            {
                "arxivId": "2003.04005",
                "title": "Coronavirus and Financial Volatility: 40 Days of Fasting and Fear",
                "abstract": "40 days after the start of the international monitoring of COVID-19, we search for the effect of official announcements regarding new cases of infection and death ratio on the financial markets volatility index (VIX). Whereas the new cases reported in China and outside China have a mixed effect on financial volatility, the death ratio positively influences VIX, that outside China triggering a more important impact. In addition, the higher the number of affected countries, the higher the financial volatility is."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2403.02591",
        "category": "econ",
        "title": "Matrix-based Prediction Approach for Intraday Instantaneous Volatility Vector",
        "abstract": "In this paper, we introduce a novel method for predicting intraday instantaneous volatility based on Ito semimartingale models using high-frequency financial data. Several studies have highlighted stylized volatility time series features, such as interday auto-regressive dynamics and the intraday U-shaped pattern. To accommodate these volatility features, we propose an interday-by-intraday instantaneous volatility matrix process that can be decomposed into low-rank conditional expected instantaneous volatility and noise matrices. To predict the low-rank conditional expected instantaneous volatility matrix, we propose the Two-sIde Projected-PCA (TIP-PCA) procedure. We establish asymptotic properties of the proposed estimators and conduct a simulation study to assess the finite sample performance of the proposed prediction method. Finally, we apply the TIP-PCA method to an out-of-sample instantaneous volatility vector prediction study using high-frequency data from the S&P 500 index and 11 sector index funds.",
        "references": [
            {
                "arxivId": "1406.3836",
                "title": "Projected Principal Component Analysis in Factor Models",
                "abstract": "This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employees principal component analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates. When it applies to high-dimensional factor analysis, the projection removes noise components. We show that the unobserved latent factors can be more accurately estimated than the conventional PCA if the projection is genuine, or more precisely, when the factor loading matrices are related to the projected linear space. When the dimensionality is large, the factors can be estimated accurately even when the sample size is finite. We propose a flexible semi-parametric factor model, which decomposes the factor loading matrix into the component that can be explained by subject-specific covariates and the orthogonal residual component. The covariates' effects on the factor loadings are further modeled by the additive model via sieve approximations. By using the newly proposed Projected-PCA, the rates of convergence of the smooth factor loading matrices are obtained, which are much faster than those of the conventional factor analysis. The convergence is achieved even when the sample size is finite and is particularly appealing in the high-dimension-low-sample-size situation. This leads us to developing nonparametric tests on whether observed covariates have explaining powers on the loadings and whether they fully explain the loadings. The proposed method is illustrated by both simulated data and the returns of the components of the S&P 500 index."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-06.json",
        "arxivId": "2403.02726",
        "category": "econ",
        "title": "Bias in Generative AI",
        "abstract": "This study analyzed images generated by three popular generative artificial intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators. Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances. Firstly, we found that all three AI generators exhibited bias against women and African Americans. Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society. Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances. For example, women were depicted as younger with more smiles and happiness, while men were depicted as older with more neutral expressions and anger, posing a risk that generative AI models may unintentionally depict women as more submissive and less competent than men. Such nuanced biases, by their less overt nature, might be more problematic as they can permeate perceptions unconsciously and may be more difficult to rectify. Although the extent of bias varied depending on the model, the direction of bias remained consistent in both commercial and open-source AI generators. As these tools become commonplace, our study highlights the urgency to identify and mitigate various biases in generative AI, reinforcing the commitment to ensuring that AI technologies benefit all of humanity for a more inclusive future.",
        "references": [
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2208.05316",
        "category": "econ",
        "title": "Welfare ordering of voting weight allocations",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2303.05968",
        "category": "econ",
        "title": "A general impossibility theorem on Pareto efficiency and Bayesian incentive compatibility",
        "abstract": null,
        "references": [
            {
                "arxivId": "2206.09574",
                "title": "The Winner-Take-All Dilemma",
                "abstract": "We consider collective decision\u2010making when society consists of groups endowed with voting weights. Each group chooses an internal rule that specifies the allocation of its weight to alternatives as a function of its members' preferences. Under fairly general conditions, we show that the winner\u2010take\u2010all rule is a dominant strategy, while the equilibrium is Pareto dominated, highlighting the dilemma structure between optimality for each group and for the whole society. We also develop a technique for asymptotic analysis and show Pareto dominance of the proportional rule."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2308.10131",
        "category": "econ",
        "title": "Agree to Disagree: Measuring Hidden Dissents in FOMC Meetings",
        "abstract": "Based on a record of dissents on FOMC votes and transcripts of the meetings from 1976 to 2017, we develop a deep learning model based on self-attention mechanism to create a measure of disagreement for members in each meeting. While dissents are rare, we find that members often have reservations with the policy decision, and the level of disagreement is mostly driven by current or predicted macroeconomic data. Using our model to evaluate speeches made by members between meetings, we find that the informational content of speeches is low if we can only compare them to speeches made by the chair. Disagreement strongly correlates with data from the Summary of Economic Projections and a measure of monetary policy sub-optimality, suggesting that disagreement is driven by both members' different preferences and their different views about the future.",
        "references": [
            {
                "arxivId": "1601.06733",
                "title": "Long Short-Term Memory-Networks for Machine Reading",
                "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2312.01209",
        "category": "econ",
        "title": "A Method of Moments Approach to Asymptotically Unbiased Synthetic Controls",
        "abstract": "A common approach to constructing a Synthetic Control unit is to fit on the outcome variable and covariates in pre-treatment time periods, but it has been shown by Ferman and Pinto (2019) that this approach does not provide asymptotic unbiasedness when the fit is imperfect and the number of controls is fixed. Many related panel methods have a similar limitation when the number of units is fixed. I introduce and evaluate a new method in which the Synthetic Control is constructed using a General Method of Moments approach where units not being included in the Synthetic Control are used as instruments. I show that a Synthetic Control Estimator of this form will be asymptotically unbiased as the number of pre-treatment time periods goes to infinity, even when pre-treatment fit is imperfect and the number of units is fixed. Furthermore, if both the number of pre-treatment and post-treatment time periods go to infinity, then averages of treatment effects can be consistently estimated. I conduct simulations and an empirical application to compare the performance of this method with existing approaches in the literature.",
        "references": [
            {
                "arxivId": "1906.06665",
                "title": "On the Properties of the Synthetic Control Estimator with Many Periods and Many Controls",
                "abstract": "Abstract We consider the asymptotic properties of the synthetic control (SC) estimator when both the number of pretreatment periods and control units are large. If potential outcomes follow a linear factor model, we provide conditions under which the SC unit asymptotically recovers the factor structure of the treated unit, even when the pretreatment fit is imperfect. This happens when there are weights diluted among an increasing number of control units such that a weighted average of the factor structure of the control units asymptotically reconstructs the factor structure of the treated unit. In this case, the SC estimator is asymptotically unbiased even when treatment assignment is correlated with time-varying unobservables. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1712.09089",
                "title": "An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls",
                "abstract": "Abstract We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available."
            },
            {
                "arxivId": "1911.08521",
                "title": "Synthetic controls with imperfect pretreatment fit",
                "abstract": "We analyze the properties of the Synthetic Control (SC) and related estimators when the pre\u2010treatment fit is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre\u2010treatment periods goes to infinity. Still, we show that a demeaned version of the SC method can improve in terms of bias and variance relative to the difference\u2010in\u2010difference estimator. We also derive a specification test for the demeaned SC estimator in this setting with imperfect pre\u2010treatment fit. Given our theoretical results, we provide practical guidance for applied researchers on how to justify the use of such estimators in empirical applications."
            },
            {
                "arxivId": "1610.07748",
                "title": "Balancing, Regression, Difference-in-Differences and Synthetic Control Methods: A Synthesis",
                "abstract": "In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units."
            },
            {
                "arxivId": "1512.01310",
                "title": "A Central Limit Theorem for Non-stationary Strongly Mixing Random Fields",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2402.16309",
        "category": "econ",
        "title": "Collective Evaluation Problem",
        "abstract": "This study focuses on situations where a finite set of alternatives is evaluated by collecting evaluations from several individuals, some of whom may not evaluate specific alternatives. The collection of subsets of alternatives that individuals (can) evaluate is referred to as an evaluability profile. For a given evaluability profile, we define a collective evaluation function whose inputs are the evaluation orders of individuals on the subsets of alternatives that they evaluate. We investigate the properties of collective evaluation functions, which are modifications of those introduced in previous studies. We identify the necessary and sufficient conditions on the evaluability profile that ensure the existence of collective evaluation functions satisfying four different combinations of these properties.",
        "references": [
            {
                "arxivId": "2305.00641",
                "title": "On extensions of partial priorities in school choice",
                "abstract": "We consider a school choice matching model where the priorities for schools are represented by binary relations that may not be weak order. We focus on the (total order) extensions of the binary relations. We introduce a class of algorithms to derive one of the extensions of a binary relation and characterize them by using the class. We show that if the binary relations are the partial orders, then for each stable matching for the profile of the binary relations, there is an extension for which it is also stable. Moreover, if there are multiple stable matchings for the profile of the binary relations that are ranked by Pareto dominance, there is an extension for which all of those matchings are stable. We provide several applications of these results."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03240",
        "category": "econ",
        "title": "Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects",
        "abstract": "This study investigates the estimation and the statistical inference about Conditional Average Treatment Effects (CATEs), which have garnered attention as a metric representing individualized causal effects. In our data-generating process, we assume linear models for the outcomes associated with binary treatments and define the CATE as a difference between the expected outcomes of these linear models. This study allows the linear models to be high-dimensional, and our interest lies in consistent estimation and statistical inference for the CATE. In high-dimensional linear regression, one typical approach is to assume sparsity. However, in our study, we do not assume sparsity directly. Instead, we consider sparsity only in the difference of the linear models. We first use a doubly robust estimator to approximate this difference and then regress the difference on covariates with Lasso regularization. Although this regression estimator is consistent for the CATE, we further reduce the bias using the techniques in double/debiased machine learning (DML) and debiased Lasso, leading to $\\sqrt{n}$-consistency and confidence intervals. We refer to the debiased estimator as the triple/debiased Lasso (TDL), applying both DML and debiased Lasso techniques. We confirm the soundness of our proposed method through simulation studies.",
        "references": [
            {
                "arxivId": "2403.02467",
                "title": "Applied Causal Inference Powered by ML and AI",
                "abstract": "An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools."
            },
            {
                "arxivId": "2310.16819",
                "title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression",
                "abstract": "In causal inference about two treatments, Conditional Average Treatment Effects (CATEs) play an important role as a quantity representing an individualized causal effect, defined as a difference between the expected outcomes of the two treatments conditioned on covariates. This study assumes two linear regression models between a potential outcome and covariates of the two treatments and defines CATEs as a difference between the linear regression models. Then, we propose a method for consistently estimating CATEs even under high-dimensional and non-sparse parameters. In our study, we demonstrate that desirable theoretical properties, such as consistency, remain attainable even without assuming sparsity explicitly if we assume a weaker assumption called implicit sparsity originating from the definition of CATEs. In this assumption, we suppose that parameters of linear models in potential outcomes can be divided into treatment-specific and common parameters, where the treatment-specific parameters take difference values between each linear regression model, while the common parameters remain identical. Thus, in a difference between two linear regression models, the common parameters disappear, leaving only differences in the treatment-specific parameters. Consequently, the non-zero parameters in CATEs correspond to the differences in the treatment-specific parameters. Leveraging this assumption, we develop a Lasso regression method specialized for CATE estimation and present that the estimator is consistent. Finally, we confirm the soundness of the proposed method by simulation studies."
            },
            {
                "arxivId": "2310.16945",
                "title": "Causal Q-Aggregation for CATE Model Selection",
                "abstract": "Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\\frac{\\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error terms related to products of errors in the nuisance functions. Crucially, our regret rate does not require that any of the candidate CATE models be close to the truth. We validate our new method on many semi-synthetic datasets and also provide extensions of our work to CATE model selection with instrumental variables and unobserved confounding."
            },
            {
                "arxivId": "2304.07726",
                "title": "Bayesian Causal Synthesis for Supra-Inference on Heterogeneous Treatment Effects",
                "abstract": "We propose a novel Bayesian methodology to mitigate misspecification and improve estimating treatment effects. A plethora of methods to estimate -- particularly the heterogeneous -- treatment effect have been proposed with varying success. It is recognized, however, that the underlying data generating mechanism, or even the model specification, can drastically affect the performance of each method, without any way to compare its performance in real world applications. Using a foundational Bayesian framework, we develop Bayesian causal synthesis; a supra-inference method that synthesizes several causal estimates to improve inference. We provide a fast posterior computation algorithm and show that the proposed method provides consistent estimates of the heterogeneous treatment effect. Several simulations and an empirical study highlight the efficacy of the proposed approach compared to existing methodologies, providing improved point and density estimation of the heterogeneous treatment effect."
            },
            {
                "arxivId": "2204.10969",
                "title": "Combining Doubly Robust Methods and Machine Learning for Estimating Average Treatment Effects for Observational Real-world Data",
                "abstract": "Observational cohort studies are increasingly being used for comparative effectiveness research to assess the safety of therapeutics. Recently, various doubly robust methods have been proposed for average treatment effect estimation by combining the treatment model and the outcome model via different vehicles, such as matching, weighting, and regression. The key advantage of doubly robust estimators is that they require either the treatment model or the outcome model to be correctly specified to obtain a consistent estimator of average treatment effects, and therefore lead to a more accurate and often more precise inference. However, little work has been done to understand how doubly robust estimators differ due to their unique strategies of using the treatment and outcome models and how machine learning techniques can be combined to boost their performance. Here we examine multiple popular doubly robust methods and compare their performance using different treatment and outcome modeling via extensive simulations and a real-world application. We found that incorporating machine learning with doubly robust estimators such as the targeted maximum likelihood estimator gives the best overall performance. Practical guidance on how to apply doubly robust estimators is provided."
            },
            {
                "arxivId": "2111.12161",
                "title": "Sensitivity analysis of individual treatment effects: A robust conformal inference approach",
                "abstract": "Significance The individual treatment effect (ITE) describes the difference between an individual\u2019s outcome when receiving a treatment versus not. This difference may vary across individuals conditional on their characteristics. In observational studies, inference for ITEs can be invalid if one ignores unmeasured confounding factors that simultaneously influence the treatment assignment and the outcomes. We propose a framework to quantitatively understand the robustness of causal conclusions on ITEs against such potential confounding factors. This yields prediction bands, which come with rigorous uncertainty quantification tools. These tools apply regardless of the machine learning model employed to learn the treatment effect, however complicated, and regardless of the sample size."
            },
            {
                "arxivId": "2106.03765",
                "title": "On Inductive Biases for Heterogeneous Treatment Effect Estimation",
                "abstract": "We investigate how to exploit structural similarities of an individual's potential outcomes (POs) under different treatments to obtain better estimates of conditional average treatment effects in finite samples. Especially when it is unknown whether a treatment has an effect at all, it is natural to hypothesize that the POs are similar - yet, some existing strategies for treatment effect estimation employ regularization schemes that implicitly encourage heterogeneity even when it does not exist and fail to fully make use of shared structure. In this paper, we investigate and compare three end-to-end learning strategies to overcome this problem - based on regularization, reparametrization and a flexible multi-task architecture - each encoding inductive bias favoring shared behavior across POs. To build understanding of their relative strengths, we implement all strategies using neural networks and conduct a wide range of semi-synthetic experiments. We observe that all three approaches can lead to substantial improvements upon numerous baselines and gain insight into performance differences across various experimental settings."
            },
            {
                "arxivId": "2101.10943",
                "title": "Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms",
                "abstract": "The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed meta-learning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes."
            },
            {
                "arxivId": "2009.14286",
                "title": "Benign overfitting in ridge regression",
                "abstract": "Classical learning theory suggests that strong regularization is needed to learn a class with large complexity. This intuition is in contrast with the modern practice of machine learning, in particular learning neural networks, where the number of parameters often exceeds the number of data points. It has been observed empirically that such overparametrized models can show good generalization performance even if trained with vanishing or negative regularization. The aim of this work is to understand theoretically how this effect can occur, by studying the setting of ridge regression. We provide non-asymptotic generalization bounds for overparametrized ridge regression that depend on the arbitrary covariance structure of the data, and show that those bounds are tight for a range of regularization parameter values. To our knowledge this is the first work that studies overparametrized ridge regression in such a general setting. We identify when small or negative regularization is sufficient for obtaining small generalization error. On the technical side, our bounds only require the data vectors to be i.i.d. sub-gaussian, while most previous work assumes independence of the components of those vectors."
            },
            {
                "arxivId": "2108.02836",
                "title": "Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion)",
                "abstract": "Congratulations to Hahn, Murray and Carvalho on a nice contribution. The authors propose the Bayesian causal forest (BCF) model. It reduces the bias and improves frequentist coverage of Bayesian credible intervals of treatment effect estimates in the presence of strong confounding and treatment effect heterogeneity. The BCF model builds upon the popular and empirically proven prediction method, Bayesian Additive Regression Trees (BART) (Chipman et al., 2010). BCF reformulates the response surface model as the sum of two functions, one modeling the prognostic impact of the control variables and one representing the treatment effect. This formulation directly incorporates estimates of the propensity score (PS) which induces a covariate-dependent prior on the regression function and regularizes treatment effect heterogeneity separately from the prognostic effect of control variables. It is impressive that the proposed method, in many settings, has better bias reduction, more consistent 95% coverage probability and shorter uncertainty intervals compared to the vanilla BART, which boasts better performance in a host of modern causal inference studies, including Hill (2011), Wendling et al. (2018), Dorie et al. (2019) and Hu et al. (2020), to name a few."
            },
            {
                "arxivId": "2004.11810",
                "title": "A Tree-Based Semi-Varying Coefficient Model for the COM-Poisson Distribution",
                "abstract": "Abstract We propose a tree-based semi-varying coefficient model for the Conway\u2013Maxwell\u2013Poisson (CMP or COM-Poisson) distribution which is a two-parameter generalization of the Poisson distribution and is flexible enough to capture both under-dispersion and over-dispersion in count data. The advantage of tree-based methods is their scalability to high-dimensional data. We develop CMPMOB, an estimation procedure for a semi-varying coefficient model, using model-based recursive partitioning (MOB). The proposed framework is broader than the existing MOB framework as it allows node-invariant effects to be included in the model. To simplify the computational burden of the exhaustive search employed in the original MOB algorithm, a new split point estimation procedure is proposed by borrowing tools from change point estimation methodology. The proposed method uses only the estimated score functions without fitting models for each split point and, therefore, is computationally simpler. Since the tree-based methods only provide a piece-wise constant approximation to the underlying smooth function, we further propose the CMPBoost semi-varying coefficient model which uses the gradient boosting procedure for estimation. The usefulness of the proposed methods are illustrated using simulation studies and a real example from a bike sharing system in Washington, DC. Supplementary files for this article are available online."
            },
            {
                "arxivId": "1908.08526",
                "title": "Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes",
                "abstract": "Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of $q$-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness."
            },
            {
                "arxivId": "1908.02399",
                "title": "Estimation of Conditional Average Treatment Effects With High-Dimensional Data",
                "abstract": "Abstract Given the unconfoundedness assumption, we propose new nonparametric estimators for the reduced dimensional conditional average treatment effect (CATE) function. In the first stage, the nuisance functions necessary for identifying CATE are estimated by machine learning methods, allowing the number of covariates to be comparable to or larger than the sample size. The second stage consists of a low-dimensional local linear regression, reducing CATE to a function of the covariate(s) of interest. We consider two variants of the estimator depending on whether the nuisance functions are estimated over the full sample or over a hold-out sample. Building on Belloni at al. and Chernozhukov et al., we derive functional limit theory for the estimators and provide an easy-to-implement procedure for uniform inference based on the multiplier bootstrap. The empirical application revisits the effect of maternal smoking on a baby\u2019s birth weight as a function of the mother\u2019s age."
            },
            {
                "arxivId": "1306.3171",
                "title": "Confidence intervals and hypothesis testing for high-dimensional regression",
                "abstract": "Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p- values for these models. \n \nWe consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. \n \nOur approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a high-throughput genomic data set about riboflavin production rate, made publicly available by Buhlmann et al. (2014)."
            },
            {
                "arxivId": "0912.4045",
                "title": "Restricted Eigenvalue Conditions on Subgaussian Random Matrices",
                "abstract": "It is natural to ask: what kinds of matrices satisfy the Restr icted Eigenvalue (RE) condition? In this paper, we associate the RE condition (Bickel-Ritov-Tsybakov 09) with the complexity of a subset of the sphere in R p , where p is the dimensionality of the data, and show that a class of random matrices with independent rows, but not necessarily independent columns, satisfy the RE condition, when the sample size is above a certain lower bound. Here we explicitly introduce an additional covariance structure to the class of random matrices that we have known by now that satisfy the Restricted Isometry Property"
            },
            {
                "arxivId": "math/0608017",
                "title": "High-dimensional graphs and variable selection with the Lasso",
                "abstract": "The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03299",
        "category": "econ",
        "title": "Understanding and avoiding the\"weights of regression\": Heterogeneous effects, misspecification, and longstanding solutions",
        "abstract": "Researchers in many fields endeavor to estimate treatment effects by regressing outcome data (Y) on a treatment (D) and observed confounders (X). Even absent unobserved confounding, the regression coefficient on the treatment reports a weighted average of strata-specific treatment effects (Angrist, 1998). Where heterogeneous treatment effects cannot be ruled out, the resulting coefficient is thus not generally equal to the average treatment effect (ATE), and is unlikely to be the quantity of direct scientific or policy interest. The difference between the coefficient and the ATE has led researchers to propose various interpretational, bounding, and diagnostic aids (Humphreys, 2009; Aronow and Samii, 2016; Sloczynski, 2022; Chattopadhyay and Zubizarreta, 2023). We note that the linear regression of Y on D and X can be misspecified when the treatment effect is heterogeneous in X. The\"weights of regression\", for which we provide a new (more general) expression, simply characterize how the OLS coefficient will depart from the ATE under the misspecification resulting from unmodeled treatment effect heterogeneity. Consequently, a natural alternative to suffering these weights is to address the misspecification that gives rise to them. For investigators committed to linear approaches, we propose relying on the slightly weaker assumption that the potential outcomes are linear in X. Numerous well-known estimators are unbiased for the ATE under this assumption, namely regression-imputation/g-computation/T-learner, regression with an interaction of the treatment and covariates (Lin, 2013), and balancing weights. Any of these approaches avoid the apparent weighting problem of the misspecified linear regression, at an efficiency cost that will be small when there are few covariates relative to sample size. We demonstrate these lessons using simulations in observational and experimental settings.",
        "references": [
            {
                "arxivId": "0904.1990",
                "title": "Average and Quantile Effects in Nonseparable Panel Models",
                "abstract": "Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time-homogeneity conditions that are like \ufffdtime is randomly assigned\ufffd or \ufffdtime is an instrument.\ufffd Partial-identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial-identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of T, the number of time periods, is shown by deriving shrinkage rates for the identified set as T grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations."
            },
            {
                "arxivId": "0804.2958",
                "title": "Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data",
                "abstract": "When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03589",
        "category": "econ",
        "title": "Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices",
        "abstract": "This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than optimizing only the propensity score. Based on this idea, in each round of our experiment, the experimenter optimizes the covariate density and propensity score based on past observations. To design an adaptive experiment, we first derive the efficient covariate density and propensity score that minimizes the semiparametric efficiency bound, a lower bound for the asymptotic variance given a fixed covariate density and a fixed propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.",
        "references": [
            {
                "arxivId": "2311.18274",
                "title": "Semiparametric Efficient Inference in Adaptive Experiments",
                "abstract": "We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting the asymptotic variance. Empirical results demonstrate that our methods yield narrower confidence sequences than those previously developed in the literature while maintaining time-uniform error control."
            },
            {
                "arxivId": "2302.02988",
                "title": "Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds",
                "abstract": "We investigate the problem of fixed-budget best arm identification (BAI) for minimizing expected simple regret. In an adaptive experiment, a decision maker draws one of multiple treatment arms based on past observations and observes the outcome of the drawn arm. After the experiment, the decision maker recommends the treatment arm with the highest expected outcome. We evaluate the decision based on the expected simple regret, which is the difference between the expected outcomes of the best arm and the recommended arm. Due to inherent uncertainty, we evaluate the regret using the minimax criterion. First, we derive asymptotic lower bounds for the worst-case expected simple regret, which are characterized by the variances of potential outcomes (leading factor). Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder (HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in recommending the best arm. Our theoretical analysis shows that the TS-HIR strategy is asymptotically minimax optimal, meaning that the leading factor of its worst-case expected simple regret matches our derived worst-case lower bound. Additionally, we consider extensions of our method, such as the asymptotic optimality for the probability of misidentification. Finally, we validate the proposed method's effectiveness through simulations."
            },
            {
                "arxivId": "2110.15573",
                "title": "A/B/n Testing with Control in the Presence of Subpopulations",
                "abstract": "Motivated by A/B/n testing applications, we consider a finite set of distributions (called \\emph{arms}), one of which is treated as a \\emph{control}. We assume that the population is stratified into homogeneous subpopulations. At every time step, a subpopulation is sampled and an arm is chosen: the resulting observation is an independent draw from the arm conditioned on the subpopulation. The quality of each arm is assessed through a weighted combination of its subpopulation means. We propose a strategy for sequentially choosing one arm per time step so as to discover as fast as possible which arms, if any, have higher weighted expectation than the control. This strategy is shown to be asymptotically optimal in the following sense: if $\\tau_\\delta$ is the first time when the strategy ensures that it is able to output the correct answer with probability at least $1-\\delta$, then $\\mathbb{E}[\\tau_\\delta]$ grows linearly with $\\log(1/\\delta)$ at the exact optimal rate. This rate is identified in the paper in three different settings: (1) when the experimenter does not observe the subpopulation information, (2) when the subpopulation of each sample is observed but not chosen, and (3) when the experimenter can select the subpopulation from which each response is sampled. We illustrate the efficiency of the proposed strategy with numerical simulations on synthetic and real data collected from an A/B/n experiment."
            },
            {
                "arxivId": "2002.11642",
                "title": "Off-Policy Evaluation and Learning for External Validity under a Covariate Shift",
                "abstract": "We consider the evaluation and training of a new policy for the evaluation data by using the historical data obtained from a different policy. The goal of off-policy evaluation (OPE) is to estimate the expected reward of a new policy over the evaluation data, and that of off-policy learning (OPL) is to find a new policy that maximizes the expected reward over the evaluation data. Although the standard OPE and OPL assume the same distribution of covariate between the historical and evaluation data, there often exists a problem of a covariate shift, i.e., the distribution of the covariate of the historical data is different from that of the evaluation data. In this paper, we derive the efficiency bound of OPE under a covariate shift. Then, we propose doubly robust and efficient estimators for OPE and OPL under a covariate shift by using an estimator of the density ratio between the distributions of the historical and evaluation data. We also discuss other possible estimators and compare their theoretical properties. Finally, we confirm the effectiveness of the proposed estimators through experiments."
            },
            {
                "arxivId": "1809.03084",
                "title": "Efficient Counterfactual Learning from Bandit Feedback",
                "abstract": "What is the most statistically efficient way to do off-policy optimization with batch data from bandit feedback? For log data generated by contextual bandit algorithms, we consider offline estimators for the expected reward from a counterfactual policy. Our estimators are shown to have lowest variance in a wide class of estimators, achieving variance reduction relative to standard estimators. We then apply our estimators to improve advertisement design by a major advertisement company. Consistent with the theoretical result, our estimators allow us to improve on the existing bandit algorithm with more statistical confidence compared to a state-of-theart benchmark."
            },
            {
                "arxivId": "1806.05127",
                "title": "Stratification Trees for Adaptive Randomization in Randomized Controlled Trials",
                "abstract": "\n This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, and the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, our results are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure, can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment."
            },
            {
                "arxivId": "1805.04735",
                "title": "Pool-Based Sequential Active Learning for Regression",
                "abstract": "Active learning (AL) is a machine-learning approach for reducing the data labeling effort. Given a pool of unlabeled samples, it tries to select the most useful ones to label so that a model built from them can achieve the best possible performance. This paper focuses on pool-based sequential AL for regression (ALR). We first propose three essential criteria that an ALR approach should consider in selecting the most useful unlabeled samples: informativeness, representativeness, and diversity, and compare four existing ALR approaches against them. We then propose a new ALR approach using passive sampling, which considers both the representativeness and the diversity in both the initialization and subsequent iterations. Remarkably, this approach can also be integrated with other existing ALR approaches in the literature to further improve the performance. Extensive experiments on 11 University of California, Irvine, Carnegie Mellon University StatLib, and University of Florida Media Core data sets from various domains verified the effectiveness of our proposed ALR approaches."
            },
            {
                "arxivId": "1603.07573",
                "title": "STATISTICAL INFERENCE FOR THE MEAN OUTCOME UNDER A POSSIBLY NON-UNIQUE OPTIMAL TREATMENT STRATEGY.",
                "abstract": "We consider challenges that arise in the estimation of the mean outcome under an optimal individualized treatment strategy defined as the treatment rule that maximizes the population mean outcome, where the candidate treatment rules are restricted to depend on baseline covariates. We prove a necessary and sufficient condition for the pathwise differentiability of the optimal value, a key condition needed to develop a regular and asymptotically linear (RAL) estimator of the optimal value. The stated condition is slightly more general than the previous condition implied in the literature. We then describe an approach to obtain root-n rate confidence intervals for the optimal value even when the parameter is not pathwise differentiable. We provide conditions under which our estimator is RAL and asymptotically efficient when the mean outcome is pathwise differentiable. We also outline an extension of our approach to a multiple time point problem. All of our results are supported by simulations."
            },
            {
                "arxivId": "1602.04589",
                "title": "Optimal Best Arm Identification with Fixed Confidence",
                "abstract": "We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis."
            },
            {
                "arxivId": "1507.08025",
                "title": "Multi-armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges.",
                "abstract": "Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice."
            },
            {
                "arxivId": "1506.03486",
                "title": "Sequential Nonparametric Testing with the Law of the Iterated Logarithm",
                "abstract": "We propose a new algorithmic framework for sequential hypothesis testing with i.i.d. data, which includes A/B testing, nonparametric two-sample testing, and independence testing as special cases. It is novel in several ways: (a) it takes linear time and constant space to compute on the fly, (b) it has the same power guarantee as a non-sequential version of the test with the same computational constraints up to a small factor, and (c) it accesses only as many samples as are required - its stopping time adapts to the unknown difficulty of the problem. All our test statistics are constructed to be zero-mean martingales under the null hypothesis, and the rejection threshold is governed by a uniform non-asymptotic law of the iterated logarithm (LIL). For the case of nonparametric two-sample mean testing, we also provide a finite sample power analysis, and the first non-asymptotic stopping time calculations for this class of problems. We verify our predictions for type I and II errors and stopping times using simulations."
            },
            {
                "arxivId": "0802.2655",
                "title": "Pure exploration in finitely-armed and continuous-armed bandits",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03597",
        "category": "econ",
        "title": "The 'Must Stock' Challenge in Academic Publishing: Pricing Implications of Transformative Agreements",
        "abstract": "The high relevance of top-notch academic journals turns them into 'must stock' products that assign its often commercial owners with extraordinary market power. Intended to tackle this, university consortia around the globe negotiate so-called 'transformative agreements' with many publishing houses. It shall pave the way towards standard open-access publishing. While several contract designs exist, the 'publish-and-read' (PAR) scheme is the one that comes closest to the ideal of an entirely open access environment: Publishers are paid a fixed case-by-case rate for each publication, which includes a fee for their extensive libraries. In turn, all subscription payments are waived. I theoretically derive that this contract design benefits the included publishers regardless of whether the number of publications in these publishers' journals grows or declines. Consequently, widespread PAR contracts are likely to raise entry barriers for new (open-access) competitors even further. Intending to lower costs for the universities, their libraries, and, ultimately, the taxpayers, this PAR fee contract design of transformative agreements might cause the opposite.",
        "references": [
            {
                "arxivId": "2402.18255",
                "title": "How open are hybrid journals included in transformative agreements?",
                "abstract": "The ongoing controversy surrounding transformative agreements, which aim to transition journal publishing to full open access, highlight the need for large-scale studies assessing the impact of these agreements on hybrid open access. By combining publicly available data from various sources, including cOAlition S Journal Checker, Crossref, and OpenAlex, this study presents a novel approach that analyses over 700 agreements. Results suggest a strong growth in open access between 2018 and 2022 from 4.3% to 15%. During this period, 11,189 hybrid journals provided open access to 742,369 out of 8,146,958 articles, representing a five-year open access proportion of 9.1%. Authors who could make use of transformative agreements at the time of publication contributed 328,957 open access articles. In 2022, 143,615 out of 249,511 open access articles in hybrid journals or 58% were enabled by transformative agreements. This trend was largely driven by the three commercial publishers Elsevier, Springer Nature, and Wiley, but the open access uptake varied substantially across journals, publishers, disciplines, and country affiliations. In particular, the OECD and BRICS areas revealed different publication trends. In conclusion, this study suggests that current levels of implementation of transformative agreements is insufficient to bring about a large-scale transition to full open access."
            },
            {
                "arxivId": "2105.12078",
                "title": "No deal: German researchers\u2019 publishing and citing behaviors after Big Deal negotiations with Elsevier",
                "abstract": "Abstract In 2014, a union of German research organizations established Projekt DEAL, a national-level project to negotiate licensing agreements with large scientific publishers. Negotiations between DEAL and Elsevier began in 2016, and broke down without a successful agreement in 2018; during this time, around 200 German research institutions canceled their license agreements with Elsevier, leading Elsevier to restrict journal access at those institutions. We investigated the effect on researchers\u2019 publishing and citing behaviors from a bibliometric perspective, using a data set of \u223c400,000 articles published by researchers at DEAL institutions during 2012\u20132020. We further investigated these effects with respect to the timing of contract cancellations, research disciplines, collaboration patterns, and article open-access status. We find evidence for a decrease in Elsevier\u2019s market share of articles from DEAL institutions, with the largest year-on-year market share decreases occurring from 2018 to 2020 following the implementation of access restrictions. We also observe year-on-year decreases in the proportion of citations, although the decrease is smaller. We conclude that negotiations with Elsevier and access restrictions have led to some reduced willingness to publish in Elsevier journals, but that researchers are not strongly affected in their ability to cite Elsevier articles, implying that researchers use other methods to access scientific literature."
            },
            {
                "arxivId": "2012.07675",
                "title": "Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases",
                "abstract": null
            },
            {
                "arxivId": "1912.12646",
                "title": "Scholarly journal publishing in transition- from restricted to open access",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03610",
        "category": "econ",
        "title": "Paying for Privacy: Pay-or-Tracking Walls",
        "abstract": "Prestigious news publishers, and more recently, Meta, have begun to request that users pay for privacy. Specifically, users receive a notification banner, referred to as a pay-or-tracking wall, that requires them to (i) pay money to avoid being tracked or (ii) consent to being tracked. These walls have invited concerns that privacy might become a luxury. However, little is known about pay-or-tracking walls, which prevents a meaningful discussion about their appropriateness. This paper conducts several empirical studies and finds that top EU publishers use pay-or-tracking walls. Their implementations involve various approaches, including bundling the pay option with advertising-free access or additional content. The price for not being tracked exceeds the advertising revenue that publishers generate from a user who consents to being tracked. Notably, publishers' traffic does not decline when implementing a pay-or-tracking wall and most users consent to being tracked; only a few users pay. In short, pay-or-tracking walls seem to provide the means for expanding the practice of tracking. Publishers profit from pay-or-tracking walls and may observe a revenue increase of 16.4% due to tracking more users than under a cookie consent banner.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03612",
        "category": "econ",
        "title": "Using the Dual-Privacy Framework to Understand Consumers\u2019 Perceived Privacy Violations Under Different Firm Practices in Online Advertising",
        "abstract": "In response to privacy concerns about collecting and using personal data, the online advertising industry has been developing privacy-enhancing technologies (PETs), e.g., under Google's Privacy Sandbox initiative. In this research, we use the dual-privacy framework, which postulates that consumers have intrinsic and instrumental preferences for privacy, to understand consumers' perceived privacy violations (PPVs) for current and proposed online advertising practices. The key idea is that different practices differ in whether individual data leaves the consumer's machine or not and in how they track and target consumers; these affect, respectively, the intrinsic and instrumental components of privacy preferences differently, leading to different PPVs for different practices. We conducted online studies focused on consumers in the United States to elicit PPVs for various advertising practices. Our findings confirm the intuition that tracking and targeting consumers under the industry status quo of behavioral targeting leads to high PPV. New technologies or proposals that ensure that data are kept on the consumer's machine lower PPV relative to behavioral targeting but, importantly, this decrease is small. Furthermore, group-level targeting does not differ significantly from individual-level targeting in reducing PPV. Under contextual targeting, where there is no tracking, PPV is significantly reduced. Interestingly, with respect to PPV, consumers are indifferent between seeing untargeted ads and no ads when they are not being tracked. We find that consumer perceptions of privacy violations under different tracking and targeting practices may differ from what technical definitions suggest. Therefore, rather than relying solely on technical perspectives, a consumer-centric approach to privacy is needed, based on, for instance, the dual-privacy framework.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-07.json",
        "arxivId": "2403.03649",
        "category": "econ",
        "title": "The Cost of Coming Out",
        "abstract": "The fear of social stigma and discrimination leads many individuals worldwide to hesitate in openly disclosing their sexual orientation. Due to the large costs of concealing identity, it is crucial to understand the extent of anti-LGB sentiments and reactions to coming out. However, disclosing one's sexual orientation is a personal choice, complicating data access and introducing endogeneity issues. This paper tackles these challenges by using an innovative data source from a popular online video game together with a natural experiment. We exploit exogenous variation in the identity of a playable character to identify the effects of disclosure on players' revealed preferences for that character. Leveraging detailed daily data, we monitor players' preferences for the character across diverse regions globally and employ synthetic control methods to isolate the effect of the disclosure on players' preferences. Our findings reveal a substantial and persistent negative impact of coming out. To strengthen the plausibility of social stigma as the primary explanation for the estimated effects, we systematically address and eliminate several alternative game-related channels.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-08.json",
        "arxivId": "2403.04354",
        "category": "econ",
        "title": "A Logarithmic Mean Divisia Index Decomposition of CO$_2$ Emissions from Energy Use in Romania",
        "abstract": "Carbon emissions have become a specific alarming indicators and intricate challenges that lead an extended argue about climate change. The growing trend in the utilization of fossil fuels for the economic progress and simultaneously reducing the carbon quantity has turn into a substantial and global challenge. The aim of this paper is to examine the driving factors of CO$_2$ emissions from energy sector in Romania during the period 2008-2022 emissions using the log mean Divisia index (LMDI) method and takes into account five items: CO$_2$ emissions, primary energy resources, energy consumption, gross domestic product and population, the driving forces of CO$_2$ emissions, based on which it was calculated the contribution of carbon intensity, energy mixes, generating efficiency, economy, and population. The results indicate that generating efficiency effect -90968.57 is the largest inhibiting index while economic effect is the largest positive index 69084.04 having the role of increasing CO$_2$ emissions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-08.json",
        "arxivId": "2403.04512",
        "category": "econ",
        "title": "A topological characterization of the existence of w-stable sets",
        "abstract": "The theory of optimal choice sets is a solution theory that has a long and well-established tradition in social choice and game theories. Some of important general solution concepts of choice problems when the set of best alternatives does not exist (this problem occurs when the preferences yielded by an economic process are cyclic) is the Stable Set (Von Neumann-Morgenstern set) and its variants (Generalized Stable set, Extended Stable set, m-Stable set and w-Stable set). The theory of w-stable sets solution is more realistic because: (1) It solves the existence problem of solution; (2) It expands the notions of maximal alternative set and (3) The concept of stability is defined in such a way as to prevent a chosen alternative from being dominated by another alternative and sets this stability within the solution. In this paper, we present a topological characterization of the existence of w-Stable sets solution of arbitrary binary relations over non-finite sets of alternatives.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "1807.05477",
        "category": "econ",
        "title": "Linear Programming Based Near-Optimal Pricing for Laminar Bayesian Online Selection",
        "abstract": "In the Bayesian online selection problem, the goal is to design a pricing scheme for a sequence of arriving buyers that maximizes the expected social-welfare (or revenue) subject to different types of structural constraints. Inspired by applications in operations management, the focus of this paper is on the cases where the set of served customers is characterized by a laminar matroid. \n \nWe give the first Polynomial-Time Approximation Scheme (PTAS) for the problem when the laminar matroid has constant depth. Our approach is based on rounding the solution of a hierarchy of linear programming relaxations that approximate the optimum online solution with any degree of accuracy plus a concentration argument that shows the rounding incurs a small loss. We also study another variation, which we call the production constrained problem, for which the allowable set of served customers is characterized by a collection of production and shipping constraints forming a certain form of laminar matroid. Using a similar LP-based approach, we design a PTAS for this problem even when the depth of the laminar matroid is not constant. The analysis exploits the negative dependency of the optimum selection rule in the lower-levels of the laminar family. \n \nFinally, we conclude with a discussion of the linear programming based approach employed in the paper and re-derive some of the classic prophet inequalities known in the literature.",
        "references": [
            {
                "arxivId": "2103.13024",
                "title": "Online stochastic matching, poisson arrivals, and the natural linear program",
                "abstract": "We study the online stochastic matching problem. Consider a bipartite graph with offline vertices on one side, and with i.i.d.online vertices on the other side. The offline vertices and the distribution of online vertices are known to the algorithm beforehand. The realization of the online vertices, however, is revealed one at a time, upon which the algorithm immediately decides how to match it. For maximizing the cardinality of the matching, we give a 0.711-competitive online algorithm, which improves the best previous ratio of 0.706. When the offline vertices are weighted, we introduce a 0.7009-competitive online algorithm for maximizing the total weight of the matched offline vertices, which improves the best previous ratio of 0.662. Conceptually, we find that the analysis of online algorithms simplifies if the online vertices follow a Poisson process, and establish an approximate equivalence between this Poisson arrival model and online stochstic matching. Technically, we propose a natural linear program for the Poisson arrival model, and demonstrate how to exploit its structure by introducing a converse of Jensen\u2019s inequality. Moreover, we design an algorithmic amortization to replace the analytic one in previous work, and as a result get the first vertex-weighted online stochastic matching algorithm that improves the results in the weaker random arrival model."
            },
            {
                "arxivId": "2103.13089",
                "title": "Single-Sample Prophet Inequalities Revisited",
                "abstract": "The study of the prophet inequality problem in the limited information regime was initiated by Azar et al. [SODA'14] in the pursuit of prior-independent posted-price mechanisms. As they show, $O(1)$-competitive policies are achievable using only a single sample from the distribution of each agent. A notable portion of their results relies on reducing the design of single-sample prophet inequalities (SSPIs) to that of order-oblivious secretary (OOS) policies. The above reduction comes at the cost of not fully utilizing the available samples. However, to date, this is essentially the only method for proving SSPIs for many combinatorial sets. Very recently, Rubinstein et al. [ITCS'20] give a surprisingly simple algorithm which achieves the optimal competitive ratio for the single-choice SSPI problem $-$ a result which is unobtainable going through the reduction to secretary problems. Motivated by this discrepancy, we study the competitiveness of simple SSPI policies directly, without appealing to results from OOS literature. In this direction, we first develop a framework for analyzing policies against a greedy-like prophet solution. Using this framework, we obtain the first SSPI for general (non-bipartite) matching environments, as well as improved competitive ratios for transversal and truncated partition matroids. Second, motivated by the observation that many OOS policies for matroids decompose the problem into independent rank-$1$ instances, we provide a meta-theorem which applies to any matroid satisfying this partition property. Leveraging the recent results by Rubinstein et al., we obtain improved competitive guarantees (most by a factor of $2$) for a number of matroids captured by the reduction of Azar et al. Finally, we discuss applications of our SSPIs to the design of mechanisms for multi-dimensional limited information settings with improved revenue and welfare guarantees."
            },
            {
                "arxivId": "2102.10261",
                "title": "Online Stochastic Max-Weight Bipartite Matching: Beyond Prophet Inequalities",
                "abstract": "The rich literature on online Bayesian selection problems has long focused on so-called prophet inequalities, which compare the gain of an online algorithm to that of a \"prophet\" who knows the future. An equally-natural, though significantly less well-studied benchmark is the optimum online algorithm, which may be omnipotent (i.e., computationally-unbounded), but not omniscient. What is the computational complexity of the optimum online? How well can a polynomial-time algorithm approximate it? Motivated by applications in ride hailing, we study the above questions for the online stochastic maximum-weight matching problem under vertex arrivals. This problem was recently introduced by Ezra, Feldman, Gravin and Tang (EC'20), who gave a 1/2-competitive algorithm for it. This is the best possible ratio, as this problem is a generalization of the original single-item prophet inequality. We present a polynomial-time algorithm which approximates optimal online within a factor of 0.51---beating the best-possible prophet inequality. At the core of our result are a new linear program formulation, an algorithm that tries to match the arriving vertices in two attempts, and an analysis that bounds the correlation resulting from the second attempts. In contrast, we show that it is PSPACE-hard to approximate this problem within some constant \u03b1 < 1."
            },
            {
                "arxivId": "1906.06361",
                "title": "Online Allocation and Pricing: Constant Regret via Bellman Inequalities",
                "abstract": "We develop a framework for designing simple and efficient policies for a family of online allocation and pricing problems that includes online packing, budget-constrained probing, dynamic pricing, and online contextual bandits with knapsacks. In each case, we evaluate the performance of our policies in terms of their regret (i.e., additive gap) relative to an offline controller that is endowed with more information than the online controller. Our framework is based on Bellman inequalities, which decompose the loss of an algorithm into two distinct sources of error: (1) arising from computational tractability issues, and (2) arising from estimation/prediction of random trajectories. Balancing these errors guides the choice of benchmarks, and leads to policies that are both tractable and have strong performance guarantees. In particular, in all our examples, we demonstrate constant-regret policies that only require resolving a linear program in each period, followed by a simple greedy action-selection rule; thus, our policies are practical as well as provably near optimal."
            },
            {
                "arxivId": "1905.12778",
                "title": "Online Matching with Stochastic Rewards: Optimal Competitive Ratio via Path Based Formulation",
                "abstract": "The problem of online matching with stochastic rewards is a generalization of the online bipartite matching problem where each edge has a probability of success. When a match is made it succeeds with the probability of the corresponding edge. Introducing this model, Mehta and Panigrahi (FOCS 2012) focused on the special case of identical edge probabilities. Comparing against a deterministic offline LP, they showed that the Ranking algorithm of Karp et al. (STOC 1990) is 0.534 competitive and proposed a new online algorithm with an improved guarantee of 0.567 for vanishingly small probabilities. For the case of vanishingly small but heterogeneous probabilities Mehta et al. (SODA 2015), gave a 0.534 competitive algorithm against the same LP benchmark. For the more general vertex-weighted version of the problem, to the best of our knowledge, no results being 1/2 were previously known even for identical probabilities. We focus on the vertex-weighted version and give two improvements. First, we show that a natural generalization of the Perturbed-Greedy algorithm of Aggarwal et al. (SODA 2011), is (1-1/e) competitive when probabilities decompose as a product of two factors, one corresponding to each vertex of the edge. This is the best achievable guarantee as it includes the case of identical probabilities and in particular, the classical online bipartite matching problem. Second, we give a deterministic 0.596 competitive algorithm for the previously well studied case of case of fully heterogeneous but vanishingly small edge probabilities. A key contribution of our approach is the use of novel path-based formulations and a generalization of the primal-dual scheme of Devanur et al. (SODA 2013). These allow us to compare against the natural benchmark of clairvoyant (offline) algorithms that know the sequence of arrivals and the edge probabilities in advance, but not the outcomes of potential matches. These ideas may be of independent interest in other online settings with post-allocation stochasticity."
            },
            {
                "arxivId": "1902.06243",
                "title": "Prophet Inequality for Bipartite Matching: Merits of Being Simple and Non Adaptive",
                "abstract": "We consider Bayesian online selection problem of a matching in bipartite graphs, i.e., online weighted matching problem with edge arrivals where online algorithm knows distributions of weights, that corresponds to the intersection of two matroids in Kleinberg and Weinberg [35] model. We consider a simple class of non adaptive vertex-additive policies that assign static prices to all vertices in the graph and accept each edge only if its weight exceeds the sum of the prices of the edge's endpoints. We show existence of a vertex-additive policy with the expected payoff of at least one third of the prophet's payoff and present gradient decent type algorithm that quickly converges to the desired vector of vertex prices. This improves the adaptive online policy of Kleinberg and Weinberg for the intersection of two matroids in two ways: our policy is non adaptive and has better approximation guarantee of 3 instead of previous guarantees 5.82 of Kleinberg and Weinberg and 2\u2022e=5.43 of Feldman et al. [23] against the prophet. We give a complementary lower bound of 2.25 for any online algorithm in the bipartite matching setting."
            },
            {
                "arxivId": "1811.11881",
                "title": "Adversarial Bandits with Knapsacks",
                "abstract": "We consider Bandits with Knapsacks (henceforth, BwK), a general model for multi-armed bandits under supply/budget constraints. In particular, a bandit algorithm needs to solve a well-known knapsack problem: find an optimal packing of items into a limited-size knapsack. The BwK problem is a common generalization of numerous motivating examples, which range from dynamic pricing to repeated auctions to dynamic ad allocation to network routing and scheduling. While the prior work on BwK focused on the stochastic version, we pioneer the other extreme in which the outcomes can be chosen adversarially. This is a considerably harder problem, compared to both the stochastic version and the \"classic\" adversarial bandits, in that regret minimization is no longer feasible. Instead, the objective is to minimize the competitive ratio: the ratio of the benchmark reward to algorithm's reward. We design an algorithm with competitive ratio O(log T) relative to the best fixed distribution over actions, where T is the time horizon; we also prove a matching lower bound. The key conceptual contribution is a new perspective on the stochastic version of the problem. We suggest a new algorithm for the stochastic version, which builds on the framework of regret minimization in repeated games and admits a substantially simpler analysis compared to prior work. We then analyze this algorithm for the adversarial version, and use it as a subroutine to solve the latter. Our algorithm is the first \"black-box reduction\" from bandits to BwK: it takes an arbitrary bandit algorithm and uses it as a subroutine. We use this reduction to derive several extensions."
            },
            {
                "arxivId": "1807.07483",
                "title": "Prophet secretary through blind strategies",
                "abstract": null
            },
            {
                "arxivId": "1807.03435",
                "title": "Improved Approximations for Free-Order Prophets and Second-Price Auctions",
                "abstract": "We study the fundamental problem of selling a single indivisible item to one of $n$ buyers with independent and potentially nonidentical value distributions. We focus on two simple and widely used selling mechanisms: the second price auction with \\emph{eager} personalized reserve prices and the sequential posted price mechanism. Using a new approach, we improve the best-known performance guarantees for these mechanisms. We show that for every value of the number of buyers $n$, the eager second price (ESP) auction and sequential posted price mechanisms respectively earn at least $0.6620$ and $0.6543$ fractions of the optimal revenue. We also provide improved performance guarantees for these mechanisms when the number of buyers is small, which is the more relevant regime for many applications of interest. This in particular implies an improved bound of $0.6543$ for free-order prophet inequalities. \nMotivated by our improved revenue bounds, we further study the problem of optimizing reserve prices in the ESP auctions when the sorted order of personalized reserve prices among bidders is exogenous. We show that this problem can be solved polynomially. In addition, by analyzing a real auction dataset from Google's advertising exchange, we demonstrate the effectiveness of order-based pricing."
            },
            {
                "arxivId": "1806.09251",
                "title": "Optimal Online Contention Resolution Schemes via Ex-Ante Prophet Inequalities",
                "abstract": "Online contention resolution schemes (OCRSs) were proposed by Feldman, Svensson, and Zenklusen as a generic technique to round a fractional solution in the matroid polytope in an online fashion. It has found applications in several stochastic combinatorial problems where there is a commitment constraint: on seeing the value of a stochastic element, the algorithm has to immediately and irrevocably decide whether to select it while always maintaining an independent set in the matroid. Although OCRSs immediately lead to prophet inequalities, these prophet inequalities are not optimal. Can we instead use prophet inequalities to design optimal OCRSs? \nWe design the first optimal $1/2$-OCRS for matroids by reducing the problem to designing a matroid prophet inequality where we compare to the stronger benchmark of an ex-ante relaxation. We also introduce and design optimal $(1-1/e)$-random order CRSs for matroids, which are similar to OCRSs but the arrival is chosen uniformly at random."
            },
            {
                "arxivId": "1805.07742",
                "title": "A PTAS for a Class of Stochastic Dynamic Programs",
                "abstract": "We develop a framework for obtaining polynomial time approximation schemes (PTAS) for a class of stochastic dynamic programs. Using our framework, we obtain the first PTAS for the following stochastic combinatorial optimization problems: \\probemax: We are given a set of $n$ items, each item $i\\in [n]$ has a value $X_i$ which is an independent random variable with a known (discrete) distribution $\\pi_i$. We can {\\em probe} a subset $P\\subseteq [n]$ of items sequentially. Each time after {probing} an item $i$, we observe its value realization, which follows the distribution $\\pi_i$. We can {\\em adaptively} probe at most $m$ items and each item can be probed at most once. The reward is the maximum among the $m$ realized values. Our goal is to design an adaptive probing policy such that the expected value of the reward is maximized. To the best of our knowledge, the best known approximation ratio is $1-1/e$, due to Asadpour \\etal~\\cite{asadpour2015maximizing}. We also obtain PTAS for some generalizations and variants of the problem and some other problems."
            },
            {
                "arxivId": "1901.05028",
                "title": "The Bayesian Prophet: A Low-Regret Framework for Online Decision Making",
                "abstract": "Motivated by the success of using black-box predictive algorithms as subroutines for online decision-making, we develop a new framework for designing online policies given access to an oracle providing statistical information about an offline benchmark. Having access to such prediction oracles enables simple and natural Bayesian selection policies, and raises the question as to how these policies perform in different settings. Our work makes two important contributions towards tackling this question: First, we develop a general technique we call *compensated coupling* which can be used to derive bounds on the expected regret (i.e., additive loss with respect to a benchmark) for any online policy and offline benchmark; Second, using this technique, we show that the Bayes Selector has constant expected regret (i.e., independent of the number of arrivals and resource levels) in any online packing and matching problem with a finite type-space. Our results generalize and simplify many existing results for online packing and matching problems, and suggest a promising pathway for obtaining oracle-driven policies for other online decision-making settings."
            },
            {
                "arxivId": "1711.01834",
                "title": "Prophet Secretary: Surpassing the 1-1/e Barrier",
                "abstract": "In the Prophet Secretary problem, samples from a known set of probability distributions arrive one by one in a uniformly random order, and an algorithm must irrevocably pick one of the samples as soon as it arrives. The goal is to maximize the expected value of the sample picked relative to the expected maximum of the distributions. This is one of the most simple and fundamental problems in online decision making that models the process selling one item to a sequence of costumers. For a closely related problem called the Prophet Inequality where the order of the random variables is adversarial, it is known that one can achieve in expectation 1/2 of the expected maximum, and no better ratio is possible. For the Prophet Secretary problem, that is, when the variables arrive in a random order, Esfandiari et al. (2015) showed that one can actually get 1-1/e of the maximum. The 1-1/e bound was recently extended to more general settings by Ehsani et al. (2018). Given these results, one might be tempted to believe that 1-1/e is the correct bound. We show that this is not the case by providing an algorithm for the Prophet Secretary problem that beats the 1-1/e bound and achieves 1-1/e+1/400 times the expected maximum. We also prove a hardness result on the performance of algorithms under a natural restriction which we call deterministic distribution-insensitivity."
            },
            {
                "arxivId": "1905.04770",
                "title": "Algorithms for Online Matching, Assortment, and Pricing with Tight Weight-dependent Competitive Ratios",
                "abstract": "Resource Allocation and Pricing in the Absence of a Demand Forecast"
            },
            {
                "arxivId": "1704.05836",
                "title": "Beating 1-1/e for ordered prophets",
                "abstract": "Hill and Kertz studied the prophet inequality on iid distributions [The Annals of Probability 1982]. They proved a theoretical bound of 1 - 1/e on the approximation factor of their algorithm. They conjectured that the best approximation factor for arbitrarily large n is 1/1+1/e \u2243 0.731. This conjecture remained open prior to this paper for over 30 years. In this paper we present a threshold-based algorithm for the prophet inequality with n iid distributions. Using a nontrivial and novel approach we show that our algorithm is a 0.738-approximation algorithm. By beating the bound of 1/1+1/e, this refutes the conjecture of Hill and Kertz. Moreover, we generalize our results to non-uniform distributions and discuss its applications in mechanism design."
            },
            {
                "arxivId": "1612.03161",
                "title": "Prophet Inequalities Made Easy: Stochastic Optimization by Pricing Non-Stochastic Inputs",
                "abstract": "We present a general framework for stochastic online maximization problems with combinatorial feasibility constraints. The framework establishes prophet inequalities by constructing price-based online approximation algorithms, a natural extension of threshold algorithms for settings beyond binary selection. Our analysis takes the form of an extension theorem: we derive sufficient conditions on prices when all weights are known in advance, then prove that the resulting approximation guarantees extend directly to stochastic settings. Our framework unifies and simplifies much of the existing literature on prophet inequalities and posted price mechanisms, and is used to derive new and improved results for combinatorial markets (with and without complements), multi-dimensional matroids, and sparse packing problems. Finally, we highlight a surprising connection between the smoothness framework for bounding the price of anarchy of mechanisms and our framework, and show that many smooth mechanisms can be recast as posted price mechanisms with comparable performance guarantees."
            },
            {
                "arxivId": "1812.01577",
                "title": "A duality based unified approach to Bayesian mechanism design",
                "abstract": "We provide a unified view of many recent developments in Bayesian mechanism design, including the black-box reductions of Cai et. al., simple auctions for additive buyers, and posted-price mechanisms for unit-demand buyers. Additionally, we show that viewing these three previously disjoint lines of work through the same lens leads to new developments as well. First, we provide a duality framework for Bayesian mechanism design, which naturally accommodates multiple agents and arbitrary objectives/feasibility constraints. Using this, we prove that either a posted-price mechanism or the VCG auction with per-bidder entry fees achieves a constant-factor of the optimal Bayesian IC revenue whenever buyers are unit-demand or additive, unifying previous breakthroughs of Chawla et. al. and Yao, and improving both approximation ratios (from 33.75 to 24 and 69 to 8). Finally, we show that this view also leads to improved structural characterizations in the Cai et. al. framework."
            },
            {
                "arxivId": "1604.00357",
                "title": "Beyond matroids: secretary problem and prophet inequality with general constraints",
                "abstract": "We study generalizations of the ``Prophet Inequality'' and ``Secretary Problem'', where the algorithm is restricted to an arbitrary downward-closed set system. For 0,1 values, we give O(n)-competitive algorithms for both problems. This is close to the Omega(n/log n) lower bound due to Babaioff, Immorlica, and Kleinberg. For general values, our results translate to O(log(n) log(r))-competitive algorithms, where r is the cardinality of the largest feasible set. This resolves (up to the O(loglog(n) log(r)) factor) an open question posed to us by Bobby Kleinberg."
            },
            {
                "arxivId": "1603.03806",
                "title": "Mechanism Design for Subadditive Agents via an Ex Ante Relaxation",
                "abstract": "We consider the problem of maximizing revenue for a monopolist offering multiple items to multiple heterogeneous buyers. We develop a simple mechanism that obtains a constant factor approximation under the assumption that the buyers' values are additive subject to a matroid feasibility constraint and independent across items. Importantly, different buyers in our setting can have different constraints on the sets of items they desire. Our mechanism is a sequential variant of two-part tariffs. Prior to our work, simple approximation mechanisms for such multi-buyer problems were known only for the special cases of all unit-demand or all additive value buyers. Our work expands upon and unifies long lines of work on unit-demand settings and additive settings. We employ the ex ante relaxation approach developed by Alaei [2011] for reducing a multiple-buyer mechanism design problem with an ex post supply constraint into single-buyer problems with ex ante supply constraints. Solving the single-agent problems requires us to significantly extend a decomposition technique developed in the context of additive values Li and Yao [2013] and its extension to subadditive values by [2015]."
            },
            {
                "arxivId": "1508.00142",
                "title": "Online Contention Resolution Schemes",
                "abstract": "We introduce a new rounding technique designed for online optimization problems, which is related to contention resolution schemes, a technique initially introduced in the context of submodular function maximization. Our rounding technique, which we call online contention resolution schemes (OCRSs), is applicable to many online selection problems, including Bayesian online selection, oblivious posted pricing mechanisms, and stochastic probing models. It allows for handling a wide set of constraints, and shares many strong properties of offline contention resolution schemes. In particular, OCRSs for different constraint families can be combined to obtain an OCRS for their intersection. Moreover, we can approximately maximize submodular functions in the online settings we consider. We, thus, get a broadly applicable framework for several online selection problems, which improves on previous approaches in terms of the types of constraints that can be handled, the objective functions that can be dealt with, and the assumptions on the strength of the adversary. Furthermore, we resolve two open problems from the literature; namely, we present the first constant-factor constrained oblivious posted price mechanism for matroid constraints, and the first constant-factor algorithm for weighted stochastic probing with deadlines."
            },
            {
                "arxivId": "1507.06738",
                "title": "Linear Contextual Bandits with Knapsacks",
                "abstract": "We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn't exceed the budget for each resource. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual), bandits with knapsacks (BwK), and the online stochastic packing problem (OSPP). We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through an optimization oracle. We combine techniques from the work on linContextual, BwK, and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases."
            },
            {
                "arxivId": "1410.7596",
                "title": "Fast Algorithms for Online Stochastic Convex Programming",
                "abstract": "We introduce the online stochastic Convex Programming (CP) problem, a very general version of stochastic online problems which allows arbitrary concave objectives and convex feasibility constraints. Many well-studied problems like online stochastic packing and covering, online stochastic matching with concave returns, etc. form a special case of online stochastic CP. We present fast algorithms for these problems, which achieve near-optimal regret guarantees for both the i.i.d. and the random permutation models of stochastic inputs. When applied to the special case online packing, our ideas yield a simpler and faster primal-dual algorithm for this well studied problem, which achieves the optimal competitive ratio. Our techniques make explicit the connection of primal-dual paradigm and online learning to online stochastic CP."
            },
            {
                "arxivId": "1405.6146",
                "title": "A Simple and Approximately Optimal Mechanism for an Additive Buyer",
                "abstract": "We consider a monopolist seller with n heterogeneous items, facing a single buyer. The buyer hasa value for each item drawn independently according to(non-identical) distributions, and his value for a set ofitems is additive. The seller aims to maximize his revenue.It is known that an optimal mechanism in this setting maybe quite complex, requiring randomization [19] and menusof infinite size [15]. Hart and Nisan [17] have initiated astudy of two very simple pricing schemes for this setting:item pricing, in which each item is priced at its monopolyreserve; and bundle pricing, in which the entire set ofitems is priced and sold as one bundle. Hart and Nisan [17]have shown that neither scheme can guarantee more thana vanishingly small fraction of the optimal revenue. Insharp contrast, we show that for any distributions, thebetter of item and bundle pricing is a constant-factorapproximation to the optimal revenue. We further discussextensions to multiple buyers and to valuations that arecorrelated across items."
            },
            {
                "arxivId": "1307.5449",
                "title": "Non-Stationary Stochastic Optimization",
                "abstract": "We consider a non-stationary variant of a sequential stochastic optimization problem, in which the underlying cost functions may change along the horizon. We propose a measure, termed variation budget , that controls the extent of said change, and study how restrictions on this budget impact achievable performance. We identify sharp conditions under which it is possible to achieve long-run average optimality and more refined performance measures such as rate optimality that fully characterize the complexity of such problems. In doing so, we also establish a strong connection between two rather disparate strands of literature: (1) adversarial online convex optimization and (2) the more traditional stochastic approximation paradigm (couched in a non-stationary setting). This connection is the key to deriving well-performing policies in the latter, by leveraging structure of optimal policies in the former. Finally, tight bounds on the minimax regret allow us to quantify the \u201cprice of non-stationarity,\u201d which mathematically captures the added complexity embedded in a temporally changing environment versus a stationary one."
            },
            {
                "arxivId": "1307.3736",
                "title": "Prophet Inequalities with Limited Information",
                "abstract": "In the classical prophet inequality, a gambler observes a sequence of stochastic rewards V1, ..., Vn and must decide, for each reward Vi, whether to keep it and stop the game or to forfeit the reward forever and reveal the next value Vi. The gambler's goal is to obtain a constant fraction of the expected reward that the optimal offline algorithm would get. Recently, prophet inequalities have been generalized to settings where the gambler can choose k items, and, more generally, where he can choose any independent set in a matroid. However, all the existing algorithms require the gambler to know the distribution from which the rewards V1, ..., Vn are drawn. \n \nThe assumption that the gambler knows the distribution from which V1, ..., Vn are drawn is very strong. Instead, we work with the much simpler assumption that the gambler only knows a few samples from this distribution. We construct the first single-sample prophet inequalities for many settings of interest, whose guarantees all match the best possible asymptotically, even with full knowledge of the distribution. Specifically, we provide a novel single-sample algorithm when the gambler can choose any k elements whose analysis is based on random walks with limited correlation. In addition, we provide a black-box method for converting specific types of solutions to the related secretary problem to single-sample prophet inequalities, and apply it to several existing algorithms. Finally, we provide a constant-sample prophet inequality for constant-degree bipartite matchings. \n \nIn addition, we apply these results to design the first posted-price and multi-dimensional auction mechanisms with limited information in settings with asymmetric bidders. Connections between prophet inequalities and posted-price mechanisms are already known, but applying the existing framework requires knowledge of the underlying distributions, as well as the so-called \"virtual values\" even when the underlying prophet inequalities do not. We therefore provide an extension of this framework that bypasses virtual values altogether, allowing our mechanisms to take full advantage of the limited information required by our new prophet inequalities."
            },
            {
                "arxivId": "1307.3192",
                "title": "Online Independent Set Beyond the Worst-Case: Secretaries, Prophets, and Periods",
                "abstract": null
            },
            {
                "arxivId": "1306.1149",
                "title": "Improvements and Generalizations of Stochastic Knapsack and Multi-Armed Bandit Approximation Algorithms: Extended Abstract",
                "abstract": "The multi-armed bandit (MAB) problem features the classical tradeoff between exploration and exploitation. The input specifies several stochastic arms which evolve with each pull, and the goal is to maximize the expected reward after a fixed budget of pulls. The celebrated work of Gittins et al., surveyed in [8], presumes a condition on the arms called the martingale assumption. In [9], A. Gupta et al. obtained an LP-based 1 48 -approximation for the problem with the martingale assumption removed. We improve the algorithm to a 4 27 -approximation, with simpler analysis. Our algorithm also generalizes to the case of MAB superprocesses with (stochastic) multi-period actions. This generalization captures the framework introduced by Guha and Munagala in [11], and yields new results for their budgeted learning problems. Also, we obtain a ( 1 2 \u2212 e)-approximation for the variant of MAB where preemption (playing an arm, switching to another arm, then coming back to the first arm) is not allowed. This contains the stochastic knapsack problem of Dean, Goemans, and Vondrak in [6] with correlated rewards, where we are given a knapsack of fixed size, and a set of jobs each with a joint distribution for its size and reward. The actual size and reward of a job can only be discovered in real-time as it is being scheduled, and the objective is to maximize expected reward before the knapsack size is exhausted. Our ( 1 2 \u2212 e)-approximation improves the 1 16 and 1 8 approximations in [9] for correlated stochastic knapsack with cancellation and no cancellation, respectively, providing to our knowledge the first tight algorithm for these problems that matches the integrality gap of 2. We sample probabilities from an exponential-sized dynamic programming solution, whose existence is guaranteed by an LP projection argument. We hope this technique can also be applied to other dynamic programming problems which can be projected down onto a small LP."
            },
            {
                "arxivId": "1305.2545",
                "title": "Bandits with Knapsacks",
                "abstract": "Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called \"bandits with knapsacks\", that combines aspects of stochastic integer programming with online learning. A distinctive feature of our problem, in comparison to the existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sub linear regret in the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems. We present two algorithms whose reward is close to the information-theoretic optimum: one is based on a novel \"balanced exploration\" paradigm, while the other is a primal-dual algorithm that uses multiplicative updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors. We illustrate the generality of the problem by presenting applications in a number of different domains including electronic commerce, routing, and scheduling. As one example of a concrete application, we consider the problem of dynamic posted pricing with limited supply and obtain the first algorithm whose regret, with respect to the optimal dynamic policy, is sub linear in the supply."
            },
            {
                "arxivId": "1211.1149",
                "title": "Stochastic combinatorial optimization via poisson approximation",
                "abstract": "We study several stochastic combinatorial problems, including the expected utility maximization problem, the stochastic knapsack problem and the stochastic bin packing problem. A common technical challenge in these problems is to optimize some function (other than the expectation) of the sum of a set of random variables. The difficulty is mainly due to the fact that the probability distribution of the sum is the convolution of a set of distributions, which is not an easy objective function to work with. To tackle this difficulty, we introduce the Poisson approximation technique. The technique is based on the Poisson approximation theorem discovered by Le Cam, which enables us to approximate the distribution of the sum of a set of random variables using a compound Poisson distribution. Using the technique, we can reduce a variety of stochastic problems to the corresponding deterministic multiple-objective problems, which either can be solved by standard dynamic programming or have known solutions in the literature. For the problems mentioned above, we obtain the following results: We first study the expected utility maximization problem introduced recently [Li and Despande, FOCS11]. For monotone and Lipschitz utility functions, we obtain an additive PTAS if there is a multidimensional PTAS for the multi-objective version of the problem, strictly generalizing the previous result. The result implies the first additive PTAS for maximizing threshold probability for the stochastic versions of global min-cut, matroid base and matroid intersection. For the stochastic bin packing problem (introduced in [Kleinberg, Rabani and Tardos, STOC97]), we show there is a polynomial time algorithm which uses at most the optimal number of bins, if we relax the size of each bin and the overflow probability by e for any constant \u03b5>0. Based on this result, we obtain a 3-approximation if only the size of each bin can be relaxed by \u03b5, improving the known O(1/\u03b5) factor for constant overflow probability. For stochastic knapsack, we show a (1+\u03b5)-approximation using \u03b5 extra capacity for any \u03b5>0, even when the size and reward of each item may be correlated and cancelations of items are allowed. This generalizes the previous work [Balghat, Goel and Khanna, SODA11] for the case without correlation and cancelation. Our algorithm is also simpler. We also present a factor 2+\u03b5 approximation algorithm for stochastic knapsack with cancelations, for any constant \u03b5>0, improving the current known approximation factor of 8 [Gupta, Krishnaswamy, Molinaro and Ravi, FOCS11]. We also study an interesting variant of the stochastic knapsack problem, where the size and the profit of each item are revealed before the decision is made. The problem falls into the framework of Bayesian online selection problems, which has been studied a lot recently."
            },
            {
                "arxivId": "1207.5518",
                "title": "Optimal Multi-dimensional Mechanism Design: Reducing Revenue to Welfare Maximization",
                "abstract": "We provide a reduction from revenue maximization to welfare maximization in multidimensional Bayesian auctions with arbitrary - possibly combinatorial - feasibility constraints and independent bidders with arbitrary - possibly combinatorial-demand constraints, appropriately extending Myerson's single-dimensional result [21] to this setting. We also show that every feasible Bayesian auction - including in particular the revenue-optimal one - can be implemented as a distribution over virtual VCG allocation rules. A virtual VCG allocation rule has the following simple form: Every bidder's type ti is transformed into a virtual type fi(ti), via a bidder-specific function. Then, the allocation maximizing virtual welfare is chosen. Using this characterization, we show how to find and run the revenue-optimal auction given only black-box access to an implementation of the VCG allocation rule. We generalize this result to arbitrarily correlated bidders, introducing the notion of a second-order VCG allocation rule. Our results are computationally efficient for all multidimensional settings where the bidders are additive, or can be efficiently mapped to be additive, albeit the feasibility and demand constraints may still remain arbitrary combinatorial. In this case, our mechanisms run in time polynomial in the number of items and the total number of bidder types, but not type profiles. This is polynomial in the number of items, the number of bidders, and the cardinality of the support of each bidder's value distribution. For generic correlated distributions, this is the natural description complexity of the problem. The runtime can be further improved to polynomial in only the number of items and the number of bidders in itemsymmetric settings by making use of techniques from [15]."
            },
            {
                "arxivId": "1201.4764",
                "title": "Matroid prophet inequalities",
                "abstract": "Consider a gambler who observes a sequence of independent, non-negative random numbers and is allowed to stop the sequence at any time, claiming a reward equal to the most recent observation. The famous prophet inequality of Krengel, Sucheston, and Garling asserts that a gambler who knows the distribution of each random variable can achieve at least half as much reward, in expectation, as a \"prophet\" who knows the sampled values of each random variable and can choose the largest one. We generalize this result to the setting in which the gambler and the prophet are allowed to make more than one selection, subject to a matroid constraint. We show that the gambler can still achieve at least half as much reward as the prophet; this result is the best possible, since it is known that the ratio cannot be improved even in the original prophet inequality, which corresponds to the special case of rank-one matroids. Generalizing the result still further, we show that under an intersection of $p$ matroid constraints, the prophet's reward exceeds the gambler's by a factor of at most $O(p)$, and this factor is also tight.\n Beyond their interest as theorems about pure online algoritms or optimal stopping rules, these results also have applications to mechanism design. Our results imply improved bounds on the ability of sequential posted-price mechanisms to approximate optimal mechanisms in both single-parameter and multi-parameter Bayesian settings. In particular, our results imply the first efficiently computable constant-factor approximations to the Bayesian optimal revenue in certain multi-parameter settings."
            },
            {
                "arxivId": "1108.4142",
                "title": "Dynamic Pricing with Limited Supply",
                "abstract": "We consider the problem of designing revenue-maximizing online posted-price mechanisms when the seller has limited supply. A seller has k identical items for sale and is facing n potential buyers (\u201cagents\u201d) that are arriving sequentially. Each agent is interested in buying one item. Each agent\u2019s value for an item is an independent sample from some fixed (but unknown) distribution with support [0,1]. The seller offers a take-it-or-leave-it price to each arriving agent (possibly different for different agents), and aims to maximize his expected revenue. We focus on mechanisms that do not use any information about the distribution; such mechanisms are called detail-free (or prior-independent). They are desirable because knowing the distribution is unrealistic in many practical scenarios. We study how the revenue of such mechanisms compares to the revenue of the optimal offline mechanism that knows the distribution (\u201coffline benchmark\u201d). We present a detail-free online posted-price mechanism whose revenue is at most O((k log n)2/3) less than the offline benchmark, for every distribution that is regular. In fact, this guarantee holds without any assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, we prove a matching lower bound. The performance guarantee for the same mechanism can be improved to O(\u221ak log n), with a distribution-dependent constant, if the ratio k/n is sufficiently small. We show that, in the worst case over all demand distributions, this is essentially the best rate that can be obtained with a distribution-specific constant. On a technical level, we exploit the connection to multiarmed bandits (MAB). While dynamic pricing with unlimited supply can easily be seen as an MAB problem, the intuition behind MAB approaches breaks when applied to the setting with limited supply. Our high-level conceptual contribution is that even the limited supply setting can be fruitfully treated as a bandit problem."
            },
            {
                "arxivId": "1106.0961",
                "title": "Bayesian Combinatorial Auctions: Expanding Single Buyer Mechanisms to Many Buyers",
                "abstract": "For Bayesian combinatorial auctions, we present a general framework for approximately reducing the mechanism design problem for multiple buyers to the mechanism design problem for each individual buyer. Our framework can be applied to any setting which roughly satisfies the following assumptions: (i) The buyer's types must be distributed independently (not necessarily identically). (ii) The objective function must be linearly separable over the set of buyers (iii) The supply constraints must be the only constraints involving more than one buyer. Our framework is general in the sense that it makes no explicit assumption about any of the following: (i) The buyer's valuations (e.g., sub modular, additive, etc). (ii) The distribution of types for each buyer. (iii) The other constraints involving individual buyers (e.g., budget constraints, etc). We present two generic $n$-buyer mechanisms that use $1$-buyer mechanisms as black boxes. Assuming that we have an$\\alpha$-approximate $1$-buyer mechanism for each buyer\\footnote{Note that we can use different $1$-buyer mechanisms to accommodate different classes of buyers.} and assuming that no buyer ever needs more than $\\frac{1}{k}$ of all copies of each item for some integer $k \\ge 1$, then our generic $n$-buyer mechanisms are $\\gamma_k\\cdot\\alpha$-approximation of the optimal$n$-buyer mechanism, in which $\\gamma_k$ is a constant which is at least $1-\\frac{1}{\\sqrt{k+3}}$. Observe that $\\gamma_k$ is at least $\\frac{1}{2}$ (for $k=1$) and approaches $1$ as $k$ increases. As a byproduct of our construction, we improve a generalization of prophet inequalities. Furthermore, as applications of our main theorem, we improve several results from the literature."
            },
            {
                "arxivId": "1008.1843",
                "title": "Mechanism design via correlation gap",
                "abstract": "For revenue and welfare maximization in single-dimensional Bayesian settings, Chawla et al. (STOC10) recently showed that sequential posted-price mechanisms (SPMs), though simple in form, can perform surprisingly well compared to the optimal mechanisms. In this paper, we give a theoretical explanation of this fact, based on a connection to the notion of correlation gap.\n Loosely speaking, for auction environments with matroid constraints, we can relate the performance of a mechanism to the expectation of a monotone submodular function over a random set. This random set corresponds to the winner set for the optimal mechanism, which is highly correlated, and corresponds to certain demand set for SPMs, which is independent. The notion of correlation gap of Agrawal et al. (SODA10) quantifies how much we \"lose\" in the expectation of the function by ignoring correlation in the random set, and hence bounds our loss in using certain SPM instead of the optimal mechanism. Furthermore, the correlation gap of a monotone and submodular function is known to be small, and it follows that certain SPM can approximate the optimal mechanism by a good constant factor.\n Exploiting this connection, we give tight analysis of a greedy-based SPM of Chawla et al. for several environments. In particular, we show that it gives an e/(e \u2212 1)-approximation for matroid environments, gives asymptotically a 1/(1--1/\u221a2\u03c0k)-approximation for the important sub-case of k-unit auctions, and gives a (p + 1)-approximation for environments with p-independent set system constraints."
            },
            {
                "arxivId": "1007.1673",
                "title": "Online stochastic matching: online actions based on offline statistics",
                "abstract": "We consider the online stochastic matching problem proposed by Feldman et al. [4] as a model of display ad allocation. We are given a bipartite graph; one side of the graph corresponds to a fixed set of bins and the other side represents the set of possible ball types. At each time step, a ball is sampled independently from the given distribution and it needs to be matched upon its arrival to an empty bin. The goal is to maximize the size of the matching.\n We present an online algorithm for this problem with a competitive ratio of 0.702. Before our result, algorithms with a competitive ratio better than 1\u22121/e were known under the assumption that the expected number of arriving balls of each type is integral. A key idea of the algorithm is to collect statistics about the decisions of the optimum offline solution using Monte Carlo sampling and use those statistics to guide the decisions of the online algorithm. We also show that no online algorithm can have a competitive ratio better than 0.823."
            },
            {
                "arxivId": "0911.2974",
                "title": "A Dynamic Near-Optimal Algorithm for Online Linear Programming",
                "abstract": "A natural optimization model that formulates many online resource allocation problems is the online linear programming LP problem in which the constraint matrix is revealed column by column along with the corresponding objective coefficient. In such a model, a decision variable has to be set each time a column is revealed without observing the future inputs, and the goal is to maximize the overall objective function. In this paper, we propose a near-optimal algorithm for this general class of online problems under the assumptions of random order of arrival and some mild conditions on the size of the LP right-hand-side input. Specifically, our learning-based algorithm works by dynamically updating a threshold price vector at geometric time intervals, where the dual prices learned from the revealed columns in the previous period are used to determine the sequential decisions in the current period. Through dynamic learning, the competitiveness of our algorithm improves over the past study of the same problem. We also present a worst case example showing that the performance of our algorithm is near optimal."
            },
            {
                "arxivId": "0907.2435",
                "title": "Multi-parameter mechanism design and sequential posted pricing",
                "abstract": "We study the classic mathematical economics problem of Bayesian optimal mechanism design where a principal aims to optimize expected revenue when allocating resources to self-interested agents with preferences drawn from a known distribution. In single parameter settings (i.e., where each agent's preference is given by a single private value for being served and zero for not being served) this problem is solved [20]. Unfortunately, these single parameter optimal mechanisms are impractical and rarely employed [1], and furthermore the underlying economic theory fails to generalize to the important, relevant, and unsolved multi-dimensional setting (i.e., where each agent's preference is given by multiple values for each of the multiple services available) [25].\n In contrast to the theory of optimal mechanisms we develop a theory of sequential posted price mechanisms, where agents in sequence are offered take-it-or-leave-it prices. We prove that these mechanisms are approximately optimal in single-dimensional settings. These posted-price mechanisms avoid many of the properties of optimal mechanisms that make the latter impractical. Furthermore, these mechanisms generalize naturally to multi-dimensional settings where they give the first known approximations to the elusive optimal multi-dimensional mechanism design problem. In particular, we solve multi-dimensional multi-unit auction problems and generalizations to matroid feasibility constraints. The constant approximations we obtain range from 1.5 to 8. For all but one case, our posted price sequences can be computed in polynomial time.\n This work can be viewed as an extension and improvement of the single-agent algorithmic pricing work of [9] to the setting of multiple agents where the designer has combinatorial feasibility constraints on which agents can simultaneously obtain each service."
            },
            {
                "arxivId": "math/0404095",
                "title": "Towards a theory of negative dependence",
                "abstract": "The FKG theorem says that the positive lattice condition, an easily checkable hypothesis which holds for many natural families of events, implies positive association, a very useful property. Thus there is a natural and useful theory of positively dependent events. There is, as yet, no corresponding theory of negatively dependent events. There is, however, a need for such a theory. This paper, unfortunately, contains no substantial theorems. Its purpose is to present examples that motivate a need for such a theory, give plausibility arguments for the existence of such a theory, outline a few possible directions such a theory might take, and state a number of specific conjectures which pertain to the examples and to a wish list of theorems."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "2107.02739",
        "category": "econ",
        "title": "Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts",
        "abstract": "Many differentiated products have key attributes that are unstructured and thus high-dimensional (e.g., design, text). Instead of treating unstructured attributes as unobservables in economic models, quantifying them can be important to answer interesting economic questions. To propose an analytical framework for these types of products, this paper considers one of the simplest design products-fonts-and investigates merger and product differentiation using an original dataset from the world's largest online marketplace for fonts. We quantify font shapes by constructing embeddings from a deep convolutional neural network. Each embedding maps a font's shape onto a low-dimensional vector. In the resulting product space, designers are assumed to engage in Hotelling-type spatial competition. From the image embeddings, we construct two alternative measures that capture the degree of design differentiation. We then study the causal effects of a merger on the merging firm's creative decisions using the constructed measures in a synthetic control method. We find that the merger causes the merging firm to increase the visual variety of font design. Notably, such effects are not captured when using traditional measures for product offerings (e.g., specifications and the number of products) constructed from structured data.",
        "references": [
            {
                "arxivId": "2305.00044",
                "title": "Hedonic Prices and Quality Adjusted Price Indices Powered by AI",
                "abstract": "Accurate, real-time measurements of price index changes using electronic records are essential for tracking inflation and productivity in today's economic environment. We develop empirical hedonic models that can process large amounts of unstructured product data (text, images, prices, quantities) and output accurate hedonic price estimates and derived indices. To accomplish this, we generate abstract product attributes, or ``features,'' from text descriptions and images using deep neural networks, and then use these attributes to estimate the hedonic price function. Specifically, we convert textual information about the product to numeric features using large language models based on transformers, trained or fine-tuned using product descriptions, and convert the product image to numeric features using a residual network model. To produce the estimated hedonic price function, we again use a multi-task neural network trained to predict a product's price in all time periods simultaneously. To demonstrate the performance of this approach, we apply the models to Amazon's data for first-party apparel sales and estimate hedonic prices. The resulting models have high predictive accuracy, with $R^2$ ranging from $80\\%$ to $90\\%$. Finally, we construct the AI-based hedonic Fisher price index, chained at the year-over-year frequency. We contrast the index with the CPI and other electronic indices."
            },
            {
                "arxivId": "1908.11412",
                "title": "GeoStyle: Discovering Fashion Trends and Events",
                "abstract": "Understanding fashion styles and trends is of great potential interest to retailers and consumers alike. The photos people upload to social media are a historical and public data source of how people dress across the world and at different times. While we now have tools to automatically recognize the clothing and style attributes of what people are wearing in these photographs, we lack the ability to analyze spatial and temporal trends in these attributes or make predictions about the future. In this paper we address this need by providing an automatic framework that analyzes large corpora of street imagery to (a) discover and forecast long-term trends of various fashion attributes as well as automatically discovered styles, and (b) identify spatio-temporally localized events that affect what people wear. We show that our framework makes long term trend forecasts that are > 20% more accurate than prior art, and identifies hundreds of socially meaningful events that impact fashion across the globe."
            },
            {
                "arxivId": "1901.09036",
                "title": "Orthogonal Statistical Learning",
                "abstract": "We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be estimated from data (a \"nuisance model\"). We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target model and one for the nuisance model. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning literature to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate the case where the target parameter belongs to a complex nonparametric class. We characterize conditions on the metric entropy such that oracle rates---rates of the same order as if we knew the nuisance model---are achieved. We also analyze the rates achieved by specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance in the literature: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data."
            },
            {
                "arxivId": "1901.02551",
                "title": "Thinking Outside the Pool: Active Training Image Creation for Relative Attributes",
                "abstract": "Current wisdom suggests more labeled image data is always better, and obtaining labels is the bottleneck. Yet curating a pool of sufficiently diverse and informative images is itself a challenge. In particular, training image curation is problematic for fine-grained attributes, where the subtle visual differences of interest may be rare within traditional image sources. We propose an active image generation approach to address this issue. The main idea is to jointly learn the attribute ranking task while also learning to generate novel realistic image samples that will benefit that task. We introduce an end-to-end framework that dynamically \"imagines\" image pairs that would confuse the current model, presents them to human annotators for labeling, then improves the predictive model with the new examples. On two datasets, we show that by thinking outside the pool of real images, our approach gains generalization accuracy on challenging fine-grained attribute comparisons."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1803.09288",
                "title": "The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings",
                "abstract": "We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences (rich \u2013 poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste."
            },
            {
                "arxivId": "1712.09089",
                "title": "An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls",
                "abstract": "Abstract We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available."
            },
            {
                "arxivId": "1705.06394",
                "title": "Fashion Forward: Forecasting Visual Style in Fashion",
                "abstract": "What is the future of fashion? Tackling this question from a data-driven vision perspective, we propose to forecast visual style trends before they occur. We introduce the first approach to predict the future popularity of styles discovered from fashion images in an unsupervised manner. Using these styles as a basis, we train a forecasting model to represent their trends over time. The resulting model can hypothesize new mixtures of styles that will become popular in the future, discover style dynamics (trendy vs. classic), and name the key visual attributes that will dominate tomorrow\u2019s fashion. We demonstrate our idea applied to three datasets encapsulating 80,000 fashion products sold across six years on Amazon. Results indicate that fashion forecasting benefits greatly from visual analysis, much more than textual or meta-data cues surrounding products."
            },
            {
                "arxivId": "1512.03385",
                "title": "Deep Residual Learning for Image Recognition",
                "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            {
                "arxivId": "1503.03832",
                "title": "FaceNet: A unified embedding for face recognition and clustering",
                "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets."
            },
            {
                "arxivId": "1412.1265",
                "title": "Deeply learned face representations are sparse, selective, and robust",
                "abstract": "This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set."
            },
            {
                "arxivId": "1301.3781",
                "title": "Efficient Estimation of Word Representations in Vector Space",
                "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "2108.05858",
        "category": "econ",
        "title": "An Optimal Transport Approach to Estimating Causal Effects via Nonlinear Difference-in-Differences",
        "abstract": "We propose a nonlinear difference-in-differences method to estimate multivariate counterfactual distributions in classical treatment and control study designs with observational data. Our approach sheds a new light on existing approaches like the changes-in-changes and the classical semiparametric difference-in-differences estimator and generalizes them to settings with multivariate heterogeneity in the outcomes. The main benefit of this extension is that it allows for arbitrary dependence and heterogeneity in the joint outcomes. We demonstrate its utility both on synthetic and real data. In particular, we revisit the classical Card \\&Krueger dataset, examining the effect of a minimum wage increase on employment in fast food restaurants; a reanalysis with our method reveals that restaurants tend to substitute full-time with part-time labor after a minimum wage increase at a faster pace. A previous version of this work was entitled\"An optimal transport approach to causal inference.",
        "references": [
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            },
            {
                "arxivId": "2109.12004",
                "title": "Entropic estimation of optimal transport maps",
                "abstract": "We develop a computationally tractable method for estimating the optimal map between two distributions over R d with rigorous \ufb01nite-sample guarantees. Leveraging an entropic version of Brenier\u2019s theorem, we show that our estimator\u2014the barycentric projection of the optimal entropic plan\u2014is easy to compute using Sinkhorn\u2019s algorithm. As a result, unlike current approaches for map estimation, which are slow to evaluate when the dimension or number of samples is large, our approach is parallelizable and extremely e\ufb03cient even for massive data sets. Under smoothness assumptions on the optimal map, we show that our estimator enjoys comparable statistical performance to other estimators in the literature, but with much lower computational cost. We showcase the e\ufb03cacy of our proposed estimator through numerical examples. Our proofs are based on a modi\ufb01ed duality principle for entropic optimal transport and on a method for approximating optimal entropic plans due to Pal (2019)."
            },
            {
                "arxivId": "2107.01718",
                "title": "Rates of Estimation of Optimal Transport Maps using Plug-in Estimators via Barycentric Projections",
                "abstract": "Optimal transport maps between two probability distributions \u03bc and \u03bd on R have found extensive applications in both machine learning and statistics. In practice, these maps need to be estimated from data sampled according to \u03bc and \u03bd. Plugin estimators are perhaps most popular in estimating transport maps in the field of computational optimal transport. In this paper, we provide a comprehensive analysis of the rates of convergences for general plug-in estimators defined via barycentric projections. Our main contribution is a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-in estimators. We illustrate the usefulness of this stability estimate by first providing rates of convergence for the natural discretediscrete and semi-discrete estimators of optimal transport maps. We then use the same stability estimate to show that, under additional smoothness assumptions of Sobolev type or Besov type, kernel smoothed or wavelet based plug-in estimators respectively speed up the rates of convergence and significantly mitigate the curse of dimensionality suffered by the natural discrete-discrete/semi-discrete estimators. As a by-product of our analysis, we also obtain faster rates of convergence for plug-in estimators of W2(\u03bc, \u03bd), the Wasserstein distance between \u03bc and \u03bd, under the aforementioned smoothness assumptions, thereby complementing recent results in Chizat et al. (2020). Finally, we illustrate the applicability of our results in obtaining rates of convergence for Wasserstein barycenter between two probability distributions and obtaining asymptotic detection thresholds for some recent optimaltransport based tests of independence."
            },
            {
                "arxivId": "2104.13106",
                "title": "On the Structure of Optimal Transportation Plans between Discrete Measures",
                "abstract": null
            },
            {
                "arxivId": "2101.05380",
                "title": "A Dimension-free Computational Upper-bound for Smooth Optimal Transport Estimation",
                "abstract": "It is well-known that plug-in statistical estimation of optimal transport suffers from the curse of dimensionality. Despite recent efforts to improve the rate of estimation with the smoothness of the problem, the computational complexity of these recently proposed methods still degrades exponentially with the dimension. In this paper, thanks to an infinite-dimensional sum-of-squares representation, we derive a statistical estimator of smooth optimal transport which achieves a precision $\\varepsilon$ from $\\tilde{O}(\\varepsilon^{-2})$ independent and identically distributed samples from the distributions, for a computational cost of $\\tilde{O}(\\varepsilon^{-4})$ when the smoothness increases, hence yielding dimension-free statistical and computational rates, with potentially exponentially dimension-dependent constants."
            },
            {
                "arxivId": "2010.04814",
                "title": "When Is Parallel Trends Sensitive to Functional Form?",
                "abstract": "This paper assesses when the validity of difference\u2010in\u2010differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger \u201cparallel trends\u201d\u2010type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form."
            },
            {
                "arxivId": "2007.00830",
                "title": "Unlinked Monotone Regression",
                "abstract": "We consider so-called univariate unlinked (sometimes \"decoupled,\" or \"shuffled\") regression when the unknown regression curve is monotone. In standard monotone regression, one observes a pair $(X,Y)$ where a response $Y$ is linked to a covariate $X$ through the model $Y= m_0(X) + \\epsilon$, with $m_0$ the (unknown) monotone regression function and $\\epsilon$ the unobserved error (assumed to be independent of $X$). In the unlinked regression setting one gets only to observe a vector of realizations from both the response $Y$ and from the covariate $X$ where now $Y \\stackrel{d}{=} m_0(X) + \\epsilon$. There is no (observed) pairing of $X$ and $Y$. Despite this, it is actually still possible to derive a consistent non-parametric estimator of $m_0$ under the assumption of monotonicity of $m_0$ and knowledge of the distribution of the noise $\\epsilon$. In this paper, we establish an upper bound on the rate of convergence of such an estimator under minimal assumption on the distribution of the covariate $X$. We discuss extensions to the case in which the distribution of the noise is unknown. We develop a gradient-descent-based algorithm for its computation, and we demonstrate its use on synthetic data. Finally, we apply our method (in a fully data driven way, without knowledge of the error distribution) on longitudinal data from the US Consumer Expenditure Survey."
            },
            {
                "arxivId": "1806.07348",
                "title": "Statistical Optimal Transport via Factored Couplings",
                "abstract": "We propose a new method to estimate Wasserstein distances and optimal transport plans between two probability distributions from samples in high dimension. Unlike plug-in rules that simply replace the true distributions by their empirical counterparts, our method promotes couplings with low transport rank, a new structural assumption that is similar to the nonnegative rank of a matrix. Regularizing based on this assumption leads to drastic improvements on high-dimensional data for various tasks, including domain adaptation in single-cell RNA sequencing data. These findings are supported by a theoretical analysis that indicates that the transport rank is key in overcoming the curse of dimensionality inherent to data-driven optimal transport."
            },
            {
                "arxivId": "2102.01716",
                "title": "A Survey of Some Recent Applications of Optimal Transport Methods to Econometrics",
                "abstract": "This paper surveys recent applications of methods from the theory of optimal transport to econometric problems."
            },
            {
                "arxivId": "1705.09634",
                "title": "Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration",
                "abstract": "Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iteration, which also directly suggests a new greedy coordinate descent algorithm, Greenkhorn, with the same theoretical guarantees. Numerical simulations illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice."
            },
            {
                "arxivId": "1604.06145",
                "title": "Estimating Semi-Parametric Panel Multinomial Choice Models Using Cyclic Monotonicity",
                "abstract": "This paper proposes a new semi\u2010parametric identification and estimation approach to multinomial choice models in a panel data setting with individual fixed effects. Our approach is based on cyclic monotonicity, which is a defining convex\u2010analytic feature of the random utility framework underlying multinomial choice models. From the cyclic monotonicity property, we derive identifying inequalities without requiring any shape restrictions for the distribution of the random utility shocks. These inequalities point identify model parameters under straightforward assumptions on the covariates. We propose a consistent estimator based on these inequalities."
            },
            {
                "arxivId": "1209.1077",
                "title": "Learning Probability Measures with respect to Optimal Transport Metrics",
                "abstract": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures."
            },
            {
                "arxivId": "1204.2762",
                "title": "On the uniform asymptotic validity of subsampling and the bootstrap",
                "abstract": "This paper provides conditions under which subsampling and the bootstrap can be used to construct estimators of the quantiles of the distribution of a root that behave well uniformly over a large class of distributions $\\mathbf{P}$. These results are then applied (i) to construct confidence regions that behave well uniformly over $\\mathbf{P}$ in the sense that the coverage probability tends to at least the nominal level uniformly over $\\mathbf{P}$ and (ii) to construct tests that behave well uniformly over $\\mathbf{P}$ in the sense that the size tends to no greater than the nominal level uniformly over $\\mathbf{P}$. Without these stronger notions of convergence, the asymptotic approximations to the coverage probability or size may be poor, even in very large samples. Specific applications include the multivariate mean, testing moment inequalities, multiple testing, the empirical process and U-statistics."
            },
            {
                "arxivId": "1111.5927",
                "title": "Distribution's template estimate with Wasserstein metrics",
                "abstract": "In this paper we tackle the problem of comparing distributions of random variables and defining a mean pattern between a sample of random events. Using barycenters of measures in the Wasserstein space, we propose an iterative version as an estimation of the mean distribution. Moreover, when the distributions are a common measure warped by a centered random operator, then the barycenter enables to recover this distribution template."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "2210.02957",
        "category": "econ",
        "title": "Vice versa: The decoupling of content and topic heterogeneity in collusion research",
        "abstract": "Collusive practices continue to be a significant threat to competition and consumer welfare. It should be of utmost importance for academic research to provide the theoretical and empirical foundations to antitrust authorities and enable them to develop proper tools to encounter new collusive practices. Utilizing topical natural language machine learning techniques allows me to analyze the evolution of economic research on collusion over the past two decades in a novel way. It enables me to review some 800 publications systematically. I extract the underlying topics from the papers and conduct a large set of uni\u2010 and multivariate time series and regression analyses on their individual prevalences. I detect a notable tendency towards monocultures in topics and an endogenous constriction of the topic variety. In contrast, the overall contents and issues addressed by these papers have grown remarkably. This caused a decoupling: Nowadays, more datasets and cartel cases are studied but with a smaller research\u00a0scope.",
        "references": [
            {
                "arxivId": "2105.00337",
                "title": "Detecting bid-rigging coalitions in different countries and auction formats",
                "abstract": null
            },
            {
                "arxivId": "2007.11604",
                "title": "Understanding the temporal evolution of COVID-19 research through machine learning and natural language processing",
                "abstract": null
            },
            {
                "arxivId": "1405.4053",
                "title": "Distributed Representations of Sentences and Documents",
                "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
            },
            {
                "arxivId": "0708.3601",
                "title": "A correlated topic model of Science",
                "abstract": "Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139--177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990--1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections."
            },
            {
                "arxivId": "1301.6705",
                "title": "Probabilistic Latent Semantic Analysis",
                "abstract": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "2309.02338",
        "category": "econ",
        "title": "Sustainability assessment of Low Earth Orbit (LEO) satellite broadband mega-constellations",
        "abstract": "The growth of megaconstellations is rapidly increasing the number of rocket launches. While Low Earth Orbit (LEO) broadband satellites help to connect unconnected communities and achieve the Sustainable Development Goals (SDGs), there are also significant environmental emissions impacts from burning rocket fuels. We present sustainability analytics for phase 1 of the three main LEO constellations including Amazon Kuiper (3,236 satellites), Eutelsat Group`s OneWeb (648 satellites), and SpaceX Starlink (4,425 satellites). We find that LEO megaconstellations provide substantially improved broadband speeds for rural and remote communities, but are roughly 6-8 times more emissions intensive (250 kg CO2eq/subscriber/year) than comparative terrestrial mobile broadband. In the worst-case emissions scenario, this rises to 12-14 times more (469 kg CO2eq/subscriber/year). Policy makers must carefully consider the trade-off between connecting unconnected communities to further the SDGs and mitigating the growing space sector environmental footprint, particularly regarding phase 2 plans to launch an order-of-magnitude more satellites.",
        "references": [
            {
                "arxivId": "2012.06182",
                "title": "Point-to-Point Communication in Integrated Satellite-Aerial 6G Networks: State-of-the-Art and Future Challenges",
                "abstract": "This paper surveys the literature on point-to-point (P2P) links for integrated satellite-aerial networks, which are envisioned to be among the key enablers of the sixth-generation (6G) of wireless networks vision. The paper first outlines the unique characteristics of such integrated large-scale complex networks, often denoted by spatial networks, and focuses on two particular space-air infrastructures, namely, satellites networks and high-altitude platforms (HAPs). The paper then classifies the connecting P2P communications links as satellite-to-satellite links at the same layer (SSLL), satellite-to-satellite links at different layers (SSLD), and HAP-to-HAP links (HHL). The paper surveys each layer of such spatial networks separately, and highlights the possible natures of the connecting links (i.e., radio-frequency or free-space optics) with a dedicated survey of the existing link-budget results. The paper, afterwards, presents the prospective merit of realizing such an integrated satellite-HAP network towards providing broadband services in under-served and remote areas. Finally, the paper sheds light on several future research directions in the context of spatial networks, namely large-scale network optimization, intelligent offloading, smart platforms, energy efficiency, multiple access schemes, distributed spatial networks, and routing."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "2402.13580",
        "category": "econ",
        "title": "Mechanism Design with Sequential-Move Games: Revelation Principle",
        "abstract": "Traditionally, mechanism design focuses on simultaneous-move games (e.g., Myerson (1981)). In this paper, we study mechanism design with sequential-move games, and provide two results on revelation principles for general solution concepts (e.g., perfect Bayesian equilibrium, obvious dominance, strong-obvious dominance). First, if a solution concept is additive, implementation in sequential-move games is equivalent to implementation in simultaneous-move games. Second, for any solution concept \\r{ho} and any social choice function f, we identify a canonical operator {\\gamma}^{(\\r{ho},f)}, which is defined on primitives. We prove that, if \\r{ho} is monotonic, f can be implemented by a sequential-move game if and only if {\\gamma}^{(\\r{ho},f)} is achievable, which translates a complicated mechanism design problem into checking some conditions defined on primitives. Most of the existing solution concepts are either additive or monotonic.",
        "references": [
            {
                "arxivId": "1610.04873",
                "title": "Gibbard-Satterthwaite Success Stories and Obvious Strategyproofness",
                "abstract": "The Gibbard-Satterthwaite Impossibility Theorem [Gibbard, 1973, Satterthwaite, 1975] holds that dictatorship is the only Pareto optimal and strategyproof social choice function on the full domain of preferences. Much of the work in mechanism design aims at getting around this impossibility theorem. Three grand success stories stand out. On the domains of single-peaked preferences, of object assignment, and of quasilinear preferences, there are appealing Pareto optimal and strategyproof social choice functions. We investigate whether these success stories are robust to strengthening strategyproofness to obvious strategyproofness, a stronger incentive property that was recently introduced by Li [2015] and has since garnered considerable attention. For single-peaked preferences, we characterize the class of OSP-implementable and unanimous social choice functions as dictatorships with safeguards against extremism -- mechanisms (which turn out to also be Pareto optimal) in which the dictator can choose the outcome, but other agents may prevent the dictator from choosing an outcome that is too extreme. Median voting is consequently not OSP-implementable. Moreover, even when there are only two possible outcomes, majority voting is not OSP-implementable, and unanimity is the only OSP-implementable supermajority rule. For object assignment, we characterize the class of OSP-implementable and Pareto optimal matching rules as sequential barter with lurkers -- a significant generalization over bossy variants of bipolar serially dictatorial rules. While Li [2015] shows that second-price auctions are OSP-implementable when only one good is sold, we show that this positive result does not extend to the case of multiple goods. Even when all agents' preferences over goods are quasilinear and additive, no welfare-maximizing auction where losers pay nothing is OSP-implementable when more than one good is sold. Our analysis makes use of a gradual revelation principle, an analog of the (direct) revelation principle for OSP mechanisms that we present and prove, and believe to be of independent interest. An integrated examination, of all of these negative and positive results, on the one hand reveals that the various mechanics that come into play within obviously strategyproof mechanisms are considerably richer and more diverse than previously demonstrated and can give rise to rather exotic and quite intricate mechanisms in some domains, however on the other hand suggests that the boundaries of obvious strategyproofness are significantly less far-reaching than one may hope in other domains. We thus observe that in a natural sense, obvious strategyproofness is neither \"too strong\" nor \"too weak\" a definition for capturing \"strategyproofness that is easy to see,\" but in fact while it performs as intuitively expected on some domains, it \"overshoots\" on some other domains, and \"undershoots\" on yet other domains."
            },
            {
                "arxivId": "1511.00452",
                "title": "Stable matching mechanisms are not obviously strategy-proof",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-11.json",
        "arxivId": "2403.05222",
        "category": "econ",
        "title": "Matching under Imperfectly Transferable Utility",
        "abstract": "In this paper, we examine matching models with imperfectly transferable utility (ITU). We provide motivating examples, discuss the theoretical foundations of ITU matching models and present methods for estimating them. We also explore connected topics and provide an overview of the related literature. This paper has been submitted as a draft chapter for the Handbook of the Economics of Matching, edited by Che, Chiappori and Salani\\'e.",
        "references": [
            {
                "arxivId": "2309.11416",
                "title": "Existence of a Competitive Equilibrium with Substitutes, with Applications to Matching and Discrete Choice Models",
                "abstract": "We propose new results for the existence and uniqueness of a general nonparametric and nonseparable competitive equilibrium with substitutes. These results ensure the invertibility of a general competitive system. The existing literature has focused on the uniqueness of a competitive equilibrium assuming that existence holds. We introduce three properties that our supply system must satisfy: weak substitutes, pivotal substitutes, and responsiveness. These properties are sufficient to ensure the existence of an equilibrium, thus providing the existence counterpart to Berry, Gandhi, and Haile (2013)'s uniqueness results. For two important classes of models, bipartite matching models with full assignment and discrete choice models, we show that both models can be reformulated as a competitive system such that our existence and uniqueness results can be readily applied. We also provide an algorithm to compute the unique competitive equilibrium. Furthermore, we argue that our results are particularly useful for studying imperfectly transferable utility matching models with full assignment and non-additive random utility models."
            },
            {
                "arxivId": "2102.02071",
                "title": "Matching Function Equilibria with Partial Assignment: Existence, Uniqueness and Estimation",
                "abstract": "In this paper, we argue that models coming from a variety of fields share a common structure that we call matching function equilibria with partial assignment. This structure revolves around an aggregate matching function and a system of nonlinear equations. This encompasses search and matching models, matching models with transferable, non-transferable and imperfectly transferable utility, and matching with peer effects. We provide a proof of existence and uniqueness of an equilibrium as well as an efficient algorithm to compute it. We show how to estimate parametric versions of these models by maximum likelihood. We also propose an approach to construct counterfactuals without estimating the matching functions for a subclass of models. We illustrate our estimation approach by analyzing the impact of the elimination of the Social Security Student Benefit Program in 1982 on the marriage market in the United States."
            },
            {
                "arxivId": "2102.06547",
                "title": "Like Attract Like? A Structural Comparison of Homogamy across Same-Sex and Different-Sex Households",
                "abstract": "In this paper, we extend Gary Becker\u2019s empirical analysis of the marriage market to same-sex couples. We build an equilibrium model of the same-sex marriage market that allows for straightforward identification of the gains of marriage. We estimate the model with 2008\u201312 American Community Survey data on California and find that positive assortative mating is weaker for same-sex couples than for different-sex couples with respect to age and race. Positive assortative mating on education is stronger among female same-sex couples but comparable for male same-sex and different-sex couples. As regards labor market outcomes, our results suggest that specialization within the household mainly applies to different-sex couples."
            },
            {
                "arxivId": "2211.07416",
                "title": "Collective models and the marriage market",
                "abstract": "In this paper, I develop an integrated approach to collective models and matching models of the marriage market. In the collective framework, both household formation and the intra-household allocation of bargaining power are taken as given. This is no longer the case in the present contribution, where both are endogenous to the determination of equilibrium on the marriage market. I characterize a class of \"proper'' collective models which can be embedded into a general matching framework with imperfectly transferable utility. In such models, the bargaining sets are parametrized by an analytical device called distance function, which plays a key role both for writing down the usual stability conditions and for estimation. In general, however, distance functions are not known in closed-form. I provide an efficient method for computing distance functions, that works even with the most complex collective models. Finally, I illustrate my results with a proof-of-concept application that uses PSID data. I identify the sharing rule and its distribution and consider two counterfactual experiments of women empowerment, both of which are shown to affect the sharing rule and the investment of couples in public goods."
            },
            {
                "arxivId": "1609.06349",
                "title": "A review of matrix scaling and Sinkhorn's normal form for matrices and positive maps",
                "abstract": "Given a nonnegative matrix $A$, can you find diagonal matrices $D_1,~D_2$ such that $D_1AD_2$ is doubly stochastic? The answer to this question is known as Sinkhorn's theorem. It has been proved with a wide variety of methods, each presenting a variety of possible generalisations. Recently, generalisations such as to positive maps between matrix algebras have become more and more interesting for applications. This text gives a review of over 70 years of matrix scaling. The focus lies on the mathematical landscape surrounding the problem and its solution as well as the generalisation to positive maps and contains hardly any nontrivial unpublished results."
            },
            {
                "arxivId": "1508.05114",
                "title": "The nonlinear Bernstein-Schr\\\"odinger equation in Economics",
                "abstract": "In this paper we relate the Equilibrium Assignment Problem (EAP), which is underlying in several economics models, to a system of nonlinear equations that we call the \"nonlinear Bernstein-Schr\\\"odinger system\", which is well-known in the linear case, but whose nonlinear extension does not seem to have been studied. We apply this connection to derive an existence result for the EAP, and an efficient computational method."
            },
            {
                "arxivId": "2106.02371",
                "title": "Cupid\u2019s Invisible Hand: Social Surplus and Identification in Matching Models",
                "abstract": "We investigate a model of one-to-one matching with transferable utility when some of the characteristics of the players are unobservable to the analyst. We allow for a wide class of distributions of unobserved heterogeneity, subject only to a separability assumption that generalizes Choo and Siow (2006). We first show that the stable matching maximizes a social gain function that trades off the average surplus due to the observable characteristics and a generalized entropy term that reflects the impact of matching on unobserved characteristics. We use this result to derive simple closed-form formulae that identify the joint surplus in every possible match and the equilibrium utilities of all participants, given any known distribution of unobserved heterogeneity. If transfers are observed, then the pre-transfer utilities of both partners are also identified. We also present a very fast algorithm that computes the optimal matching for any specification of the joint surplus. We conclude by discussing some empirical approaches suggested by these results."
            },
            {
                "arxivId": "2102.07476",
                "title": "Personality Traits and the Marriage Market",
                "abstract": "Which and how many attributes are relevant for the sorting of agents in a matching market? This paper addresses these questions by constructing indices of mutual attractiveness that aggregate information about agents\u2019 attributes. The first k indices for agents on each side of the market provide the best approximation of the matching surplus by a k-dimensional model. The methodology is applied on a unique Dutch household survey containing information about education, height, body mass index, health, attitude toward risk, and personality traits of spouses."
            },
            {
                "arxivId": "1012.1904",
                "title": "Unique equilibria and substitution effects in a stochastic model of the marriage market",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2006.14869",
        "category": "econ",
        "title": "Revealing Choice Bracketing",
        "abstract": "Experiments suggest that people fail to take into account interdependencies between their choices -- they do not broadly bracket. Researchers often instead assume that people narrowly bracket, but existing designs do not test it. We design a novel experiment and revealed preference tests for how someone brackets their choices. In portfolio allocation under risk, social allocation, and induced-value shopping experiments, 40-43% of subjects are consistent with narrow bracketing and 0-16% with broad bracketing. Adjusting for each model's predictive precision, 74% of subjects are best described by narrow bracketing, 13% by broad bracketing, and 6% by intermediate cases.",
        "references": [
            {
                "arxivId": "2101.04529",
                "title": "Narrow Bracketing in Work Choices",
                "abstract": "Many important economic outcomes result from cumulative effects of smaller choices, so the best outcomes require accounting for other choices at each decision point. We document narrow bracketing \u2014 the neglect of such accounting \u2014 in work choices in a pre-registered experiment on MTurk: bracketing changes average willingness to work by 13-28%. In our experiment, broad bracketing is so simple to implement that narrow bracketing cannot possibly be due to optimal conservation of cognitive resources, so it must be suboptimal. We jointly estimate disutility of work and bracketing, finding gender differences in convexity of disutility, but not in bracketing. JEL Classifications: C91, D91, J01"
            },
            {
                "arxivId": "2010.08033",
                "title": "Background Risk and Small-Stakes Risk Aversion.",
                "abstract": "We show that under plausible levels of background risk, no theory of choice under risk---such as expected utility theory, prospect theory, or rank dependent utility---can simultaneously satisfy the following three economic postulates: (i) Decision makers are risk-averse over small gambles, (ii) they respect stochastic dominance, and (iii) they account for background risk."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2112.14249",
        "category": "econ",
        "title": "Nested Nonparametric Instrumental Variable Regression: Long Term, Mediated, and Time Varying Treatment Effects",
        "abstract": "Several causal parameters in short panel data models are scalar summaries of a function called a nested nonparametric instrumental variable regression (nested NPIV). Examples include long term, mediated, and time varying treatment effects identified using proxy variables. However, it appears that no prior estimators or guarantees for nested NPIV exist, preventing flexible estimation and inference for these causal parameters. A major challenge is compounding ill posedness due to the nested inverse problems. We analyze adversarial estimators of nested NPIV, and provide sufficient conditions for efficient inference on the causal parameter. Our nonasymptotic analysis has three salient features: (i) introducing techniques that limit how ill posedness compounds; (ii) accommodating neural networks, random forests, and reproducing kernel Hilbert spaces; and (iii) extending to causal functions, e.g. long term heterogeneous treatment effects. We measure long term heterogeneous treatment effects of Project STAR and mediated proximal treatment effects of the Job Corps.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2207.07318",
        "category": "econ",
        "title": "Flexible global forecast combinations",
        "abstract": null,
        "references": [
            {
                "arxivId": "2308.05263",
                "title": "Solving the Forecast Combination Puzzle",
                "abstract": "We demonstrate that the forecasting combination puzzle is a consequence of the methodology commonly used to produce forecast combinations. By the combination puzzle, we refer to the empirical finding that predictions formed by combining multiple forecasts in ways that seek to optimize forecast performance often do not out-perform more naive, e.g. equally-weighted, approaches. In particular, we demonstrate that, due to the manner in which such forecasts are typically produced, tests that aim to discriminate between the predictive accuracy of competing combination strategies can have low power, and can lack size control, leading to an outcome that favours the naive approach. We show that this poor performance is due to the behavior of the corresponding test statistic, which has a non-standard asymptotic distribution under the null hypothesis of no inferior predictive accuracy, rather than the {standard normal distribution that is} {typically adopted}. In addition, we demonstrate that the low power of such predictive accuracy tests in the forecast combination setting can be completely avoided if more efficient estimation strategies are used in the production of the combinations, when feasible. We illustrate these findings both in the context of forecasting a functional of interest and in terms of predictive densities. A short empirical example {using daily financial returns} exemplifies how researchers can avoid the puzzle in practical settings."
            },
            {
                "arxivId": "2205.04216",
                "title": "Forecast combinations: An over 50-year review",
                "abstract": null
            },
            {
                "arxivId": "2012.15059",
                "title": "Ensembles of Localised Models for Time Series Forecasting",
                "abstract": null
            },
            {
                "arxivId": "2008.00444",
                "title": "Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality",
                "abstract": null
            },
            {
                "arxivId": "1707.08114",
                "title": "A Survey on Multi-Task Learning",
                "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL."
            },
            {
                "arxivId": "1704.04110",
                "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks",
                "abstract": null
            },
            {
                "arxivId": "1410.4726",
                "title": "Nonparametric Stein-type shrinkage covariance matrix estimators in high-dimensional settings",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2212.13449",
        "category": "econ",
        "title": "Self-progressive choice models",
        "abstract": "Consider a population of heterogenous agents whose choice behaviors are partially comparable according to given primitive orderings. The set of choice functions admissible in the population specifies a choice model. A choice model is self-progressive if each aggregate choice behavior consistent with the model is uniquely representable as a probability distribution over admissible choice functions that are comparable. We establish an equivalence between self-progressive choice models and well-known algebraic structures called lattices. This equivalence provides for a precise recipe to restrict or extend any choice model for unique orderly representation. To prove out, we characterize the minimal self-progressive extension of rational choice functions, explaining why agents might exhibit choice overload. We provide necessary and sufficient conditions for the identification of a (unique) primitive ordering that renders our choice overload representation to a choice model.",
        "references": [
            {
                "arxivId": "2209.14258",
                "title": "Helly-Type Theorems for the Ordering of the Vertices of a Hypergraph",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2212.14411",
        "category": "econ",
        "title": "Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations",
        "abstract": "Sequential tests and their implied confidence sequences, which are valid at arbitrary stopping times, promise flexible statistical inference and on-the-fly decision making. However, strong guarantees are limited to parametric sequential tests that under-cover in practice or concentration-bound-based sequences that over-cover and have suboptimal rejection times. In this work, we consider classic delayed-start normal-mixture sequential probability ratio tests, and we provide the first asymptotic type-I-error and expected-rejection-time guarantees under general non-parametric data generating processes, where the asymptotics are indexed by the test's burn-in time. The type-I-error results primarily leverage a martingale strong invariance principle and establish that these tests (and their implied confidence sequences) have type-I error rates asymptotically equivalent to the desired (possibly varying) $\\alpha$-level. The expected-rejection-time results primarily leverage an identity inspired by It\\^o's lemma and imply that, in certain asymptotic regimes, the expected rejection time is asymptotically equivalent to the minimum possible among $\\alpha$-level tests. We show how to apply our results to sequential inference on parameters defined by estimating equations, such as average treatment effects. Together, our results establish these (ostensibly parametric) tests as general-purpose, non-parametric, and near-optimal. We illustrate this via numerical simulations and a real-data application to A/B testing at Netflix.",
        "references": [
            {
                "arxivId": "2203.04485",
                "title": "A composite generalization of Ville's martingale theorem",
                "abstract": "We provide a composite version of Ville's theorem that an event has zero measure if and only if there exists a nonnegative martingale which explodes to infinity when that event occurs. This is a classic result connecting measure-theoretic probability to the sequence-by-sequence game-theoretic probability, recently developed by Shafer and Vovk. Our extension of Ville's result involves appropriate composite generalizations of nonnegative martingales and measure-zero events: these are respectively provided by ``e-processes'', and a new inverse capital outer measure. We then develop a novel line-crossing inequality for sums of random variables which are only required to have a finite first moment, which we use to prove a composite version of the strong law of large numbers (SLLN). This allows us to show that violation of the SLLN is an event of outer measure zero and that our e-process explodes to infinity on every such violating sequence, while this is provably not achievable with a nonnegative (super)martingale."
            },
            {
                "arxivId": "1810.08240",
                "title": "Time-uniform, nonparametric, nonasymptotic confidence sequences",
                "abstract": "A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. Our work develops confidence sequences whose widths go to zero, with nonasymptotic coverage guarantees under nonparametric conditions. We draw connections between the Cram\\'er-Chernoff method for exponential concentration, the law of the iterated logarithm (LIL), and the sequential probability ratio test---our confidence sequences are time-uniform extensions of the first; provide tight, nonasymptotic characterizations of the second; and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes, and matrix martingales. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein bound growing at a LIL rate, as well as a novel upper LIL for the maximum eigenvalue of a sum of random matrices. Finally, we apply our methods to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman-Rubin potential outcomes model."
            },
            {
                "arxivId": "math/0507434",
                "title": "Nonanticipating estimation applied to sequential analysis and changepoint detection",
                "abstract": "Suppose a process yields independent observations whose distributions belong to a family parameterized by theta E Theta. When the process is in control, the observations are i.i.d. with a known parameter value theta(0). When the process is out of control, the parameter changes. We apply an idea of Robbins and Siegmund [Proc. Sixth Berkeley Symp. Math. Statist. Probab. 4 (1972) 37-41] to construct a class of sequential tests and detection schemes whereby the unknown post-change parameters are estimated. This approach is especially useful in situations where the parametric space is intricate and mixture-type rules are operationally or conceptually difficult to formulate. We exemplify our approach by applying it to the problem of detecting a change in the shape parameter of a Gamma distribution, in both a univariate and a multivariate setting."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2301.09379",
        "category": "econ",
        "title": "Revisiting Panel Data Discrete Choice Models with Lagged Dependent Variables",
        "abstract": "This paper revisits the identification and estimation of a class of semiparametric (distribution-free) panel data binary choice models with lagged dependent variables, exogenous covariates, and entity fixed effects. We provide a novel identification strategy, using an\"identification at infinity\"argument. In contrast with the celebrated Honore and Kyriazidou (2000), our method permits time trends of any form and does not suffer from the\"curse of dimensionality\". We propose an easily implementable conditional maximum score estimator. The asymptotic properties of the proposed estimator are fully characterized. A small-scale Monte Carlo study demonstrates that our approach performs satisfactorily in finite samples. We illustrate the usefulness of our method by presenting an empirical application to enrollment in private hospital insurance using the Household, Income and Labour Dynamics in Australia (HILDA) Survey data.",
        "references": [
            {
                "arxivId": "2311.00013",
                "title": "Semiparametric Discrete Choice Models for Bundles",
                "abstract": "We propose new identi cation and estimation approaches to semiparametric discrete choice models for bundles in both cross-sectional and panel data settings. The random utility functions of these models take the usual parametric form, while no distributional assumption is imposed on the stochastic disturbances. Our proposed methods permit certain forms of heteroskedasticity and arbitrary correlation in the disturbances across choices. Our identi cation approach is matching-based; it matches observed covariates across agents for the cross-sectional case, and over time for the panel data case. For the cross-sectional model, we propose a kernel-weighted rank procedure and establish N-asymptotic normality of the resulting estimators. We show the validity of the nonparametric bootstrap for the inference. For the panel data model, we propose localized maximum score type estimators which have a non-standard asymptotic distribution. We show that the numerical bootstrap developed by Hong and Li (2020) is a valid inference method for our panel data estimators. Monte Carlo experiments demonstrate that our proposed estimation and inference procedures perform adequately in nite samples."
            },
            {
                "arxivId": "2202.12062",
                "title": "SEMIPARAMETRIC ESTIMATION OF DYNAMIC BINARY CHOICE PANEL DATA MODELS",
                "abstract": "We propose a new approach to the semiparametric analysis of panel data binary choice models with fixed effects and dynamics (lagged dependent variables). The model under consideration has the same random utility framework as in Honor\u00e9 and Kyriazidou (2000, Econometrica 68, 839\u2013874). We demonstrate that, with additional serial dependence conditions on the process of deterministic utility and tail restrictions on the error distribution, the (point) identification of the model can proceed in two steps, and requires matching only the value of an index function of explanatory variables over time, rather than the value of each explanatory variable. Our identification method motivates an easily implementable, two-step maximum score (2SMS) procedure \u2013 producing estimators whose rates of convergence, in contrast to Honor\u00e9 and Kyriazidou\u2019s (2000, Econometrica 68, 839\u2013874) methods, are independent of the model dimension. We then analyze the asymptotic properties of the 2SMS procedure and propose bootstrap-based distributional approximations for inference. Evidence from Monte Carlo simulations indicates that our procedure performs satisfactorily in finite samples."
            },
            {
                "arxivId": "2107.03253",
                "title": "Dynamic ordered panel logit models",
                "abstract": "This paper studies a dynamic ordered logit model for panel data with fixed effects. The main contribution of the paper is to construct a set of valid moment conditions that are free of the fixed effects. The moment functions can be computed using four or more periods of data, and the paper presents sufficient conditions for the moment conditions to identify the common parameters of the model, namely the regression coefficients, the autoregressive parameters, and the threshold parameters. The availability of moment conditions suggests that these common parameters can be estimated using the generalized method of moments, and the paper documents the performance of this estimator using Monte Carlo simulations and an empirical illustration to self-reported health status using the British Household Panel Survey."
            },
            {
                "arxivId": "2104.04590",
                "title": "Identification of Dynamic Panel Logit Models with Fixed Effects",
                "abstract": "We show that the identification problem for a class of dynamic panel logit models with fixed effects has a connection to the truncated moment problem in mathematics. We use this connection to show that the sharp identified set of the structural parameters is characterized by a set of moment equality and inequality conditions. This result provides sharp bounds in models where moment equality conditions do not exist or do not point identify the parameters. We also show that the sharp identified set of the non-parametric latent distribution of the fixed effects is characterized by a vector of its generalized moments, and that the number of moments grows linearly in T . This final result lets us point identify, or sharply bound, specific classes of functionals, without solving an optimization problem with respect to the latent distribution. We illustrate our identification result with several examples, and an empirical application on modeling children\u2019s respiratory conditions."
            },
            {
                "arxivId": "2008.05517",
                "title": "A Dynamic Ordered Logit Model With Fixed Effects",
                "abstract": "\n We study a fixed-T panel data logit model for ordered outcomes that accommodates fixed effects and state dependence. We provide identification results for the autoregressive parameter, regression coefficients, and the threshold parameters in this model. Our results require only four observations on the outcome variable. We provide conditions under which a composite conditional maximum likelihood estimator is consistent and asymptotically normal. We use our estimator to explore the determinants of self-reported health in a panel of European countries over the period 2003\u20132016 and find evidence for state dependence in self-reported health."
            },
            {
                "arxivId": "2005.05942",
                "title": "Moment Conditions for Dynamic Panel Logit Models with Fixed Effects",
                "abstract": "This paper builds on Bonhomme (2012) to develop a method to systematically construct moment conditions for dynamic panel data logit models with fixed effects. After introducing the moment conditions obtained in this way, we explore their implications for identification and estimation of the model parameters that are common to all individuals, and we find that those common model parameters are estimable at root-$n$ rate for many more dynamic panel logit models than has been appreciated by the existing literature. In the case where the model contains one lagged variable, the moment conditions in Kitazawa (2013, 2016) are transformations of a subset of ours. A GMM estimator that is based on the moment conditions is shown to perform well in Monte Carlo simulations and in an empirical illustration to labor force participation."
            },
            {
                "arxivId": "1610.02753",
                "title": "Local M-estimation with discontinuous criterion for dependent and limited observations",
                "abstract": "This paper examines asymptotic properties of local M-estimators under three sets of high-level conditions. These conditions are sufficiently general to cover the minimum volume predictive region, conditional maximum score estimator for a panel data discrete choice model, and many other widely used estimators in statistics and econometrics. Specifically, they allow for discontinuous criterion functions of weakly dependent observations, which may be localized by kernel smoothing and contain nuisance parameters whose dimension may grow to infinity. Furthermore, the localization can occur around parameter values rather than around a fixed point and the observation may take limited values, which leads to set estimators. Our theory produces three different nonparametric cube root rates and enables valid inference for the local M-estimators, building on novel maximal inequalities for weakly dependent data. Our results include the standard cube root asymptotics as a special case. To illustrate the usefulness of our results, we verify our conditions for various examples such as the Hough transform estimator with diminishing bandwidth, maximum score-type set estimator, and many others."
            },
            {
                "arxivId": "1604.06145",
                "title": "Estimating Semi-Parametric Panel Multinomial Choice Models Using Cyclic Monotonicity",
                "abstract": "This paper proposes a new semi\u2010parametric identification and estimation approach to multinomial choice models in a panel data setting with individual fixed effects. Our approach is based on cyclic monotonicity, which is a defining convex\u2010analytic feature of the random utility framework underlying multinomial choice models. From the cyclic monotonicity property, we derive identifying inequalities without requiring any shape restrictions for the distribution of the random utility shocks. These inequalities point identify model parameters under straightforward assumptions on the covariates. We propose a consistent estimator based on these inequalities."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2303.10117",
        "category": "econ",
        "title": "Estimation of Grouped Time-Varying Network Vector Autoregression Models",
        "abstract": "This paper introduces a flexible time-varying network vector autoregressive model framework for large-scale time series. A latent group structure is imposed on the heterogeneous and node-specific time-varying momentum and network spillover effects so that the number of unknown time-varying coefficients to be estimated can be reduced considerably. A classic agglomerative clustering algorithm with nonparametrically estimated distance matrix is combined with a ratio criterion to consistently estimate the latent group number and membership. A post-grouping local linear smoothing method is proposed to estimate the group-specific time-varying momentum and network effects, substantially improving the convergence rates of the preliminary estimates which ignore the latent structure. We further modify the methodology and theory to allow for structural breaks in either the group membership, group number or group-specific coefficient functions. Numerical studies including Monte-Carlo simulation and an empirical application are presented to examine the finite-sample performance of the developed model and methodology.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2304.12245",
        "category": "econ",
        "title": "Inferring comparative advantage via entropy maximization",
        "abstract": "We revise the procedure proposed by Balassa to infer comparative advantage, which is a standard tool in Economics to analyze specialization (of countries, regions, etc). Balassa\u2019s approach compares a country\u2019s export of a given product with what would be expected from a benchmark based on the total volumes of countries and product flows. Based on results in the literature, we show that implementing Balassa\u2019s idea leads to conditions for estimating parameters conflicting with the information content of the model itself. Moreover, Balassa\u2019s approach does not implement any statistical validation. Hence, we propose an alternative procedure to overcome such a limitation, based upon the framework of entropy maximization and implementing a proper test of hypothesis: the \u2018key products\u2019 of a country are, now, the ones whose production is significantly larger than expected, under a null-model constraining the same amount of information defining Balassa\u2019s approach. What we found is that country diversification is always observed, regardless of the strictness of the validation procedure. Besides, the ranking of countries\u2019 fitnesses is only partially affected by the details of the validation scheme employed for the analysis while large differences are found to affect the rankings of product complexities. The routine for implementing the entropy-based filtering procedures employed here is freely available through the official Python Package Index PyPI.",
        "references": [
            {
                "arxivId": "1707.05146",
                "title": "Unfolding the innovation system for the development of countries: coevolution of Science, Technology and Production",
                "abstract": null
            },
            {
                "arxivId": "1607.01735",
                "title": "A maximum entropy approach to separating noise from signal in bimodal affiliation networks",
                "abstract": "In practice, many empirical networks, including co-authorship and collocation networks are unimodal projections of a bipartite data structure where one layer represents entities, the second layer consists of a number of sets representing affiliations, attributes, groups, etc., and an inter-layer link indicates membership of an entity in a set. The edge weight in the unimodal projection, which we refer to as a co-occurrence network, counts the number of sets to which both end-nodes are linked. Interpreting such dense networks requires statistical analysis that takes into account the bipartite structure of the underlying data. Here we develop a statistical significance metric for such networks based on a maximum entropy null model which preserves both the frequency sequence of the individuals/entities and the size sequence of the sets. Solving the maximum entropy problem is reduced to solving a system of nonlinear equations for which fast algorithms exist, thus eliminating the need for expensive Monte-Carlo sampling techniques. We use this metric to prune and visualize a number of empirical networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2308.07172",
        "category": "econ",
        "title": "Economic complexity and the sustainability transition: A review of data, methods, and literature",
        "abstract": "Economic Complexity (EC) methods have gained increasing popularity across fields and disciplines. In particular, the EC toolbox has proved particularly promising in the study of complex and interrelated phenomena, such as the transition towards a greener economy. Using the EC approach, scholars have been investigating the relationship between EC and sustainability, proposing to identify the distinguishing characteristics of green products and to assess the readiness of productive and technological structures for the sustainability transition. This article proposes to review and summarize the data, methods, and empirical literature that are relevant to the study of the sustainability transition from an EC perspective. We review three distinct but connected blocks of literature on EC and environmental sustainability. First, we survey the evidence linking measures of EC to indicators related to environmental sustainability. Second, we review articles that strive to assess the green competitiveness of productive systems. Third, we examine evidence on green technological development and its connection to non-green knowledge bases. Finally, we summarize the findings for each block and identify avenues for further research in this recent and growing body of empirical literature.",
        "references": [
            {
                "arxivId": "2304.12245",
                "title": "Inferring comparative advantage via entropy maximization",
                "abstract": "We revise the procedure proposed by Balassa to infer comparative advantage, which is a standard tool in Economics to analyze specialization (of countries, regions, etc). Balassa\u2019s approach compares a country\u2019s export of a given product with what would be expected from a benchmark based on the total volumes of countries and product flows. Based on results in the literature, we show that implementing Balassa\u2019s idea leads to conditions for estimating parameters conflicting with the information content of the model itself. Moreover, Balassa\u2019s approach does not implement any statistical validation. Hence, we propose an alternative procedure to overcome such a limitation, based upon the framework of entropy maximization and implementing a proper test of hypothesis: the \u2018key products\u2019 of a country are, now, the ones whose production is significantly larger than expected, under a null-model constraining the same amount of information defining Balassa\u2019s approach. What we found is that country diversification is always observed, regardless of the strictness of the validation procedure. Besides, the ranking of countries\u2019 fitnesses is only partially affected by the details of the validation scheme employed for the analysis while large differences are found to affect the rankings of product complexities. The routine for implementing the entropy-based filtering procedures employed here is freely available through the official Python Package Index PyPI."
            },
            {
                "arxivId": "2209.08382",
                "title": "Multidimensional economic complexity and inclusive green growth",
                "abstract": null
            },
            {
                "arxivId": "2202.01804",
                "title": "The different structure of economic ecosystems at the scales of companies and countries",
                "abstract": "A key element to understand complex systems is the relationship between the spatial scale of investigation and the structure of the interrelation among its elements. When it comes to economic systems, it is now well-known that the country-product bipartite network exhibits a nested structure, which is the foundation of different algorithms that have been used to scientifically investigate countries\u2019 development and forecast national economic growth. Changing the subject from countries to companies, a significantly different scenario emerges. Through the analysis of a unique dataset of Italian firms\u2019 exports and a worldwide dataset comprising countries\u2019 exports, here we find that, while a globally nested structure is observed at the country level, a local, in-block nested structure emerges at the level of firms. This in-block nestedness is statistically significant with respect to suitable null models and the algorithmic partitions of products into blocks correspond well with the UN-COMTRADE product classification. These findings lay a solid foundation for developing a scientific approach based on the physics of complex systems to the analysis of companies, which has been lacking until now."
            },
            {
                "arxivId": "2103.06017",
                "title": "Relatedness in the era of machine learning",
                "abstract": null
            },
            {
                "arxivId": "1909.05604",
                "title": "The Emergence of Innovation Complexity at Different Geographical and Technological Scales",
                "abstract": "We define a novel quantitative strategy inspired by the ecological notion of nestedness to single out the scale at which innovation complexity emerges from the aggregation of specialized building blocks. Our analysis not only suggests that the innovation space can be interpreted as a natural system in which advantageous capabilities are selected by evolutionary pressure, but also that the emerging structure of capabilities is not independent of the scale of observation at which they are observed. Expanding on this insight allows us to understand whether the capabilities characterizing the innovation space at a given scale are compatible with a complex evolutionary dynamics or, rather, a set of essentially independent activities allowing to reduce the system at that scale to a set of disjoint non interacting sub-systems. This yields a measure of the innovation complexity of the system, i.e. of the degree of interdependence between the sets of capabilities underlying the system's building blocks."
            },
            {
                "arxivId": "1808.10428",
                "title": "The Role of Complex Analysis in Modelling Economic Growth",
                "abstract": "Development and growth are complex and tumultuous processes. Modern economic growth theories identify some key determinants of economic growth. However, the relative importance of the determinants remains unknown, and additional variables may help clarify the directions and dimensions of the interactions. The novel stream of literature on economic complexity goes beyond aggregate measures of productive inputs and considers instead a more granular and structural view of the productive possibilities of countries, i.e., their capabilities. Different endowments of capabilities are crucial ingredients in explaining differences in economic performances. In this paper we employ economic fitness, a measure of productive capabilities obtained through complex network techniques. Focusing on the combined roles of fitness and some more traditional drivers of growth\u2014GDP per capita, capital intensity, employment ratio, life expectancy, human capital and total factor productivity\u2014we build a bridge between economic growth theories and the economic complexity literature. Our findings show that fitness plays a crucial role in fostering economic growth and, when it is included in the analysis, can be either complementary to traditional drivers of growth or can completely overshadow them. Notably, for the most complex countries, which have the most diversified export baskets and the largest endowments of capabilities, fitness is complementary to the chosen growth determinants in enhancing economic growth. The empirical findings are in agreement with neoclassical and endogenous growth theories. By contrast, for countries with intermediate and low capability levels, fitness emerges as the key growth driver. This suggests that economic models should account for capabilities; in fact, describing the technological possibilities of countries solely in terms of their production functions may lead to a misinterpretation of the roles of factors."
            },
            {
                "arxivId": "1709.05272",
                "title": "Economic Complexity: \"Buttarla in caciara\" vs a constructive approach",
                "abstract": "This note is a contribution to the debate about the optimal algorithm for Economic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2] eventually agree that the ECI+ algorithm [1] consists just in a renaming of the Fitness algorithm we introduced in 2012, as we explicitly showed in [3]. However, they omit any comment on the fact that their extensive numerical tests claimed to demonstrate that the same algorithm works well if they name it ECI+, but not if its name is Fitness. They should realize that this eliminates any credibility to their numerical methods and therefore also to their new analysis, in which they consider many algorithms [2]. Since by their own admission the best algorithm is the Fitness one, their new claim became that the search for the best algorithm is pointless and all algorithms are alike. This is exactly the opposite of what they claimed a few days ago and it does not deserve much comments. After these clarifications we also present a constructive analysis of the status of Economic Complexity, its algorithms, its successes and its perspectives. For us the discussion closes here, we will not reply to further comments."
            },
            {
                "arxivId": "1708.03511",
                "title": "Technology networks: the autocatalytic origins of innovation",
                "abstract": "We analyse the autocatalytic structure of technological networks and evaluate its significance for the dynamics of innovation patenting. To this aim, we define a directed network of technological fields based on the International Patents Classification, in which a source node is connected to a receiver node via a link if patenting activity in the source field anticipates patents in the receiver field in the same region more frequently than we would expect at random. We show that the evolution of the technology network is compatible with the presence of a growing autocatalytic structure, i.e. a portion of the network in which technological fields mutually benefit from being connected to one another. We further show that technological fields in the core of the autocatalytic set display greater fitness, i.e. they tend to appear in a greater number of patents, thus suggesting the presence of positive spillovers as well as positive reinforcement. Finally, we observe that core shifts take place whereby different groups of technology fields alternate within the autocatalytic structure; this points to the importance of recombinant innovation taking place between close as well as distant fields of the hierarchical classification of technological fields."
            },
            {
                "arxivId": "1511.08622",
                "title": "Complex Economies Have a Lateral Escape from the Poverty Trap",
                "abstract": "We analyze the decisive role played by the complexity of economic systems at the onset of the industrialization process of countries over the past 50 years. Our analysis of the input growth dynamics, considering a further dimension through a recently introduced measure of economic complexity, reveals that more differentiated and more complex economies face a lower barrier (in terms of GDP per capita) when starting the transition towards industrialization. As a consequence, we can extend the classical concept of a one-dimensional poverty trap, by introducing a two-dimensional poverty trap: a country will start the industrialization process if it is rich enough (as in neo-classical economic theories), complex enough (using this new dimension and laterally escaping from the poverty trap), or a linear combination of the two. This naturally leads to the proposal of a Complex Index of Relative Development (CIRD) which shows, when analyzed as a function of the growth due to input, a shape of an upside down parabola similar to that expected from the standard economic theories when considering only the GDP per capita dimension."
            },
            {
                "arxivId": "1507.02099",
                "title": "A review of the literature on citation impact indicators",
                "abstract": null
            },
            {
                "arxivId": "1505.07907",
                "title": "Linking Economic Complexity, Institutions and Income Inequality",
                "abstract": null
            },
            {
                "arxivId": "1503.05098",
                "title": "Randomizing bipartite networks: the case of the World Trade Web",
                "abstract": null
            },
            {
                "arxivId": "1408.2138",
                "title": "How the Taxonomy of Products Drives the Economic Development of Countries",
                "abstract": "We introduce an algorithm able to reconstruct the relevant network structure on which the time evolution of country-product bipartite networks takes place. The significant links are obtained by selecting the largest values of the projected matrix. We first perform a number of tests of this filtering procedure on synthetic cases and a toy model. Then we analyze the bipartite network constituted by countries and exported products, using two databases for a total of almost 50 years. It is then possible to build a hierarchically directed network, in which the taxonomy of products emerges in a natural way. We study the influence of the structure of this taxonomy network on countries' development; in particular, guided by an example taken from the industrialization of South Korea, we link the structure of the taxonomy network to the empirical temporal connections between product activations, finding that the most relevant edges for countries' development are the ones suggested by our network. These results suggest paths in the product space which are easier to achieve, and so can drive countries' policies in the industrialization process."
            },
            {
                "arxivId": "1108.2590",
                "title": "A Network Analysis of Countries\u2019 Export Flows: Firm Grounds for the Building Blocks of the Economy",
                "abstract": "In this paper we analyze the bipartite network of countries and products from UN data on country production. We define the country-country and product-product projected networks and introduce a novel method of filtering information based on elements\u2019 similarity. As a result we find that country clustering reveals unexpected socio-geographic links among the most competing countries. On the same footings the products clustering can be efficiently used for a bottom-up classification of produced goods. Furthermore we mathematically reformulate the \u201creflections method\u201d introduced by Hidalgo and Hausmann as a fixpoint problem; such formulation highlights some conceptual weaknesses of the approach. To overcome such an issue, we introduce an alternative methodology (based on biased Markov chains) that allows to rank countries in a conceptually consistent way. Our analysis uncovers a strong non-linear interaction between the diversification of a country and the ubiquity of its products, thus suggesting the possible need of moving towards more efficient and direct non-linear fixpoint algorithms to rank countries and products in the global market."
            },
            {
                "arxivId": "1101.1707",
                "title": "The network structure of economic output",
                "abstract": null
            },
            {
                "arxivId": "0708.2090",
                "title": "The Product Space Conditions the Development of Nations",
                "abstract": "Economies grow by upgrading the products they produce and export. The technology, capital, institutions, and skills needed to make newer products are more easily adapted from some products than from others. Here, we study this network of relatedness between products, or \u201cproduct space,\u201d finding that more-sophisticated products are located in a densely connected core whereas less-sophisticated products occupy a less-connected periphery. Empirically, countries move through the product space by developing goods close to those they currently produce. Most countries can reach the core only by traversing empirically infrequent distances, which may help explain why poor countries have trouble developing more competitive exports and fail to converge to the income levels of rich countries."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2310.19788",
        "category": "econ",
        "title": "Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget",
        "abstract": "This study investigates the experimental design problem for identifying the arm with the highest expected outcome, referred to as best arm identification (BAI). In our experiments, the number of treatment-allocation rounds is fixed. During each round, a decision-maker allocates an arm and observes a corresponding outcome, which follows a Gaussian distribution with variances that can differ among the arms. At the end of the experiment, the decision-maker recommends one of the arms as an estimate of the best arm. To design an experiment, we first discuss lower bounds for the probability of misidentification. Our analysis highlights that the available information on the outcome distribution, such as means (expected outcomes), variances, and the choice of the best arm, significantly influences the lower bounds. Because available information is limited in actual experiments, we develop a lower bound that is valid under the unknown means and the unknown choice of the best arm, which are referred to as the worst-case lower bound. We demonstrate that the worst-case lower bound depends solely on the variances of the outcomes. Then, under the assumption that the variances are known, we propose the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an extension of the Neyman allocation proposed by Neyman (1934). We show that the GNA-EBA strategy is asymptotically optimal in the sense that its probability of misidentification aligns with the lower bounds as the sample size increases infinitely and the differences between the expected outcomes of the best and other suboptimal arms converge to the same values across arms. We refer to such strategies as asymptotically worst-case optimal.",
        "references": [
            {
                "arxivId": "2308.12000",
                "title": "On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget",
                "abstract": "We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we first introduce the natural class of {\\it consistent} and {\\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof then proceeds by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \\cite{qin2022open}."
            },
            {
                "arxivId": "2306.07549",
                "title": "Fixed-Budget Best-Arm Identification with Heterogeneous Reward Variances",
                "abstract": "We study the problem of best-arm identification (BAI) in the fixed-budget setting with heterogeneous reward variances. We propose two variance-adaptive BAI algorithms for this setting: SHVar for known reward variances and SHAdaVar for unknown reward variances. Our algorithms rely on non-uniform budget allocations among the arms where the arms with higher reward variances are pulled more often than those with lower variances. The main algorithmic novelty is in the design of SHAdaVar, which allocates budget greedily based on overestimating the unknown reward variances. We bound probabilities of misidentifying the best arms in both SHVar and SHAdaVar. Our analyses rely on novel lower bounds on the number of pulls of an arm that do not require closed-form solutions to the budget allocation problem. Since one of our budget allocation problems is analogous to the optimal experiment design with unknown variances, we believe that our results are of a broad interest. Our experiments validate our theory, and show that SHVar and SHAdaVar outperform algorithms from prior works with analytical guarantees."
            },
            {
                "arxivId": "2303.09468",
                "title": "On the Existence of a Complexity in Fixed Budget Bandit Identification",
                "abstract": "In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by the same algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate."
            },
            {
                "arxivId": "2303.00950",
                "title": "Open Problem: Optimal Best Arm Identification with Fixed Budget",
                "abstract": "Best arm identification or pure exploration problems have received much attention in the COLT community since Bubeck et al. (2009) and Audibert et al. (2010). For any bandit instance with a unique best arm, its asymptotic complexity in the so-called fixed-confidence setting has been completely characterized in Garivier and Kaufmann (2016) and Chernoff (1959), while little is known about the asymptotic complexity in its\"dual\"setting called fixed-budget setting. This note discusses the open problems and conjectures about the instance-dependent asymptotic complexity in the fixed-budget setting."
            },
            {
                "arxivId": "2302.03117",
                "title": "Asymptotic Representations for Sequential Decisions, Adaptive Experiments, and Batched Bandits",
                "abstract": "We develop asymptotic approximation results that can be applied to sequential estimation and inference problems, adaptive randomized controlled trials, and other statistical decision problems that involve multiple decision nodes with structured and possibly endogenous information sets. Our results extend the classic asymptotic representation theorem used extensively in efficiency bound theory and local power analysis. In adaptive settings where the decision at one stage can affect the observation of variables in later stages, we show that a limiting data environment characterizes all limit distributions attainable through a joint choice of an adaptive design rule and statistics applied to the adaptively generated data, under local alternatives. We illustrate how the theory can be applied to study the choice of adaptive rules and end-of-sample statistical inference in batched (groupwise) sequential adaptive experiments."
            },
            {
                "arxivId": "2210.00974",
                "title": "Dealing with Unknown Variances in Best-Arm Identification",
                "abstract": "The problem of identifying the best arm among a collection of items having Gaussian rewards distribution is well understood when the variances are known. Despite its practical relevance for many applications, few works studied it for unknown variances. In this paper we introduce and analyze two approaches to deal with unknown variances, either by plugging in the empirical variance or by adapting the transportation costs. In order to calibrate our two stopping rules, we derive new time-uniform concentration inequalities, which are of independent interest. Then, we illustrate the theoretical and empirical performances of our two sampling rule wrappers on Track-and-Stop and on a Top Two algorithm. Moreover, by quantifying the impact on the sample complexity of not knowing the variances, we reveal that it is rather small."
            },
            {
                "arxivId": "2206.04646",
                "title": "Minimax Optimal Algorithms for Fixed-Budget Best Arm Identification",
                "abstract": "We consider the fixed-budget best arm identification problem where the goal is to find the arm of the largest mean with a fixed number of samples. It is known that the probability of misidentifying the best arm is exponentially small to the number of rounds. However, limited characterizations have been discussed on the rate (exponent) of this value. In this paper, we characterize the minimax optimal rate as a result of an optimization over all possible parameters. We introduce two rates, $R^{\\mathrm{go}}$ and $R^{\\mathrm{go}}_{\\infty}$, corresponding to lower bounds on the probability of misidentification, each of which is associated with a proposed algorithm. The rate $R^{\\mathrm{go}}$ is associated with $R^{\\mathrm{go}}$-tracking, which can be efficiently implemented by a neural network and is shown to outperform existing algorithms. However, this rate requires a nontrivial condition to be achievable. To address this issue, we introduce the second rate $R^{\\mathrm{go}}_\\infty$. We show that this rate is indeed achievable by introducing a conceptual algorithm called delayed optimal tracking (DOT)."
            },
            {
                "arxivId": "2205.02726",
                "title": "Asymptotic Efficiency Bounds for a Class of Experimental Designs",
                "abstract": "We consider an experimental design setting in which units are assigned to treatment after being sampled sequentially from an in\ufb01nite population. We derive asymptotic ef-\ufb01ciency bounds that apply to data from any experiment that assigns treatment as a (possibly randomized) function of covariates and past outcome data, including strati\ufb01cation on covariates and adaptive designs. For estimating the average treatment e\ufb00ect of a binary treatment, our results show that no further \ufb01rst order asymptotic e\ufb03ciency improvement is possible relative to an estimator that achieves the Hahn (1998) bound in an experimental design where the propensity score is chosen to minimize this bound. Our results also apply to settings with multiple treatments with possible constraints on treatment, as well as covariate based sampling of a single outcome."
            },
            {
                "arxivId": "2204.05527",
                "title": "Neyman allocation is minimax optimal for best arm identification with two arms",
                "abstract": ". This note describes the optimal policy rule, according to the local asymptotic minimax regret criterion, for best arm identi\ufb01cation when there are only two treatments. It is shown that the optimal sampling rule is the Neyman allocation, which allocates a constant fraction of units to each treatment in a manner that is proportional to the standard deviation of the treatment outcomes. When the variances are equal, the optimal ratio is one-half. This policy is independent of the data, so there is no adaptation to previous outcomes. At the end of the experiment, the policy maker adopts the treatment with higher average outcomes."
            },
            {
                "arxivId": "2112.06363",
                "title": "Risk and optimal policies in bandit experiments",
                "abstract": "We provide a decision theoretic analysis of bandit experiments under local asymptotics. Working within the framework of diffusion processes, we define suitable notions of asymptotic Bayes and minimax risk for these experiments. For normally distributed rewards, the minimal Bayes risk can be characterized as the solution to a second-order partial differential equation (PDE). Using a limit of experiments approach, we show that this PDE characterization also holds asymptotically under both parametric and non-parametric distributions of the rewards. The approach further describes the state variables it is asymptotically sufficient to restrict attention to, and thereby suggests a practical strategy for dimension reduction. The PDEs characterizing minimal Bayes risk can be solved efficiently using sparse matrix routines or Monte-Carlo methods. We derive the optimal Bayes and minimax policies from their numerical solutions. These optimal policies substantially dominate existing methods such as Thompson sampling; the risk of the latter is often twice as high."
            },
            {
                "arxivId": "2109.08229",
                "title": "Policy Choice and Best Arm Identification: Asymptotic Analysis of Exploration Sampling",
                "abstract": "We consider the\"policy choice\"problem -- otherwise known as best arm identification in the bandit literature -- proposed by Kasy and Sautmann (2021) for adaptive experimental design. Theorem 1 of Kasy and Sautmann (2021) provides three asymptotic results that give theoretical guarantees for exploration sampling developed for this setting. We first show that the proof of Theorem 1 (1) has technical issues, and the proof and statement of Theorem 1 (2) are incorrect. We then show, through a counterexample, that Theorem 1 (3) is false. For the former two, we correct the statements and provide rigorous proofs. For Theorem 1 (3), we propose an alternative objective function, which we call posterior weighted policy regret, and derive the asymptotic optimality of exploration sampling."
            },
            {
                "arxivId": "2108.09265",
                "title": "Efficient Online Estimation of Causal Effects by Deciding What to Observe",
                "abstract": "Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we aim to estimate any functional of a probabilistic model (e.g., a causal effect) as efficiently as possible, by deciding, at each time, which data source to query. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by current estimates of the moments. We propose two selection strategies: (1) explore-then-commit (OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources may include subsets of mediators, confounders, and instrumental variables."
            },
            {
                "arxivId": "2106.10417",
                "title": "Variance-Dependent Best Arm Identification",
                "abstract": "We study the problem of identifying the best arm in a stochastic multi-armed bandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is associated with an unknown reward distribution supported on $[0,1]$ with mean $\\theta_i$ and variance $\\sigma_i^2$. Assume $\\theta_1>\\theta_2 \\geq \\cdots \\geq\\theta_n$. We propose an adaptive algorithm which explores the gaps and variances of the rewards of the arms and makes future decisions based on the gathered information using a novel approach called \\textit{grouped median elimination}. The proposed algorithm guarantees to output the best arm with probability $(1-\\delta)$ and uses at most $O \\left(\\sum_{i = 1}^n \\left(\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i}\\right)(\\ln \\delta^{-1} + \\ln \\ln \\Delta_i^{-1})\\right)$ samples, where $\\Delta_i$ ($i \\geq 2$) denotes the reward gap between arm $i$ and the best arm and we define $\\Delta_1 = \\Delta_2$. This achieves a significant advantage over the variance-independent algorithms in some favorable scenarios and is the first result that removes the extra $\\ln n$ factor on the best arm compared with the state-of-the-art. We further show that $\\Omega \\left( \\sum_{i = 1}^n \\left( \\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i} \\right) \\ln \\delta^{-1} \\right)$ samples are necessary for an algorithm to achieve the same goal, thereby illustrating that our algorithm is optimal up to doubly logarithmic terms."
            },
            {
                "arxivId": "2008.00249",
                "title": "Review on ranking and selection: A new perspective",
                "abstract": null
            },
            {
                "arxivId": "2002.05308",
                "title": "Efficient Adaptive Experimental Design for Average Treatment Effect Estimation",
                "abstract": "The goal of many scientific experiments including A/B testing is to estimate the average treatment effect (ATE), which is defined as the difference between the expected outcomes of two or more treatments. In this paper, we consider a situation where an experimenter can assign a treatment to research subjects sequentially. In adaptive experimental design, the experimenter is allowed to change the probability of assigning a treatment using past observations for estimating the ATE efficiently. However, with this approach, it is difficult to apply a standard statistical method to construct an estimator because the observations are not independent and identically distributed. We thus propose an algorithm for efficient experiments with estimators constructed from dependent samples. We also introduce a sequential testing framework using the proposed estimator. To justify our proposed approach, we provide finite and infinite sample analyses. Finally, we experimentally show that the proposed algorithm exhibits preferable performance."
            },
            {
                "arxivId": "1911.02768",
                "title": "Confidence intervals for policy evaluation in adaptive experiments",
                "abstract": "Significance Randomized controlled trials are central to the scientific process, but they can be costly. For example, a clinical trial may assign patients to treatments that are detrimental to them. Adaptive experimental designs, such as multiarmed bandit algorithms, reduce costs by increasing the probability of assigning promising treatments over the course of the experiment. However, because observations collected by these methods are dependent and their distribution is nonstationary, statistical inference can be challenging. We propose a treatment-effect estimator that has an asymptotically unbiased and normal test statistic under straightforward, relatively weak conditions on the adaptive design. This estimator generalizes for a variety of parameters of interest. Adaptive experimental designs can dramatically improve efficiency in randomized trials. But with adaptively collected data, common estimators based on sample means and inverse propensity-weighted means can be biased or heavy-tailed. This poses statistical challenges, in particular when the experimenter would like to test hypotheses about parameters that were not targeted by the data-collection mechanism. In this paper, we present a class of test statistics that can handle these challenges. Our approach is to adaptively reweight the terms of an augmented inverse propensity-weighting estimator to control the contribution of each term to the estimator\u2019s variance. This scheme reduces overall variance and yields an asymptotically normal test statistic. We validate the accuracy of the resulting estimates and their CIs in numerical experiments and show that our methods compare favorably to existing alternatives in terms of mean squared error, coverage, and CI size."
            },
            {
                "arxivId": "1910.10945",
                "title": "Fixed-Confidence Guarantees for Bayesian Best-Arm Identification",
                "abstract": "We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors."
            },
            {
                "arxivId": "1811.11419",
                "title": "Mixture Martingales Revisited with Applications to Sequential Tests and Confidence Intervals",
                "abstract": "This paper presents new deviation inequalities that are valid uniformly in time under adaptive sampling in a multi-armed bandit model. The deviations are measured using the Kullback-Leibler divergence in a given one-dimensional exponential family, and may take into account several arms at a time. They are obtained by constructing for each arm a mixture martingale based on a hierarchical prior, and by multiplying those martingales. Our deviation inequalities allow us to analyze stopping rules based on generalized likelihood ratios for a large class of sequential identification problems, and to construct tight confidence intervals for some functions of the means of the arms."
            },
            {
                "arxivId": "1806.05127",
                "title": "Stratification Trees for Adaptive Randomization in Randomized Controlled Trials",
                "abstract": "\n This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, and the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, our results are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure, can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment."
            },
            {
                "arxivId": "1705.10033",
                "title": "Improving the Expected Improvement Algorithm",
                "abstract": "The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude."
            },
            {
                "arxivId": "1605.09004",
                "title": "Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem",
                "abstract": "We consider the problem of \\textit{best arm identification} with a \\textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, with arms distribution defined on $[0,1]$. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity $H$, will misidentify the best arm with probability lower bounded by $$\\exp\\Big(-\\frac{T}{\\log(K)H}\\Big),$$ where $H$ is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by $\\exp(-T/H)$. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem."
            },
            {
                "arxivId": "1602.08448",
                "title": "Simple Bayesian Algorithms for Best Arm Identification",
                "abstract": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. Just as the multiarmed bandit problem crystallizes the tradeoff between exploration and exploitation, this \u201cpure exploration\u201d variant crystallizes the challenge of rapidly gathering information before committing to a final decision. The paper proposes several simple Bayesian algorithms for allocating measurement effort and, by characterizing fundamental asymptotic limits on the performance of any algorithm, formalizes a sense in which these seemingly naive algorithms are the best possible."
            },
            {
                "arxivId": "1602.04589",
                "title": "Optimal Best Arm Identification with Fixed Confidence",
                "abstract": "We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis."
            },
            {
                "arxivId": "1407.4443",
                "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models",
                "abstract": "The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m \u2265 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixedcon fidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest: a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1)."
            },
            {
                "arxivId": "1312.7308",
                "title": "lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits",
                "abstract": "The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples. The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCB-type algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art."
            },
            {
                "arxivId": "0802.2655",
                "title": "Pure exploration in finitely-armed and continuous-armed bandits",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2401.07818",
        "category": "econ",
        "title": "A general approach for computing a consensus in group decision making that integrates multiple ethical principles",
        "abstract": null,
        "references": [
            {
                "arxivId": "2001.09768",
                "title": "Artificial Intelligence, Values, and Alignment",
                "abstract": null
            },
            {
                "arxivId": "1907.07167",
                "title": "Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression",
                "abstract": "Linear regression in $\\ell_p$-norm is a canonical optimization problem that arises in several applications, including sparse recovery, semi-supervised learning, and signal processing. Generic convex optimization algorithms for solving $\\ell_p$-regression are slow in practice. Iteratively Reweighted Least Squares (IRLS) is an easy to implement family of algorithms for solving these problems that has been studied for over 50 years. However, these algorithms often diverge for p > 3, and since the work of Osborne (1985), it has been an open problem whether there is an IRLS algorithm that is guaranteed to converge rapidly for p > 3. We propose p-IRLS, the first IRLS algorithm that provably converges geometrically for any $p \\in [2,\\infty).$ Our algorithm is simple to implement and is guaranteed to find a $(1+\\varepsilon)$-approximate solution in $O(p^{3.5} m^{\\frac{p-2}{2(p-1)}} \\log \\frac{m}{\\varepsilon}) \\le O_p(\\sqrt{m} \\log \\frac{m}{\\varepsilon} )$ iterations. Our experiments demonstrate that it performs even better than our theoretical bounds, beats the standard Matlab/CVX implementation for solving these problems by 10--50x, and is the fastest among available implementations in the high-accuracy regime."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2402.14003",
        "category": "econ",
        "title": "Multidimensional Signaling with a Resource Constraint",
        "abstract": "We study multidimensional signaling (cognitive/non-cognitive) as a sender's portfolio choice with a resource constraint. We establish the existence of a unique monotone D1 equilibrium where the cognitive (non-cognitive) signal increases (decreases) in sender type and the sum of the two increases in sender type. The equilibrium is characterized by two threshold sender types. The low threshold is one where a kink occurs in signaling. The constraint is binding only for sender types above it. The high threshold is the other one, above which all types spend all the resources in cognitive signal with pooling and discontinuity on the top.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.05803",
        "category": "econ",
        "title": "Semiparametric Inference for Regression-Discontinuity Designs",
        "abstract": "Treatment effects in regression discontinuity designs (RDDs) are often estimated using local regression methods. However, global approximation methods are generally deemed inefficient. In this paper, we propose a semiparametric framework tailored for estimating treatment effects in RDDs. Our global approach conceptualizes the identification of treatment effects within RDDs as a partially linear modeling problem, with the linear component capturing the treatment effect. Furthermore, we utilize the P-spline method to approximate the nonparametric function and develop procedures for inferring treatment effects within this framework. We demonstrate through Monte Carlo simulations that the proposed method performs well across various scenarios. Furthermore, we illustrate using real-world datasets that our global approach may result in more reliable inference.",
        "references": [
            {
                "arxivId": "1911.09511",
                "title": "A Practical Introduction to Regression Discontinuity Designs",
                "abstract": "In this Element and its accompanying Element, Matias D. Cattaneo, Nicolas Idrobo, and Rocio Titiunik provide an accessible and practical guide for the analysis and interpretation of Regression Discontinuity (RD) designs that encourages the use of a common set of practices and facilitates the accumulation of RD-based empirical evidence. In this Element, the authors discuss the foundations of the canonical Sharp RD design, which has the following features: (i) the score is continuously distributed and has only one dimension, (ii) there is only one cutoff, and (iii) compliance with the treatment assignment is perfect. In the accompanying Element, the authors discuss practical and conceptual extensions to the basic RD setup."
            },
            {
                "arxivId": "1809.03904",
                "title": "Regression Discontinuity Designs Using Covariates",
                "abstract": "Abstract We study regression discontinuity designs when covariates are included in the estimation. We examine local polynomial estimators that include discrete or continuous covariates in an additive separable way, but without imposing any parametric restrictions on the underlying population regression functions. We recommend a covariate-adjustment approach that retains consistency under intuitive conditions and characterize the potential for estimation and inference improvements. We also present new covariate-adjusted mean-squared error expansions and robust bias-corrected inference procedures, with heteroskedasticity-consistent and cluster-robust standard errors. We provide an empirical illustration and an extensive simulation study. All methods are implemented in R and Stata software packages."
            },
            {
                "arxivId": "1606.04086",
                "title": "Inference in Regression Discontinuity Designs with a Discrete Running Variable",
                "abstract": "We consider inference in regression discontinuity designs when the running variable only takes a moderate number of distinct values. In particular, we study the common practice of using confidence intervals (CIs) based on standard errors that are clustered by the running variable as a means to make inference robust to model misspecification (Lee and Card 2008). We derive theoretical results and present simulation and empirical evidence showing that these CIs do not guard against model misspecification, and that they have poor coverage properties. We therefore recommend against using these CIs in practice. We instead propose two alternative CIs with guaranteed coverage properties under easily interpretable restrictions on the conditional expectation function. (JEL C13, C51, J13, J31, J64, J65)"
            },
            {
                "arxivId": "1406.5823",
                "title": "Fitting Linear Mixed-Effects Models Using lme4",
                "abstract": "Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.05850",
        "category": "econ",
        "title": "Estimating Causal Effects of Discrete and Continuous Treatments with Binary Instruments",
        "abstract": "We propose an instrumental variable framework for identifying and estimating average and quantile effects of discrete and continuous treatments with binary instruments. The basis of our approach is a local copula representation of the joint distribution of the potential outcomes and unobservables determining treatment assignment. This representation allows us to introduce an identifying assumption, so-called copula invariance, that restricts the local dependence of the copula with respect to the treatment propensity. We show that copula invariance identifies treatment effects for the entire population and other subpopulations such as the treated. The identification results are constructive and lead to straightforward semiparametric estimation procedures based on distribution regression. An application to the effect of sleep on well-being uncovers interesting patterns of heterogeneity.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.05913",
        "category": "econ",
        "title": "Network formation and efficiency in linear-quadratic games: An experimental study",
        "abstract": "We experimentally study effort provision and network formation in the linear-quadratic game characterized by positive externality and complementarity of effort choices among network neighbors. We compare experimental outcomes to the equilibrium and efficient allocations and study the impact of group size and linking costs. We find that individuals overprovide effort relative to the equilibrium level on the network they form. However, their payoffs are lower than the equilibrium payoffs because they create fewer links than it is optimal which limits the beneficial spillovers of effort provision. Reducing the linking costs does not significantly increase the connectedness of the network and the welfare loss is higher in larger groups. Individuals connect to the highest effort providers in the group and ignore links to relative low effort providers, even if those links would be beneficial to form. This effect explains the lack of links in the network.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.05999",
        "category": "econ",
        "title": "Locally Regular and Efficient Tests in Non-Regular Semiparametric Models",
        "abstract": "This paper considers hypothesis testing in semiparametric models which may be non-regular. I show that C($\\alpha$) style tests are locally regular under mild conditions, including in cases where locally regular estimators do not exist, such as models which are (semi-parametrically) weakly identified. I characterise the appropriate limit experiment in which to study local (asymptotic) optimality of tests in the non-regular case, permitting the generalisation of classical power bounds to this case. I give conditions under which these power bounds are attained by the proposed C($\\alpha$) style tests. The application of the theory to a single index model and an instrumental variables model is worked out in detail.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.06150",
        "category": "econ",
        "title": "Algorithmic Collusion and Price Discrimination: The Over-Usage of Data",
        "abstract": "As firms' pricing strategies increasingly rely on algorithms, two concerns have received much attention: algorithmic tacit collusion and price discrimination. This paper investigates the interaction between these two issues through simulations. In each period, a new buyer arrives with independently and identically distributed willingness to pay (WTP), and each firm, observing private signals about WTP, adopts Q-learning algorithms to set prices. We document two novel mechanisms that lead to collusive outcomes. Under asymmetric information, the algorithm with information advantage adopts a Bait-and-Restrained-Exploit strategy, surrendering profits on some signals by setting higher prices, while exploiting limited profits on the remaining signals by setting much lower prices. Under a symmetric information structure, competition on some signals facilitates convergence to supra-competitive prices on the remaining signals. Algorithms tend to collude more on signals with higher expected WTP. Both uncertainty and the lack of correlated signals exacerbate the degree of collusion, thereby reducing both consumer surplus and social welfare. A key implication is that the over-usage of data, both payoff-relevant and non-relevant, by AIs in competitive contexts will reduce the degree of collusion and consequently lead to a decline in industry profits.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.06246",
        "category": "econ",
        "title": "Estimating Factor-Based Spot Volatility Matrices with Noisy and Asynchronous High-Frequency Data",
        "abstract": "We propose a new estimator of high-dimensional spot volatility matrices satisfying a low-rank plus sparse structure from noisy and asynchronous high-frequency data collected for an ultra-large number of assets. The noise processes are allowed to be temporally correlated, heteroskedastic, asymptotically vanishing and dependent on the efficient prices. We define a kernel-weighted pre-averaging method to jointly tackle the microstructure noise and asynchronicity issues, and we obtain uniformly consistent estimates for latent prices. We impose a continuous-time factor model with time-varying factor loadings on the price processes, and estimate the common factors and loadings via a local principal component analysis. Assuming a uniform sparsity condition on the idiosyncratic volatility structure, we combine the POET and kernel-smoothing techniques to estimate the spot volatility matrices for both the latent prices and idiosyncratic errors. Under some mild restrictions, the estimated spot volatility matrices are shown to be uniformly consistent under various matrix norms. We provide Monte-Carlo simulation and empirical studies to examine the numerical performance of the developed estimation methodology.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-12.json",
        "arxivId": "2403.06657",
        "category": "econ",
        "title": "Data-Driven Tuning Parameter Selection for High-Dimensional Vector Autoregressions",
        "abstract": "Lasso-type estimators are routinely used to estimate high-dimensional time series models. The theoretical guarantees established for Lasso typically require the penalty level to be chosen in a suitable fashion often depending on unknown population quantities. Furthermore, the resulting estimates and the number of variables retained in the model depend crucially on the chosen penalty level. However, there is currently no theoretically founded guidance for this choice in the context of high-dimensional time series. Instead one resorts to selecting the penalty level in an ad hoc manner using, e.g., information criteria or cross-validation. We resolve this problem by considering estimation of the perhaps most commonly employed multivariate time series model, the linear vector autoregressive (VAR) model, and propose a weighted Lasso estimator with penalization chosen in a fully data-driven way. The theoretical guarantees that we establish for the resulting estimation and prediction error match those currently available for methods based on infeasible choices of penalization. We thus provide a first solution for choosing the penalization in high-dimensional time series models.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-13.json",
        "arxivId": "2403.07152",
        "category": "econ",
        "title": "Success functions in large contests",
        "abstract": "We consider contests with a large set (continuum) of participants and axiomatize contest success functions that arise when performance is composed of both effort and a random element, and when winners are those whose performance exceeds a cutoff determined by a market clearing condition. A co-monotonicity property is essentially all that is needed for a representation in the general case, but significantly stronger conditions must hold to obtain an additive structure. We illustrate the usefulness of this framework by revisiting some of the classic questions in the contests literature.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-13.json",
        "arxivId": "2403.07530",
        "category": "econ",
        "title": "Carbon Economics of Different Agricultural Practices for Farming Soil",
        "abstract": "The loss of soil organic carbon (SOC) poses a severe danger to agricultural sustainability around the World. This review examines various farming practices and their impact on soil organic carbon storage. After a careful review of the literature, most of the research indicated that different farming practices, such as organic farming, cover crops, conservation tillage, and agroforestry, play vital roles in increasing the SOC content of the soil sustainably. Root exudation from cover crops increases microbial activity and helps break down complex organic compounds into organic carbon. Conservation tillage enhances the soil structure and maintains carbon storage without disturbing the soil. Agroforestry systems boost organic carbon input and fasten nutrient cycling because the trees and crops have symbiotic relationships. Intercropping and crop rotations have a role in changing the composition of plant residues and promoting carbon storage. There were many understanding on the complex interactions between soil organic carbon dynamics and agricultural practices. Based on the study, the paper reveals, the role of different agricultural practices like Carbon storage through cover crops, crop rotation, mulching Conservation tillage, conventional tillage, zero tillage and organic amendments in organic carbon storage in the soil for maximum crop yield to improve the economic condition of the cultivators.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2106.16081",
        "category": "econ",
        "title": "Quantal Response Equilibrium and Rationalizability: Inside the Black Box",
        "abstract": "This paper aims to connect epistemic and behavioral game theory by examining the epistemic foundations of quantal response equilibrium (QRE) in static games. We focus on how much information agents possess about the probability distributions of idiosyncratic payoff shocks, in addition to the standard assumptions of rationality and common belief in rationality. When these distributions are transparent, we obtain a solution concept called $\\Delta^p$-rationalizability, which includes action distributions derived from QRE; we also give a condition under which this relationship holds true in reverse. When agents only have common belief in the monotonicity of these distributions (for example, extreme value distributions), we obtain another solution concept called $\\Delta^M$-rationalizability, which includes action distributions derived from rank-dependent choice equilibrium, a parameter-free variant of QRE. Our solution concepts also provide insights for interpreting experimental and empirical data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2206.04643",
        "category": "econ",
        "title": "On the Performance of the Neyman Allocation with Small Pilots",
        "abstract": "The Neyman Allocation is used in many papers on experimental design, which typically assume that researchers have access to large pilot studies. This may be unrealistic. To understand the properties of the Neyman Allocation with small pilots, we study its behavior in an asymptotic framework that takes pilot size to be fixed even as the size of the main wave tends to infinity. Our analysis shows that the Neyman Allocation can lead to estimates of the ATE with higher asymptotic variance than with (non-adaptive) balanced randomization. In particular, this happens when the outcome variable is relatively homoskedastic with respect to treatment status or when it exhibits high kurtosis. We provide a series of empirical examples showing that such situations can arise in practice. Our results suggest that researchers with small pilots should not use the Neyman Allocation if they believe that outcomes are homoskedastic or heavy-tailed. We examine some potential methods for improving the finite sample performance of the FNA via simulations.",
        "references": [
            {
                "arxivId": "1906.00288",
                "title": "At What Level Should One Cluster Standard Errors in Paired and Small-Strata Experiments?",
                "abstract": "In matched pairs experiments in which one cluster per pair of clusters is assigned to treatment, to estimate treatment effects, researchers often regress their outcome on a treatment indicator and pair fixed effects, clustering standard errors at the unit-of-randomization level. We show that even if the treatment has no effect, a 5 percent\u2013level t-test based on this regression will wrongly conclude that the treatment has an effect up to 16.5 percent of the time. To fix this problem, researchers should instead cluster standard errors at the pair level. Using simulations, we show that similar results apply to clustered experiments with small strata. (JEL C21, C90, G21, O16, O18)"
            },
            {
                "arxivId": "1806.05127",
                "title": "Stratification Trees for Adaptive Randomization in Randomized Controlled Trials",
                "abstract": "\n This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, and the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, our results are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure, can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2208.08471",
        "category": "econ",
        "title": "An Unexpected Stochastic Dominance: Pareto Distributions, Dependence, and Diversification",
        "abstract": "Diversification is generally regarded as an efficient tool to reduce portfolio risks. In \u201cAn unexpected stochastic dominance: Pareto distributions, dependence, and diversification,\u201d Chen, Embrechts, and Wang showed that the weighted average of independent and identically distributed (i.i.d.) Pareto random variables with infinite mean is larger than one such random variable in the sense of first-order stochastic dominance, and thus diversification is, surprisingly, worse than no diversification. The relation implies superadditivity of value-at-risk, a regulatory risk measure used in the finance and insurance sectors. The obtained relation also holds under some form of negative dependence.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2310.11969",
        "category": "econ",
        "title": "Survey calibration for causal inference: a simple method to balance covariate distributions",
        "abstract": "This paper proposes a~simple, yet powerful, method for balancing distributions of covariates for causal inference based on observational studies. The method makes it possible to balance an arbitrary number of quantiles (e.g., medians, quartiles, or deciles) together with means if necessary. The proposed approach is based on the theory of calibration estimators (Deville and S\\\"arndal 1992), in particular, calibration estimators for quantiles, proposed by Harms and Duchesne (2006). The method does not require numerical integration, kernel density estimation or assumptions about the distributions. Valid estimates can be obtained by drawing on existing asymptotic theory. An~illustrative example of the proposed approach is presented for the entropy balancing method and the covariate balancing propensity score method. Results of a~simulation study indicate that the method efficiently estimates average treatment effects on the treated (ATT), the average treatment effect (ATE), the quantile treatment effect on the treated (QTT) and the quantile treatment effect (QTE), especially in the presence of non-linearity and mis-specification of the models. The proposed approach can be further generalized to other designs (e.g. multi-category, continuous) or methods (e.g. synthetic control method). An open source software implementing proposed methods is available.",
        "references": [
            {
                "arxivId": "2107.07086",
                "title": "Independence weights for causal inference with continuous treatments",
                "abstract": "Studying causal effects of continuous treatments is important for gaining a deeper understanding of many interventions, policies, or medications, yet researchers are often left with observational studies for doing so. In the observational setting, confounding is a barrier to the estimation of causal effects. Weighting approaches seek to control for confounding by reweighting samples so that confounders are comparable across different treatment values. Yet, for continuous treatments, weighting methods are highly sensitive to model misspecification. In this paper we elucidate the key property that makes weights effective in estimating causal quantities involving continuous treatments. We show that to eliminate confounding, weights should make treatment and confounders independent on the weighted scale. We develop a measure that characterizes the degree to which a set of weights induces such independence. Further, we propose a new model-free method for weight estimation by optimizing our measure. We study the theoretical properties of our measure and our weights, and prove that our weights can explicitly mitigate treatment-confounder dependence. The empirical effectiveness of our approach is demonstrated in a suite of challenging numerical experiments, where we find that our weights are quite robust and work well under a broad range of settings."
            },
            {
                "arxivId": "2004.13962",
                "title": "Energy balancing of covariate distributions",
                "abstract": "\n Bias in causal comparisons has a correspondence with distributional imbalance of covariates between treatment groups. Weighting strategies such as inverse propensity score weighting attempt to mitigate bias by either modeling the treatment assignment mechanism or balancing specified covariate moments. This article introduces a new weighting method, called energy balancing, which instead aims to balance weighted covariate distributions. By directly targeting distributional imbalance, the proposed weighting strategy can be flexibly utilized in a wide variety of causal analyses without the need for careful model or moment specification. Our energy balancing weights (EBW) approach has several advantages over existing weighting techniques. First, it offers a model-free and robust approach for obtaining covariate balance that does not require tuning parameters, obviating the need for modeling decisions of secondary nature to the scientific question at hand. Second, since this approach is based on a genuine measure of distributional balance, it provides a means for assessing the balance induced by a given set of weights for a given dataset. We demonstrate the effectiveness of this EBW approach in a suite of simulation experiments, and in studies on the safety of right heart catheterization and on three additional studies using electronic health record data."
            },
            {
                "arxivId": "2001.06118",
                "title": "Distributional Synthetic Controls",
                "abstract": "The method of synthetic controls is a fundamental tool for evaluating causal effects of policy changes in settings with observational data. In many settings where it is applicable, researchers want to identify causal effects of policy changes on a treated unit at an aggregate level while having access to data at a finer granularity. This article proposes an extension of the synthetic controls estimator that takes advantage of this additional structure and provides nonparametric estimates of the heterogeneity within the aggregate unit. The idea is to replicate the quantile function associated with the treated unit by a weighted average of quantile functions of the control units. This estimator relies on the same mathematical theory as the changes\u2010in\u2010changes estimator and can be applied in both repeated cross\u2010sections and panel data with as little as a single pre\u2010treatment period. It also provides a unique counterfactual quantile function for any type of distribution."
            },
            {
                "arxivId": "1812.08683",
                "title": "Robust Estimation of Causal Effects via High-Dimensional Covariate Balancing Propensity Score.",
                "abstract": "In this paper, we propose a robust method to estimate the average treatment effects in observational studies when the number of potential confounders is possibly much greater than the sample size. We first use a class of penalized M-estimators for the propensity score and outcome models. We then calibrate the initial estimate of the propensity score by balancing a carefully selected subset of covariates that are predictive of the outcome. Finally, the estimated propensity score is used to construct the inverse probability weighting estimator. We prove that the proposed estimator, which has the sample boundedness property, is root-n consistent, asymptotically normal, and semiparametrically efficient when the propensity score model is correctly specified and the outcome model is linear in covariates. More importantly, we show that our estimator remains root-n consistent and asymptotically normal so long as either the propensity score model or the outcome model is correctly specified. We provide valid confidence intervals in both cases and further extend these results to the case where the outcome model is a generalized linear model. In simulation studies, we find that the proposed methodology often estimates the average treatment effect more accurately than the existing methods. We also present an empirical application, in which we estimate the average causal effect of college attendance on adulthood political participation. Open-source software is available for implementing the proposed methodology."
            },
            {
                "arxivId": "1810.01370",
                "title": "Covariate Distribution Balance via Propensity Scores",
                "abstract": "This paper proposes new estimators for the propensity score that aim to maximize the covariate distribution balance among different treatment groups. Heuristically, our proposed procedure attempts to estimate a propensity score model by making the underlying covariate distribution of different treatment groups as close to each other as possible. Our estimators are data-driven, do not rely on tuning parameters such as bandwidths, admit an asymptotic linear representation, and can be used to estimate different treatment effect parameters under different identifying assumptions, including unconfoundedness and local treatment effects. We derive the asymptotic properties of inverse probability weighted estimators for the average, distributional, and quantile treatment effects based on the proposed propensity score estimator and illustrate their finite sample performance via Monte Carlo simulations and two empirical applications."
            },
            {
                "arxivId": "1601.05890",
                "title": "Covariate balancing propensity score by tailored loss functions",
                "abstract": "In observational studies, propensity scores are commonly estimated by maxi- mum likelihood but may fail to balance high-dimensional pre-treatment covariates even after specification search. We introduce a general framework that unifies and generalizes several recent proposals to improve covariate balance when designing an observational study. In- stead of the likelihood function, we propose to optimize special loss functions---covariate balancing scoring rules (CBSR)---to estimate the propensity score. A CBSR is uniquely determined by the link function in the GLM and the estimand (a weighted average treatment effect). We show CBSR does not lose asymptotic efficiency to the Bernoulli likelihood in estimating the weighted average treatment effect compared, but CBSR is much more robust in finite sample. Borrowing tools developed in statistical learning, we propose practical strategies to balance covariate functions in rich function classes. This is useful to estimate the maximum bias of the inverse probability weighting (IPW) estimators and construct honest confidence interval in finite sample. Lastly, we provide several numerical examples to demonstrate the trade-off of bias and variance in the IPW-type estimators and the trade-off in balancing different function classes of the covariates."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2403.04029",
        "category": "econ",
        "title": "Two-Person adversarial games are zero-sum: A resolution of the Luce-Raiffa-Aumann (LRA) conjecture",
        "abstract": "This letter: (i) reformulates the theorems of Adler-Daskalakis-Papadimitriou (2009) and Raimondo (2023) on two-player adversarial games as a generalized result with a simplified proof, (ii) forges connections to work on strategically zero-sum games by Moulin-Vial (1978), and on axiomatizations of multi-linear utilities of n-person games by Fishburn-Roberts (1976, 1978). The simplification and the connections on offer give prominence to two-person zero-sum games studied by Aumann (1961), Shapley (1964) and Rosenthal (1974), and also to recent algorithmic work in computer science. We give a productive reorientation to the subject by bringing the two communities together under the rubric of adversarial games.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2403.07896",
        "category": "econ",
        "title": "SACR\u00c9 BLEU: Self-Assessed Creator Royalties \u00c9nforced by Balancing Liquidity Estimation & Utility (A formal definition and analysis of Ethereum Request for Comment ERC-7526)",
        "abstract": "The secondary market for Ethereum non-fungible tokens (NFTs) has resulted in over $1.8bn being paid to creators in the form of a sales tax commonly called creator royalties. This was despite royalty payments being enforced by no more than social contract alone. Predictably, such an incentive structure led to zero-royalty alternatives becoming abundant and payments dwindled. A purely programmatic solution to royalty enforcement is hampered by the prevailing NFT standard, ERC-721, which is ignorant of sale values and royalty enforcement therefore relies on (potentially dishonest) third parties. We thus introduce an incentive-compatible mechanism for which there is a single rationalisable solution, in which royalties are paid in full, while maintaining full ERC-721 compatibility. The mechanism constitutes the core of ERC-7526.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2403.08031",
        "category": "econ",
        "title": "Score-based mechanisms",
        "abstract": "We propose a mechanism design framework that incorporates both soft information, which can be freely manipulated, and semi-hard information, which entails a cost for falsification. The framework captures various contexts such as school choice, public housing, organ transplant and manipulations of classification algorithms. We first provide a canonical class of mechanisms for these settings. The key idea is to treat the submission of hard information as an observable and payoff-relevant action and the contractible part of the mechanism as a mapping from submitted scores to a distribution over decisions (a score-based decision rule). Each type report triggers a distribution over score submission requests and a distribution over decision rules. We provide conditions under which score-based mechanisms are without loss of generality. In other words, situations under which the agent does not make any type reports and decides without a mediator what score to submit in a score-based decision rule. We proceed to characterize optimal approval mechanisms in the presence of manipulable hard information. In several leading settings optimal mechanisms are score-based (and thus do not rely on soft information) and involve costly screening. The solution methodology we employ is suitable both for concave cost functions and quadratic costs and is applicable to a wide range of contexts in economics and in computer science.",
        "references": [
            {
                "arxivId": "2005.08377",
                "title": "The Role of Randomness and Noise in Strategic Classification",
                "abstract": "We investigate the problem of designing optimal classifiers in the strategic classification setting, where the classification is part of a game in which players can modify their features to attain a favorable classification outcome (while incurring some cost). Previously, the problem has been considered from a learning-theoretic perspective and from the algorithmic fairness perspective. Our main contributions include 1. Showing that if the objective is to maximize the efficiency of the classification process (defined as the accuracy of the outcome minus the sunk cost of the qualified players manipulating their features to gain a better outcome), then using randomized classifiers (that is, ones where the probability of a given feature vector to be accepted by the classifier is strictly between 0 and 1) is necessary. 2. Showing that in many natural cases, the imposed optimal solution (in terms of efficiency) has the structure where players never change their feature vectors (the randomized classifier is structured in a way, such that the gain in the probability of being classified as a 1 does not justify the expense of changing one's features). 3. Observing that the randomized classification is not a stable best-response from the classifier's viewpoint, and that the classifier doesn't benefit from randomized classifiers without creating instability in the system. 4. Showing that in some cases, a noisier signal leads to better equilibria outcomes -- improving both accuracy and fairness when more than one subpopulation with different feature adjustment costs are involved. This is interesting from a policy perspective, since it is hard to force institutions to stick to a particular randomized classification strategy (especially in a context of a market with multiple classifiers), but it is possible to alter the information environment to make the feature signals inherently noisier."
            },
            {
                "arxivId": "1909.01888",
                "title": "Scoring Strategic Agents",
                "abstract": "I introduce a model of scoring. An intermediary aggregates multiple features of a sender into a score. A receiver sees this score and takes a decision. The receiver wants his decision to match the sender's latent characteristic. But the sender wants the most favorable decision, and she can distort each of her features at a private cost. I characterize the receiver-optimal scoring rule. This rule underweights some features to deter sender distortion, and overweights other features to keep the score unbiased. The receiver prefers this score to seeing the sender's full features because the coarser information mitigates his commitment problem."
            },
            {
                "arxivId": "1908.10330",
                "title": "Improving Information from Manipulable Data",
                "abstract": "\n Data-based decision making must account for the manipulation of data by agents who are aware of how decisions are being made and want to affect their allocations. We study a framework in which, due to such manipulation, data become less informative when decisions depend more strongly on data. We formalize why and how a decision maker should commit to underutilizing data. Doing so attenuates information loss and thereby improves allocation accuracy."
            },
            {
                "arxivId": "1808.08646",
                "title": "The Disparate Effects of Strategic Manipulation",
                "abstract": "When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed \"strategic manipulation,\" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to \"trick\" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off---even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's \"quality\" when agents' capacities to adaptively respond differ."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2403.08102",
        "category": "econ",
        "title": "Tournament Auctions",
        "abstract": "We examine ``tournament'' second-price auctions in which $N$ bidders compete for the right to participate in a second stage and contend against bidder $N+1$. When the first $N$ bidders are committed so that their bids cannot be changed in the second stage, the analysis yields some unexpected results. The first $N$ bidders consistently bid above their values in equilibrium. When bidder $N+1$ is sufficiently stronger than the first $N$, overbidding leads to an increase in expected revenue in comparison to the standard second-price auction when $N$ is large.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2403.08130",
        "category": "econ",
        "title": "Imputation of Counterfactual Outcomes when the Errors are Predictable",
        "abstract": "A crucial input into causal inference is the imputed counterfactual outcome. Imputation error can arise because of sampling uncertainty from estimating the prediction model using the untreated observations, or from out-of-sample information not captured by the model. While the literature has focused on sampling uncertainty, it vanishes with the sample size. Often overlooked is the possibility that the out-of-sample error can be informative about the missing counterfactual outcome if it is mutually or serially correlated. Motivated by the best linear unbiased predictor (\\blup) of \\citet{goldberger:62} in a time series setting, we propose an improved predictor of potential outcome when the errors are correlated. The proposed \\pup\\; is practical as it is not restricted to linear models, can be used with consistent estimators already developed, and improves mean-squared error for a large class of strong mixing error processes. Ignoring predictability in the errors can distort conditional inference. However, the precise impact will depend on the choice of estimator as well as the realized values of the residuals.",
        "references": [
            {
                "arxivId": "2303.01863",
                "title": "Constructing High Frequency Economic Indicators by Imputation",
                "abstract": "\n Monthly and weekly economic indicators are often taken to be the largest common factor estimated from high and low frequency data, either separately or jointly. To incorporate mixed frequency information without directly modeling them, we target a low frequency diffusion index that is already available, and treat high frequency values as missing. We impute these values using multiple factors estimated from the high frequency data. In the empirical examples considered, static matrix completion that does not account for serial correlation in the idiosyncratic errors yields imprecise estimates of the missing values irrespective of how the factors are estimated. Single equation\u00a0and systems-based dynamic procedures that account for serial correlation yield imputed values that are closer to the observed low frequency ones. This is the case in the counterfactual exercise that imputes the monthly values of consumer sentiment series before 1978 when the data was released only on a quarterly basis. This is also the case for a weekly version of the CFNAI index of economic activity that is imputed using seasonally unadjusted data. The imputed series reveals episodes of increased variability of weekly economic information that are masked by the monthly data, notably around the 2014-15 collapse in oil prices."
            },
            {
                "arxivId": "2202.12078",
                "title": "Confidence intervals of treatment effects in panel data models with interactive fixed effects",
                "abstract": null
            },
            {
                "arxivId": "2011.03996",
                "title": "Do We Exploit all Information for Counterfactual Analysis? Benefits of Factor Models and Idiosyncratic Correction",
                "abstract": "Abstract Optimal pricing, that is determining the price level that maximizes profit or revenue of a given product, is a vital task for the retail industry. To select such a quantity, one needs first to estimate the price elasticity from the product demand. Regression methods usually fail to recover such elasticities due to confounding effects and price endogeneity. Therefore, randomized experiments are typically required. However, elasticities can be highly heterogeneous depending on the location of stores, for example. As the randomization frequently occurs at the municipal level, standard difference-in-differences methods may also fail. Possible solutions are based on methodologies to measure the effects of treatments on a single (or just a few) treated unit(s) based on counterfactuals constructed from artificial controls. For example, for each city in the treatment group, a counterfactual may be constructed from the untreated locations. In this article, we apply a novel high-dimensional statistical method to measure the effects of price changes on daily sales from a major retailer in Brazil. The proposed methodology combines principal components (factors) and sparse regressions, resulting in a method called Factor-Adjusted Regularized Method for Treatment evaluation (FarmTreat). The data consist of daily sales and prices of five different products over more than 400 municipalities. The products considered belong to the sweet and candies category and experiments have been conducted over the years of 2016 and 2017. Our results confirm the hypothesis of a high degree of heterogeneity yielding very different pricing strategies over distinct municipalities. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "2006.06138",
                "title": "Conformal inference of counterfactuals and individual treatment effects",
                "abstract": "Evaluating treatment effect heterogeneity widely informs treatment decision making. At the moment, much emphasis is placed on the estimation of the conditional average treatment effect via flexible machine learning algorithms. While these methods enjoy some theoretical appeal in terms of consistency and convergence rates, they generally perform poorly in terms of uncertainty quantification. This is troubling since assessing risk is crucial for reliable decision\u2010making in sensitive and uncertain environments. In this work, we propose a conformal inference\u2010based approach that can produce reliable interval estimates for counterfactuals and individual treatment effects under the potential outcome framework. For completely randomized or stratified randomized experiments with perfect compliance, the intervals have guaranteed average coverage in finite samples regardless of the unknown data generating mechanism. For randomized experiments with ignorable compliance and general observational studies obeying the strong ignorability assumption, the intervals satisfy a doubly robust property which states the following: the average coverage is approximately controlled if either the propensity score or the conditional quantiles of potential outcomes can be estimated accurately. Numerical studies on both synthetic and real data sets empirically demonstrate that existing methods suffer from a significant coverage deficit even in simple models. In contrast, our methods achieve the desired coverage with reasonably short intervals."
            },
            {
                "arxivId": "1912.07120",
                "title": "Prediction Intervals for Synthetic Control Methods",
                "abstract": "Abstract Uncertainty quantification is a fundamental problem in the analysis and interpretation of synthetic control (SC) methods. We develop conditional prediction intervals in the SC framework, and provide conditions under which these intervals offer finite-sample probability guarantees. Our method allows for covariate adjustment and nonstationary data. The construction begins by noting that the statistical uncertainty of the SC prediction is governed by two distinct sources of randomness: one coming from the construction of the (likely misspecified) SC weights in the pretreatment period, and the other coming from the unobservable stochastic error in the post-treatment period when the treatment effect is analyzed. Accordingly, our proposed prediction intervals are constructed taking into account both sources of randomness. For implementation, we propose a simulation-based approach along with finite-sample-based probability bound arguments, naturally leading to principled sensitivity analysis methods. We illustrate the numerical performance of our methods using empirical applications and a small simulation study. Python, R and Stata software packages implementing our methodology are available. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1910.06677",
                "title": "Matrix Completion, Counterfactuals, and Factor Analysis of Missing Data",
                "abstract": "Abstract This article proposes an imputation procedure that uses the factors estimated from a tall block along with the re-rotated loadings estimated from a wide block to impute missing values in a panel of data. Assuming that a strong factor structure holds for the full panel of data and its sub-blocks, it is shown that the common component can be consistently estimated at four different rates of convergence without requiring regularization or iteration. An asymptotic analysis of the estimation error is obtained. An application of our analysis is estimation of counterfactuals when potential outcomes have a factor structure. We study the estimation of average and individual treatment effects on the treated and establish a normal distribution theory that can be useful for hypothesis testing."
            },
            {
                "arxivId": "1909.01782",
                "title": "Inference in Difference\u2010in\u2010Differences: How Much Should We Trust in Independent Clusters?",
                "abstract": "We analyze the challenges for inference in difference-in-differences (DID) when there is spatial correlation. We present novel theoretical insights and empirical evidence on the settings in which ignoring spatial correlation should lead to more or less distortions in DID applications. We show that details such as the time frame used in the estimation, the choice of the treated and control groups, and the choice of the estimator, are key determinants of distortions due to spatial correlation. We also analyze the feasibility and trade-offs involved in a series of alternatives to take spatial correlation into account. Given that, we provide relevant recommendations for applied researchers on how to mitigate and assess the possibility of inference distortions due to spatial correlation."
            },
            {
                "arxivId": "1811.04170",
                "title": "The Augmented Synthetic Control Method",
                "abstract": "Abstract The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The \u201csynthetic control\u201d is a weighted average of control units that balances the treated unit\u2019s pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package."
            },
            {
                "arxivId": "1712.09089",
                "title": "An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls",
                "abstract": "Abstract We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available."
            },
            {
                "arxivId": "1911.08521",
                "title": "Synthetic controls with imperfect pretreatment fit",
                "abstract": "We analyze the properties of the Synthetic Control (SC) and related estimators when the pre\u2010treatment fit is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre\u2010treatment periods goes to infinity. Still, we show that a demeaned version of the SC method can improve in terms of bias and variance relative to the difference\u2010in\u2010difference estimator. We also derive a specification test for the demeaned SC estimator in this setting with imperfect pre\u2010treatment fit. Given our theoretical results, we provide practical guidance for applied researchers on how to justify the use of such estimators in empirical applications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-14.json",
        "arxivId": "2403.08230",
        "category": "econ",
        "title": "A vicious cycle along busy bus corridors and how to abate it",
        "abstract": "We unveil that a previously-unreported vicious cycle can be created when bus queues form at curbside stops along a corridor. Buses caught in this cycle exhibit growing variation in headways as they travel from stop to stop. Bus (and patron) delays accumulate in like fashion and can grow large on long, busy corridors. We show that this damaging cycle can be abated in simple ways. Present solutions entail holding buses at a corridor entrance and releasing them as per various strategies proposed in the literature. We introduce a modest variant to the simplest of these strategies. It releases buses at headways that are slightly less than, or equal to, the scheduled values. It turns out that periodically releasing buses at slightly smaller headways can substantially reduce bus delays caused by holding so that benefits can more readily outweigh costs in corridors that contain a sufficient number of serial bus stops. The simple variant is shown to perform about as well as, or better than, other bus-holding strategies in terms of saving delays, and is more effective than other strategies in regularizing bus headways. We also show that grouping buses from across multiple lines and holding them by group can be effective when patrons have the flexibility to choose buses from across all lines in a group. Findings come by formulating select models of bus-corridor dynamics and using these to simulate part of the Bus Rapid Transit corridor in Guangzhou, China.",
        "references": [
            {
                "arxivId": "1806.09254",
                "title": "Effect of transit signal priority on bus service reliability",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-15.json",
        "arxivId": "2403.08886",
        "category": "econ",
        "title": "Measuring the bioeconomy economically: exploring the connections between concepts, methods, data, indicators and their limitations",
        "abstract": "Despite its relevance, measuring the contributions of the bioeconomy to national economies remains an arduous task that faces limitations. Part of the difficulty is associated with the lack of a clear and widely accepted concept of the bioeconomy and moves on to the connections between methods, data and indicators. The present study aims to define the concepts of bioeconomy and to explore the connections between concepts, methods, data and indicators when measuring the bioeconomy economically, and the limitations involved in this process. The bioeconomy concepts were defined based on a literature review and a content analysis of 84 documents selected through snowballing procedures to find articles measuring 'how big is the bioeconomy?'. The content of the 84 documents was uploaded to the QDA Miner software and coded according to the bioeconomy concept, the methods or models used, the data sources accessed, the indicators calculated, and the limitations reported by the authors. The results of the occurrence and co-occurrence of the codes were extracted and analyzed statistically, indicating that the measurement of bioeconomy (i) need recognize and pursue the proposed concept of holistic bioeconomy; (ii) rarely considered aspects of holistic bioeconomy (3.5%); (iii) is primarily based on the concept of biomass-based bioeconomy (BmBB) (94%); (iv) the association with the concept of biosphere (BsBB) appeared in 26% of the studies; (v) the biotech-based bioeconomy (BtBB) was the least frequent (1.2%); (vi) there is a diversity of methods and models, but the most common are those traditionally used to measure macroeconomic activities, especially input-output models; (vii) depending on the prevailing methods, the data comes from various official statistical databases, such as national accounts and economic activity classification systems;...",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-15.json",
        "arxivId": "2403.09045",
        "category": "econ",
        "title": "Entangled vs. Separable Choice",
        "abstract": "We study joint probabilistic choice rules that describe the behavior of two decision makers, each facing a possibly different menu. These choice rules are separable when they can be factored into autonomous choices from each individual solely correlated through their individual probabilistic choice rules. Despite recent interest in studying such rules, a complete characterization of the restrictions on them remains an open question. A reasonable conjecture is that such restrictions on separable joint choice can be factored into individual choice restrictions. We name these restrictions separable and show that this conjecture is true if and only if the probabilistic choice rule of at least one decision maker uniquely identifies the distribution over deterministic choice rules. Otherwise, entangled choice rules exist that satisfy separable restrictions yet are not separable. The possibility of entangled choice complicates the characterization of separable choice since one needs to augment the separable restrictions with the new emerging ones.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-15.json",
        "arxivId": "2403.09155",
        "category": "econ",
        "title": "News Media as Suppliers of Narratives (and Information)",
        "abstract": "We present a model of news media that shape consumer beliefs by providing information (signals about an exogenous state) and narratives (models of what determines outcomes). To amplify consumers' engagement, media maximize consumers' anticipatory utility. Focusing on a class of separable consumer preferences, we show that a monopolistic media platform facing homogenous consumers provides a false\"empowering\"narrative coupled with an optimistically biased signal. Consumer heterogeneity gives rise to a novel menu-design problem due to a\"data externality\"among consumers. The optimal menu features multiple narratives and creates polarized beliefs. These effects also arise in a competitive media market model.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-15.json",
        "arxivId": "2403.09470",
        "category": "econ",
        "title": "Climate Immobility Traps: A Household-Level Test",
        "abstract": "The complex relationship between climate shocks, migration, and adaptation hampers a rigorous understanding of the heterogeneous mobility outcomes of farm households exposed to climate risk. To unpack this heterogeneity, the analysis combines longitudinal multi-topic household survey data from Nigeria with a causal machine learning approach, tailored to a conceptual framework bridging economic migration theory and the poverty traps literature. The results show that pre-shock asset levels, in situ adaptive capacity, and cumulative shock exposure drive not just the magnitude but also the sign of the impact of agriculture-relevant weather anomalies on the mobility outcomes of farming households. While local adaptation acts as a substitute for migration, the roles played by wealth constraints and repeated shock exposure suggest the presence of climate-induced immobility traps.",
        "references": [
            {
                "arxivId": "1902.07409",
                "title": "Estimating Treatment Effects with Causal Forests: An Application",
                "abstract": "Abstract:We apply causal forests to a dataset derived from the National Study of Learning Mindsets, and discusses resulting practical and conceptual challenges. This note will appear in an upcoming issue of Observational Studies, Empirical Investigation of Methods for Heterogeneity, that compiles several analyses of the same dataset."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2104.14737",
        "category": "econ",
        "title": "Automatic Debiased Machine Learning via Riesz Regression",
        "abstract": "A variety of interesting parameters may depend on high dimensional regressions. Machine learning can be used to estimate such parameters. However estimators based on machine learners can be severely biased by regularization and/or model selection. Debiased machine learning uses Neyman orthogonal estimating equations to reduce such biases. Debiased machine learning generally requires estimation of unknown Riesz representers. A primary innovation of this paper is to provide Riesz regression estimators of Riesz representers that depend on the parameter of interest, rather than explicit formulae, and that can employ any machine learner, including neural nets and random forests. End-to-end algorithms emerge where the researcher chooses the parameter of interest and the machine learner and the debiasing follows automatically. Another innovation here is debiased machine learners of parameters depending on generalized regressions, including high-dimensional generalized linear models. An empirical example of automatic debiased machine learning using neural nets is given. We find in Monte Carlo examples that automatic debiasing sometimes performs better than debiasing via inverse propensity scores and never worse. Finite sample mean square error bounds for Riesz regression estimators and asymptotic theory are also given.",
        "references": [
            {
                "arxivId": "2110.03031",
                "title": "RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests",
                "abstract": "Many causal and policy effects of interest are defined by linear functionals of high-dimensional or non-parametric regression functions. $\\sqrt{n}$-consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, which leads to properties such as semi-parametric efficiency, double robustness, and Neyman orthogonality. We implement an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method only relies on black-box evaluation oracle access to the linear functional and does not require knowledge of its analytic form. We propose a multitasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Riesz representer and regression loss, while sharing representation layers for the two functions. We also propose a Random Forest method which learns a locally linear representation of the Riesz function. Even though our method applies to arbitrary functionals, we experimentally find that it performs well compared to the state of art neural net based algorithm of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the problem of estimating average marginal effects with continuous treatments, using semi-synthetic data of gasoline price changes on gasoline demand."
            },
            {
                "arxivId": "2102.11076",
                "title": "Debiased Kernel Methods",
                "abstract": "I propose a practical procedure based on bias correction and sample splitting to calculate confidence intervals for functionals of generic kernel methods, i.e. nonparametric estimators learned in a reproducing kernel Hilbert space (RKHS). For example, an analyst may desire confidence intervals for functionals of kernel ridge regression or kernel instrumental variable regression. The framework encompasses (i) evaluations over discrete domains, (ii) treatment effects of discrete treatments, and (iii) incremental treatment effects of continuous treatments. For the target quantity, whether it is (i)-(iii), I prove pointwise root-n consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. I show that the classic assumptions of RKHS learning theory also imply inference."
            },
            {
                "arxivId": "2101.00009",
                "title": "Adversarial Estimation of Riesz Representers",
                "abstract": "Many causal parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Our estimators are highly compatible with targeted and debiased machine learning with sample splitting; our guarantees directly verify general conditions for inference that allow mis-specification. We also use our guarantees to prove inference without sample splitting, based on stability or complexity. Our estimators achieve nominal coverage in highly nonlinear simulations where some previous methods break down. They shed new light on the heterogeneous effects of matching grants."
            },
            {
                "arxivId": "2012.05187",
                "title": "Smoothed Quantile Regression with Large-Scale Inference.",
                "abstract": null
            },
            {
                "arxivId": "2010.04855",
                "title": "Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves",
                "abstract": "\n We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous and incremental response curves. The treatment and covariates may be discrete or continuous in general spaces. Due to a decomposition property specific to the reproducing kernel Hilbert space, our estimators have simple closed form solutions. We prove uniform consistency with finite sample rates via an original analysis of generalized kernel ridge regression. We extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. We achieve state-of-the-art performance in nonlinear simulations with many covariates, and conduct a policy evaluation of the US Job Corps training program for disadvantaged youths."
            },
            {
                "arxivId": "2007.03210",
                "title": "Estimation and Inference with Trees and Forests in High Dimensions",
                "abstract": "We analyze the finite sample mean squared error (MSE) performance of regression trees and forests in the high dimensional regime with binary features, under a sparsity constraint. We prove that if only $r$ of the $d$ features are relevant for the mean outcome function, then shallow trees built greedily via the CART empirical MSE criterion achieve MSE rates that depend only logarithmically on the ambient dimension $d$. We prove upper bounds, whose exact dependence on the number relevant variables $r$ depends on the correlation among the features and on the degree of relevance. For strongly relevant features, we also show that fully grown honest forests achieve fast MSE rates and their predictions are also asymptotically normal, enabling asymptotically valid inference that adapts to the sparsity of the regression function."
            },
            {
                "arxivId": "1909.05244",
                "title": "Double Robustness for Complier Parameters and a Semiparametric Test for Complier Characteristics",
                "abstract": "\n We propose a semiparametric test to evaluate (a) whether different instruments induce subpopulations of compliers with the same observable characteristics, on average; and (b) whether compliers have observable characteristics that are the same as the full population, treated subpopulation, or untreated subpopulation, on average. The test is a flexible robustness check for the external validity of instruments. To justify the test, we characterize the doubly robust moment for Abadie Abadie (2003)'s class of complier parameters, and we analyse a machine learning update to \u03ba weighting that we call the automatic \u03ba weight. We use the test to reinterpret the difference in local average treatment effect estimates that Angrist and Evans (1998a) obtain when using different instrumental variables."
            },
            {
                "arxivId": "1901.09036",
                "title": "Orthogonal Statistical Learning",
                "abstract": "We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be estimated from data (a \"nuisance model\"). We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target model and one for the nuisance model. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning literature to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate the case where the target parameter belongs to a complex nonparametric class. We characterize conditions on the metric entropy such that oracle rates---rates of the same order as if we knew the nuisance model---are achieved. We also analyze the rates achieved by specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance in the literature: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data."
            },
            {
                "arxivId": "1809.05224",
                "title": "Automatic Debiased Machine Learning of Causal and Structural Effects",
                "abstract": "Many causal and structural effects depend on regressions. Examples include policy effects, average derivatives, regression decompositions, average treatment effects, causal mediation, and parameters of economic structural models. The regressions may be high\u2010dimensional, making machine learning useful. Plugging machine learners into identifying equations can lead to poor inference due to bias from regularization and/or model selection. This paper gives automatic debiasing for linear and nonlinear functions of regressions. The debiasing is automatic in using Lasso and the function of interest without the full form of the bias correction. The debiasing can be applied to any regression learner, including neural nets, random forests, Lasso, boosting, and other high\u2010dimensional methods. In addition to providing the bias correction, we give standard errors that are robust to misspecification, convergence rates for the bias correction, and primitive conditions for asymptotic inference for estimators of a variety of estimators of structural and causal effects. The automatic debiased machine learning is used to estimate the average treatment effect on the treated for the NSW job training data and to estimate demand elasticities from Nielsen scanner data while allowing preferences to be correlated with prices and income."
            },
            {
                "arxivId": "1802.08667",
                "title": "Double/De-Biased Machine Learning of Global and Local Parameters Using Regularized Riesz Representers",
                "abstract": "We provide adaptive inference methods, based on l1 regularization methods, for regular (semi-parametric) and non-regular (nonparametric) linear functionals of the conditional expectation function. Examples of regular functionals include average treatment effects, policy effects from covariate distribution shifts and stochastic transformations, and average derivatives. Examples of non-regular functionals include the local linear functionals defined as local averages that approximate perfectly localized quantities: average treatment, average policy effects, and average derivatives, conditional on a covariate subvector fixed at a point. Our construction relies on building Neyman orthogonal equations for the target parameter that are approximately invariant to small perturbations of the nuisance parameters. To achieve this property we include the linear Riesz representer for the functionals in the equations as the additional nuisance parameter. We use l1-regularized methods to learn approximations to the regression function and the linear representer, in settings where dimension of (possibly overcomplete) dictionary of basis functions P is much larger than N. We then estimate the linear functional by the solution to the empirical analog of the orthogonal equations. Our key result is that under weak assumptions the estimator of the functional concentrates in a L/root(n) neighborhood of the target with deviations controlled by the Gaussian law, provided L/root(n) \\to 0; L is the operator norm of the functional, measuring the degree of its non-regularity, with L diverging for local functionals (or under weak identification of the global functionals)."
            },
            {
                "arxivId": "1712.00038",
                "title": "Augmented minimax linear estimation",
                "abstract": "Many statistical estimands can expressed as continuous linear functionals of a conditional expectation function. This includes the average treatment effect under unconfoundedness and generalizations for continuous-valued and personalized treatments. In this paper, we discuss a general approach to estimating such quantities: we begin with a simple plug-in estimator based on an estimate of the conditional expectation function, and then correct the plug-in estimator by subtracting a minimax linear estimate of its error. We show that our method is semiparametrically efficient under weak conditions and observe promising performance on both real and simulated data."
            },
            {
                "arxivId": "1801.09138",
                "title": "Cross-fitting and fast remainder rates for semiparametric estimation",
                "abstract": "There are many interesting and widely used estimators of a functional with ?nite semi-parametric variance bound that depend on nonparametric estimators of nuisance func-tions. We use cross-?tting to construct such estimators with fast remainder rates. We give cross-?t doubly robust estimators that use separate subsamples to estimate di?erent nuisance functions. We show that a cross-?t doubly robust spline regression estimator of the expected conditional covariance is semiparametric e?cient under minimal conditions. Corresponding estimators of other average linear functionals of a conditional expectation are shown to have the fastest known remainder rates under certain smoothness conditions. The cross-?t plug-in estimator shares some of these properties but has a remainder term that is larger than the cross-?t doubly robust estimator. As speci?c examples we consider the expected conditional covariance, mean with randomly missing data, and a weighted average derivative."
            },
            {
                "arxivId": "1708.06633",
                "title": "Nonparametric regression using deep neural networks with ReLU activation function",
                "abstract": "Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to $\\log n$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates."
            },
            {
                "arxivId": "1608.00033",
                "title": "Locally Robust Semiparametric Estimation",
                "abstract": "Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where first steps have no effect, locally, on average moment functions. Using these orthogonal moments reduces model selection and regularization bias, as is important in many applications, especially for machine learning first steps. Also, associated standard errors are robust to misspecification when there is the same number of moment functions as parameters of interest.\n We use these orthogonal moments and cross\u2010fitting to construct debiased machine learning estimators of functions of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that additional first steps needed for the orthogonal moment functions have no effect, globally, on average orthogonal moment functions. We give a general approach to estimating those additional first steps. We characterize double robustness and give a variety of new doubly robust moment functions. We give general and simple regularity conditions for asymptotic theory."
            },
            {
                "arxivId": "1602.08927",
                "title": "High-Dimensional $L_2$Boosting: Rate of Convergence",
                "abstract": "Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional setting. Moreover, we introduce so-called \\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal Boosting\\textquotedblright\\ where after each step an orthogonal projection is conducted. We show that both post-$L_2$Boosting and the orthogonal boosting achieve the same rate of convergence as LASSO in a sparse, high-dimensional setting. We show that the rate of convergence of the classical $L_2$Boosting depends on the design matrix described by a sparse eigenvalue constant. To show the latter results, we derive new approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. Our results also allow a direct comparison between LASSO and boosting which has been missing from the literature. Finally, we present simulation studies and applications to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms LASSO."
            },
            {
                "arxivId": "1508.01378",
                "title": "The influence function of semiparametric estimators",
                "abstract": "There are many economic parameters that depend on nonparametric first steps. Examples include games, dynamic discrete choice, average exact consumer surplus, and treatment effects. Often estimators of these parameters are asymptotically equivalent to a sample average of an object referred to as the influence function. The influence function is useful in local policy analysis, in evaluating local sensitivity of estimators, and constructing debiased machine learning estimators. We show that the influence function is a Gateaux derivative with respect to a smooth deviation evaluated at a point mass. This result generalizes the classic Von Mises (1947) and Hampel (1974) calculation to estimators that depend on smooth nonparametric first steps. We give explicit influence functions for first steps that satisfy exogenous or endogenous orthogonality conditions. We use these results to generalize the omitted variable bias formula for regression to policy analysis for and sensitivity to structural changes. We apply this analysis and find no sensitivity to endogeneity of average equivalent variation estimates in a gasoline demand application."
            },
            {
                "arxivId": "0801.1095",
                "title": "SIMULTANEOUS ANALYSIS OF LASSO AND DANTZIG SELECTOR",
                "abstract": "We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the l p estimation loss for 1 \u2264 p \u2264 2 in the linear model when the number of variables can be much larger than the sample size."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2205.08104",
        "category": "econ",
        "title": "Restricting Entries to All-Pay Contests",
        "abstract": "We study an all-pay contest where players with low abilities are filtered prior to the round of competing for prizes. These are often practiced due to limited resources or to enhance the competitiveness of the contest. We consider a setting where the designer admits a certain number of top players into the contest. The players admitted into the contest update their beliefs about their opponents based on the signal that their abilities are among the top. We find that their posterior beliefs, even with IID priors, are correlated and depend on players' private abilities, representing a unique feature of this game. We explicitly characterize the symmetric and unique Bayesian equilibrium strategy. We find that each admitted player's equilibrium effort is in general not monotone with the number of admitted players. Despite this non-monotonicity, surprisingly, all players exert their highest efforts when all players are admitted. This result holds generally -- it is true under any ranking-based prize structure, ability distribution, and cost function. We also discuss a two-stage extension where players with top first-stage efforts can proceed to the second stage competing for prizes.",
        "references": [
            {
                "arxivId": "2205.05207",
                "title": "Prizes and effort in contests with private information",
                "abstract": "Contests are situations in which agents compete with one another by exerting costly effort to win valuable prizes. In this paper, we consider contest environments where participants have private information about their ability and the contest designer can manipulate the values of different prizes to influence the effort exerted by them. In such environments, with the goal of understanding how different contests compare in terms of the effort they induce, we study the effect of two different interventions on effort: (1) increase in value of prizes (2) increase in competition (transfer of value from worse to better prizes) The effect of increasing the value of a prize on effort depends upon the rank of the prize. While increasing the value of the first prize encourages effort for all agent types and increasing the value of the last prize discourages effort for all agent types, the effect of increasing the value of any intermediate prize depends qualitatively on the distribution of abilities in the population. More precisely, we find that if the density of agents is increasing in ability so that productive agents are more likely than unproductive agents, increasing any intermediate prize discourages effort. And if the density of agents is decreasing in ability so that unproductive agents are more likely than productive agents, increasing any intermediate prize encourages effort. Intuitively, this is because increasing any intermediate prize reduces effort of the most productive agents while increasing the effort of the less productive agents. The overall expected effect then depends on the relative likelihood of the more productive and less productive agents."
            },
            {
                "arxivId": "1111.2893",
                "title": "Optimal Crowdsourcing Contests",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2305.05998",
        "category": "econ",
        "title": "On the Time-Varying Structure of the Arbitrage Pricing Theory using the Japanese Sector Indices",
        "abstract": "This paper is the first study to examine the time instability of the APT in the Japanese stock market. In particular, we measure how changes in each risk factor affect the stock risk premiums to investigate the validity of the APT over time, applying the rolling window method to Fama and MacBeth's (1973) two-step regression and Kamstra and Shi's (2023) generalized GRS test. We summarize our empirical results as follows: (1) the changes in monetary policy by major central banks greatly affect the validity of the APT in Japan, and (2) the time-varying estimates of the risk premiums for each factor are also unstable over time, and they are affected by the business cycle and economic crises. Therefore, we conclude that the validity of the APT as an appropriate model to explain the Japanese sector index is not stable over time.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2309.04193",
        "category": "econ",
        "title": "Robust equilibria in cheap-talk games with fairly transparent motives",
        "abstract": "For cheap-talk games with a binary state space in which the sender has state-independent preferences, we characterize equilibria that are robust to introducing slight state-dependence on the side of the sender. Not all equilibria are robust, but the sender-optimum is always achieved at some robust equilibrium.",
        "references": [
            {
                "arxivId": "2402.06765",
                "title": "Perfect Bayesian Persuasion",
                "abstract": "A sender commits to an experiment to persuade a receiver. Accounting for the sender's experiment-choice incentives, and not presupposing a receiver tie-breaking rule when indifferent, we characterize when the sender's equilibrium payoff is unique and so coincides with her\"Bayesian persuasion\"value. A sufficient condition in finite models is that every action which is receiver-optimal at some belief is uniquely optimal at some other belief -- a generic property. We similarly show the equilibrium sender payoff is typically unique in ordered models. In an extension, we show uniqueness generates robustness to imperfect sender commitment."
            },
            {
                "arxivId": "2302.00281",
                "title": "Informationally Robust Cheap-Talk",
                "abstract": "We study the robustness of cheap-talk equilibria to infinitesimal private information of the receiver in a model with a binary state-space and state-independent sender-preferences."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2311.10685",
        "category": "econ",
        "title": "High-Throughput Asset Pricing",
        "abstract": "We use empirical Bayes (EB) to mine data on 140,000 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. This\"high-throughput asset pricing\"produces out-of-sample performance comparable to strategies in top finance journals. But unlike the published strategies, the data-mined strategies are free of look-ahead bias. EB predicts that high returns are concentrated in accounting strategies, small stocks, and pre-2004 samples, consistent with limited attention theories. The intuition is seen in the cross-sectional distribution of t-stats, which is far from the null for equal-weighted accounting strategies. High-throughput methods provide a rigorous, unbiased method for documenting asset pricing facts.",
        "references": [
            {
                "arxivId": "2212.10317",
                "title": "Does Peer-Reviewed Research Help Predict Stock Returns?",
                "abstract": "Mining 29,000 accounting ratios for t-statistics over 2.0 leads to cross-sectional predictability similar to the peer review process. For both methods, about 50% of predictability remains after the original sample periods. Data mining generates other features of peer review including the rise in returns as original sample periods end, the speed of post-sample decay, and themes like investment, issuance, and accruals. Predictors supported by peer-reviewed risk explanations underperform data mining. Similarly, the relationship between modeling rigor and post-sample returns is negative. Our results suggest peer review systematically mislabels mispricing as risk, though only 18% of predictors are attributed to risk."
            },
            {
                "arxivId": "2204.10275",
                "title": "Do t-Statistic Hurdles Need to be Raised?",
                "abstract": "Many scholars have called for raising statistical hurdles to guard against false discoveries in academic publications. I show these calls may be difficult to justify empirically. Published data exhibit bias: results that fail to meet existing hurdles are often unobserved. These unobserved results must be extrapolated, which can lead to weak identification of revised hurdles. In contrast, statistics that can target only published findings (e.g. empirical Bayes shrinkage and the FDR) can be strongly identified, as data on published findings is plentiful. I demonstrate these results theoretically and in an empirical analysis of the cross-sectional return predictability literature."
            },
            {
                "arxivId": "2006.04269",
                "title": "False (and Missed) Discoveries in Financial Economics",
                "abstract": "Multiple testing plagues many important questions in finance such as fund and factor selection. We propose a new way to calibrate both Type I and Type II errors. Next, using a double-bootstrap method, we establish a t-statistic hurdle that is associated with a specific false discovery rate (e.g., 5%). We also establish a hurdle that is associated with a certain acceptable ratio of misses to false discoveries (Type II error scaled by Type I error), which effectively allows for differential costs of the two types of mistakes. Evaluating current methods, we find that they lack power to detect outperforming managers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2312.14355",
        "category": "econ",
        "title": "Optimal Strategies for the Decumulation of Retirement Savings under Differing Appetites for Liquidity and Investment Risks",
        "abstract": "A retiree's appetite for risk is a common input into the lifetime utility models that are traditionally used to find optimal strategies for the decumulation of retirement savings. In this work, we consider a retiree with potentially differing appetites for the key financial risks of decumulation: liquidity risk and investment risk. We set out to determine whether these differing risk appetites have a significant impact on the retiree's optimal choice of decumulation strategy. To do so, we design and implement a framework which selects the optimal decumulation strategy from a general set of admissible strategies in line with a retiree's goals, and under differing appetites for the key risks of decumulation. Overall, we find significant evidence to suggest that a retiree's differing appetites for different decumulation risks will impact their optimal choice of strategy at retirement. Through an illustrative example calibrated to the Australian context, we find results which are consistent with actual behaviours in this jurisdiction (in particular, a shallow market for annuities), which lends support to our framework and may provide some new insight into the so-called annuity puzzle.",
        "references": [
            {
                "arxivId": "1206.6268",
                "title": "Maximizing utility of consumption subject to a constraint on the probability of lifetime ruin",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2403.09899",
        "category": "econ",
        "title": "Prediction of retail chain failure: examples of recent U.S. retail failures",
        "abstract": "Over the last several years, several well-established and prominent brick-and-mortar retail chains have ceased operations, raising concerns for something that some have referred to as a retail apocalypse. While the demise of brick-and-mortar is far from certain, scholars have attempted to model the likelihood that a retailer is about to fail using different approaches. This paper examines the failures of Bed Bath and Beyond, J.C. Penney, Rite Aid, and Sears Holdings in the United States between 2013 and 2022. A model of retail failure is presented that considers internal and external firm factors using both annual report and macroeconomic data. The findings suggest that certain revenue-based financial ratios and the annual average U.S. inflation rates are statistically significant predictors of failure. Furthermore, the failure model demonstrated that it can provide a nontrivial early warning signal at least the year before failure. The paper concludes with a discussion and directions for future research.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2403.10208",
        "category": "econ",
        "title": "Irrational Random Utility Models",
        "abstract": "We show that the set of aggregate choices of a population of rational decision-makers - random utility models (RUMs) - can be represented by a population of irrational ones if, and only if, their preferences are sufficiently uncorrelated. We call this representation: Irrational RUM. We then show that almost all RUMs can be represented by a population in which at least some decision-makers are irrational and that under specific conditions their irrational behavior is unconstrained.",
        "references": [
            {
                "arxivId": "2102.05570",
                "title": "Identification in the random utility model",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2403.10239",
        "category": "econ",
        "title": "A Big Data Approach to Understand Sub-national Determinants of FDI in Africa",
        "abstract": "Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based on text mining and social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in determining foreign ownership.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-18.json",
        "arxivId": "2403.10352",
        "category": "econ",
        "title": "Goodness-of-Fit for Conditional Distributions: An Approach Using Principal Component Analysis and Component Selection",
        "abstract": "This paper introduces a novel goodness-of-fit test technique for parametric conditional distributions. The proposed tests are based on a residual marked empirical process, for which we develop a conditional Principal Component Analysis. The obtained components provide a basis for various types of new tests in addition to the omnibus one. Component tests that based on each component serve as experts in detecting certain directions. Smooth tests that assemble a few components are also of great use in practice. To further improve testing efficiency, we introduce a component selection approach, aiming to identify the most contributory components. The finite sample performance of the proposed tests is illustrated through Monte Carlo experiments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "1905.13660",
        "category": "econ",
        "title": "On Policy Evaluation with Aggregate Time-Series Shocks",
        "abstract": "We propose a general strategy for estimating treatment effects, in contexts where the only source of exogenous variation is a sequence of aggregate time-series shocks. We start by arguing that commonly used estimation procedures tend to ignore the crucial time-series aspects of the data. Next, we develop a graphical tool and a novel test to illustrate the issues of the design using data from influential studies in development economics [Nunn and Qian, 2014] and macroeconomics [Nakamura and Steinsson, 2014]. Motivated by these studies, we construct a new estimator, which is based on the time-series model for the aggregate shock. We analyze the statistical properties of our estimator in the practically relevant case, where both cross-sectional and time-series dimensions are of similar size. Finally, to provide causal interpretation for our estimator, we analyze a new causal model that allows taking into account both rich unobserved heterogeneity in potential outcomes and unobserved aggregate shocks.",
        "references": [
            {
                "arxivId": "2306.14311",
                "title": "Simple estimation of semiparametric models with measurement errors",
                "abstract": "We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a\"corrected\"set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates, and settings with multivariate or non-classical EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models."
            },
            {
                "arxivId": "1909.09412",
                "title": "Double-Robust Identification for Causal Panel Data Models",
                "abstract": "\n We study identification and estimation of causal effects in settings with panel data. Traditionally researchers follow model-based identification strategies relying on assumptions governing the relation between the potential outcomes and the observed and unobserved confounders. We focus on a different, complementary approach to identification where assumptions are made about the connection between the treatment assignment and the unobserved confounders. Such strategies are common in cross-section\u00a0settings but rarely used with panel data. We introduce different sets of assumptions that follow the two paths to identification and develop a double robust approach. We propose estimation methods that build on these identification strategies."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1808.05293",
                "title": "Design-Based Analysis in Difference-in-Differences Settings with Staggered Adoption",
                "abstract": "In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is is an unbiased estimator of a particular weighted average causal effect. We characterize the proeperties of this estimand, and show that the standard variance estimator is conservative."
            },
            {
                "arxivId": "1806.07928",
                "title": "Shift-Share Designs: Theory and Inference",
                "abstract": "\n We study inference in shift-share regression designs, such as when a regional outcome is regressed on a weighted average of sectoral shocks, using regional sector shares as weights. We conduct a placebo exercise in which we estimate the effect of a shift-share regressor constructed with randomly generated sectoral shocks on actual labor market outcomes across U.S. commuting zones. Tests based on commonly used standard errors with 5% nominal significance level reject the null of no effect in up to 55% of the placebo samples. We use a stylized economic model to show that this overrejection problem arises because regression residuals are correlated across regions with similar sectoral shares, independent of their geographic location. We derive novel inference methods that are valid under arbitrary cross-regional correlation in the regression residuals. We show using popular applications of shift-share designs that our methods may lead to substantially wider confidence intervals in practice."
            },
            {
                "arxivId": "1806.01888",
                "title": "High-dimensional econometrics and regularized GMM",
                "abstract": "This chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. High-dimensional models are characterized by having a number of unknown parameters that is not vanishingly small relative to the sample size. We first present results in a framework where estimators of parameters of interest may be represented directly as approximate means. Within this context, we review fundamental results including high-dimensional central limit theorems, bootstrap approximation of high-dimensional limit distributions, and moderate deviation theory. We also review key concepts underlying inference when many parameters are of interest such as multiple testing with family-wise error rate or false discovery rate control. We then turn to a general high-dimensional minimum distance framework with a special focus on generalized method of moments problems where we present results for estimation and inference about model parameters. The presented results cover a wide array of econometric applications, and we discuss several leading special cases including high-dimensional linear regression and linear instrumental variables models to illustrate the general results."
            },
            {
                "arxivId": "1806.01221",
                "title": "Quasi-Experimental Shift-Share Research Designs",
                "abstract": "Many studies use shift-share (or \"Bartik\") instruments, which average a set of shocks with exposure share weights. We provide a new econometric framework for shift-share instrumental variable (SSIV) regressions in which identification follows from the quasi-random assignment of shocks, while exposure shares are allowed to be endogenous. The framework is motivated by an equivalence result: the orthogonality between a shift-share instrument and an unobserved residual can be represented as the orthogonality between the underlying shocks and a shock-level unobservable. SSIV regression coefficients can similarly be obtained from an equivalent shock-level regression, motivating shock-level conditions for their consistency. We discuss and illustrate several practical insights of this framework in the setting of Autor et al. (2013), estimating the effect of Chinese import competition on manufacturing employment across U.S. commuting zones."
            },
            {
                "arxivId": "1802.06475",
                "title": "A multivariate Berry\u2013Esseen theorem with explicit constants",
                "abstract": "We provide a Lyapunov type bound in the multivariate central limit theorem for sums of independent, but not necessarily identically distributed random vectors. The error in the normal approximation is estimated for certain classes of sets, which include the class of measurable convex sets. The error bound is stated with explicit constants. The result is proved by means of Stein\u2019s method. In addition, we improve the constant in the bound of the Gaussian perimeter of convex sets."
            },
            {
                "arxivId": "1710.10251",
                "title": "Matrix Completion Methods for Causal Panel Data Models",
                "abstract": "Abstract In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \u201cmissing\u201d elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data."
            },
            {
                "arxivId": "1706.01778",
                "title": "Sampling\u2010Based versus Design\u2010Based Uncertainty in Regression Analysis",
                "abstract": "Consider a researcher estimating the parameters of a regression function based on data for all 50 states in the United States or on data for all visits to a website. What is the interpretation of the estimated parameters and the standard errors? In practice, researchers typically assume that the sample is randomly drawn from a large population of interest and report standard errors that are designed to capture sampling variation. This is common even in applications where it is difficult to articulate what that population of interest is, and how it differs from the sample. In this article, we explore an alternative approach to inference, which is partly design\u2010based. In a design\u2010based setting, the values of some of the regressors can be manipulated, perhaps through a policy intervention. Design\u2010based uncertainty emanates from lack of knowledge about the values that the regression outcome would have taken under alternative interventions. We derive standard errors that account for design\u2010based uncertainty instead of, or in addition to, sampling\u2010based uncertainty. We show that our standard errors in general are smaller than the usual infinite\u2010population sampling\u2010based standard errors and provide conditions under which they coincide."
            },
            {
                "arxivId": "1610.07748",
                "title": "Balancing, Regression, Difference-in-Differences and Synthetic Control Methods: A Synthesis",
                "abstract": "In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units."
            },
            {
                "arxivId": "1607.06801",
                "title": "High-dimensional regression adjustments in randomized experiments",
                "abstract": "Significance As datasets get larger and more complex, there is a growing interest in using machine-learning methods to enhance scientific analysis. In many settings, considerable work is required to make standard machine-learning methods useful for specific scientific applications. We find, however, that in the case of treatment effect estimation with randomized experiments, regression adjustments via machine-learning methods designed to minimize test set error directly induce efficient estimates of the average treatment effect. Thus, machine-learning methods can be used out of the box for this task, without any special-case adjustments. We study the problem of treatment effect estimation in randomized experiments with high-dimensional covariate information and show that essentially any risk-consistent regression adjustment can be used to obtain efficient estimates of the average treatment effect. Our results considerably extend the range of settings where high-dimensional regression adjustments are guaranteed to provide valid inference about the population average treatment effect. We then propose cross-estimation, a simple method for obtaining finite-sample\u2013unbiased treatment effect estimates that leverages high-dimensional regression adjustments. Our method can be used when the regression model is estimated using the lasso, the elastic net, subset selection, etc. Finally, we extend our analysis to allow for adaptive specification search via cross-validation and flexible nonparametric regression adjustments with machine-learning methods such as random forests or neural networks."
            },
            {
                "arxivId": "1306.2872",
                "title": "Hanson-Wright inequality and sub-gaussian concentration",
                "abstract": "In this expository note, we give a modern proof of Hanson-Wright inequality for quadratic forms in sub-gaussian random variables.We deduce a useful concentration inequality for sub-gaussian random vectors.Two examples are given to illustrate these results: a concentration of distances between random vectors and subspaces, and a bound on the norms of products of random and deterministic matrices."
            },
            {
                "arxivId": "1208.2301",
                "title": "Agnostic notes on regression adjustments to experimental data: Reexamining Freedman's critique",
                "abstract": "Freedman [Adv. in Appl. Math. 40 (2008) 180-193; Ann. Appl. Stat. 2 (2008) 176-196] critiqued ordinary least squares regression adjustment of estimated treatment effects in randomized experiments, using Neyman's model for randomization inference. Contrary to conventional wisdom, he argued that adjustment can lead to worsened asymptotic precision, invalid measures of precision, and small-sample bias. This paper shows that in sufficiently large samples, those problems are either minor or easily fixed. OLS adjustment cannot hurt asymptotic precision when a full set of treatment-covariate interactions is included. Asymptotically valid confidence intervals can be constructed with the Huber-White sandwich standard error estimator. Checks on the asymptotic approximations are illustrated with data from Angrist, Lang, and Oreopoulos's [Am. Econ. J.: Appl. Econ. 1:1 (2009) 136--163] evaluation of strategies to improve college students' achievement. The strongest reasons to support Freedman's preference for unadjusted estimates are transparency and the dangers of specification search."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2112.14514",
        "category": "econ",
        "title": "Technology, Institution, and Regional Growth: Evidence from Mineral Mining Industry in Industrializing Japan",
        "abstract": "Coal extraction was an influential economic activity in interwar Japan. In the initial stage, coal mines used not only males but also females as the core workforce in the pits. However, the innovation of labor-saving technologies and the renewal of traditional extraction methodology induced institutional change through the revision of labor regulations on female miners in the early 1930s. This dramatically changed the mines as the place where skilled males were the principal miners engaged in the underground works. I investigate the impact of coal mining on regional growth and assess how the institutional change induced by the labor regulations affected its process. By linking the location information of mines with registration- and census-based statistics, I found that coal mines led to remarkable population growth. The labor regulations did not stagnate but accelerated the local population growth as they forced female miners to exit from the labor market and form their families. The regulations prohibited risky underground works by female miners. This reduction in occupational hazards also improved early-life mortality via the mortality selection mechanism in utero.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2210.02736",
        "category": "econ",
        "title": "Toward environmental sustainability: an empirical study on airports efficiency",
        "abstract": "The transition to more environmentally sustainable production processes and managerial practices is an increasingly important topic. Many industries need to undergo radical change to meet environmental sustainability requirements; the tourism industry is no exception. In this respect, a particular aspect that needs further attention is the relationship between airport performances and investments in environmental sustainability policies. This work represents a first attempt to provide empirical evidences about this relationship. Through the application of a non-parametrical method, we first assess the efficiency of the Italian airports industry. Secondly, we investigated the relationship between airports performance and management commitment toward the ecological transition using a Tobit regression model. The results show that airports adherence to formal multi-year ecological transition programs has a positive and consistent impact on their performance.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2306.14311",
        "category": "econ",
        "title": "Simple estimation of semiparametric models with measurement errors",
        "abstract": "We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a\"corrected\"set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates, and settings with multivariate or non-classical EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models.",
        "references": [
            {
                "arxivId": "2403.11309",
                "title": "Nonparametric identification and estimation with non-classical errors-in-variables",
                "abstract": "This paper considers nonparametric identification and estimation of the regression function when a covariate is mismeasured. The measurement error need not be classical. Employing the small measurement error approximation, we establish nonparametric identification under weak and easy-to-interpret conditions on the instrumental variable. The paper also provides nonparametric estimators of the regression function and derives their rates of convergence."
            },
            {
                "arxivId": "2104.00473",
                "title": "Normalizations and misspecification in skill formation models",
                "abstract": "An important class of structural models studies the determinants of skill formation and the optimal timing of interventions. In this paper, I provide new identification results for these models and investigate the effects of seemingly innocuous scale and location restrictions on parameters of interest. To do so, I first characterize the identified set of all parameters without these additional restrictions and show that important policy-relevant parameters are point identified under weaker assumptions than commonly used in the literature. The implications of imposing standard scale and location restrictions depend on how the model is specified, but they generally impact the interpretation of parameters and can affect counterfactuals. Importantly, with the popular CES production function, commonly used scale restrictions are overidentifying and lead to biased estimators. Consequently, simply changing the units of measurements of observed variables might yield ineffective investment strategies and misleading policy recommendations. I show how existing estimators can easily be adapted to solve these issues. As a byproduct, this paper also presents a general and formal definition of when restrictions are truly normalizations."
            },
            {
                "arxivId": "1808.07387",
                "title": "Sensitivity Analysis using Approximate Moment Condition Models",
                "abstract": "We consider inference in models defined by approximate moment conditions. We show that near\u2010optimal confidence intervals (CIs) can be formed by taking a generalized method of moments (GMM) estimator, and adding and subtracting the standard error times a critical value that takes into account the potential bias from misspecification of the moment conditions. In order to optimize performance under potential misspecification, the weighting matrix for this GMM estimator takes into account this potential bias and, therefore, differs from the one that is optimal under correct specification. To formally show the near\u2010optimality of these CIs, we develop asymptotic efficiency bounds for inference in the locally misspecified GMM setting. These bounds may be of independent interest, due to their implications for the possibility of using moment selection procedures when conducting inference in moment condition models. We apply our methods in an empirical application to automobile demand, and show that adjusting the weighting matrix can shrink the CIs by a factor of 3 or more."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2310.08536",
        "category": "econ",
        "title": "Real-time Prediction of the Great Recession and the COVID-19 Recession",
        "abstract": "A series of standard and penalized logistic regression models is employed to model and forecast the Great Recession and the Covid-19 recession in the US. These two recessions are scrutinized by closely examining the movement of five chosen predictors, their regression coefficients, and the predicted probabilities of recession. The empirical analysis explores the predictive content of numerous macroeconomic and financial indicators with respect to NBER recession indicator. The predictive ability of the underlying models is evaluated using a set of statistical evaluation metrics. The results strongly support the application of penalized logistic regression models in the area of recession prediction. Specifically, the analysis indicates that a mixed usage of different penalized logistic regression models over different forecast horizons largely outperform standard logistic regression models in the prediction of Great recession in the US, as they achieve higher predictive accuracy across 5 different forecast horizons. The Great Recession is largely predictable, whereas the Covid-19 recession remains unpredictable, given that the Covid-19 pandemic is a real exogenous event. The results are validated by constructing via principal component analysis (PCA) on a set of selected variables a recession indicator that suffers less from publication lags and exhibits a very high correlation with the NBER recession indicator.",
        "references": [
            {
                "arxivId": "1905.11744",
                "title": "Evaluating time series forecasting models: an empirical study on performance estimation methods",
                "abstract": null
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1105.0828",
                "title": "MissForest - non-parametric missing value imputation for mixed-type data",
                "abstract": "MOTIVATION\nModern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data, the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a non-parametric method which can cope with different types of variables simultaneously.\n\n\nRESULTS\nWe compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees, random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest, we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple datasets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10% to 30%. We show that missForest can successfully handle missing values, particularly in datasets including different types of variables. In our comparative study, missForest outperforms other methods of imputation especially in data settings where complex interactions and non-linear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data.\n\n\nAVAILABILITY\nThe package missForest is freely available from http://stat.ethz.ch/CRAN/.\n\n\nCONTACT\nstekhoven@stat.math.ethz.ch; buhlmann@stat.math.ethz.ch"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2310.17571",
        "category": "econ",
        "title": "Inside the black box: Neural network-based real-time prediction of US recessions (preprint)",
        "abstract": "Feedforward neural network (FFN) and two specific types of recurrent neural network, long short-term memory (LSTM) and gated recurrent unit (GRU), are used for modeling US recessions in the period from 1967 to 2021. The estimated models are then employed to conduct real-time predictions of the Great Recession and the Covid-19 recession in US. Their predictive performances are compared to those of the traditional linear models, the logistic regression model both with and without the ridge penalty. The out-of-sample performance suggests the application of LSTM and GRU in the area of recession forecasting, especially for the long-term forecasting tasks. They outperform other types of models across 5 forecasting horizons with respect to different types of statistical performance metrics. Shapley additive explanations (SHAP) method is applied to the fitted GRUs across different forecasting horizons to gain insight into the feature importance. The evaluation of predictor importance differs between the GRU and ridge logistic regression models, as reflected in the variable order determined by SHAP values. When considering the top 5 predictors, key indicators such as the S\\&P 500 index, real GDP, and private residential fixed investment consistently appear for short-term forecasts (up to 3 months). In contrast, for longer-term predictions (6 months or more), the term spread and producer price index become more prominent. These findings are supported by both local interpretable model-agnostic explanations (LIME) and marginal effects.",
        "references": [
            {
                "arxivId": "1704.02685",
                "title": "Learning Important Features Through Propagating Activation Differences",
                "abstract": "The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH."
            },
            {
                "arxivId": "1503.02101",
                "title": "Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition",
                "abstract": "We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points. In this paper we identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee."
            },
            {
                "arxivId": "1406.1078",
                "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
                "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2312.12741",
        "category": "econ",
        "title": "Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances",
        "abstract": "We address the problem of best arm identification (BAI) with a fixed budget for two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the best arm, an arm with the highest expected reward, through an adaptive experiment. Kaufmann et al. (2016) develops a lower bound for the probability of misidentifying the best arm. They also propose a strategy, assuming that the variances of rewards are known, and show that it is asymptotically optimal in the sense that its probability of misidentification matches the lower bound as the budget approaches infinity. However, an asymptotically optimal strategy is unknown when the variances are unknown. For this open issue, we propose a strategy that estimates variances during an adaptive experiment and draws arms with a ratio of the estimated standard deviations. We refer to this strategy as the Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW) strategy. We then demonstrate that this strategy is asymptotically optimal by showing that its probability of misidentification matches the lower bound when the budget approaches infinity, and the gap between the expected rewards of two arms approaches zero (small-gap regime). Our results suggest that under the worst-case scenario characterized by the small-gap regime, our strategy, which employs estimated variance, is asymptotically optimal even when the variances are unknown.",
        "references": [
            {
                "arxivId": "2312.12137",
                "title": "Best Arm Identification with Fixed Budget: A Large Deviation Perspective",
                "abstract": "We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated \\sr (Successive Rejects) algorithm \\citep{audibert2010best}, and (ii) to devise and analyze new algorithms. In particular, we present \\sred (Continuous Rejects), a truly adaptive algorithm that can reject arms in {\\it any} round based on the observed empirical gaps between the rewards of various arms. Applying our Large Deviation results, we prove that \\sred enjoys better performance guarantees than existing algorithms, including \\sr. Extensive numerical experiments confirm this observation."
            },
            {
                "arxivId": "2310.19788",
                "title": "Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget",
                "abstract": "This study investigates the experimental design problem for identifying the arm with the highest expected outcome, referred to as best arm identification (BAI). In our experiments, the number of treatment-allocation rounds is fixed. During each round, a decision-maker allocates an arm and observes a corresponding outcome, which follows a Gaussian distribution with variances that can differ among the arms. At the end of the experiment, the decision-maker recommends one of the arms as an estimate of the best arm. To design an experiment, we first discuss lower bounds for the probability of misidentification. Our analysis highlights that the available information on the outcome distribution, such as means (expected outcomes), variances, and the choice of the best arm, significantly influences the lower bounds. Because available information is limited in actual experiments, we develop a lower bound that is valid under the unknown means and the unknown choice of the best arm, which are referred to as the worst-case lower bound. We demonstrate that the worst-case lower bound depends solely on the variances of the outcomes. Then, under the assumption that the variances are known, we propose the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an extension of the Neyman allocation proposed by Neyman (1934). We show that the GNA-EBA strategy is asymptotically optimal in the sense that its probability of misidentification aligns with the lower bounds as the sample size increases infinitely and the differences between the expected outcomes of the best and other suboptimal arms converge to the same values across arms. We refer to such strategies as asymptotically worst-case optimal."
            },
            {
                "arxivId": "2308.12000",
                "title": "On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget",
                "abstract": "We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we first introduce the natural class of {\\it consistent} and {\\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof then proceeds by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \\cite{qin2022open}."
            },
            {
                "arxivId": "2303.09468",
                "title": "On the Existence of a Complexity in Fixed Budget Bandit Identification",
                "abstract": "In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by the same algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate."
            },
            {
                "arxivId": "2303.00950",
                "title": "Open Problem: Optimal Best Arm Identification with Fixed Budget",
                "abstract": "Best arm identification or pure exploration problems have received much attention in the COLT community since Bubeck et al. (2009) and Audibert et al. (2010). For any bandit instance with a unique best arm, its asymptotic complexity in the so-called fixed-confidence setting has been completely characterized in Garivier and Kaufmann (2016) and Chernoff (1959), while little is known about the asymptotic complexity in its\"dual\"setting called fixed-budget setting. This note discusses the open problems and conjectures about the instance-dependent asymptotic complexity in the fixed-budget setting."
            },
            {
                "arxivId": "2210.00974",
                "title": "Dealing with Unknown Variances in Best-Arm Identification",
                "abstract": "The problem of identifying the best arm among a collection of items having Gaussian rewards distribution is well understood when the variances are known. Despite its practical relevance for many applications, few works studied it for unknown variances. In this paper we introduce and analyze two approaches to deal with unknown variances, either by plugging in the empirical variance or by adapting the transportation costs. In order to calibrate our two stopping rules, we derive new time-uniform concentration inequalities, which are of independent interest. Then, we illustrate the theoretical and empirical performances of our two sampling rule wrappers on Track-and-Stop and on a Top Two algorithm. Moreover, by quantifying the impact on the sample complexity of not knowing the variances, we reveal that it is rather small."
            },
            {
                "arxivId": "2206.04646",
                "title": "Minimax Optimal Algorithms for Fixed-Budget Best Arm Identification",
                "abstract": "We consider the fixed-budget best arm identification problem where the goal is to find the arm of the largest mean with a fixed number of samples. It is known that the probability of misidentifying the best arm is exponentially small to the number of rounds. However, limited characterizations have been discussed on the rate (exponent) of this value. In this paper, we characterize the minimax optimal rate as a result of an optimization over all possible parameters. We introduce two rates, $R^{\\mathrm{go}}$ and $R^{\\mathrm{go}}_{\\infty}$, corresponding to lower bounds on the probability of misidentification, each of which is associated with a proposed algorithm. The rate $R^{\\mathrm{go}}$ is associated with $R^{\\mathrm{go}}$-tracking, which can be efficiently implemented by a neural network and is shown to outperform existing algorithms. However, this rate requires a nontrivial condition to be achievable. To address this issue, we introduce the second rate $R^{\\mathrm{go}}_\\infty$. We show that this rate is indeed achievable by introducing a conceptual algorithm called delayed optimal tracking (DOT)."
            },
            {
                "arxivId": "2205.02726",
                "title": "Asymptotic Efficiency Bounds for a Class of Experimental Designs",
                "abstract": "We consider an experimental design setting in which units are assigned to treatment after being sampled sequentially from an in\ufb01nite population. We derive asymptotic ef-\ufb01ciency bounds that apply to data from any experiment that assigns treatment as a (possibly randomized) function of covariates and past outcome data, including strati\ufb01cation on covariates and adaptive designs. For estimating the average treatment e\ufb00ect of a binary treatment, our results show that no further \ufb01rst order asymptotic e\ufb03ciency improvement is possible relative to an estimator that achieves the Hahn (1998) bound in an experimental design where the propensity score is chosen to minimize this bound. Our results also apply to settings with multiple treatments with possible constraints on treatment, as well as covariate based sampling of a single outcome."
            },
            {
                "arxivId": "2204.05527",
                "title": "Neyman allocation is minimax optimal for best arm identification with two arms",
                "abstract": ". This note describes the optimal policy rule, according to the local asymptotic minimax regret criterion, for best arm identi\ufb01cation when there are only two treatments. It is shown that the optimal sampling rule is the Neyman allocation, which allocates a constant fraction of units to each treatment in a manner that is proportional to the standard deviation of the treatment outcomes. When the variances are equal, the optimal ratio is one-half. This policy is independent of the data, so there is no adaptation to previous outcomes. At the end of the experiment, the policy maker adopts the treatment with higher average outcomes."
            },
            {
                "arxivId": "2111.09885",
                "title": "Rate-Optimal Bayesian Simple Regret in Best Arm Identification",
                "abstract": "We consider best arm identification in the multiarmed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization, the leading term in the Bayesian simple regret derives from the region in which the gap between optimal and suboptimal arms is smaller than [Formula: see text]. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings."
            },
            {
                "arxivId": "2109.08229",
                "title": "Policy Choice and Best Arm Identification: Asymptotic Analysis of Exploration Sampling",
                "abstract": "We consider the\"policy choice\"problem -- otherwise known as best arm identification in the bandit literature -- proposed by Kasy and Sautmann (2021) for adaptive experimental design. Theorem 1 of Kasy and Sautmann (2021) provides three asymptotic results that give theoretical guarantees for exploration sampling developed for this setting. We first show that the proof of Theorem 1 (1) has technical issues, and the proof and statement of Theorem 1 (2) are incorrect. We then show, through a counterexample, that Theorem 1 (3) is false. For the former two, we correct the statements and provide rigorous proofs. For Theorem 1 (3), we propose an alternative objective function, which we call posterior weighted policy regret, and derive the asymptotic optimality of exploration sampling."
            },
            {
                "arxivId": "2002.05308",
                "title": "Efficient Adaptive Experimental Design for Average Treatment Effect Estimation",
                "abstract": "The goal of many scientific experiments including A/B testing is to estimate the average treatment effect (ATE), which is defined as the difference between the expected outcomes of two or more treatments. In this paper, we consider a situation where an experimenter can assign a treatment to research subjects sequentially. In adaptive experimental design, the experimenter is allowed to change the probability of assigning a treatment using past observations for estimating the ATE efficiently. However, with this approach, it is difficult to apply a standard statistical method to construct an estimator because the observations are not independent and identically distributed. We thus propose an algorithm for efficient experiments with estimators constructed from dependent samples. We also introduce a sequential testing framework using the proposed estimator. To justify our proposed approach, we provide finite and infinite sample analyses. Finally, we experimentally show that the proposed algorithm exhibits preferable performance."
            },
            {
                "arxivId": "1806.05127",
                "title": "Stratification Trees for Adaptive Randomization in Randomized Controlled Trials",
                "abstract": "\n This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, and the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, our results are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure, can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment."
            },
            {
                "arxivId": "1605.09004",
                "title": "Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem",
                "abstract": "We consider the problem of \\textit{best arm identification} with a \\textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, with arms distribution defined on $[0,1]$. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity $H$, will misidentify the best arm with probability lower bounded by $$\\exp\\Big(-\\frac{T}{\\log(K)H}\\Big),$$ where $H$ is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by $\\exp(-T/H)$. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem."
            },
            {
                "arxivId": "1602.08448",
                "title": "Simple Bayesian Algorithms for Best Arm Identification",
                "abstract": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. Just as the multiarmed bandit problem crystallizes the tradeoff between exploration and exploitation, this \u201cpure exploration\u201d variant crystallizes the challenge of rapidly gathering information before committing to a final decision. The paper proposes several simple Bayesian algorithms for allocating measurement effort and, by characterizing fundamental asymptotic limits on the performance of any algorithm, formalizes a sense in which these seemingly naive algorithms are the best possible."
            },
            {
                "arxivId": "1602.04589",
                "title": "Optimal Best Arm Identification with Fixed Confidence",
                "abstract": "We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis."
            },
            {
                "arxivId": "1510.04740",
                "title": "Semiparametric theory and empirical processes in causal inference",
                "abstract": null
            },
            {
                "arxivId": "1312.7308",
                "title": "lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits",
                "abstract": "The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples. The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCB-type algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art."
            },
            {
                "arxivId": "0802.2655",
                "title": "Pure exploration in finitely-armed and continuous-armed bandits",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.00347",
        "category": "econ",
        "title": "Set-Valued Control Functions",
        "abstract": "The control function approach allows the researcher to identify various causal effects of interest. While powerful, it requires a strong invertibility assumption, which limits its applicability. This paper expands the scope of the nonparametric control function approach by allowing the control function to be set-valued and derive sharp bounds on structural parameters. The proposed generalization accommodates a wide range of selection processes involving discrete endogenous variables, random coefficients, treatment selections with interference, and dynamic treatment selections.",
        "references": [
            {
                "arxivId": "2401.11046",
                "title": "Information based inference in models with set-valued predictions and misspecification",
                "abstract": "This paper proposes an information-based inference method for partially identified parameters in incomplete models that is valid both when the model is correctly specified and when it is misspecified. Key features of the method are: (i) it is based on minimizing a suitably defined Kullback-Leibler information criterion that accounts for incompleteness of the model and delivers a non-empty pseudo-true set; (ii) it is computationally tractable; (iii) its implementation is the same for both correctly and incorrectly specified models; (iv) it exploits all information provided by variation in discrete and continuous covariates; (v) it relies on Rao's score statistic, which is shown to be asymptotically pivotal."
            },
            {
                "arxivId": "2009.13861",
                "title": "A computational approach to identification of treatment effects for policy evaluation",
                "abstract": null
            },
            {
                "arxivId": "1903.09679",
                "title": "Identification and Estimation of a Partially Linear Regression Model Using Network Data",
                "abstract": "I study a regression model in which one covariate is an unknown function of a latent driver of link formation in a network. Rather than specify and fit a parametric network formation model, I introduce a new method based on matching pairs of agents with similar columns of the squared adjacency matrix, the \n ijth entry of which contains the number of other agents linked to both agents \n i and \n j. The intuition behind this approach is that for a large class of network formation models the columns of the squared adjacency matrix characterize all of the identifiable information about individual linking behavior. In this paper, I describe the model, formalize this intuition, and provide consistent estimators for the parameters of the regression model.\n"
            },
            {
                "arxivId": "1809.05706",
                "title": "Control variables, discrete instruments, and identification of structural functions",
                "abstract": "Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects. We illustrate our results with an empirical application to Engel curve estimation."
            },
            {
                "arxivId": "1805.09397",
                "title": "Identification in Nonparametric Models for Dynamic Treatment Effects",
                "abstract": "This paper develops a nonparametric model that represents how sequences of outcomes and treatment choices influence one another in a dynamic manner. In this setting, we are interested in identifying the average outcome for individuals in each period, had a particular treatment sequence been assigned. The identification of this quantity allows us to identify the average treatment effects (ATE's) and the ATE's on transitions, as well as the optimal treatment regimes, namely, the regimes that maximize the (weighted) sum of the average potential outcomes, possibly less the cost of the treatments. The main contribution of this paper is to relax the sequential randomization assumption widely used in the biostatistics literature by introducing a flexible choice-theoretic framework for a sequence of endogenous treatments. We show that the parameters of interest are identified under each period's two-way exclusion restriction, i.e., with instruments excluded from the outcome-determining process and other exogenous variables excluded from the treatment-selection process. We also consider partial identification in the case where the latter variables are not available. Lastly, we extend our results to a setting where treatments do not appear in every period."
            },
            {
                "arxivId": "1711.02184",
                "title": "Semiparametric estimation of structural functions in nonseparable triangular models",
                "abstract": "Triangular systems with nonadditively separable unobserved heterogeneity provide a theoretically appealing framework for the modeling of complex structural relationships. However, they are not commonly used in practice due to the need for exogenous variables with large support for identification, the curse of dimensionality in estimation, and the lack of inferential tools. This paper introduces two classes of semiparametric nonseparable triangular models that address these limitations. They are based on distribution and quantile regression modeling of the reduced form conditional distributions of the endogenous variables. We show that average, distribution, and quantile structural functions are identified in these systems through a control function approach that does not require a large support condition. We propose a computationally attractive three\u2010stage procedure to estimate the structural functions where the first two stages consist of quantile or distribution regressions. We provide asymptotic theory and uniform inference methods for each stage. In particular, we derive functional central limit theorems and bootstrap functional central limit theorems for the distribution regression estimators of the structural functions. These results establish the validity of the bootstrap for three\u2010stage estimators of structural functions, and lead to simple inference algorithms. We illustrate the implementation and applicability of all our methods with numerical simulations and an empirical application to demand analysis."
            },
            {
                "arxivId": "1706.05982",
                "title": "On Heckits, Late, and Numerical Equivalence",
                "abstract": "Structural econometric methods are often criticized for being sensitive to functional form assumptions. We study parametric estimators of the local average treatment effect (LATE) derived from a widely used class of latent threshold crossing models and show they yield LATE estimates algebraically equivalent to the instrumental variables (IV) estimator. Our leading example is Heckman's (1979) two\u2010step (\u201cHeckit\u201d) control function estimator which, with two\u2010sided non\u2010compliance, can be used to compute estimates of a variety of causal parameters. Equivalence with IV is established for a semiparametric family of control function estimators and shown to hold at interior solutions for a class of maximum likelihood estimators. Our results suggest differences between structural and IV estimates often stem from disagreements about the target parameter rather than from functional form assumptions per se. In cases where equivalence fails, reporting structural estimates of LATE alongside IV provides a simple means of assessing the credibility of structural extrapolation exercises."
            },
            {
                "arxivId": "1605.00499",
                "title": "Monte Carlo Confidence Sets for Identified Sets",
                "abstract": "In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows."
            },
            {
                "arxivId": "0907.3503",
                "title": "Intersection bounds: estimation and inference",
                "abstract": "We develop a practical and novel method for inference on intersection bounds, namely bounds defined by either the infimum or supremum of a parametric or nonparametric function, or equivalently, the value of a linear programming problem with a potentially infinite constraint set. Our approach is especially convenient in models comprised of a continuum of inequalities that are separable in parameters, and also applies to models with inequalities that are non-separable in parameters. Since analog estimators for intersection bounds can be severely biased in finite samples, routinely underestimating the length of the identified set, we also offer a (downward/upward) median unbiased estimator of these (upper/lower) bounds as a natural by-product of our inferential procedure. Furthermore, our method appears to be the first and currently only method for inference in nonparametric models with a continuum of inequalities. We develop asymptotic theory for our method based on the strong approximation of a sequence of studentized empirical processes by a sequence of Gaussian or other pivotal processes. We provide conditions for the use of nonparametric kernel and series estimators, including a novel result that establishes strong approximation for general series estimators, which may be of independent interest. We illustrate the usefulness of our method with Monte Carlo experiments and an empirical example."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.10614",
        "category": "econ",
        "title": "Gentrification, displacement, and income trajectory of incumbents",
        "abstract": "Gentrification is associated with rapid demographic changes within inner-city neighborhoods. While many fear that gentrification drives low-income people from their homes and communities, there is limited evidence of the consequences of these changes. I use Canadian administrative tax files to track the movements of incumbent workers and their income trajectory as their neighborhood gentrifies. I exploit the timing at which neighborhoods gentrify in a matched staggered event-study framework. I find no evidence of displacement effects, even for low socioeconomic status households. In fact, families living in gentrifying neighborhoods are more likely to stay longer. I suggest that this might be related to tenant rights protection laws. When they endogenously decide to leave, low-income families do not relocate to worse neighborhoods. Finally, I find no adverse effects on their income trajectory, suggesting no repercussions on their labor market outcomes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.10618",
        "category": "econ",
        "title": "Limits of Approximating the Median Treatment Effect",
        "abstract": "Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\\mathbf{a}, \\mathbf{b}$, much of the prior work focused on estimating median$(\\mathbf{a}) -$ median$(\\mathbf{b})$, where median($\\mathbf x$) denotes the median value in the sorted ordering of all the values in vector $\\mathbf x$. It is known that estimating the difference of medians is easier than the desired estimand of median$(\\mathbf{a-b})$, called the Median Treatment Effect (MTE). The fundamental problem of causal inference -- for every individual $i$, we can only observe one of the potential outcome values, i.e., either the value $a_i$ or $b_i$, but not both, makes estimating MTE particularly challenging. In this work, we argue that MTE is not estimable and detail a novel notion of approximation that relies on the sorted order of the values in $\\mathbf{a-b}$. Next, we identify a quantity called variability that exactly captures the complexity of MTE estimation. By drawing connections to instance-optimality studied in theoretical computer science, we show that every algorithm for estimating the MTE obtains an approximation error that is no better than the error of an algorithm that computes variability. Finally, we provide a simple linear time algorithm for computing the variability exactly. Unlike much prior work, a particular highlight of our work is that we make no assumptions about how the potential outcome vectors are generated or how they are correlated, except that the potential outcome values are $k$-ary, i.e., take one of $k$ discrete values.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.10636",
        "category": "econ",
        "title": "Resilient by Design: Simulating Street Network Disruptions across Every Urban Area in the World",
        "abstract": null,
        "references": [
            {
                "arxivId": "2009.09106",
                "title": "Street Network Models and Indicators for Every Urban Area in the World",
                "abstract": "Cities worldwide exhibit a variety of street network patterns and configurations that shape human mobility, equity, health, and livelihoods. This study models and analyzes the street networks of every urban area in the world, using boundaries derived from the Global Human Settlement Layer. Street network data are acquired and modeled from OpenStreetMap with the open\u2010source OSMnx software. In total, this study models over 160 million OpenStreetMap street network nodes and over 320 million edges across 8,914 urban areas in 178 countries, and attaches elevation and grade data. This article presents the study\u2019s reproducible computational workflow, introduces two new open data repositories of ready\u2010to\u2010use global street network models and calculated indicators, and discusses summary findings on street network form worldwide. It makes four contributions. First, it reports the methodological advances of this open\u2010source workflow. Second, it produces an open data repository containing street network models for each urban area. Third, it analyzes these models to produce an open data repository containing street network form indicators for each urban area. No such global urban street network indicator data set has previously existed. Fourth, it presents a summary analysis of urban street network form, reporting the first such worldwide results in the literature."
            },
            {
                "arxivId": "1211.0259",
                "title": "Urban Street Networks, a Comparative Analysis of Ten European Cities",
                "abstract": "We compare the structural properties of the street networks of ten different European cities using their primal representation. We investigate the properties of the geometry of the networks and a set of centrality measures highlighting differences and similarities between cases. In particular, we found that cities share structural similarities due to their quasiplanarity but that there are also several distinctive geometrical properties. A principal component analysis is performed on the distributions of centralities and their respective moments, which is used to find distinctive characteristics by which we can classify cities into families. We believe that, beyond the improvement of the empirical knowledge on streets' network properties, our findings can open new perspectives into the scientific relationship between city planning and complex networks, stimulating the debate on the effectiveness of the set of knowledge that statistical physics can contribute for city planning and urban-morphology studies."
            },
            {
                "arxivId": "0708.4360",
                "title": "Modeling urban street patterns.",
                "abstract": "Urban street patterns form planar networks whose empirical properties cannot be accounted for by simple models such as regular grids or Voronoi tesselations. Striking statistical regularities across different cities have been recently empirically found, suggesting that a general and detail-independent mechanism may be in action. We propose a simple model based on a local optimization process combined with ideas previously proposed in studies of leaf pattern formation. The statistical properties of this model are in good agreement with the observed empirical patterns. Our results thus suggest that in the absence of a global design strategy, the evolution of many different transportation networks indeed follows a simple universal mechanism."
            },
            {
                "arxivId": "physics/0506009",
                "title": "The Network Analysis of Urban Streets: A Primal Approach",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0309436",
                "title": "Betweenness centrality in large complex networks",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0309045",
                "title": "A measure of betweenness centrality based on random walks",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.11010",
        "category": "econ",
        "title": "How Periodic Forecast Updates Influence MRP Planning Parameters: A Simulation Study",
        "abstract": "In many supply chains, the current efforts at digitalization have led to improved information exchanges between manufacturers and their customers. Specifically, demand forecasts are often provided by the customers and regularly updated as the related customer information improves. In this paper, we investigate the influence of forecast updates on the production planning method of Material Requirements Planning (MRP). A simulation study was carried out to assess how updates in information affect the setting of planning parameters in a rolling horizon MRP planned production system. An intuitive result is that information updates lead to disturbances in the production orders for the MRP standard, and, therefore, an extension for MRP to mitigate these effects is developed. A large numerical simulation experiment shows that the MRP safety stock exploitation heuristic, that has been developed, leads to significantly improved results as far as inventory and backorder costs are concerned. An interesting result is that the fixed-order-quantity lotsizing policy performs - in most instances - better than the fixed-order-period lotsizing policy, when periodic forecast updates occur. In addition, the simulation study shows that underestimating demand is marginally more costly than overestimating it, based on the comparative analysis of all instances. Furthermore, the results indicate that the MRP safety stock exploitation heuristic can mitigate the negative effects of biased forecasts.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.11016",
        "category": "econ",
        "title": "Comprehensive Oos Evaluation of Predictive Algorithms with Statistical Decision Theory",
        "abstract": "We argue that comprehensive out-of-sample (OOS) evaluation using statistical decision theory (SDT) should replace the current practice of K-fold and Common Task Framework validation in machine learning (ML) research. SDT provides a formal framework for performing comprehensive OOS evaluation across all possible (1) training samples, (2) populations that may generate training data, and (3) populations of prediction interest. Regarding feature (3), we emphasize that SDT requires the practitioner to directly confront the possibility that the future may not look like the past and to account for a possible need to extrapolate from one population to another when building a predictive algorithm. SDT is simple in abstraction, but it is often computationally demanding to implement. We discuss progress in tractable implementation of SDT when prediction accuracy is measured by mean square error or by misclassification rate. We summarize research studying settings in which the training data will be generated from a subpopulation of the population of prediction interest. We also consider conditional prediction with alternative restrictions on the state space of possible populations that may generate training data. We conclude by calling on ML researchers to join with econometricians and statisticians in expanding the domain within which implementation of SDT is tractable.",
        "references": [
            {
                "arxivId": "2308.05171",
                "title": "Statistical decision theory respecting stochastic dominance",
                "abstract": null
            },
            {
                "arxivId": "2208.03381",
                "title": "Partial Identification of Personalized Treatment Response with Trial-reported Analyses of Binary Subgroups.",
                "abstract": "Medical journals have adhered to a reporting practice that seriously limits the usefulness of published trial findings. Medical decision makers commonly observe many patient covariates and seek to use this information to personalize treatment choices. Yet standard summaries of trial findings only partition subjects into broad subgroups, typically binary categories. Given this reporting practice, we study the problem of inference on long mean treatment outcomes E[y(t)|x], where t is a treatment, y(t) is a treatment outcome, and the covariate vector x has length K, each component being a binary variable. The available data are estimates of {E[y(t)|xk = 0], E[y(t)|xk = 1], P(xk)}, k = 1, . . . , K reported in journal articles. We show that reported trial findings partially identify {E[y(t)|x], P(x)}. Illustrative computations demonstrate that the summaries of trial findings in journal articles may imply only wide bounds on long mean outcomes. One can realistically tighten inferences if one can combine reported trial findings with credible assumptions having identifying power, such as bounded-variation assumptions."
            },
            {
                "arxivId": "2110.00864",
                "title": "Probabilistic Prediction for Binary Treatment Choice: with Focus on Personalized Medicine",
                "abstract": "This paper carries further my research applying statistical decision theory to treatment choice with sample data, using maximum regret to evaluate the performance of treatment rules. The specific new contribution is to study as-if optimization using estimates of illness probabilities, when choosing between surveillance and aggressive treatment. With this motivation, I introduce a new form of analysis of kernel estimation. Beyond its specifics, the paper sends a broad message. Biostatisticians and computer scientists have addressed medical risk assessment in alternative indirect ways, the former applying classical statistical theory and the latter measuring ex-post prediction accuracy in test samples. Neither approach is satisfactory. Statistical decision theory provides a coherent, generally applicable methodology. I am grateful for comments from Michael Gmeiner, Valentyn Litvin, and Filip Obradovic. I am grateful to Litvin and Gmeiner for programming the computations in Sections 4.1 and 4.2 respectively."
            },
            {
                "arxivId": "1912.08726",
                "title": "Econometrics for Decision Making: Building Foundations Sketched by Haavelmo and Wald",
                "abstract": "Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes use of statistical decision theory to evaluate the performance of models in decision making. I consider the common practice of \n as\u2010if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as\u2010if optimization or any other model\u2010based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancing computation is the primary task to complete the foundations sketched by Haavelmo and Wald.\n"
            },
            {
                "arxivId": "1708.06633",
                "title": "Nonparametric regression using deep neural networks with ReLU activation function",
                "abstract": "Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to $\\log n$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates."
            },
            {
                "arxivId": "1611.00740",
                "title": "Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review",
                "abstract": null
            },
            {
                "arxivId": "1509.07913",
                "title": "Sufficient trial size to inform clinical practice",
                "abstract": "Significance A core objective of trials comparing alternative medical treatments is to inform treatment choice in clinical practice, and yet conventional practice in designing trials has been to choose a sample size that yields specified statistical power. Power, a concept in the theory of hypothesis testing, is at most loosely connected to effective treatment choice. This paper develops an alternative principle for trial design that aims to directly benefit medical decision making. We propose choosing a sample size that enables implementation of near-optimal treatment rules. Near optimality means that treatment choices are suitably close to the best that could be achieved if clinicians were to know with certainty mean treatment response in their patient populations. Medical research has evolved conventions for choosing sample size in randomized clinical trials that rest on the theory of hypothesis testing. Bayesian statisticians have argued that trials should be designed to maximize subjective expected utility in settings of clinical interest. This perspective is compelling given a credible prior distribution on treatment response, but there is rarely consensus on what the subjective prior beliefs should be. We use Wald\u2019s frequentist statistical decision theory to study design of trials under ambiguity. We show that \u03b5-optimal rules exist when trials have large enough sample size. An \u03b5-optimal rule has expected welfare within \u03b5 of the welfare of the best treatment in every state of nature. Equivalently, it has maximum regret no larger than \u03b5. We consider trials that draw predetermined numbers of subjects at random within groups stratified by covariates and treatments. We report exact results for the special case of two treatments and binary outcomes. We give simple sufficient conditions on sample sizes that ensure existence of \u03b5-optimal treatment rules when there are multiple treatments and outcomes are bounded. These conditions are obtained by application of Hoeffding large deviations inequalities to evaluate the performance of empirical success rules."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.11022",
        "category": "econ",
        "title": "Auctions with Dynamic Scoring",
        "abstract": "We study the design of auctions with dynamic scoring, which allocate a single item according to a given scoring rule. We are motivated by online advertising auctions when users interact with a platform over the course of a session. The platform ranks ads based on a combination of bids and quality scores, and updates the quality scores throughout the session based on the user's online activity. The platform must decide when to show an ad during the session. By delaying the auction, the auctioneer acquires information about an ad's quality, improving her chances of selecting a high quality ad. However information is costly, because delay reduces market thickness and in turn revenue. When should the auctioneer allocate the impression to balance these forces? We develop a theoretical model to study the effect of market design on the trade-off between market thickness and information. In particular, we focus on first- and second-price auctions. The auctioneer can commit to the auction format, but not to its timing: her decision can thus be cast as a real options problem. We show that under optimal stopping the first-price auction allocates efficiently but with delay. Instead, the second-price auction generates more revenue by avoiding delay. The auctioneer benefits from introducing reserve prices, more so in a first-price auction.",
        "references": [
            {
                "arxivId": "2311.18138",
                "title": "Algorithmic Persuasion Through Simulation: Information Design in the Age of Generative AI",
                "abstract": "We study a Bayesian persuasion problem where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (binary) state of the world, such as whether the quality of the product is high or low, but only has limited information about the receiver's beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in generative AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver's behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender's optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender's expected utility in this Bayesian persuasion game. We also consider approximate oracles, more general query structures, and costly queries."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.11028",
        "category": "econ",
        "title": "What Makes Systemic Discrimination,\"Systemic?\"Exposing the Amplifiers of Inequity",
        "abstract": "Drawing on work spanning economics, public health, education, sociology, and law, I formalize theoretically what makes systemic discrimination\"systemic.\"Injustices do not occur in isolation, but within a complex system of interdependent factors; and their effects may amplify as a consequence. I develop a taxonomy of these amplification mechanisms, connecting them to well-understood concepts in economics that are precise, testable and policy-oriented. This framework reveals that these amplification mechanisms can either be directly disrupted, or exploited to amplify the effects of equity-focused interventions instead. In other words, it shows how to use the machinery of systemic discrimination against itself. Real-world examples discussed include but are not limited to reparations for slavery and Jim Crow, vouchers or place-based neighborhood interventions, police shootings, affirmative action, and Covid-19.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-19.json",
        "arxivId": "2403.11240",
        "category": "econ",
        "title": "Speed, Accuracy, and Complexity",
        "abstract": "This paper re-examines the validity of using response time to infer problem complexity. It revisits a canonical Wald model of optimal stopping, taking signal-to-noise ratio as a measure of problem complexity. While choice quality is monotone in problem complexity, expected stopping time is inverse $U$-shaped. Indeed decisions are fast in both very simple and very complex problems: in simple problems it is quick to understand which alternative is best, while in complex problems it would be too costly -- an insight which extends to general costly information acquisition models. This non-monotonicity also underlies an ambiguous relationship between response time and ability, whereby higher ability entails slower decisions in very complex problems, but faster decisions in simple problems. Finally, this paper proposes a new method to correctly infer problem complexity based on the finding that choices react more to changes in incentives in more complex problems.",
        "references": [
            {
                "arxivId": "1812.00849",
                "title": "Strategically Simple Mechanisms",
                "abstract": "We define and investigate a property of mechanisms that we call \u201cstrategic simplicity,\u201d and that is meant to capture the idea that, in strategically simple mechanisms, strategic choices require limited strategic sophistication. We define a mechanism to be strategically simple if choices can be based on first\u2010order beliefs about the other agents' preferences and first\u2010order certainty about the other agents' rationality alone, and there is no need for agents to form higher\u2010order beliefs, because such beliefs are irrelevant to the optimal strategies. All dominant strategy mechanisms are strategically simple. But many more mechanisms are strategically simple. In particular, strategically simple mechanisms may be more flexible than dominant strategy mechanisms in the bilateral trade problem and the voting problem."
            },
            {
                "arxivId": "math/9207212",
                "title": "User\u2019s guide to viscosity solutions of second order partial differential equations",
                "abstract": "The notion of viscosity solutions of scalar fully nonlinear partial differential equations of second order provides a framework in which startling comparison and uniqueness theorems, existence theorems, and theorems about continuous dependence may now be proved by very efficient and striking arguments. The range of important applications of these results is enormous. This article is a self-contained exposition of the basic theory of viscosity solutions"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2209.05570",
        "category": "econ",
        "title": "Attention Capture",
        "abstract": "A decision maker (DM) learns about an uncertain state via a dynamic information structure chosen by a designer and, at each history, optimally chooses between learning more and stopping to act. Thus, dynamic information structures induce joint distributions over outcomes (actions, states, and stopping times). We show there is no commitment gap: for arbitrary preferences over outcomes, designer-optimal structures can always be modified to be sequentially optimal. These modifications exploit the irreversibility of information to discipline the designer's future self. We then show all feasible distributions over outcomes are implementable with dynamic information structures for which stopping beliefs are extremal and continuation beliefs follow a deterministic path. We use these results to solve the problem of a designer with (i) nonlinear preferences over DM's stopping times: optimal structures have a block structure such that DM's indifference times coincide with the support of her stopping time; and (ii) preferences over actions and stopping times when they are additive or supermodular. Our results speak directly to the attention economy.",
        "references": [
            {
                "arxivId": "2207.00685",
                "title": "Engagement Maximization",
                "abstract": "We consider the problem of a Bayesian agent receiving signals over time and then taking an action. The agent chooses when to stop and take an action based on her current beliefs, and prefers (all else equal) to act sooner rather than later. The signals received by the agent are determined by a principal, whose objective is to maximize engagement (the total attention paid by the agent to the signals). We show that engagement maximization by the principal minimizes the agent's welfare; the agent does no better than if she gathered no information. Relative to a benchmark in which the agent chooses the signals, engagement maximization induces excessive information acquisition and extreme beliefs. An optimal strategy for the principal involves\"suspensive signals\"that lead the agent's belief to become\"less certain than the prior\"and\"decisive signals\"that lead the agent's belief to jump to the stopping region."
            },
            {
                "arxivId": "1910.11392",
                "title": "The Persuasion Duality",
                "abstract": "We present a unified duality approach to Bayesian persuasion. The optimal dual variable, interpreted as a price function, is shown to be a supergradient of the concave closure of the objective function at the prior belief. Under regularity conditions, our general duality result implies known results for the case when the objective function depends only on the expected state. We apply our approach to characterize the optimal signal in the case when the state is two-dimensional."
            },
            {
                "arxivId": "1910.07015",
                "title": "Dynamically Aggregating Diverse Information",
                "abstract": "An agent has access to multiple information sources, each modeled as a Brownian motion whose drift provides information about a different component of an unknown Gaussian state. Information is acquired continuously---where the agent chooses both which sources to sample from, and also how to allocate attention across them---until an endogenously chosen time, at which point a decision is taken. We demonstrate conditions on the agent's prior belief under which it is possible to exactly characterize the optimal information acquisition strategy. We then apply this characterization to derive new results regarding: (1) endogenous information acquisition for binary choice, (2) the dynamic consequences of attention manipulation, and (3) strategic information provision by biased news sources."
            },
            {
                "arxivId": "2303.09675",
                "title": "Dynamic Information Provision: Rewarding the Past and Guiding the Future",
                "abstract": "I study the optimal provision of information in a long\u2010term relationship between a sender and a receiver. The sender observes a persistent, evolving state and commits to send signals over time to the receiver, who sequentially chooses public actions that affect the welfare of both players. I solve for the sender's optimal policy in closed form: the sender reports the value of the state with a delay that shrinks over time and eventually vanishes. Even when the receiver knows the current state, the sender retains leverage by threatening to conceal the future evolution of the state."
            },
            {
                "arxivId": "1812.06967",
                "title": "Optimal Dynamic Allocation of Attention",
                "abstract": "We consider a decision maker (DM) who, before taking an action, seeks information by allocating her limited attention dynamically over different news sources that are biased toward alternative actions. Endogenous choice of information generates rich dynamics: the chosen news source either reinforces or weakens the prior, shaping subsequent attention choices, belief updating, and the final action. The DM adopts a learning strategy biased toward the current belief when the belief is extreme and against that belief when it is moderate. Applied to consumption of news media, observed behavior exhibits an \u201c echo-chamber\u201d effect for partisan voters and a novel \u201c anti-echo-chamber\u201d effect for moderates. (JEL D72, D83, D91, L82)"
            },
            {
                "arxivId": "1407.5649",
                "title": "Optimal dynamic information provision",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2309.08740",
        "category": "econ",
        "title": "Learning Source Biases: Multi-sourced Misspecifications and Consequences",
        "abstract": "We study how a decision maker (DM) learns about the biases of unfamiliar information sources. Absent any friction, a rational DM uses familiar sources as yardsticks to discern the true bias of a source. If the DM has misspecified beliefs, this process fails. We derive long-run beliefs, behavior, welfare, and corresponding comparative statics when the DM holds dogmatic, incorrect beliefs about the biases of several familiar sources. The distortion due to misspecified learning aggregates misspecifications associated with familiar sources independently of sources the DM learns about. This has implications for labor market discrimination, media bias, and project finance and oversight.",
        "references": [
            {
                "arxivId": "2012.15007",
                "title": "Evolutionarily Stable (Mis)specifications: Theory and Applications",
                "abstract": "We introduce an evolutionary framework to evaluate competing (mis)specifications in strategic situations, focusing on which misspecifications can persist over a correct specification. Agents with heterogeneous specifications coexist in a society and repeatedly match against random opponents to play a stage game. They draw Bayesian inferences about the environment based on personal experience, so their learning depends on the distribution of specifications and matching assortativity in the society. One specification is evolutionarily stable against another if, whenever sufficiently prevalent, its adherents obtain higher expected objective payoffs than their counterparts. The learning channel leads to novel stability phenomena compared to frameworks where the heritable unit of cultural transmission is a single belief instead of a specification (i.e., set of feasible beliefs). We apply the framework to linear-quadratic-normal games where players receive correlated signals but possibly misperceive the information structure. The correct specification is not evolutionarily stable against a correlational error, whose direction depends on matching assortativity. As another application, the framework also endogenizes coarse analogy classes in centipede games. The full paper can be found at https://kevinhe.net/papers/theory_evolution.pdf"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2311.00439",
        "category": "econ",
        "title": "Robustify and Tighten the Lee Bounds: A Sample Selection Model under Stochastic Monotonicity and Symmetry Assumptions",
        "abstract": "In the presence of sample selection, Lee's (2009) nonparametric bounds are a popular tool for estimating a treatment effect. However, the Lee bounds rely on the monotonicity assumption, whose empirical validity is sometimes unclear. Furthermore, the bounds are often regarded to be wide and less informative even under monotonicity. To address these issues, this study introduces a stochastic version of the monotonicity assumption alongside a nonparametric distributional shape constraint. The former enhances the robustness of the Lee bounds with respect to monotonicity, while the latter helps tighten these bounds. The obtained bounds do not rely on the exclusion restriction and can be root-$n$ consistently estimable, making them practically viable. The potential usefulness of the proposed methods is illustrated by their application on experimental data from the after-school instruction programme studied by Muralidharan, Singh, and Ganimian (2019).",
        "references": [
            {
                "arxivId": "0907.3503",
                "title": "Intersection bounds: estimation and inference",
                "abstract": "We develop a practical and novel method for inference on intersection bounds, namely bounds defined by either the infimum or supremum of a parametric or nonparametric function, or equivalently, the value of a linear programming problem with a potentially infinite constraint set. Our approach is especially convenient in models comprised of a continuum of inequalities that are separable in parameters, and also applies to models with inequalities that are non-separable in parameters. Since analog estimators for intersection bounds can be severely biased in finite samples, routinely underestimating the length of the identified set, we also offer a (downward/upward) median unbiased estimator of these (upper/lower) bounds as a natural by-product of our inferential procedure. Furthermore, our method appears to be the first and currently only method for inference in nonparametric models with a continuum of inequalities. We develop asymptotic theory for our method based on the strong approximation of a sequence of studentized empirical processes by a sequence of Gaussian or other pivotal processes. We provide conditions for the use of nonparametric kernel and series estimators, including a novel result that establishes strong approximation for general series estimators, which may be of independent interest. We illustrate the usefulness of our method with Monte Carlo experiments and an empirical example."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.05671",
        "category": "econ",
        "title": "Investigating Changes of Water Quality in Reservoirs based on Flood and Inflow Fluctuations",
        "abstract": "Water temperature and dissolved oxygen are essential indicators of water quality and ecosystem sustainability. Lately, heavy rainfalls are happening frequently and forcefully affecting the thermal structure and mixing layers in depth by sharply increasing the volume of inflow entitled flash flood. It can occur by sudden intense precipitation and develop within minutes or hours. Because of heavy debris load and speedy water, this phenomenon has remarkable effects on water quality. A higher flow during floods may worsens water quality at lakes and reservoirs that are thermally stratified (with separate density layers) and decrease dissolved oxygen content. However, it is unclear how well these parameters represent the response of lakes to changes in volume discharge. To address this question, researchers simulate the thermal structure in two stratified reservoirs, considering the Rajae reservoir as a representative reservoir in the north of Iran and Minab reservoir in the south. In this study, the model realistically represented variations of dissolved oxygen and temperature of dams Lake response to flash floods. The model performance was evaluated using observed data from stations on the dams lake. In this case, the inflow charge considered in a 10-day flash flood from April 6th to April 16th during the yearly normal flow. The complete mixture in a part of the thermal structure has been proved in Rajaee reservoir. The nonpermanent impact of the massive inflow of storm runoff caused an increase in oxygen-consuming, leading to a severe decrease in dissolved oxygen on epilimnion and metalimnion. The situation in Minab reservoir was relatively different from Rajae reservoir. The inflow changes not only cause mixture but also help expanding stratification.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.09138",
        "category": "econ",
        "title": "Study on Standardizing Working Time: A Case of XYZ Retail Store in Bandung, Indonesia",
        "abstract": "Work time standardization helps to find and reduce wasteful movements and time in the workplace, such as chatting, mobile phone use, insufficient rest, or unproductive tasks. This study aims to map the process of displaying products from the warehouse to the shelves and calculate and determine the standard working time of employees of the Operations Division of PT XYZ Branch who oversee displaying X Milk and Y Bread. The data was collected six times in three weeks, including interviews and observations, and took a sample of 20 pieces on each product to carry out data analysis such as data sufficiency tests and control charts. Several time deviations were found in the display process of X Milk products on all observation days in different activities. Whereas in the process of displaying Y Bread, only the deviation of working time was found on the 4th observation day, which proves that the process needs to have a standard working time so that the activity work time is more controlled. Therefore, the analysis is carried out with the calculation of performance rating, time allowance, normal time, and standard time. The result of the standard time calculation for the display process of X Milk products is 15.83 minutes and Y Bread is 9.18 minutes for each product of 20 units.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.10982",
        "category": "econ",
        "title": "Financial Performance and Innovation: Evidence From USA, 1998 - 2023",
        "abstract": "This study explores the relationship between R&D intensity, as a measure of innovation, and financial performance among S&P 500 companies over 100 quarters from 1998 to 2023, including multiple crisis periods. It challenges the conventional wisdom that larger companies are more prone to innovate, using a comprehensive dataset across various industries. The analysis reveals diverse associations between innovation and key financial indicators such as firm size, assets, EBITDA, and tangibility. Our findings underscore the importance of innovation in enhancing firm competitiveness and market positioning, highlighting the effectiveness of countercyclical innovation policies. This research contributes to the debate on the role of R&D investments in driving firm value, offering new insights for both academic and policy discussions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12093",
        "category": "econ",
        "title": "Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach",
        "abstract": "Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \\textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \\textit{Stackelberg mean-field reinforcement learning }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms of performance, efficiency-equity tradeoff, and SMFG assumption analysis. This paper significantly contributes to the domain of AI for economics by providing a powerful tool for modeling and solving optimal macroeconomic policies.",
        "references": [
            {
                "arxivId": "2311.05229",
                "title": "Mean Field Games in a Stackelberg problem with an informed major player",
                "abstract": "We investigate a stochastic differential game in which a major player has a private information (the knowledge of a random variable), which she discloses through her control to a population of small players playing in a Nash Mean Field Game equilibrium. The major player's cost depends on the distribution of the population, while the cost of the population depends on the random variable known by the major player. We show that the game has a relaxed solution and that the optimal control of the major player is approximatively optimal in games with a large but finite number of small players."
            },
            {
                "arxivId": "2302.10440",
                "title": "A Machine Learning Method for Stackelberg Mean Field Games",
                "abstract": "We propose a single-level numerical approach to solve Stackelberg mean field game (MFG) problems. In Stackelberg MFG, an infinite population of agents play a non-cooperative game and choose their controls to optimize their individual objectives while interacting with the principal and other agents through the population distribution. The principal can influence the mean field Nash equilibrium at the population level through policies, and she optimizes her own objective, which depends on the population distribution. This leads to a bi-level problem between the principal and mean field of agents that cannot be solved using traditional methods for MFGs. We propose a reformulation of this problem as a single-level mean field optimal control problem through a penalization approach. We prove convergence of the reformulated problem to the original problem. We propose a machine learning method based on (feed-forward and recurrent) neural networks and illustrate it on several examples from the literature."
            },
            {
                "arxivId": "2210.04110",
                "title": "Optimization frameworks and sensitivity analysis of Stackelberg mean-field games",
                "abstract": "This paper proposes and studies a class of discrete-time \ufb01nite-time-horizon Stackelberg mean-\ufb01eld games, with one leader and an in\ufb01nite number of identical and indistinguishable followers. In this game, the objective of the leader is to maximize her reward considering the worst-case cost over all possible \u01eb -Nash equilibria among followers. A new analytical paradigm is established by showing the equivalence between this Stackelberg mean-\ufb01eld game and a minimax optimization problem. This optimization framework facilitates studying both analytically and numerically the set of Nash equilibria for the game; and leads to the sensitivity and the robustness analysis of the game value. In particular, when there is model uncertainty, the game value for the leader su\ufb00ers non-vanishing sub-optimality as the perturbed model converges to the true model. In order to obtain a near-optimal solution, the leader needs to be more pessimistic with anticipation of model errors and adopts a relaxed version of the original Stackelberg game."
            },
            {
                "arxivId": "2208.05568",
                "title": "The emergence of division of labour through decentralized social sanctioning",
                "abstract": "Human ecological success relies on our characteristic ability to flexibly self-organize into cooperative social groups, the most successful of which employ substantial specialization and division of labour. Unlike most other animals, humans learn by trial and error during their lives what role to take on. However, when some critical roles are more attractive than others, and individuals are self-interested, then there is a social dilemma: each individual would prefer others take on the critical but unremunerative roles so they may remain free to take one that pays better. But disaster occurs if all act thus and a critical role goes unfilled. In such situations learning an optimum role distribution may not be possible. Consequently, a fundamental question is: how can division of labour emerge in groups of self-interested lifetime-learning individuals? Here, we show that by introducing a model of social norms, which we regard as emergent patterns of decentralized social sanctioning, it becomes possible for groups of self-interested individuals to learn a productive division of labour involving all critical roles. Such social norms work by redistributing rewards within the population to disincentivize antisocial roles while incentivizing prosocial roles that do not intrinsically pay as well as others."
            },
            {
                "arxivId": "2206.05835",
                "title": "Deep Reinforcement Learning for Optimal Investment and Saving Strategy Selection in Heterogeneous Profiles: Intelligent Agents working towards retirement",
                "abstract": "The transition from defined benefit to defined contribution pension plans shifts the responsibility for saving toward retirement from governments and institutions to the individuals. Determining optimal saving and investment strategy for individuals is paramount for stable financial stance and for avoiding poverty during work-life and retirement, and it is a particularly challenging task in a world where form of employment and income trajectory experienced by different occupation groups are highly diversified. We introduce a model in which agents learn optimal portfolio allocation and saving strategies that are suitable for their heterogeneous profiles. We use deep reinforcement learning to train agents. The environment is calibrated with occupation and age dependent income evolution dynamics. The research focuses on heterogeneous income trajectories dependent on agent profiles and incorporates the behavioural parameterisation of agents. The model provides a flexible methodology to estimate lifetime consumption and investment choices for heterogeneous profiles under varying scenarios."
            },
            {
                "arxivId": "2205.06760",
                "title": "Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning",
                "abstract": "Advances in artificial intelligence often stem from the development of new environments that abstract real-world situations into a form where research can be done conveniently. This paper contributes such an environment based on ideas inspired by elementary Microeconomics. Agents learn to produce resources in a spatially complex world, trade them with one another, and consume those that they prefer. We show that the emergent production, consumption, and pricing behaviors respond to environmental conditions in the directions predicted by supply and demand shifts in Microeconomics. We also demonstrate settings where the agents' emergent prices for goods vary over space, reflecting the local abundance of goods. After the price disparities emerge, some agents then discover a niche of transporting goods between regions with different prevailing prices -- a profitable strategy because they can buy goods where they are cheap and sell them where they are expensive. Finally, in a series of ablation experiments, we investigate how choices in the environmental rewards, bartering actions, agent architecture, and ability to consume tradable goods can either aid or inhibit the emergence of this economic behavior. This work is part of the environment development branch of a research program that aims to build human-like artificial general intelligence through multi-agent interactions in simulated societies. By exploring which environment features are needed for the basic phenomena of elementary microeconomics to emerge automatically from learning, we arrive at an environment that differs from those studied in prior multi-agent reinforcement learning work along several dimensions. For example, the model incorporates heterogeneous tastes and physical abilities, and agents negotiate with one another as a grounded form of communication."
            },
            {
                "arxivId": "2201.01163",
                "title": "Analyzing Micro-Founded General Equilibrium Models with Many Agents using Deep Reinforcement Learning",
                "abstract": "Real economies can be modeled as a sequential imperfect-information game with many heterogeneous agents, such as consumers, firms, and governments. Dynamic general equilibrium (DGE) models are often used for macroeconomic analysis in this setting. However, finding general equilibria is challenging using existing theoretical or computational methods, especially when using microfoundations to model individual agents. Here, we show how to use deep multi-agent reinforcement learning (MARL) to find $\\epsilon$-meta-equilibria over agent types in microfounded DGE models. Whereas standard MARL fails to learn non-trivial solutions, our structured learning curricula enable stable convergence to meaningful solutions. Conceptually, our approach is more flexible and does not need unrealistic assumptions, e.g., continuous market clearing, that are commonly used for analytical tractability. Furthermore, our end-to-end GPU implementation enables fast real-time convergence with a large number of RL economic agents. We showcase our approach in open and closed real-business-cycle (RBC) models with 100 worker-consumers, 10 firms, and a social planner who taxes and redistributes. We validate the learned solutions are $\\epsilon$-meta-equilibria through best-response analyses, show that they align with economic intuitions, and show our approach can learn a spectrum of qualitatively distinct $\\epsilon$-meta-equilibria in open RBC models. As such, we show that hardware-accelerated MARL is a promising framework for modeling the complexity of economies based on microfoundations."
            },
            {
                "arxivId": "2110.11582",
                "title": "An Economy of Neural Networks: Learning from Heterogeneous Experiences",
                "abstract": "This paper proposes a new way to model behavioral agents in dynamic macro-financial environments. Agents are described as neural networks and learn policies from idiosyncratic past experiences. I investigate the feedback between irrationality and past outcomes in an economy with heterogeneous shocks similar to Aiyagari (1994). In the model, the rational expectations assumption is seriously violated because learning of a decision rule for savings is unstable. Agents who fall into learning traps save either excessively or save nothing, which provides a candidate explanation for several empirical puzzles about wealth distribution. Neural network agents have a higher average MPC and exhibit excess sensitivity of consumption. Learning can negatively affect intergenerational mobility."
            },
            {
                "arxivId": "2110.02474",
                "title": "Can an AI agent hit a moving target?",
                "abstract": "I model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply) with a deep reinforcement learning algorithm in the AI literature. I show that when the money supply accelerates, the learning agents only adjust their actions, which include consumption and demand for real balance, after gathering learning experience for many periods. This delayed adjustments leads to low returns during transition periods. Once they start adjusting to the new environment, their welfare improves. Their changes in beliefs and actions lead to temporary inflation volatility. I also show that, 1. the AI agents who explores their environment more adapt to the policy regime change quicker, which leads to welfare improvements and less inflation volatility, and 2. the AI agents who have experienced a structural change adjust their beliefs and behaviours quicker than an inexperienced learning agent."
            },
            {
                "arxivId": "2110.01127",
                "title": "Deep Learning for Principal-Agent Mean Field Games",
                "abstract": "Here, we develop a deep learning algorithm for solving Principal-Agent (PA) mean field games with market-clearing conditions \u2013 a class of problems that have thus far not been studied and one that poses difficulties for standard numerical methods. We use an actor-critic approach to optimization, where the agents form a Nash equilibria according to the principal\u2019s penalty function, and the principal evaluates the resulting equilibria. The inner problem\u2019s Nash equilibria is obtained using a variant of the deep backward stochastic differential equation (BSDE) method modified for McKean-Vlasov forward-backward SDEs that includes dependence on the distribution over both the forward and backward processes. The outer problem\u2019s loss is further approximated by a neural net by sampling over the space of penalty functions. We apply our approach to a stylized PA problem arising in Renewable Energy Certificate (REC) markets, where agents may rent clean energy production capacity, trade RECs, and expand their long-term capacity to navigate the market at maximum profit. Our numerical results illustrate the efficacy of the algorithm and lead to interesting insights into the nature of optimal PA interactions in the mean-field limit of these markets."
            },
            {
                "arxivId": "2108.02904",
                "title": "Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist",
                "abstract": "Optimizing economic and public policy is critical to address socioeconomic issues and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex mechanism design problem. A policy designer needs to consider multiple objectives, policy levers, and behavioral responses from strategic actors who optimize for their individual objectives. Moreover, real-world policies should be explainable and robust to simulation-to-reality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow set of policy levers or objectives that are hard to measure, do not yield explicit optimal policies, or do not consider strategic behavior, for example. Hence, it remains challenging to optimize policy in real-world scenarios. Here we show that the AI Economist framework enables effective, flexible, and interpretable policy design using two-level reinforcement learning (RL) and data-driven simulations. We validate our framework on optimizing the stringency of \\USState{} policies and Federal subsidies during a pandemic, e.g., COVID-19, using a simulation fitted to real data. We find that log-linear policies trained using RL significantly improve social welfare, based on both public health and economic outcomes, compared to past outcomes. Their behavior can be explained, e.g., well-performing policies respond strongly to changes in recovery and vaccination rates. They are also robust to calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world policymaking has not seen adoption of machine learning methods at large, including RL and AI-driven simulations. Our results show the potential of AI to guide policy design and improve social welfare amidst the complexity of the real world."
            },
            {
                "arxivId": "2106.06060",
                "title": "AI-driven Prices for Externalities and Sustainability in Production Markets",
                "abstract": "Traditional competitive markets do not account for negative externalities; indirect costs that some participants impose on others, such as the cost of over-appropriating a common-pool resource (which diminishes future stock, and thus harvest, for everyone). Quantifying appropriate interventions to market prices has proven to be quite challenging. We propose a practical approach to computing market prices and allocations via a deep reinforcement learning policymaker agent, operating in an environment of other learning agents. Our policymaker allows us to tune the prices with regard to diverse objectives such as sustainability and resource wastefulness, fairness, buyers' and sellers' welfare, etc. As a highlight of our findings, our policymaker is significantly more successful in maintaining resource sustainability, compared to the market equilibrium outcome, in scarce resource environments."
            },
            {
                "arxivId": "2105.10099",
                "title": "Learning from Zero: How to Make Consumption-Saving Decisions in a Stochastic Environment with an AI Algorithm",
                "abstract": "This exercise offers an innovative learning mechanism to model economic agent\u2019s decision-making process using a deep reinforcement learning algorithm. In particular, this AI agent is born in an economic environment with no information on the underlying economic structure and its own preference. I model how the AI agent learns from square one in terms of how it collects and processes information. It is able to learn in real time through constantly interacting with the environment and adjusting its actions accordingly (i.e., online learning). I illustrate that the economic agent under deep reinforcement learning is adaptive to changes in a given environment in real time. AI agents differ in their ways of collecting and processing information, and this leads to different learning behaviours and welfare distinctions. The chosen economic structure can be generalised to other decision-making processes and economic models."
            },
            {
                "arxivId": "2104.09368",
                "title": "Deep Reinforcement Learning in a Monetary Model",
                "abstract": "We propose using deep reinforcement learning to solve dynamic stochastic general equilibrium models. Agents are represented by deep arti\ufb01cial neural networks and learn to solve their dynamic optimisation problem by interacting with the model environment, of which they have no a priori knowledge. Deep reinforcement learning o\ufb00ers a \ufb02exible yet principled way to model bounded rationality within this general class of models. We apply our proposed approach to a classical model from the adaptive learning literature in macroeconomics which looks at the interaction of monetary and \ufb01scal policy. We \ufb01nd that, contrary to adaptive learning, the arti\ufb01cially intelligent household can solve the model in all policy regimes."
            },
            {
                "arxivId": "2103.16977",
                "title": "Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning",
                "abstract": "General equilibrium macroeconomic models are a core tool used by policymakers to understand a nation's economy. They represent the economy as a collection of forward-looking actors whose behaviours combine, possibly with stochastic effects, to determine global variables (such as prices) in a dynamic equilibrium. However, standard semi-analytical techniques for solving these models make it difficult to include the important effects of heterogeneous economic actors. The COVID-19 pandemic has further highlighted the importance of heterogeneity, for example in age and sector of employment, in macroeconomic outcomes and the need for models that can more easily incorporate it. We use techniques from reinforcement learning to solve such models incorporating heterogeneous agents in a way that is simple, extensible, and computationally efficient. We demonstrate the method's accuracy and stability on a toy problem for which there is a known analytical solution, its versatility by solving a general equilibrium problem that includes global stochasticity, and its flexibility by solving a combined macroeconomic and epidemiological model to explore the economic and health implications of a pandemic. The latter successfully captures plausible economic behaviours induced by differential health risks by age."
            },
            {
                "arxivId": "2011.09533",
                "title": "Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?",
                "abstract": "Most recently developed approaches to cooperative multi-agent reinforcement learning in the \\emph{centralized training with decentralized execution} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning. We also compare IPPO to several variants; the results suggest that IPPO's strong performance may be due to its robustness to some forms of environment non-stationarity."
            },
            {
                "arxivId": "2011.03105",
                "title": "Optimal Incentives to Mitigate Epidemics: A Stackelberg Mean Field Game Approach",
                "abstract": "Motivated by models of epidemic control in large populations, we consider a Stackelberg mean field game model between a principal and a mean field of agents evolving on a finite state space. The agents play a non-cooperative game in which they can control their transition rates between states to minimize an individual cost. The principal can influence the resulting Nash equilibrium through incentives so as to optimize its own objective. We analyze this game using a probabilistic approach. We then propose an application to an epidemic model of SIR type in which the agents control their interaction rate and the principal is a regulator acting with non pharmaceutical interventions. To compute the solutions, we propose an innovative numerical approach based on Monte Carlo simulations and machine learning tools for stochastic optimization. We conclude with numerical experiments by illustrating the impact of the agents' and the regulator's optimal decisions in two models: a basic SIR model with semi-explicit solutions and in a complex model which incorporates more states."
            },
            {
                "arxivId": "1809.04401",
                "title": "Mean-Field Leader-Follower Games with Terminal State Constraint",
                "abstract": "We analyze linear McKean-Vlasov forward-backward SDEs arising in leader-follower games with mean-field type control and terminal state constraints on the state process. We establish an existence and uniqueness of solutions result for such systems in time-weighted spaces as well as a convergence result of the solutions with respect to certain perturbations of the drivers of both the forward and the backward component. The general results are used to solve a novel single-player model of portfolio liquidation under market impact with expectations feedback as well as a novel Stackelberg game of optimal portfolio liquidation with asymmetrically informed players."
            },
            {
                "arxivId": "1802.05438",
                "title": "Mean Field Multi-Agent Reinforcement Learning",
                "abstract": "Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of user interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution. Experiments on resource allocation, Ising model estimation, and battle game tasks verify the learning effectiveness of our mean field approaches in handling many-agent interactions in population."
            },
            {
                "arxivId": "1706.02275",
                "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
                "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies."
            },
            {
                "arxivId": "1509.02971",
                "title": "Continuous control with deep reinforcement learning",
                "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12107",
        "category": "econ",
        "title": "Scenarios for the Transition to AGI",
        "abstract": "We analyze how output and wages behave under different scenarios for technological progress that may culminate in Artificial General Intelligence (AGI), defined as the ability of AI systems to perform all tasks that humans can perform. We assume that human work can be decomposed into atomistic tasks that differ in their complexity. Advances in technology make ever more complex tasks amenable to automation. The effects on wages depend on a race between automation and capital accumulation. If the distribution of task complexity exhibits a sufficiently thick infinite tail, then there is always enough work for humans, and wages may rise forever. By contrast, if the complexity of tasks that humans can perform is bounded and full automation is reached, then wages collapse. But declines may occur even before if large-scale automation outpaces capital accumulation and makes labor too abundant. Automating productivity growth may lead to broad-based gains in the returns to all factors. By contrast, bottlenecks to growth from irreproducible scarce factors may exacerbate the decline in wages.",
        "references": [
            {
                "arxivId": "2309.11690",
                "title": "Explosive growth from AI automation: A review of the arguments",
                "abstract": "We examine whether substantial AI automation could accelerate global economic growth by about an order of magnitude, akin to the economic growth effects of the Industrial Revolution. We identify three primary drivers for such growth: 1) the scalability of an AI ``labor force\"restoring a regime of increasing returns to scale, 2) the rapid expansion of an AI labor force, and 3) a massive increase in output from rapid automation occurring over a brief period of time. Against this backdrop, we evaluate nine counterarguments, including regulatory hurdles, production bottlenecks, alignment issues, and the pace of automation. We tentatively assess these arguments, finding most are unlikely deciders. We conclude that explosive growth seems plausible with AI capable of broadly substituting for human labor, but high confidence in this claim seems currently unwarranted. Key questions remain about the intensity of regulatory responses to AI, physical bottlenecks in production, the economic value of superhuman abilities, and the rate at which AI automation could occur."
            },
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            },
            {
                "arxivId": "2303.01157",
                "title": "How will Language Modelers like ChatGPT Affect Occupations and Industries?",
                "abstract": "Recent dramatic increases in AI language modeling capabilities has led to many questions about the effect of these technologies on the economy. In this paper we present a methodology to systematically assess the extent to which occupations, industries and geographies are exposed to advances in AI language modeling capabilities. We find that the top occupations exposed to language modeling include telemarketers and a variety of post-secondary teachers such as English language and literature, foreign language and literature, and history teachers. We find the top industries exposed to advances in language modeling are legal services and securities, commodities, and investments. We also find a positive correlation between wages and exposure to AI language modeling."
            },
            {
                "arxivId": "2212.08198",
                "title": "Economic impacts of AI-augmented R&D",
                "abstract": "Since its emergence around 2010, deep learning has rapidly become the most important technique in Arti\ufb01cial Intelligence (AI), producing an array of scienti\ufb01c \ufb01rsts in areas as diverse as protein folding, drug discovery, integrated chip design, and weather prediction. As more scientists and engineers adopt deep learning, it is important to consider what e\ufb00ect widespread deployment would have on scienti\ufb01c progress and, ultimately, economic growth. We assess this impact by estimating the idea production function for AI in two computer vision tasks that are considered key test-beds for deep learning and show that AI idea production is notably more capital-intensive than traditional R&D. Because increasing the capital-intensity of R&D accelerates the investments that make scientists and engineers more productive, our work suggests that AI-augmented R&D has the potential to speed up technological change and economic growth."
            },
            {
                "arxivId": "2204.01691",
                "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "abstract": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's\"hands and eyes,\"while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/."
            },
            {
                "arxivId": "2202.05924",
                "title": "Compute Trends Across Three Eras of Machine Learning",
                "abstract": "Compute, data, and algorithmic advances are the three fundamental factors that drive progress in modern Machine Learning (ML). In this paper we study trends in the most readily quantified factor - compute. We make three novel contributions: (1) we curate a dataset with the training compute of 123 milestone ML systems, 3\u00d7 larger than previous such datasets. (2) We frame the trends in compute in in three eras - the Pre Deep Learning Era, the Deep Learning Era, and the Large-Scale Era, based on our identification of a novel trend emerging around 2015. (3) We find a Deep Learning Era compute doubling time of around 6 months, significantly longer than previous findings. Overall, our work highlights the fast-growing compute requirements for training advanced ML systems."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12108",
        "category": "econ",
        "title": "Does AI help humans make better decisions? A methodological framework for experimental evaluation",
        "abstract": "The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. We apply the proposed methodology to the data from our own randomized controlled trial of a pretrial risk assessment instrument. We find that AI recommendations do not improve the classification accuracy of a judge's decision to impose cash bail. Our analysis also shows that AI-alone decisions generally perform worse than human decisions with or without AI assistance. Finally, AI recommendations tend to impose cash bail on non-white arrestees more often than necessary when compared to white arrestees.",
        "references": [
            {
                "arxivId": "2306.07566",
                "title": "Learning under Selective Labels with Data from Heterogeneous Decision-makers: An Instrumental Variable Approach",
                "abstract": "We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction rules robust to the label selection bias in both identification settings. Finally, we apply our proposed approach to a semi-synthetic financial dataset and demonstrate its superior performance in the presence of selection bias."
            },
            {
                "arxivId": "2302.06503",
                "title": "Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making",
                "abstract": "A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on \u201cground truth\u2019\u2019 labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision \u2013 including latent constructs that are not directly observable, such as disease status, the \u201ctoxicity\u201d of online comments, or future \u201cjob performance\u201d \u2013 predictive models target proxy labels that are readily available in existing datasets. Predictive models\u2019 reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research."
            },
            {
                "arxivId": "2204.05478",
                "title": "Heterogeneity in Algorithm-Assisted Decision-Making: A Case Study in Child Abuse Hotline Screening",
                "abstract": "Algorithmic risk assessment tools are now commonplace in public sector domains such as criminal justice and human services. These tools are intended to aid decision makers in systematically using rich and complex data captured in administrative systems. In this study we investigate sources of heterogeneity in the alignment between worker decisions and algorithmic risk scores in the context of a real world child abuse hotline screening use case. Specifically, we focus on heterogeneity related to worker experience. We find that senior workers are far more likely to screen in referrals for investigation, even after we control for the observed algorithmic risk score and other case characteristics. We also observe that the decisions of less-experienced workers are more closely aligned with algorithmic risk scores than those of senior workers who had decision-making experience prior to the tool being introduced. While screening decisions vary across child race, we do not find evidence of racial differences in the relationship between worker experience and screening decisions. Our findings indicate that it is important for agencies and system designers to consider ways of preserving institutional knowledge when introducing algorithms into high employee turnover settings such as child welfare call screening."
            },
            {
                "arxivId": "2101.00352",
                "title": "Characterizing Fairness Over the Set of Good Models Under Selective Labels",
                "abstract": "Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the\"Rashomon Effect.\"These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or\"the set of good models.\"Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) replace an existing model with one that has better fairness properties; or 2) audit for predictive bias. We illustrate these uses cases on a real-world credit-scoring task and a recidivism prediction task."
            },
            {
                "arxivId": "2012.15816",
                "title": "Fairness in Machine Learning",
                "abstract": null
            },
            {
                "arxivId": "2012.02845",
                "title": "Experimental Evaluation of Algorithm-Assisted Human Decision-Making: Application to Pretrial Public Safety Assessment",
                "abstract": "\n Despite an increasing reliance on fully-automated algorithmic decision-making in our day-to-day lives, human beings still make highly consequential decisions. As frequently seen in business, healthcare, and public policy, recommendations produced by algorithms are provided to human decision-makers to guide their decisions. While there exists a fast-growing literature evaluating the bias and fairness of such algorithmic recommendations, an overlooked question is whether they help humans make better decisions. We develop a general statistical methodology for experimentally evaluating the causal impacts of algorithmic recommendations on human decisions. We also show how to examine whether algorithmic recommendations improve the fairness of human decisions and derive the optimal decision rules under various settings. We apply the proposed methodology to preliminary data from the first-ever randomized controlled trial that evaluates the pretrial Public Safety Assessment (PSA) in the criminal justice system. A goal of the PSA is to help judges decide which arrested individuals should be released. On the basis of the preliminary data available, we find that providing the PSA to the judge has little overall impact on the judge\u2019s decisions and subsequent arrestee behavior. Our analysis, however, yields some potentially suggestive evidence that the PSA may help avoid unnecessarily harsh decisions for female arrestees regardless of their risk levels while it encourages the judge to make stricter decisions for male arrestees who are deemed to be risky. In terms of fairness, the PSA appears to increase an existing gender difference while having little effect on any racial differences in judges\u2019 decision. Finally, we find that the PSA\u2019s recommendations might be unnecessarily severe unless the cost of a new crime is sufficiently high."
            },
            {
                "arxivId": "2005.10400",
                "title": "Principal Fairness for Human and Algorithmic Decision-Making",
                "abstract": "Using the concept of principal stratification from the causal inference literature, we introduce a new notion of fairness, called principal fairness, for human and algorithmic decision-making. The key idea is that one should not discriminate among individuals who would be similarly affected by the decision. Unlike the existing statistical definitions of fairness, principal fairness explicitly accounts for the fact that individuals can be impacted by the decision. We propose an axiomatic assumption that all groups are created equal. This assumption is motivated by a belief that protected attributes such as race and gender should have no direct causal effects on potential outcomes. Under this assumption, we show that principal fairness implies all three existing statistical fairness criteria once we account for relevant covariates. This result also highlights the essential role of conditioning covariates in resolving the previously recognized tradeoffs between the existing statistical fairness criteria. Finally, we discuss how to empirically choose conditioning covariates and then evaluate the principal fairness of a particular decision."
            },
            {
                "arxivId": "1909.00066",
                "title": "Counterfactual risk assessments, evaluation, and fairness",
                "abstract": "Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice."
            },
            {
                "arxivId": "1801.10408",
                "title": "'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions",
                "abstract": "Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions."
            },
            {
                "arxivId": "1611.09925",
                "title": "Bounded, efficient and multiply robust estimation of average treatment effects using instrumental variables",
                "abstract": "Instrumental variables are widely used for estimating causal effects in the presence of unmeasured confounding. Under the standard instrumental variable model, however, the average treatment effect is only partially identifiable. To address this, we propose novel assumptions that enable identification of the average treatment effect. Our identification assumptions are clearly separated from model assumptions that are needed for estimation, so researchers are not required to commit to a specific observed data model in establishing identification. We then construct multiple estimators that are consistent under three different observed data models, and multiply robust estimators that are consistent in the union of these observed data models. We pay special attention to the case of binary outcomes, for which we obtain bounded estimators of the average treatment effect that are guaranteed to lie between \u22121 and 1. Our approaches are illustrated with simulations and a data analysis evaluating the causal effect of education on earnings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12183",
        "category": "econ",
        "title": "Fragile Stable Matchings",
        "abstract": "We show how fragile stable matchings are in a decentralized one-to-one matching setting. The classical work of Roth and Vande Vate (1990) suggests simple decentralized dynamics in which randomly-chosen blocking pairs match successively. Such decentralized interactions guarantee convergence to a stable matching. Our first theorem shows that, under mild conditions, any unstable matching -- including a small perturbation of a stable matching -- can culminate in any stable matching through these dynamics. Our second theorem highlights another aspect of fragility: stabilization may take a long time. Even in markets with a unique stable matching, where the dynamics always converge to the same matching, decentralized interactions can require an exponentially long duration to converge. A small perturbation of a stable matching may lead the market away from stability and involve a sizable proportion of mismatched participants for extended periods. Our results hold for a broad class of dynamics.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12260",
        "category": "econ",
        "title": "The Best of Many Robustness Criteria in Decision Making: Formulation and Application to Robust Pricing",
        "abstract": "In robust decision-making under non-Bayesian uncertainty, different robust optimization criteria, such as maximin performance, minimax regret, and maximin ratio, have been proposed. In many problems, all three criteria are well-motivated and well-grounded from a decision-theoretic perspective, yet different criteria give different prescriptions. This paper initiates a systematic study of overfitting to robustness criteria. How good is a prescription derived from one criterion when evaluated against another criterion? Does there exist a prescription that performs well against all criteria of interest? We formalize and study these questions through the prototypical problem of robust pricing under various information structures, including support, moments, and percentiles of the distribution of values. We provide a unified analysis of three focal robust criteria across various information structures and evaluate the relative performance of mechanisms optimized for each criterion against the others. We find that mechanisms optimized for one criterion often perform poorly against other criteria, highlighting the risk of overfitting to a particular robustness criterion. Remarkably, we show it is possible to design mechanisms that achieve good performance across all three criteria simultaneously, suggesting that decision-makers need not compromise among criteria.",
        "references": [
            {
                "arxivId": "2205.09008",
                "title": "Distributional Robustness: From Pricing to Auctions",
                "abstract": "We study robust mechanism design for revenue maximization when selling a single item in an auction, assuming that only the mean of the value distribution and an upper bound on the bidders' valuations for the item are known. Robust mechanism design is a rising alternative to Bayesian mechanism design, which yields designs that do not rely on assumptions like full distributional knowledge, but rather only partial knowledge of the distributions. We seek a mechanism that maximizes revenue over the worst-case distribution compatible with the known parameters. Such a mechanism arises as an equilibrium of a zero-sum game between the seller and an adversary who chooses the distribution, and so can be referred to as the max-min mechanism. Carrasco et al. [2018] derive the max-min pricing when the seller faces a single bidder for the item. We go from max-min pricing to max-min auctions by studying the canonical setting of two i.i.d. bidders, and show the max-min mechanism is the second-price auction with a randomized reserve. We derive a closed-form solution for the distribution over reserve prices, as well as the worst-case value distribution, for which there is simple economic intuition. We also derive a closed-form solution for the max-min reserve price distribution for any number of bidders, and we show that unlike the case of two bidders, a second-price auction with a randomized reserve cannot be an equilibrium for more than two bidders. Our technique for solving the zero-sum game is quite different than that of Carrasco et al. -- we focus on a reduced zero-sum game, where the seller can only choose a distribution for a second-price auction with a randomized reserve price (rather than any mechanism). We then analyze a discretized version of the setting to find conditions an equilibrium would satisfy. By refining the discretization grid, we are able to achieve differential equations, and solving them yields closed-form non-discretized distributions. The resulting distributions for the seller and the adversary are later shown to be an equilibrium for the reduced zero-sum game. For the two-bidder case, we expand our result to an equilibrium of the original zero-sum game, where the seller is not limited to second price auctions with reserve. The full version of the paper is available at https://arxiv.org/abs/2205.09008."
            },
            {
                "arxivId": "2204.10478",
                "title": "On the Robustness of Second-Price Auctions in Prior-Independent Mechanism Design",
                "abstract": "Classical Bayesian mechanism design relies on the common prior assumption, but the common prior is often not available in practice. We study the design of prior-independent mechanisms that relax this assumption: the seller is selling an indivisible item to n buyers such that the buyers' valuations are drawn from a joint distribution that is unknown to both the buyers and the seller; buyers do not need to form beliefs about competitors, and the seller assumes the distribution is adversarially chosen from a specified class. We measure performance through the worst-caseregret, or the difference between the expected revenue achievable with perfect knowledge of buyers' valuations and the actual mechanism revenue. We study a broad set of classes of valuation distributions that capture a wide spectrum of possible dependencies: independent and identically distributed (i.i.d.) distributions, mixtures of i.i.d. distributions, affiliated and exchangeable distributions, exchangeable distributions, and all joint distributions. We derive in quasi closed form the minimax values and the associated optimal mechanism. In particular, we show that the first three classes admit the same minimax regret value, which is decreasing with the number of competitors, while the last two have the same minimax regret equal to that of the case n = 1. Furthermore, we show that the minimax optimal mechanisms have a simple form across all settings: asecond-price auction with random reserve prices, which shows its robustness in prior-independent mechanism design. En route to our results, we also develop a principled methodology to determine the form of the optimal mechanism and worst-case distribution via first-order conditions that should be of independent interest in other minimax problems. The full paper is available at https://arxiv.org/abs/2204.10478 and https://ssrn.com/abstract=4090071."
            },
            {
                "arxivId": "2103.05611",
                "title": "Optimal Pricing with a Single Point",
                "abstract": "We study the following fundamental data-driven pricing problem. How can/should a decision-maker price its product based on observations at a single historical price? The decision-maker optimizes over (potentially randomized) pricing policies to maximize the worst-case ratio of the revenue it can garner compared to an oracle with full knowledge of the distribution of values, when the latter is only assumed to belong to broad non-parametric set. In particular, our framework applies to the widely used regular and monotone non-decreasing hazard rate (mhr) classes of distributions. For settings where the seller knows the exact probability of sale associated with one historical price or only a confidence interval for it, we fully characterize optimal performance and near-optimal pricing algorithms that adjust to the information at hand. As examples, against mhr distributions, we show that it is possible to guarantee $85%$ of oracle performance if one knows that half of the customers have bought at the historical price, and if only $1%$ of the customers bought, it still possible to guarantee $51%$ of oracle performance. The framework we develop leads to new insights on the value of information for pricing, as well as the value of randomization. In addition, it is general and allows to characterize optimal deterministic mechanisms and incorporate uncertainty in the probability of sale."
            },
            {
                "arxivId": "2006.05192",
                "title": "An Optimal Distributionally Robust Auction",
                "abstract": "An indivisible object may be sold to one of $n$ agents who know their valuations of the object. The seller would like to use a revenue-maximizing mechanism but her knowledge of the valuations' distribution is scarce: she knows only the means (which may be different) and an upper bound for valuations. Valuations may be correlated. Using a constructive approach based on duality, we prove that a mechanism that maximizes the worst-case expected revenue among all deterministic dominant-strategy incentive compatible, ex post individually rational mechanisms takes the following form: (1) the bidders submit bids $b_i$; (2) for each bidder, a linear score $s_i=\\beta_ib_i-\\alpha_i$ is calculated where $\\alpha_i$, $\\beta_i$ are fixed parameters; (3) the object is awarded to the agent with the highest score, provided it's nonnegative; (4) the winning bidder pays the minimal amount he would need to bid to still win in the auction. The set of optimal mechanisms includes other mechanisms but all those have to be close to the optimal linear score auction in a certain sense. When means are high, all optimal mechanisms share the linearity property. Second-price auction without a reserve is an optimal mechanism when the number of symmetric bidders is sufficiently high."
            },
            {
                "arxivId": "2001.10157",
                "title": "Benchmark Design and Prior-independent Optimization",
                "abstract": "This paper compares two leading approaches for robust optimization in the models of online algorithms and mechanism design. Competitive analysis compares the performance of an online algorithm to an offline benchmark in worst-case over inputs, and prior-independent mechanism design compares the expected performance of a mechanism on an unknown distribution (of inputs, i.e., agent values) to the optimal mechanism for the distribution in worst case over distributions. For competitive analysis, a critical concern is the choice of benchmark. This paper gives a method for selecting a good benchmark. We show that optimal algorithm/mechanism for the optimal benchmark is equal to the prior-independent optimal algorithm/mechanism. We solve a central open question in prior-independent mechanism design, namely we identify the prior-independent revenue-optimal mechanism for selling a single item to two agents with i.i.d. and regularly distributed values. We use this solution to solve the corresponding benchmark design problem. Via this solution and the above equivalence of prior-independent mechanism design and competitive analysis (a.k.a. prior-free mechanism design) we show that the standard method for lower bounds of prior-free mechanisms is not generally tight for the benchmark design program.11For the full version of this work, see https://arxiv.org/abs/2001.10157."
            },
            {
                "arxivId": "1908.05659",
                "title": "Frameworks and Results in Distributionally Robust Optimization",
                "abstract": "The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. Statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. A modeling framework, called distributionally robust optimization (DRO), has recently received significant attention in both the operations research and statistical learning communities. This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization."
            },
            {
                "arxivId": "1812.11896",
                "title": "Approximately Optimal Mechanism Design",
                "abstract": "The field of optimal mechanism design enjoys a beautiful and well-developed theory, as well as several killer applications. Rules of thumb produced by the field influence everything from how governments sell wireless spectrum licenses to how the major search engines auction off online advertising. There are, however, some basic problems for which the traditional optimal mechanism design approach is ill suited\u2014either because it makes overly strong assumptions or because it advocates overly complex designs. This article reviews several common issues with optimal mechanisms, including exorbitant communication, computation, and informational requirements; it also presents several examples demonstrating that relaxing the goal to designing an approximately optimal mechanism allows us to reason about fundamental questions that seem out of reach of the traditional theory."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12456",
        "category": "econ",
        "title": "Inflation Target at Risk: A Time-Varying Parameter Distributional Regression",
        "abstract": "Macro variables frequently display time-varying distributions, driven by the dynamic and evolving characteristics of economic, social, and environmental factors that consistently reshape the fundamental patterns and relationships governing these variables. To better understand the distributional dynamics beyond the central tendency, this paper introduces a novel semi-parametric approach for constructing time-varying conditional distributions, relying on the recent advances in distributional regression. We present an efficient precision-based Markov Chain Monte Carlo algorithm that simultaneously estimates all model parameters while explicitly enforcing the monotonicity condition on the conditional distribution function. Our model is applied to construct the forecasting distribution of inflation for the U.S., conditional on a set of macroeconomic and financial indicators. The risks of future inflation deviating excessively high or low from the desired range are carefully evaluated. Moreover, we provide a thorough discussion about the interplay between inflation and unemployment rates during the Global Financial Crisis, COVID, and the third quarter of 2023.",
        "references": [
            {
                "arxivId": "2103.03632",
                "title": "Modeling tail risks of inflation using unobserved component quantile regressions",
                "abstract": null
            },
            {
                "arxivId": "1910.10779",
                "title": "Fast and Flexible Bayesian Inference in Time-varying Parameter Regression Models",
                "abstract": "Abstract In this article, we write the time-varying parameter (TVP) regression model involving K explanatory variables and T observations as a constant coefficient regression model with KT explanatory variables. In contrast with much of the existing literature which assumes coefficients to evolve according to a random walk, a hierarchical mixture model on the TVPs is introduced. The resulting model closely mimics a random coefficients specification which groups the TVPs into several regimes. These flexible mixtures allow for TVPs that feature a small, moderate or large number of structural breaks. We develop computationally efficient Bayesian econometric methods based on the singular value decomposition of the KT regressors. In artificial data, we find our methods to be accurate and much faster than standard approaches in terms of computation time. In an empirical exercise involving inflation forecasting using a large number of predictors, we find our models to forecast better than alternative approaches and document different patterns of parameter change than are found with approaches which assume random walk evolution of parameters."
            },
            {
                "arxivId": "2201.07303",
                "title": "Large Hybrid Time-Varying Parameter VARs",
                "abstract": "Abstract Time-varying parameter VARs with stochastic volatility are routinely used for structural analysis and forecasting in settings involving a few endogenous variables. Applying these models to high-dimensional datasets has proved to be challenging due to intensive computations and over-parameterization concerns. We develop an efficient Bayesian sparsification method for a class of models we call hybrid TVP-VARs\u2014VARs with time-varying parameters in some equations but constant coefficients in others. Specifically, for each equation, the new method automatically decides whether the VAR coefficients and contemporaneous relations among variables are constant or time-varying. Using U.S. datasets of various dimensions, we find evidence that the parameters in some, but not all, equations are time varying. The large hybrid TVP-VAR also forecasts better than many standard benchmarks."
            },
            {
                "arxivId": "1612.00111",
                "title": "Bayesian non-parametric simultaneous quantile regression for complete and grid data",
                "abstract": null
            },
            {
                "arxivId": "1603.04166",
                "title": "The normal law under linear restrictions: simulation and estimation via minimax tilting",
                "abstract": "Simulation from the truncated multivariate normal distribution in high dimensions is a recurrent problem in statistical computing and is typically only feasible by using approximate Markov chain Monte Carlo sampling. We propose a minimax tilting method for exact independently and identically distributed data simulation from the truncated multivariate normal distribution. The new methodology provides both a method for simulation and an efficient estimator to hitherto intractable Gaussian integrals. We prove that the estimator has a rare vanishing relative error asymptotic property. Numerical experiments suggest that the scheme proposed is accurate in a wide range of set\u2010ups for which competing estimation schemes fail. We give an application to exact independently and identically distributed data simulation from the Bayesian posterior of the probit regression model."
            },
            {
                "arxivId": "1502.01115",
                "title": "Regression Adjustment for Noncrossing Bayesian Quantile Regression",
                "abstract": "ABSTRACT A two-stage approach is proposed to overcome the problem in quantile regression, where separately fitted curves for several quantiles may cross. The standard Bayesian quantile regression model is applied in the first stage, followed by a Gaussian process regression adjustment, which monotonizes the quantile function while borrowing strength from nearby quantiles. The two-stage approach is computationally efficient, and more general than existing techniques. The method is shown to be competitive with alternative approaches via its performance in simulated examples. Supplementary materials for the article are available online."
            },
            {
                "arxivId": "1205.0310",
                "title": "Bayesian Inference for Logistic Models Using P\u00f3lya\u2013Gamma Latent Variables",
                "abstract": "We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of P\u00f3lya\u2013Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis\u2013Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the P\u00f3lya\u2013Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "0904.0951",
                "title": "Inference on Counterfactual Distributions",
                "abstract": "In this paper we develop procedures for performing inference in regression models about how potential policy interventions affect the entire marginal distribution of an outcome of interest. These policy interventions consist of either changes in the distribution of covariates related to the outcome holding the conditional distribution of the outcome given covariates fixed, or changes in the conditional distribution of the outcome given covariates holding the marginal distribution of the covariates fixed. Under either of these assumptions, we obtain uniformly consistent estimates and functional central limit theorems for the counterfactual and status quo marginal distributions of the outcome as well as other function-valued effects of the policy, including, for example, the effects of the policy on the marginal distribution function, quantile function, and other related functionals. We construct simultaneous confidence sets for these functions; these sets take into account the sampling variation in the estimation of the relationship between the outcome and covariates. Our procedures rely on, and our theory covers, all main regression approaches for modeling and estimating conditional distributions, focusing especially on classical, quantile, duration, and distribution regressions. Our procedures are general and accommodate both simple unitary changes in the values of a given covariate as well as changes in the distribution of the covariates or the conditional distribution of the outcome given covariates of general form. We apply the procedures to examine the effects of labor market institutions on the U.S. wage distribution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12653",
        "category": "econ",
        "title": "Composite likelihood estimation of stationary Gaussian processes with a view toward stochastic volatility",
        "abstract": "We develop a framework for composite likelihood inference of parametric continuous-time stationary Gaussian processes. We derive the asymptotic theory of the associated maximum composite likelihood estimator. We implement our approach on a pair of models that has been proposed to describe the random log-spot variance of financial asset returns. A simulation study shows that it delivers good performance in these settings and improves upon a method-of-moments estimation. In an application, we inspect the dynamic of an intraday measure of spot variance computed with high-frequency data from the cryptocurrency market. The empirical evidence supports a mechanism, where the short- and long-term correlation structure of stochastic volatility are decoupled in order to capture its properties at different time scales.",
        "references": [
            {
                "arxivId": "2206.14114",
                "title": "On the universality of the volatility formation process: when machine learning and rough volatility agree",
                "abstract": "We train an LSTM network based on a pooled dataset made of hundreds of liquid stocks aiming to forecast the next daily realized volatility for all stocks. Showing the consistent outperformance of this universal LSTM relative to other asset-specific parametric models, we uncover nonparametric evidences of a universal volatility formation mechanism across assets relating past market realizations, including daily returns and volatilities, to current volatilities. A parsimonious parametric forecasting device combining the rough fractional stochastic volatility and quadratic rough Heston models with fixed parameters results in the same level of performance as the universal LSTM, which confirms the universality of the volatility formation process from a parametric perspective."
            },
            {
                "arxivId": "2107.03674",
                "title": "Inference and forecasting for continuous-time integer-valued trawl processes",
                "abstract": null
            },
            {
                "arxivId": "2010.04610",
                "title": "A GMM approach to estimate the roughness of stochastic volatility",
                "abstract": null
            },
            {
                "arxivId": "1707.00610",
                "title": "Option pricing under fast-varying and rough stochastic volatility",
                "abstract": null
            },
            {
                "arxivId": "1610.00332",
                "title": "Decoupling the Short- and Long-Term Behavior of Stochastic Volatility",
                "abstract": "\n We introduce a new class of continuous-time models of the stochastic volatility of asset prices. The models can simultaneously incorporate roughness and slowly decaying autocorrelations, including proper long memory, which are two stylized facts often found in volatility data. Our prime model is based on the so-called Brownian semistationary process and we derive a number of theoretical properties of this process, relevant to volatility modeling. Applying the models to realized volatility measures covering a vast panel of assets, we find evidence consistent with the hypothesis that time series of realized measures of volatility are both rough and very persistent. Lastly, we illustrate the utility of the models in an extensive forecasting study; we find that the models proposed in this article outperform a wide array of benchmarks considerably, indicating that it pays off to exploit both roughness and persistence in volatility forecasting."
            },
            {
                "arxivId": "1608.01895",
                "title": "Semiparametric inference on the fractal index of Gaussian and conditionally Gaussian time series data",
                "abstract": "Using theory on (conditionally) Gaussian processes with stationary increments developed in Barndorff-Nielsen et al. (2009, 2011), this paper presents a general semiparametric approach to conducting inference on the fractal index, $\\alpha$, of a time series. Our setup encompasses a large class of Gaussian processes and we show how to extend it to a large class of non-Gaussian models as well. It is proved that the asymptotic distribution of the estimator of $\\alpha$ does not depend on the specifics of the data generating process for the observations, but only on the value of $\\alpha$ and a \"heteroskedasticity\" factor. Using this, we propose a simulation-based approach to inference, which is easily implemented and is valid more generally than asymptotic analysis. We detail how the methods can be applied to test whether a stochastic process is a non-semimartingale. Finally, the methods are illustrated in two empirical applications motivated from finance. We study time series of log-prices and log-volatility from $29$ individual US stocks; no evidence of non-semimartingality in asset prices is found, but we do find evidence of non-semimartingality in volatility. This confirms a recently proposed conjecture that stochastic volatility processes of financial assets are rough (Gatheral et al., 2014)."
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            },
            {
                "arxivId": "1308.5115",
                "title": "Power-law models for infectious disease spread",
                "abstract": "Short-time human travel behaviour can be described by a power law with respect to distance. We incorporate this information in space\u2013time models for infectious disease surveillance data to better capture the dynamics of disease spread. Two previously established model classes are extended, which both decompose disease risk additively into endemic and epidemic components: a spatio-temporal point process model for individual-level data and a multivariate time-series model for aggregated count data. In both frameworks, a power-law decay of spatial interaction is embedded into the epidemic component and estimated jointly with all other unknown parameters using (penalised) likelihood inference. Whereas the power law can be based on Euclidean distance in the point process model, a novel formulation is proposed for count data where the power law depends on the order of the neighbourhood of discrete spatial units. The performance of the new approach is investigated by a reanalysis of individual cases of invasive meningococcal disease in Germany (2002\u20132008) and count data on influenza in 140 administrative districts of Southern Germany (2001\u20132008). In both applications, the power law substantially improves model fit and predictions, and is reasonably close to alternative qualitative formulations, where distance and order of neighbourhood, respectively, are treated as a factor. Implementation in the R package surveillance allows the approach to be applied in other settings."
            },
            {
                "arxivId": "1309.4667",
                "title": "Volatility occupation times",
                "abstract": "We propose nonparametric estimators of the occupation measure and the occupation density of the diffusion coefficient (stochastic volatility) of a discretely observed It\\^{o} semimartingale on a fixed interval when the mesh of the observation grid shrinks to zero asymptotically. In a first step we estimate the volatility locally over blocks of shrinking length, and then in a second step we use these estimates to construct a sample analogue of the volatility occupation time and a kernel-based estimator of its density. We prove the consistency of our estimators and further derive bounds for their rates of convergence. We use these results to estimate nonparametrically the quantiles associated with the volatility occupation measure."
            },
            {
                "arxivId": "0909.0827",
                "title": "Estimation of Volatility Functionals in the Simultaneous Presence of Microstructure Noise and Jumps",
                "abstract": "We propose a new concept of modulated bipower variation for diffusion models with microstructure noise. We show that this method provides simple estimates for such important quantities as integrated volatility or integrated quarticity. Under mild conditions the consistency of modulated bipower variation is proven. Under further assumptions we prove stable convergence of our estimates with the optimal rate n^(-1/4). Moreover, we construct estimates which are robust to finite activity jumps."
            },
            {
                "arxivId": "math/0607378",
                "title": "Non\u2010parametric Threshold Estimation for Models with Stochastic Diffusion Coefficient and Jumps",
                "abstract": "Abstract.\u2002 We consider a stochastic process driven by diffusions and jumps. Given a discrete record of observations, we devise a technique for identifying the times when jumps larger than a suitably defined threshold occurred. This allows us to determine a consistent non\u2010parametric estimator of the integrated volatility when the infinite activity jump component is L\u00e9vy. Jump size estimation and central limit results are proved in the case of finite activity jumps. Some simulations illustrate the applicability of the methodology in finite samples and its superiority on the multipower variations especially when it is not possible to use high frequency data."
            },
            {
                "arxivId": "physics/0109031",
                "title": "Stochastic Models That Separate Fractal Dimension and the Hurst Effect",
                "abstract": "Fractal behavior and long-range dependence have been observed in an astonishing number of physical, biological, geological, and socioeconomic systems. Time series, profiles, and surfaces have been characterized by their fractal dimension, a measure of roughness, and by the Hurst coefficient, a measure of long-memory dependence. Both phenomena have been modeled and explained by self-affine random functions, such as fractional Gaussian noise and fractional Brownian motion. The assumption of statistical self-affinity implies a linear relationship between fractal dimension and Hurst coefficient and thereby links the two phenomena. This article introduces stochastic models that allow for any combination of fractal dimension and Hurst coefficient. Associated software for the synthesis of images with arbitrary, prespecified fractal properties and power-law correlations is available. The new models suggest a test for self-affinity that assesses coupling and decoupling of local and global behavior."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12694",
        "category": "econ",
        "title": "Performance, Knowledge Acquisition and Satisfaction in Self-selected Groups: Evidence from a Classroom Field Experiment",
        "abstract": "We investigate how to efficiently set up work groups to boost group productivity, individual satisfaction, and learning. Therefore, we conduct a natural field experiment in a compulsory undergraduate course and study differences between self-selected and randomly assigned groups. We find that self-selected groups perform significantly worse on group assignments. Yet, students in self-selected groups learn more and are more satisfied than those in randomly assigned groups. The effect of allowing students to pick group members dominates the effect of different group compositions in self-selected groups: When controlling for the skill, gender, and home region composition of groups, the differences between self-selected and randomly formed groups persist almost unaltered. The distribution of GitHub commits per group reveals that the better average performance of randomly assigned groups is mainly driven by highly skilled individuals distributed over more groups due to the assignment mechanism. Moreover, these highly skilled individuals contribute more to the group in randomly formed groups. We argue that this mechanism explains why self-selected groups perform worse on the projects but acquire more knowledge than randomly formed groups. These findings are relevant for setting up workgroups in academic, business, and governmental organizations when tasks are not constrained to the skill set of specific individuals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-20.json",
        "arxivId": "2403.12917",
        "category": "econ",
        "title": "When is Trust Robust?",
        "abstract": "We examine an economy in which interactions are more productive if agents can trust others to refrain from cheating. Some agents are scoundrels, who always cheat, while others cheat only if the cost of cheating, a decreasing function of the proportion of cheaters, is sufficiently low. The economy exhibits multiple equilibria. As the proportion of scoundrels in the economy declines, the high-trust equilibrium can be disrupted by arbitrarily small perturbations or infusions of low-trust agents, while the low-trust equilibrium becomes impervious to perturbations and infusions of high-trust agents. The resilience of trust may thus hinge upon the prevalence of scoundrels.",
        "references": [
            {
                "arxivId": "1711.09504",
                "title": "A typology of social capital and associated network measures",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2210.13562",
        "category": "econ",
        "title": "Prediction intervals for economic fixed-event forecasts",
        "abstract": "The fixed-event forecasting setup is common in economic policy. It involves a sequence of forecasts of the same (`fixed') predictand, so that the difficulty of the forecasting problem decreases over time. Fixed-event point forecasts are typically published without a quantitative measure of uncertainty. To construct such a measure, we consider forecast postprocessing techniques tailored to the fixed-event case. We develop regression methods that impose constraints motivated by the problem at hand, and use these methods to construct prediction intervals for gross domestic product (GDP) growth in Germany and the US.",
        "references": [
            {
                "arxivId": "2205.04216",
                "title": "Forecast combinations: An over 50-year review",
                "abstract": null
            },
            {
                "arxivId": "2105.03101",
                "title": "Consistent Estimation of Distribution Functions under Increasing Concave and Convex Stochastic Ordering",
                "abstract": "Abstract A random variable Y 1 is said to be smaller than Y 2 in the increasing concave stochastic order if for all increasing concave functions for which the expected values exist, and smaller than Y 2 in the increasing convex order if for all increasing convex \u03c8. This article develops nonparametric estimators for the conditional cumulative distribution functions of a response variable Y given a covariate X, solely under the assumption that the conditional distributions are increasing in x in the increasing concave or increasing convex order. Uniform consistency and rates of convergence are established both for the K-sample case and for continuously distributed X."
            },
            {
                "arxivId": "2101.10359",
                "title": "A benchmark model for fixed-target Arctic sea ice forecasting",
                "abstract": null
            },
            {
                "arxivId": "2006.05527",
                "title": "Accelerating the Pool-Adjacent-Violators Algorithm for Isotonic Distributional Regression",
                "abstract": null
            },
            {
                "arxivId": "2005.12881",
                "title": "Evaluating epidemic forecasts in an interval format",
                "abstract": "For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction."
            },
            {
                "arxivId": "1106.1638",
                "title": "Combining Predictive Distributions",
                "abstract": "Predictive distributions need to be aggregated when probabilistic forecasts are merged, or when expert opinions expressed in terms of probability distributions are fused. We take a prediction space approach that applies to discrete, mixed discrete-continuous and continuous predictive distributions alike, and study combination formulas for cumulative distribution functions from the perspectives of coherence, probabilistic and conditional calibration, and dispersion. Both linear and non-linear aggregation methods are investigated, including generalized, spread-adjusted and beta-transformed linear pools. The effects and techniques are demonstrated theoretically, in simulation examples, and in case studies on density forecasts for S&P 500 returns and daily maximum temperature at Seattle-Tacoma Airport."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2305.00777",
        "category": "econ",
        "title": "Signaling with Commitment",
        "abstract": "We study the canonical signaling game, endowing the sender with commitment power: before learning the state, sender designs a strategy, which maps the state into a probability distribution over actions. We provide a geometric characterization of the sender's attainable payoffs, described by the topological join of the graphs of the interim payoff functions associated with different sender actions. We extend the sender's commitment power to the design of a communication protocol, characterizing whether and how sender benefits from revealing information about the state, beyond what is inferred from his action. We apply our results to the design of adjudication procedures, rating or grading systems, and sequencing algorithms for online platforms.",
        "references": [
            {
                "arxivId": "1812.04211",
                "title": "The Cost of Information: The Case of Constant Marginal Costs",
                "abstract": "We develop an axiomatic theory of information acquisition that captures the idea of constant marginal costs in information production: the cost of generating two independent signals is the sum of their costs, and generating a signal with probability half costs half its original cost. Together with Blackwell monotonicity and a continuity condition, these axioms determine the cost of a signal up to a vector of parameters. These parameters have a clear economic interpretation and determine the difficulty of distinguishing states. (JEL D82, D83)"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2308.05912",
        "category": "econ",
        "title": "Ideological ambiguity and political spectrum",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2309.05816",
        "category": "econ",
        "title": "A Duality Between Utility Transforms and Probability Distortions",
        "abstract": "In this paper, we establish a mathematical duality between utility transforms and probability distortions. These transforms play a central role in decision under risk by forming the foundation for the classic theories of expected utility, dual utility, and rank-dependent utility. Our main results establish that probability distortions are characterized by commutation with utility transforms, and utility transforms are characterized by commutation with probability distortions. These results require no additional conditions, and hence each class can be axiomatized with only one property. Moreover, under monotonicity, rank-dependent utility transforms can be characterized by set commutation with either utility transforms or probability distortions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2403.08183",
        "category": "econ",
        "title": "Causal Interpretation of Estimands Defined by Exposure Mappings",
        "abstract": "In settings with interference, researchers commonly define estimands using exposure mappings to summarize neighborhood variation in treatment assignments. This paper studies the causal interpretation of these estimands under weak restrictions on interference. We demonstrate that the estimands can exhibit unpalatable sign reversals under conventional identification conditions. This motivates the formulation of sign preservation criteria for causal interpretability. To satisfy preferred criteria, it is necessary to impose restrictions on interference, either in potential outcomes or selection into treatment. We provide sufficient conditions and show that they can be satisfied by nonparametric models with interference in both the outcome and selection stages.",
        "references": [
            {
                "arxivId": "2312.10333",
                "title": "Logit-based alternatives to two-stage least squares",
                "abstract": "We propose logit-based IV and augmented logit-based IV estimators that serve as alternatives to the traditionally used 2SLS estimator in the model where both the endogenous treatment variable and the corresponding instrument are binary. Our novel estimators are as easy to compute as the 2SLS estimator but have an advantage over the 2SLS estimator in terms of causal interpretability. In particular, in certain cases where the probability limits of both our estimators and the 2SLS estimator take the form of weighted-average treatment effects, our estimators are guaranteed to yield non-negative weights whereas the 2SLS estimator is not."
            },
            {
                "arxivId": "2210.08698",
                "title": "A Design-Based Riesz Representation Framework for Randomized Experiments",
                "abstract": "We describe a new design-based framework for drawing causal inference in randomized experiments. Causal effects in the framework are defined as linear functionals evaluated at potential outcome functions. Knowledge and assumptions about the potential outcome functions are encoded as function spaces. This makes the framework expressive, allowing experimenters to formulate and investigate a wide range of causal questions. We describe a class of estimators for estimands defined using the framework and investigate their properties. The construction of the estimators is based on the Riesz representation theorem. We provide necessary and sufficient conditions for unbiasedness and consistency. Finally, we provide conditions under which the estimators are asymptotically normal, and describe a conservative variance estimator to facilitate the construction of confidence intervals for the estimands."
            },
            {
                "arxivId": "2106.05024",
                "title": "Contamination Bias in Linear Regressions",
                "abstract": "We study regressions with multiple treatments and a set of controls that is flexible enough to purge omitted variable bias. We show that these regressions generally fail to estimate convex averages of heterogeneous treatment effects -- instead, estimates of each treatment's effect are contaminated by non-convex averages of the effects of other treatments. We discuss three estimation approaches that avoid such contamination bias, including the targeting of easiest-to-estimate weighted average effects. A re-analysis of nine empirical applications finds economically and statistically meaningful contamination bias in observational studies; contamination bias in experimental studies is more limited due to idiosyncratic effect heterogeneity."
            },
            {
                "arxivId": "1911.07085",
                "title": "Causal Inference Under Approximate Neighborhood Interference",
                "abstract": "This paper studies causal inference in randomized experiments under network interference. Commonly used models of interference posit that treatments assigned to alters beyond a certain network distance from the ego have no effect on the ego's response. However, this assumption is violated in common models of social interactions. We propose a substantially weaker model of \u201capproximate neighborhood interference\u201d (ANI) under which treatments assigned to alters further from the ego have a smaller, but potentially nonzero, effect on the ego's response. We formally verify that ANI holds for well\u2010known models of social interactions. Under ANI, restrictions on the network topology, and asymptotics under which the network size increases, we prove that standard inverse\u2010probability weighting estimators consistently estimate useful exposure effects and are approximately normal. For inference, we consider a network HAC variance estimator. Under a finite population model, we show that the estimator is biased but that the bias can be interpreted as the variance of unit\u2010level exposure effects. This generalizes Neyman's well\u2010known result on conservative variance estimation to settings with interference."
            },
            {
                "arxivId": "1705.08527",
                "title": "Causal Inference for Social Network Data",
                "abstract": "We extend recent work by van der Laan (2014) on causal inference for causally connected units to more general social network settings. Our asymptotic results allow for dependence of each observation on a growing number of other units as sample size increases. We are not aware of any previous methods for inference about network members in observational settings that allow the number of ties per node to increase as the network grows. While previous methods have generally implicitly focused on one of two possible sources of dependence among social network observations, we allow for both dependence due to contagion, or transmission of information across network ties, and for dependence due to latent similarities among nodes sharing ties. We describe estimation and inference for causal effects that are specifically of interest in social network settings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2403.13222",
        "category": "econ",
        "title": "On Equilibrium Determinacy in Overlapping Generations Models with Money",
        "abstract": "This paper provides a detailed analysis of the local determinacy of monetary and non-monetary steady states in Tirole (1985)'s classical two-period overlapping generations model with capital and production. We show that the sufficient condition for local determinacy in endowment economies provided by Scheinkman (1980) does not generalize to models with production: there are robust examples with arbitrary utility functions in which the non-monetary steady state is locally determinate or indeterminate. In contrast, the monetary steady state is locally determinate under fairly weak conditions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2403.13361",
        "category": "econ",
        "title": "Multifractal wavelet dynamic mode decomposition modeling for marketing time series",
        "abstract": "Marketing is the way we ensure our sales are the best in the market, our prices the most accessible, and our clients satisfied, thus ensuring our brand has the widest distribution. This requires sophisticated and advanced understanding of the whole related network. Indeed, marketing data may exist in different forms such as qualitative and quantitative data. However, in the literature, it is easily noted that large bibliographies may be collected about qualitative studies, while only a few studies adopt a quantitative point of view. This is a major drawback that results in marketing science still focusing on design, although the market is strongly dependent on quantities such as money and time. Indeed, marketing data may form time series such as brand sales in specified periods, brand-related prices over specified periods, market shares, etc. The purpose of the present work is to investigate some marketing models based on time series for various brands. This paper aims to combine the dynamic mode decomposition and wavelet decomposition to study marketing series due to both prices, and volume sales in order to explore the effect of the time scale on the persistence of brand sales in the market and on the forecasting of such persistence, according to the characteristics of the brand and the related market competition or competitors. Our study is based on a sample of Saudi brands during the period 22 November 2017 to 30 December 2021.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2403.13388",
        "category": "econ",
        "title": "Optimal VPPI strategy under Omega ratio with stochastic benchmark",
        "abstract": "This paper studies a variable proportion portfolio insurance (VPPI) strategy. The objective is to determine the risk multiplier by maximizing the extended Omega ratio of the investor's cushion, using a binary stochastic benchmark. When the stock index declines, investors aim to maintain the minimum guarantee. Conversely, when the stock index rises, investors seek to track some excess returns. The optimization problem involves the combination of a non-concave objective function with a stochastic benchmark, which is effectively solved based on the stochastic version of concavification technique. We derive semi-analytical solutions for the optimal risk multiplier, and the value functions are categorized into three distinct cases. Intriguingly, the classification criteria are determined by the relationship between the optimal risky multiplier in Zieling et al. (2014 and the value of 1. Simulation results confirm the effectiveness of the VPPI strategy when applied to real market data calibrations.",
        "references": [
            {
                "arxivId": "2101.06675",
                "title": "A Framework of State-dependent Utility Optimization with General Benchmarks",
                "abstract": "Benchmarks in the utility function have various interpretations, including performance guarantees and risk constraints in fund contracts and reference levels in cumulative prospect theory. In most literature, benchmarks are a deterministic constant or a fraction of the underlying wealth variable; thus, the utility is also a function of the wealth. In this paper, we propose a general framework of state-dependent utility optimization with stochastic benchmark variables, which includes stochastic reference levels as typical examples. We provide the optimal solution(s) and investigate the issues of well-definedness, feasibility, finiteness, and attainability. The major difficulties include: (i) various reasons for the non-existence of the Lagrange multiplier and corresponding results on the optimal solution; (ii) measurability issues of the concavification of a state-dependent utility and the selection of the optimal solutions. Finally, we show how to apply the framework to solve some constrained utility optimization problems with state-dependent performance and risk benchmarks as some nontrivial examples."
            },
            {
                "arxivId": "1305.5915",
                "title": "Model-Free CPPI",
                "abstract": "We consider Constant Proportion Portfolio Insurance (CPPI) and its dynamic extension, which may be called Dynamic Proportion Portfolio Insurance (DPPI). It is shown that these investment strategies work within the setting of Follmer's pathwise Ito calculus, which makes no probabilistic assumptions whatsoever. This shows, on one hand, that CPPI and DPPI are completely independent of any choice of a particular model for the dynamics of asset prices. They even make sense beyond the class of semimartingale sample paths and can be successfully defined for models admitting arbitrage, including some models based on fractional Brownian motion. On the other hand, the result can be seen as a case study for the general issue of robustness in the face of model uncertainty in finance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2403.13725",
        "category": "econ",
        "title": "Robust Inference in Locally Misspecified Bipartite Networks",
        "abstract": "This paper introduces a methodology to conduct robust inference in bipartite networks under local misspecification. We focus on a class of dyadic network models with misspecified conditional moment restrictions. The framework of misspecification is local, as the effect of misspecification varies with the sample size. We utilize this local asymptotic approach to construct a robust estimator that is minimax optimal for the mean square error within a neighborhood of misspecification. Additionally, we introduce bias-aware confidence intervals that account for the effect of the local misspecification. These confidence intervals have the correct asymptotic coverage for the true parameter of interest under sparse network asymptotics. Monte Carlo experiments demonstrate that the robust estimator performs well in finite samples and sparse networks. As an empirical illustration, we study the formation of a scientific collaboration network among economists.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-21.json",
        "arxivId": "2403.13738",
        "category": "econ",
        "title": "Policy Relevant Treatment Effects with Multidimensional Unobserved Heterogeneity",
        "abstract": "This paper provides a framework for the policy relevant treatment effects using instrumental variables. In the framework, a treatment selection may or may not satisfy the classical monotonicity condition and can accommodate multidimensional unobserved heterogeneity. We can bound the target parameter by extracting information from identifiable estimands. We also provide a more conservative yet computationally simpler bound by applying a convex relaxation method. Linear shape restrictions can be easily incorporated to further improve the bounds. Numerical and simulation results illustrate the informativeness of our convex-relaxation bounds, i.e., that our bounds are sufficiently tight.",
        "references": [
            {
                "arxivId": "2310.05311",
                "title": "Identification and Estimation in a Class of Potential Outcomes Models",
                "abstract": "This paper develops a class of potential outcomes models characterized by three main features: (i) Unobserved heterogeneity can be represented by a vector of potential outcomes and a type describing the manner in which an instrument determines the choice of treatment; (ii) The availability of an instrumental variable that is conditionally independent of unobserved heterogeneity; and (iii) The imposition of convex restrictions on the distribution of unobserved heterogeneity. The proposed class of models encompasses multiple classical and novel research designs, yet possesses a common structure that permits a unifying analysis of identification and estimation. In particular, we establish that these models share a common necessary and sufficient condition for identifying certain causal parameters. Our identification results are constructive in that they yield estimating moment conditions for the parameters of interest. Focusing on a leading special case of our framework, we further show how these estimating moment conditions may be modified to be doubly robust. The corresponding double robust estimators are shown to be asymptotically normally distributed, bootstrap based inference is shown to be asymptotically valid, and the semi-parametric efficiency bound is derived for those parameters that are root-n estimable. We illustrate the usefulness of our results for developing, identifying, and estimating causal models through an empirical evaluation of the role of mental health as a mediating variable in the Moving To Opportunity experiment."
            },
            {
                "arxivId": "2011.06695",
                "title": "When Should We (Not) Interpret Linear IV Estimands as Late?",
                "abstract": "In this paper I revisit the interpretation of the linear instrumental variables (IV) estimand as a weighted average of conditional local average treatment effects (LATEs). I focus on a practically relevant situation in which additional covariates are required for identification but the reduced-form and first-stage regressions are possibly misspecified as a result of neglected heterogeneity in the effects of the instrument. If we also allow for conditional monotonicity, i.e. the existence of compliers but no defiers at some covariate values and the existence of defiers but no compliers elsewhere, then the weights on some conditional LATEs are negative and the IV estimand is no longer interpretable as a causal effect. Even if monotonicity holds unconditionally, the IV estimand is not interpretable as the unconditional LATE parameter unless the groups that are encouraged and not encouraged to get treated are roughly equal sized."
            },
            {
                "arxivId": "2009.00553",
                "title": "A vector monotonicity assumption for multiple instruments",
                "abstract": null
            },
            {
                "arxivId": "1410.0163",
                "title": "Instrumental Variables: An Econometrician's Perspective",
                "abstract": "I review recent work in the statistics literature on instrumental variables methods from an econometrics perspective. I discuss some of the older, economic, applications including supply and demand models and relate them to the recent applications in settings of randomized experiments with noncompliance. I discuss the assumptions underlying instrumental variables methods and in what settings these may be plausible. By providing context to the current applications a better understanding of the applicability of these methods may arise."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "1905.07812",
        "category": "econ",
        "title": "Iterative Estimation of Nonparametric Regressions with Continuous Endogenous Variables and Discrete Instruments",
        "abstract": "We consider a nonparametric regression model with continuous endogenous independent variables when only discrete instruments are available that are independent of the error term. While this framework is very relevant for applied research, its implementation is cumbersome, as the regression function becomes the solution to a nonlinear integral equation. We propose a simple iterative procedure to estimate such models and showcase some of its asymptotic properties. In a simulation experiment, we discuss the details of its implementation in the case when the instrumental variable is binary. We conclude with an empirical application in which we examine the effect of pollution on house prices in a short panel of U.S. counties.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2201.06694",
        "category": "econ",
        "title": "Homophily in preferences or meetings? Identifying and estimating an iterative network formation model",
        "abstract": "Is homophily in social and economic networks driven by a taste for homogeneity (preferences) or by a higher probability of meeting individuals with similar attributes (opportunity)? This paper studies identification and estimation of an iterative network game that distinguishes between these two mechanisms. Our approach enables us to assess the counterfactual effects of changing the meeting protocol between agents. As an application, we study the role of preferences and meetings in shaping classroom friendship networks in Brazil. In a network structure in which homophily due to preferences is stronger than homophily due to meeting opportunities, tracking students may improve welfare. Still, the relative benefit of this policy diminishes over the school year.",
        "references": [
            {
                "arxivId": "1901.00373",
                "title": "Nash Equilibria on (Un)Stable Networks",
                "abstract": "In response to a change, individuals may choose to follow the responses of their friends or, alternatively, to change their friends. To model these decisions, consider a game where players choose their behaviors and friendships. In equilibrium, players internalize the need for consensus in forming friendships and choose their optimal strategies on subsets of \n k players\u2014a form of bounded rationality. The \n k\u2010player consensual dynamic delivers a probabilistic ranking of a game's equilibria, and via a varying \n k, facilitates estimation of such games.\n \n Applying the model to adolescents' smoking suggests that: (a) the response of the friendship network to changes in tobacco price amplifies the intended effect of price changes on smoking, (b) racial desegregation of high schools decreases the overall smoking prevalence, (c) peer effect complementarities are substantially stronger between smokers compared to between nonsmokers."
            },
            {
                "arxivId": "1611.07658",
                "title": "A Network Formation Model Based on Subgraphs",
                "abstract": "We develop a new class of random-graph models for the statistical estimation of network formation that allow for substantial correlation in links. Various subgraphs (e.g., links, triangles, cliques, stars) are generated and their union results in a network. The challenge in estimating the frequencies with which subgraphs 'truly' form is that subgraphs can overlap and may also incidentally generate new subgraphs, and so the true rate of formation of the subgraphs cannot generally be inferred just by counting their presence in the resulting network. We provide estimation techniques for recovering the rates at which the underlying subgraphs were formed from the observation of a single (large) network. We provide results on identification of the true underlying rates of subgraph formation from various statistics, as well as a new Central Limit Theorem for correlated random variables that establishes asymptotic normality for our estimators. We also show that if the network is sparse enough then direct counts of subgraphs are consistent and asymptotically normal estimators. We illustrate the models with applications."
            },
            {
                "arxivId": "1512.00205",
                "title": "Divide and conquer in ABC: Expectation-Progagation algorithms for likelihood-free inference",
                "abstract": "ABC algorithms are notoriously expensive in computing time, as they require simulating many complete artificial datasets from the model. We advocate in this paper a \"divide and conquer\" approach to ABC, where we split the likelihood into n factors, and combine in some way n \"local\" ABC approximations of each factor. This has two advantages: (a) such an approach is typically much faster than standard ABC and (b) it makes it possible to use local summary statistics (i.e. summary statistics that depend only on the data-points that correspond to a single factor), rather than global summary statistics (that depend on the complete dataset). This greatly alleviates the bias introduced by summary statistics, and even removes it entirely in situations where local summary statistics are simply the identity function. \nWe focus on EP (Expectation-Propagation), a convenient and powerful way to combine n local approximations into a global approximation. Compared to the EP- ABC approach of Barthelm\\'e and Chopin (2014), we present two variations, one based on the parallel EP algorithm of Cseke and Heskes (2011), which has the advantage of being implementable on a parallel architecture, and one version which bridges the gap between standard EP and parallel EP. We illustrate our approach with an expensive application of ABC, namely inference on spatial extremes."
            },
            {
                "arxivId": "1506.03481",
                "title": "On the Asymptotic Efficiency of Approximate Bayesian Computation Estimators",
                "abstract": "Many statistical applications involve models for which it is difficult to evaluate the likelihood, but from which it is relatively easy to sample. Approximate Bayesian computation is a likelihood-free method for implementing Bayesian inference in such cases. We present results on the asymptotic variance of estimators obtained using approximate Bayesian computation in a large-data limit. Our key assumption is that the data are summarized by a fixed-dimensional summary statistic that obeys a central limit theorem. We prove asymptotic normality of the mean of the approximate Bayesian computation posterior. This result also shows that, in terms of asymptotic variance, we should use a summary statistic that is the same dimension as the parameter vector, p; and that any summary statistic of higher dimension can be reduced, through a linear transformation, to dimension p in a way that can only reduce the asymptotic variance of the posterior mean. We look at how the Monte Carlo error of an importance sampling algorithm that samples from the approximate Bayesian computation posterior affects the accuracy of estimators. We give conditions on the importance sampling proposal distribution such that the variance of the estimator will be the same order as that of the maximum likelihood estimator based on the summary statistics used. This suggests an iterative importance sampling algorithm, which we evaluate empirically on a stochastic volatility model."
            },
            {
                "arxivId": "1201.4564",
                "title": "Homophily and Long-Run Integration in Social Networks",
                "abstract": "We model network formation when heterogeneous nodes enter sequentially and form connections through both random meetings and network-based search, but with type-dependent biases. We show that there is \u201clong-run integration\u201d, whereby the composition of types in sufficiently old nodes\u02bc neighborhoods approaches the global type-distribution, provided that the network-based search is unbiased. However, younger nodes\u02bc connections still reflect the biased meetings process. We derive the type-based degree distributions and group-level homophily patterns when there are two types and location-based biases. Finally, we illustrate aspects of the model with an empirical application to data on citations in physics journals."
            },
            {
                "arxivId": "1107.5959",
                "title": "Expectation Propagation for Likelihood-Free Inference",
                "abstract": "Many models of interest in the natural and social sciences have no closed-form likelihood function, which means that they cannot be treated using the usual techniques of statistical inference. In the case where such models can be efficiently simulated, Bayesian inference is still possible thanks to the approximate Bayesian computation (ABC) algorithm. Although many refinements have been suggested, ABC inference is still far from routine. ABC is often excruciatingly slow due to very low acceptance rates. In addition, ABC requires introducing a vector of \u201csummary statistics\u201d s(y), the choice of which is relatively arbitrary, and often require some trial and error, making the whole process laborious for the user. We introduce in this work the EP-ABC algorithm, which is an adaptation to the likelihood-free context of the variational approximation algorithm known as expectation propagation. The main advantage of EP-ABC is that it is faster by a few orders of magnitude than standard algorithms, while producing an overall approximation error that is typically negligible. A second advantage of EP-ABC is that it replaces the usual global ABC constraint \u2016s(y) \u2212 s(y\u22c6)\u2016 \u2a7d \u03f5, where s(y\u22c6) is the vector of summary statistics computed on the whole dataset, by n local constraints of the form \u2016si(yi) \u2212 si(y\u22c6i)\u2016 \u2a7d \u03f5 that apply separately to each data point. In particular, it is often possible to take si(yi) = yi, making it possible to do away with summary statistics entirely. In that case, EP-ABC makes it possible to approximate directly the evidence (marginal likelihood) of the model. Comparisons are performed in three real-world applications that are typical of likelihood-free inference, including one application in neuroscience that is novel, and possibly too challenging for standard ABC techniques."
            },
            {
                "arxivId": "0904.0635",
                "title": "Approximate Bayesian Computation: A Nonparametric Perspective",
                "abstract": "Approximate Bayesian Computation is a family of likelihood-free inference techniques that are well suited to models defined in terms of a stochastic generating mechanism. In a nutshell, Approximate Bayesian Computation proceeds by computing summary statistics sobs from the data and simulating summary statistics for different values of the parameter \u0398. The posterior distribution is then approximated by an estimator of the conditional density g(\u0398|sobs). In this paper, we derive the asymptotic bias and variance of the standard estimators of the posterior distribution which are based on rejection sampling and linear adjustment. Additionally, we introduce an original estimator of the posterior distribution based on quadratic adjustment and we show that its bias contains a fewer number of terms than the estimator with linear adjustment. Although we find that the estimators with adjustment are not universally superior to the estimator based on rejection sampling, we find that they can achieve better performance when there is a nearly homoscedastic relationship between the summary statistics and the parameter of interest. To make this relationship as homoscedastic as possible, we propose to use transformations of the summary statistics. In different examples borrowed from the population genetics and epidemiological literature, we show the potential of the methods with adjustment and of the transformations of the summary statistics. Supplemental materials containing the details of the proofs are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2204.11318",
        "category": "econ",
        "title": "Identification and Statistical Decision Theory",
        "abstract": "Econometricians have usefully separated study of estimation into identification and statistical components. Identification analysis, which assumes knowledge of the probability distribution generating observable data, places an upper bound on what may be learned about population parameters of interest with finite sample data. Yet Wald's statistical decision theory studies decision making with sample data without reference to identification, indeed without reference to estimation. This paper asks if identification analysis is useful to statistical decision theory. The answer is positive, as it can yield an informative and tractable upper bound on the achievable finite sample performance of decision criteria. The reasoning is simple when the decision relevant parameter is point identified. It is more delicate when the true state is partially identified and a decision must be made under ambiguity. Then the performance of some criteria, such as minimax regret, is enhanced by randomizing choice of an action. This may be accomplished by making choice a function of sample data. I find it useful to recast choice of a statistical decision function as selection of choice probabilities for the elements of the choice set. Using sample data to randomize choice conceptually differs from and is complementary to its traditional use to estimate population parameters.",
        "references": [
            {
                "arxivId": "1912.08726",
                "title": "Econometrics for Decision Making: Building Foundations Sketched by Haavelmo and Wald",
                "abstract": "Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes use of statistical decision theory to evaluate the performance of models in decision making. I consider the common practice of \n as\u2010if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as\u2010if optimization or any other model\u2010based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancing computation is the primary task to complete the foundations sketched by Haavelmo and Wald.\n"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2205.13773",
        "category": "econ",
        "title": "Wildfire Modeling: Designing a Market to Restore Assets",
        "abstract": "In the past decade, summer wildfires have become the norm in California, and the United States of America. These wildfires are caused due to variety of reasons. The state collects wildfire funds to help the impacted customers. However, the funds are eligible only under certain conditions and are collected uniformly throughout California. Therefore, the overall idea of this project is to look for quantitative results on how electrical corporations cause wildfires and how they can help to collect the wildfire funds or charge fairly to the customers to maximize the social impact. The research project aims to propose the implication of wildfire risk associated with vegetation, and due to power lines and incorporate that in dollars. Therefore, the project helps to solve the problem of collecting wildfire funds associated with each location and incorporate energy prices to charge their customers according to their wildfire risk related to the location to maximize the social surplus for the society. The thesis findings will help to calculate the risk premium involving wildfire risk associated with the location and incorporate the risk into pricing. The research of this submitted proposal provides the potential contribution towards detecting the utilities associated wildfire risk in the power lines, which can prevent wildfires by controlling the line flows of the system. Ultimately, the goal of this proposal is a social benefit to save money for the electrical corporations and their customers in California, who pay flat charges for Wildfire Fund each month $0.00580/kWh (in dollars). Therefore, this proposal will propose new method to collect wildfire fund with maximum customer surplus for future generations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2308.08430",
        "category": "econ",
        "title": "A Majority Rule Philosophy for Instant Runoff Voting",
        "abstract": "We present the core support criterion, a voting criterion satisfied by Instant Runoff Voting (IRV) that is analogous to the Condorcet criterion but reflective of a different majority rule philosophy. Condorcet methods can be thought of as conducting elections between each pair of candidates, counting all ballots to determine the winner of each pair-election. IRV can also be thought of as conducting elections between all pairs of candidates but for each pair-election only counting ballots from voters who do not prefer another major candidate (as determined self-consistently from the IRV social ranking) to the two candidates in contention. The appropriateness of including all ballots or a subset of ballots for a pair-election, depends on whether the society deems the entire or a selected ballot set in compliance with freedom of association (which implies freedom of non-association) for a given pair election. Arguments based on freedom of association rely on more information about an electorate than can be learned from ranked ballots alone. We present a freedom-of-association based argument to explain why IRV may be preferable to Condorcet in some circumstances, including the 2022 Alaska special congressional election, based on the political context of that election.",
        "references": [
            {
                "arxivId": "2301.12075",
                "title": "An Examination of Ranked-Choice Voting in the United States, 2004\u20132022",
                "abstract": "From the perspective of social choice theory, ranked-choice voting (RCV) is known to have many flaws. RCV can fail to elect a Condorcet winner and is susceptible to monotonicity paradoxes and the spoiler effect, for example. We use a database of 182 American ranked-choice elections for political office from the years 2004-2022 to investigate empirically how frequently RCV's deficiencies manifest in practice. Our general finding is that RCV's weaknesses are rarely observed in real-world elections, with the exception that ballot exhaustion frequently causes majoritarian failures."
            },
            {
                "arxivId": "2209.04764",
                "title": "A Mathematical Analysis of the 2022 Alaska Special Election for US House",
                "abstract": "In the aftermath of the August 2022 Special Election for US House in Alaska, which used ranked choice voting (RCV) to elect the winner, losing candidate Sarah Palin called RCV \u201ccrazy,\u201d \u201cconvoluted,\u201d and \u201cconfusing.\u201d These sentiments were echoed by some of her political allies. Senator Tom Cotton from Arkansas was the most blunt: \u201cRanked choice voting is a scam to rig elections,\u201d he stated. These comments raise the question: is RCV some kind of crazy scam to rig elections? The short answer is: No. (The long answer is: Nooooooo.) However, RCV does have its drawbacks, which were on full display in this election. In this article we\u2019ll discuss the deficiencies of RCV from the mathematical perspective of voting theory, using the Alaska House election as a case study. We note that most of the findings of this article were independently and concurrently discovered by Jiri Navratil and Warren Smith (see https://litarvan.substack.com/p/when-mess-explodes-the-irv-election)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2310.01666",
        "category": "econ",
        "title": "The dictator\u2019s dilemma: The distortion of information flow in autocratic regimes and its consequences",
        "abstract": null,
        "references": [
            {
                "arxivId": "2009.03131",
                "title": "Noisy bounded confidence models for opinion dynamics: the effect of boundary conditions on phase transitions",
                "abstract": "We study SDE and PDE models for opinion dynamics under bounded confidence, for a range of different boundary conditions, with and without the inclusion of a radical population. We perform exhaustive numerical studies with pseudospectral methods to determine the effects of the boundary conditions, suggesting that the no-flux case most faithfully reproduce the underlying mechanisms in the associated deterministic models of Hegselmann and Krause. We also compare the SDE and PDE models, and use tools from analysis to study phase transitions, including a systematic description of an order parameter."
            },
            {
                "arxivId": "1903.04786",
                "title": "Statistical Physics Of Opinion Formation: Is it a SPOOF?",
                "abstract": null
            },
            {
                "arxivId": "1204.3151",
                "title": "Phase transitions in the q-voter model with two types of stochastic driving.",
                "abstract": "We study a nonlinear q-voter model with stochastic driving on a complete graph. We investigate two types of stochasticity that, using the language of social sciences, can be interpreted as different kinds of nonconformity. From a social point of view, it is very important to distinguish between two types nonconformity, so-called anticonformity and independence. A majority of work has suggested that these social differences may be completely irrelevant in terms of microscopic modeling that uses tools of statistical physics and that both types of nonconformity play the role of so-called social temperature. In this paper we clarify the concept of social temperature and show that different types of noise may lead to qualitatively different emergent properties. In particular, we show that in the model with anticonformity the critical value of noise increases with parameter q, whereas in the model with independence the critical value of noise decreases with q. Moreover, in the model with anticonformity the phase transition is continuous for any value of q, whereas in the model with independence the transition is continuous for q \u2264 5 and discontinuous for q>5."
            },
            {
                "arxivId": "0907.4662",
                "title": "Continuous-Time Average-Preserving Opinion Dynamics with Opinion-Dependent Communications",
                "abstract": "We study a simple continuous-time multiagent system related to Krause's model of opinion dynamics: each agent holds a real value, and this value is continuously attracted by every other value differing from it by less than 1, with an intensity proportional to the difference. We prove convergence to a set of clusters, with the agents in each cluster sharing a common value, and provide a lower bound on the distance between clusters at a stable equilibrium, under a suitable notion of multiagent system stability. To better understand the behavior of the system for a large number of agents, we introduce a variant involving a continuum of agents. We prove, under some conditions, the existence of a solution to the system dynamics, convergence to clusters, and a nontrivial lower bound on the distance between clusters. Finally, we establish that the continuum model accurately represents the asymptotic behavior of a system with a finite but large number of agents."
            },
            {
                "arxivId": "nlin/0506020",
                "title": "Formation of clumps and patches in self-aggregation of finite-size particles",
                "abstract": null
            },
            {
                "arxivId": "nlin/0501009",
                "title": "Aggregation of finite-size particles with variable mobility.",
                "abstract": "New model equations are derived for dynamics of aggregation of finite-size particles. The differences from standard Debye-H\u00fcckel and Keller-Segel models are that the mobility of particles depends on the configuration of their neighbors and linear diffusion acts on locally averaged particle density. The evolution of collapsed states in these models reduces exactly to finite-dimensional dynamics of interacting particle clumps. Simulations show these collapsed (clumped) states emerge from smooth initial conditions, even in one spatial dimension. Extensions to two and three dimensions are also discussed."
            },
            {
                "arxivId": "cond-mat/0101130",
                "title": "Opinion evolution in closed community",
                "abstract": "A simple Ising spin model which can describe a mechanism of making a decision in a closed community is proposed. It is shown via standard Monte Carlo simulations that very simple rules lead to rather complicated dynamics and to a power law in the decision time distribution. It is found that a closed community has to evolve either to a dictatorship or a stalemate state (inability to take any common decision). A common decision can be taken in a \"democratic way\" only by an open community."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2403.13983",
        "category": "econ",
        "title": "Robust Communication Between Parties with Nearly Independent Preferences",
        "abstract": "We study finite-state communication games in which the sender's preference is perturbed by random private idiosyncrasies. Persuasion is generically impossible within the class of statistically independent sender/receiver preferences -- contrary to prior research establishing persuasive equilibria when the sender's preference is precisely transparent. Nevertheless, robust persuasion may occur when the sender's preference is only slightly state-dependent/idiosyncratic. This requires approximating an `acyclic' equilibrium of the transparent preference game, generically implying that this equilibrium is also `connected' -- a generalization of partial-pooling equilibria. It is then necessary and sufficient that the sender's preference satisfy a monotonicity condition relative to the approximated equilibrium. If the sender's preference further satisfies a `semi-local' version of increasing differences, then this analysis extends to sender preferences that rank pure actions (but not mixed actions) according to a state-independent order. We apply these techniques to study (1) how ethical considerations, such as empathy for the receiver, may improve or impede comm",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2403.14036",
        "category": "econ",
        "title": "Fused LASSO as Non-Crossing Quantile Regression",
        "abstract": "Quantile crossing has been an ever-present thorn in the side of quantile regression. This has spurred research into obtaining densities and coefficients that obey the quantile monotonicity property. While important contributions, these papers do not provide insight into how exactly these constraints influence the estimated coefficients. This paper extends non-crossing constraints and shows that by varying a single hyperparameter ($\\alpha$) one can obtain commonly used quantile estimators. Namely, we obtain the quantile regression estimator of Koenker and Bassett (1978) when $\\alpha=0$, the non crossing quantile regression estimator of Bondell et al. (2010) when $\\alpha=1$, and the composite quantile regression estimator of Koenker (1984) and Zou and Yuan (2008) when $\\alpha\\rightarrow\\infty$. As such, we show that non-crossing constraints are simply a special type of fused-shrinkage.",
        "references": [
            {
                "arxivId": "1507.03130",
                "title": "Joint Estimation of Quantile Planes Over Arbitrary Predictor Spaces",
                "abstract": "ABSTRACT In spite of the recent surge of interest in quantile regression, joint estimation of linear quantile planes remains a great challenge in statistics and econometrics. We propose a novel parameterization that characterizes any collection of noncrossing quantile planes over arbitrarily shaped convex predictor domains in any dimension by means of unconstrained scalar, vector and function valued parameters. Statistical models based on this parameterization inherit a fast computation of the likelihood function, enabling penalized likelihood or Bayesian approaches to model fitting. We introduce a complete Bayesian methodology by using Gaussian process prior distributions on the function valued parameters and develop a robust and efficient Markov chain Monte Carlo parameter estimation. The resulting method is shown to offer posterior consistency under mild tail and regularity conditions. We present several illustrative examples where the new method is compared against existing approaches and is found to offer better accuracy, coverage and model fit. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-22.json",
        "arxivId": "2403.14216",
        "category": "econ",
        "title": "A Gaussian smooth transition vector autoregressive model: An application to the macroeconomic effects of severe weather shocks",
        "abstract": "We introduce a new smooth transition vector autoregressive model with a Gaussian conditional distribution and transition weights that, for a $p$th order model, depend on the full distribution of the preceding $p$ observations. Specifically, the transition weight of each regime increases in its relative weighted likelihood. This data-driven approach facilitates capturing complex switching dynamics, enhancing the identification of gradual regime shifts. In an empirical application to the macroeconomic effects of a severe weather shock, we find that in monthly U.S. data from 1961:1 to 2022:3, the impacts of the shock are stronger in the regime prevailing in the early part of the sample and in certain crisis periods than in the regime dominating the latter part of the sample. This suggests overall adaptation of the U.S. economy to increased severe weather over time.",
        "references": [
            {
                "arxivId": "2003.05221",
                "title": "A mixture autoregressive model based on Gaussian and Student\u2019s t-distributions",
                "abstract": "Abstract We introduce a new mixture autoregressive model which combines Gaussian and Student\u2019s t mixture components. The model has very attractive properties analogous to the Gaussian and Student\u2019s t mixture autoregressive models, but it is more flexible as it enables to model series which consist of both conditionally homoscedastic Gaussian regimes and conditionally heteroscedastic Student\u2019s t regimes. The usefulness of our model is demonstrated in an empirical application to the monthly U.S. interest rate spread between the 3-month Treasury bill rate and the effective federal funds rate."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2206.06840",
        "category": "econ",
        "title": "The number of boundedly rational choices on four elements",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2311.15458",
        "category": "econ",
        "title": "Causal Models for Longitudinal and Panel Data: A Survey",
        "abstract": "This survey discusses the recent causal panel data literature. This recent literature has focused on credibly estimating causal effects of binary interventions in settings with longitudinal data, with an emphasis on practical advice for empirical researchers. It pays particular attention to heterogeneity in the causal effects, often in situations where few units are treated and with particular structures on the assignment pattern. The literature has extended earlier work on difference-in-differences or two-way-fixed-effect estimators. It has more generally incorporated factor models or interactive fixed effects. It has also developed novel methods using synthetic control approaches.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15111",
        "category": "econ",
        "title": "Fast TTC Computation",
        "abstract": "This paper proposes a fast Markov Matrix-based methodology for computing Top Trading Cycles (TTC) that delivers O(1) computational speed, that is speed independent of the number of agents and objects in the system. The proposed methodology is well suited for complex large-dimensional problems like housing choice. The methodology retains all the properties of TTC, namely, Pareto-efficiency, individual rationality and strategy-proofness.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15200",
        "category": "econ",
        "title": "Teamwork and Spillover Effects in Performance Evaluations",
        "abstract": "This article shows how coworker performance affects individual performance evaluation in a teamwork setting at the workplace. We use high-quality data on football matches to measure an important component of individual performance, shooting performance, isolated from collaborative effects. Employing causal machine learning methods, we address the assortative matching of workers and estimate both average and heterogeneous effects. There is substantial evidence for spillover effects in performance evaluations. Coworker shooting performance, meaningfully impacts both, manager decisions and third-party expert evaluations of individual performance. Our results underscore the significant role coworkers play in shaping career advancements and highlight a complementary channel, to productivity gains and learning effects, how coworkers impact career advancement. We characterize the groups of workers that are most and least affected by spillover effects and show that spillover effects are reference point dependent. While positive deviations from a reference point create positive spillover effects, negative deviations are not harmful for coworkers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15220",
        "category": "econ",
        "title": "Modelling with Discretized Variables",
        "abstract": "This paper deals with econometric models in which the dependent variable, some explanatory variables, or both are observed as censored interval data. This discretization often happens due to confidentiality of sensitive variables like income. Models using these variables cannot point identify regression parameters as the conditional moments are unknown, which led the literature to use interval estimates. Here, we propose a discretization method through which the regression parameters can be point identified while preserving data confidentiality. We demonstrate the asymptotic properties of the OLS estimator for the parameters in multivariate linear regressions for cross-sectional data. The theoretical findings are supported by Monte Carlo experiments and illustrated with an application to the Australian gender wage gap.",
        "references": [
            {
                "arxivId": "2004.11751",
                "title": "Microeconometrics with Partial Identi\ufb01cation",
                "abstract": "This chapter reviews the microeconometrics literature on partial identification, focusing on the developments of the last thirty years. The topics presented illustrate that the available data combined with credible maintained assumptions may yield much information about a parameter of interest, even if they do not reveal it exactly. Special attention is devoted to discussing the challenges associated with, and some of the solutions put forward to, (1) obtain a tractable characterization of the values for the parameters of interest which are observationally equivalent, given the available data and maintained assumptions; (2) estimate this set of values; (3) conduct test of hypotheses and make confidence statements. The chapter reviews advances in partial identification analysis both as applied to learning (functionals of) probability distributions that are well-defined in the absence of models, as well as to learning parameters that are well-defined only in the context of particular models. A simple organizing principle is highlighted: the source of the identification problem can often be traced to a collection of random variables that are consistent with the available data and maintained assumptions. This collection may be part of the observed data or be a model implication. In either case, it can be formalized as a random set. Random set theory is then used as a mathematical framework to unify a number of special results and produce a general methodology to carry out partial identification analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15258",
        "category": "econ",
        "title": "Tests for almost stochastic dominance",
        "abstract": "We introduce a 2-dimensional stochastic dominance (2DSD) index to characterize both strict and almost stochastic dominance. Based on this index, we derive an estimator for the minimum violation ratio (MVR), also known as the critical parameter, of the almost stochastic ordering condition between two variables. We determine the asymptotic properties of the empirical 2DSD index and MVR for the most frequently used stochastic orders. We also provide conditions under which the bootstrap estimators of these quantities are strongly consistent. As an application, we develop consistent bootstrap testing procedures for almost stochastic dominance. The performance of the tests is checked via simulations and the analysis of real data.",
        "references": [
            {
                "arxivId": "1404.3763",
                "title": "Inference on Directionally Differentiable Functions",
                "abstract": "This paper studies an asymptotic framework for conducting inference on parameters of the form ( 0), where is a known directionally dierentiable function and 0 is estimated by ^ n. In these settings, the asymptotic distribution of the plug-in estimator ( ^ n) can be readily derived employing existing extensions to the Delta method. We show, however, that the \\standard\" bootstrap is only consistent under overly stringent conditions { in particular we establish that dierentiability of is a necessary and sucient condition for bootstrap consistency whenever the limiting distribution of ^ n is Gaussian. An alternative resampling scheme is proposed which remains consistent when the bootstrap fails, and is shown to provide local size control under restrictions on the directional derivative of . We illustrate the utility of our results by developing a test of whether a Hilbert space valued parameter belongs to a convex set { a setting that includes moment inequality problems and certain tests of shape restrictions as special cases."
            },
            {
                "arxivId": "1403.6296",
                "title": "On Consistent Hypothesis Testing",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15281",
        "category": "econ",
        "title": "Measuring Gender and Racial Biases in Large Language Models",
        "abstract": "In traditional decision making processes, social biases of human decision makers can lead to unequal economic outcomes for underrepresented social groups, such as women, racial or ethnic minorities. Recently, the increasing popularity of Large language model based artificial intelligence suggests a potential transition from human to AI based decision making. How would this impact the distributional outcomes across social groups? Here we investigate the gender and racial biases of OpenAIs GPT, a widely used LLM, in a high stakes decision making setting, specifically assessing entry level job candidates from diverse social groups. Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications. These biases may result in a 1 or 2 percentage point difference in hiring probabilities for otherwise similar candidates at a certain threshold and are consistent across various job positions and subsamples. Meanwhile, we also find stronger pro female and weaker anti black male patterns in democratic states. Our results demonstrate that this LLM based AI system has the potential to mitigate the gender bias, but it may not necessarily cure the racial bias. Further research is needed to comprehend the root causes of these outcomes and develop strategies to minimize the remaining biases in AI systems. As AI based decision making tools are increasingly employed across diverse domains, our findings underscore the necessity of understanding and addressing the potential unequal outcomes to ensure equitable outcomes across social groups.",
        "references": [
            {
                "arxivId": "2311.14703",
                "title": "ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management",
                "abstract": "Recent breakthroughs in large language models (LLMs) have led to their rapid dissemination and widespread use. One early application has been to medicine, where LLMs have been investigated to streamline clinical workflows and facilitate clinical analysis and decision-making. However, a leading barrier to the deployment of Artificial Intelligence (AI) and in particular LLMs has been concern for embedded gender and racial biases. Here, we evaluate whether a leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical management of acute coronary syndrome (ACS). We find that specifying patients as female, African American, or Hispanic resulted in a decrease in guideline recommended medical management, diagnosis, and symptom management of ACS. Most notably, the largest disparities were seen in the recommendation of coronary angiography or stress testing for the diagnosis and further intervention of ACS and recommendation of high intensity statins. These disparities correlate with biases that have been observed clinically and have been implicated in the differential gender and racial morbidity and mortality outcomes of ACS and coronary artery disease. Furthermore, we find that the largest disparities are seen during unstable angina, where fewer explicit clinical guidelines exist. Finally, we find that through asking ChatGPT 3.5 to explain its reasoning prior to providing an answer, we are able to improve clinical accuracy and mitigate instances of gender and racial biases. This is among the first studies to demonstrate that the gender and racial biases that LLMs exhibit do in fact affect clinical management. Additionally, we demonstrate that existing strategies that improve LLM performance not only improve LLM performance in clinical management, but can also be used to mitigate gender and racial biases."
            },
            {
                "arxivId": "2305.12763",
                "title": "The emergence of economic rationality of GPT.",
                "abstract": "As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory. We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2302.13971",
                "title": "LLaMA: Open and Efficient Foundation Language Models",
                "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
            },
            {
                "arxivId": "2212.08073",
                "title": "Constitutional AI: Harmlessness from AI Feedback",
                "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels."
            },
            {
                "arxivId": "2203.02155",
                "title": "Training language models to follow instructions with human feedback",
                "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
            },
            {
                "arxivId": "1903.03862",
                "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
                "abstract": "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15293",
        "category": "econ",
        "title": "Human behaviour through a LENS: How Linguistic content triggers Emotions and Norms and determines Strategy choices",
        "abstract": "Over the last two decades, a growing body of experimental research has provided evidence that linguistic frames influence human behaviour in economic games, beyond the economic consequences of the available actions. This article proposes a novel framework that transcends the traditional confines of outcome-based preference models. According to the LENS model, the Linguistic description of the decision problem triggers Emotional responses and suggests potential Norms of behaviour, which then interact to shape an individual's Strategic choice. The article reviews experimental evidence that supports each path of the LENS model. Furthermore, it identifies and discusses several critical research questions that arise from this model, pointing towards avenues for future inquiry.",
        "references": [
            {
                "arxivId": "2403.07678",
                "title": "MoralBERT: Detecting Moral Values in Social Discourse",
                "abstract": "Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label. We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation. Our results showed that in-domain prediction models significantly outperformed traditional models. While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models. Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task. The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues."
            },
            {
                "arxivId": "2403.08944",
                "title": "Language-based game theory in the age of artificial intelligence",
                "abstract": "Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions. We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analysing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions. Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes. We discuss future research directions. We hope this work sets the stage for a novel game-theoretical approach that emphasizes the importance of language in human decisions."
            },
            {
                "arxivId": "2206.07300",
                "title": "From Outcome-Based to Language-Based Preferences",
                "abstract": "We review the literature on models that try to explain human behavior in social interactions described by normal-form games with monetary payoffs. We start by covering social and moral preferences. We then focus on the growing body of research showing that people react to the language with which actions are described, especially when it activates moral concerns. We conclude by arguing that behavioral economics is in the midst of a paradigm shift toward language-based preferences, which will require an exploration of new models and experimental setups.(JEL C70, C90, D11, D90, Z13)"
            },
            {
                "arxivId": "2106.03553",
                "title": "Playing with words: Do people exploit loaded language to affect others' decisions for their own benefit?",
                "abstract": "In this article, we study whether people in the position of describing a decision problem to decision-makers exploit this opportunity for their benefit, by choosing descriptions that may be potentially beneficial for themselves. To this end, we design, pre-register, and conduct an experiment in which dictator game recipients are asked to choose the instructions used to introduce the game to dictators, among six different instructions that are known from previous research to affect dictators\u2019 decisions. The results demonstrate that some dictator game recipients tend to choose instructions that make them more likely to receive a higher payoff. Finally, we found some evidence that young age and deliberative thinking are associated with this tendency."
            },
            {
                "arxivId": "2101.08193",
                "title": "Mathematical foundations of moral preferences",
                "abstract": "One-shot anonymous unselfishness in economic games is commonly explained by social preferences, which assume that people care about the monetary pay-offs of others. However, during the last 10 years, research has shown that different types of unselfish behaviour, including cooperation, altruism, truth-telling, altruistic punishment and trustworthiness are in fact better explained by preferences for following one's own personal norms\u2014internal standards about what is right or wrong in a given situation. Beyond better organizing various forms of unselfish behaviour, this moral preference hypothesis has recently also been used to increase charitable donations, simply by means of interventions that make the morality of an action salient. Here we review experimental and theoretical work dedicated to this rapidly growing field of research, and in doing so we outline mathematical foundations for moral preferences that can be used in future models to better understand selfless human actions and to adjust policies accordingly. These foundations can also be used by artificial intelligence to better navigate the complex landscape of human morality."
            },
            {
                "arxivId": "1901.02314",
                "title": "The power of moral words: Loaded language generates framing effects in the extreme dictator game",
                "abstract": "Understanding whether preferences are sensitive to the frame has been a major topic of debate in the last decades. For example, several works have explored whether the dictator game in the give frame gives rise to a different rate of pro-sociality than the same game in the take frame, leading to mixed results. Here we contribute to this debate with two experiments. In Study 1 (N=567) we implement an extreme dictator game in which the dictator either gets $0.50 and the recipient gets nothing, or the opposite (i.e., the recipient gets $0.50 and the dictator gets nothing). We experimentally manipulate the words describing the available actions using six terms, from very negative (e.g., stealing) to very positive (e.g., donating) connotations. We find that the rate of pro-sociality is affected by the words used to describe the available actions. In Study 2 (N=221) we ask brand new participants to rate each of the words used in Study 1 from \u201cextremely wrong\u201d to \u201cextremely right\u201d. We find that these moral judgments can explain the framing effect in Study 1. In sum, our studies provide evidence that framing effects in an extreme Dictator game can be generated using morally loaded language."
            },
            {
                "arxivId": "1711.05492",
                "title": "Increasing altruistic and cooperative behaviour with simple moral nudges",
                "abstract": null
            },
            {
                "arxivId": "1308.6297",
                "title": "CROWDSOURCING A WORD\u2013EMOTION ASSOCIATION LEXICON",
                "abstract": "Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how the combined strength and wisdom of the crowds can be used to generate a large, high\u2010quality, word\u2013emotion and word\u2013polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help to identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help to obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotion\u2010annotation questions, and show that asking if a term is associated with an emotion leads to markedly higher interannotator agreement than that obtained by asking if a term evokes an emotion."
            },
            {
                "arxivId": "1010.3003",
                "title": "Twitter mood predicts the stock market",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-25.json",
        "arxivId": "2403.15319",
        "category": "econ",
        "title": "Discounted Subjective Expected Utility in Continuous Time",
        "abstract": "By embedding uncertainty into time, we obtain a conjoint axiomatic characterization of both Exponential Discounting and Subjective Expected Utility that accommodates arbitrary state and outcome spaces. In doing so, we provide a novel and simple time-interpretation of subjective probability. The subjective probability of an event is calibrated using time discounting.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "1904.00989",
        "category": "econ",
        "title": "Counterfactual Sensitivity and Robustness",
        "abstract": "We propose a framework for analyzing the sensitivity of counterfactuals to parametric assumptions about the distribution of latent variables in structural models. In particular, we derive bounds on counterfactuals as the distribution of latent variables spans nonparametric neighborhoods of a given parametric specification while other \u201cstructural\u201d features of the model are maintained. Our approach recasts the infinite\u2010dimensional problem of optimizing the counterfactual with respect to the distribution of latent variables (subject to model constraints) as a finite\u2010dimensional convex program. We also develop an MPEC version of our method to further simplify computation in models with endogenous parameters (e.g., value functions) defined by equilibrium constraints. We propose plug\u2010in estimators of the bounds and two methods for inference. We also show that our bounds converge to the sharp nonparametric bounds on counterfactuals as the neighborhood size becomes large. To illustrate the broad applicability of our procedure, we present empirical applications to matching models with transferable utility and dynamic discrete choice models.",
        "references": [
            {
                "arxivId": "1605.00499",
                "title": "Monte Carlo Confidence Sets for Identified Sets",
                "abstract": "In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows."
            },
            {
                "arxivId": "1509.06311",
                "title": "Constrained Conditional Moment Restriction Models",
                "abstract": "Shape restrictions have played a central role in economics as both testable implications of theory and sufficient conditions for obtaining informative counterfactual predictions. In this paper, we provide a general procedure for inference under shape restrictions in identified and partially identified models defined by conditional moment restrictions. Our test statistics and proposed inference methods are based on the minimum of the generalized method of moments (GMM) objective function with and without shape restrictions. Uniformly valid critical values are obtained through a bootstrap procedure that approximates a subset of the true local parameter space. In an empirical analysis of the effect of childbearing on female labor supply, we show that employing shape restrictions in linear instrumental variables (IV) models can lead to shorter confidence regions for both local and average treatment effects. Other applications we discuss include inference for the variability of quantile IV treatment effects and for bounds on average equivalent variation in a demand model with general heterogeneity."
            },
            {
                "arxivId": "2106.02371",
                "title": "Cupid\u2019s Invisible Hand: Social Surplus and Identification in Matching Models",
                "abstract": "We investigate a model of one-to-one matching with transferable utility when some of the characteristics of the players are unobservable to the analyst. We allow for a wide class of distributions of unobserved heterogeneity, subject only to a separability assumption that generalizes Choo and Siow (2006). We first show that the stable matching maximizes a social gain function that trades off the average surplus due to the observable characteristics and a generalized entropy term that reflects the impact of matching on unobserved characteristics. We use this result to derive simple closed-form formulae that identify the joint surplus in every possible match and the equilibrium utilities of all participants, given any known distribution of unobserved heterogeneity. If transfers are observed, then the pre-transfer utilities of both partners are also identified. We also present a very fast algorithm that computes the optimal matching for any specification of the joint surplus. We conclude by discussing some empirical approaches suggested by these results."
            },
            {
                "arxivId": "1404.3763",
                "title": "Inference on Directionally Differentiable Functions",
                "abstract": "This paper studies an asymptotic framework for conducting inference on parameters of the form ( 0), where is a known directionally dierentiable function and 0 is estimated by ^ n. In these settings, the asymptotic distribution of the plug-in estimator ( ^ n) can be readily derived employing existing extensions to the Delta method. We show, however, that the \\standard\" bootstrap is only consistent under overly stringent conditions { in particular we establish that dierentiability of is a necessary and sucient condition for bootstrap consistency whenever the limiting distribution of ^ n is Gaussian. An alternative resampling scheme is proposed which remains consistent when the bootstrap fails, and is shown to provide local size control under restrictions on the directional derivative of . We illustrate the utility of our results by developing a test of whether a Hilbert space valued parameter belongs to a convex set { a setting that includes moment inequality problems and certain tests of shape restrictions as special cases."
            },
            {
                "arxivId": "1202.0666",
                "title": "Generalized minimizers of convex integral functionals, Bregman distance, Pythagorean identities",
                "abstract": "Integral functionals based on convex normal integrands are minimized subject to finitely many moment constraints. The integrands are finite on the positive and infinite on the negative numbers, strictly convex but not necessarily differentiable. The minimization is viewed as a primal problem and studied together with a dual one in the framework of convex duality. The effective domain of the value function is described by a conic core, a modification of the earlier concept of convex core. Minimizers and generalized minimizers are explicitly constructed from solutions of modified dual problems, not assuming the primal constraint qualification. A generalized Pythagorean identity is presented using Bregman distance and a correction term for lack of essential smoothness in integrands. Results are applied to minimization of Bregman distances. Existence of a generalized dual solution is established whenever the dual value is finite, assuming the dual constraint qualification. Examples of `irregular' situations are included, pointing to the limitations of generality of certain key results."
            },
            {
                "arxivId": "2102.12249",
                "title": "Set Identification in Models with Multiple Equilibria",
                "abstract": "We propose a computationally feasible way of deriving the identified features of models with multiple equilibria in pure or mixed strategies. It is shown that in the case of Shapley regular normal form games, the identified set is characterized by the inclusion of the true data distribution within the core of a Choquet capacity, which is interpreted as the generalized likelihood of the model. In turn, this inclusion is characterized by a finite set of inequalities and efficient and easily implementable combinatorial methods are described to check them. In all normal form games, the identified set is characterized in terms of the value of a submodular or convex optimization program. Efficient algorithms are then given and compared to check inclusion of a parameter in this identified set. The latter are illustrated with family bargaining games and oligopoly entry games. Copyright 2011, Oxford University Press."
            },
            {
                "arxivId": "2102.04162",
                "title": "Optimal transportation and the falsifiability of incompletely specified economic models",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2003.00930",
        "category": "econ",
        "title": "Continuum and thermodynamic limits for a simple random-exchange model",
        "abstract": null,
        "references": [
            {
                "arxivId": "1703.08811",
                "title": "Derivation of mean-field equations for stochastic particle systems",
                "abstract": null
            },
            {
                "arxivId": "1512.00503",
                "title": "Isotropic Wave Turbulence with simplified kernels: existence, uniqueness and mean-field limit for a class of instantaneous coagulation-fragmentation processes",
                "abstract": "The isotropic 4-wave kinetic equation is considered in its weak formulation using model (simplified) homogeneous kernels. Existence and uniqueness of solutions is proven in a particular setting where the kernels have a rate of growth at most linear. We also consider finite stochastic particle systems undergoing instantaneous coagulation- fragmentation phenomena and give conditions in which this system approximates the solution of the equation (mean-field limit)."
            },
            {
                "arxivId": "1505.07433",
                "title": "Opinion dynamics: inhomogeneous Boltzmann-type equations modelling opinion leadership and political segregation",
                "abstract": "We propose and investigate different kinetic models for opinion formation, when the opinion formation process depends on an additional independent variable, e.g. a leadership or a spatial variable. More specifically, we consider (i) opinion dynamics under the effect of opinion leadership, where each individual is characterized not only by its opinion but also by another independent variable which quantifies leadership qualities; (ii) opinion dynamics modelling political segregation in \u2018The Big Sort\u2019, a phenomenon that US citizens increasingly prefer to live in neighbourhoods with politically like-minded individuals. Based on microscopic opinion consensus dynamics such models lead to inhomogeneous Boltzmann-type equations for the opinion distribution. We derive macroscopic Fokker\u2013Planck-type equations in a quasi-invariant opinion limit and present results of numerical experiments."
            },
            {
                "arxivId": "1208.5753",
                "title": "From Newton to Boltzmann: Hard Spheres and Short-range Potentials",
                "abstract": "We provide a rigorous derivation of the Boltzmann equation as the mesoscopic limit of systems of hard spheres, or Newtonian particles interacting via a short-range potential, as the number of particles $N$ goes to infinity and the characteristic length of interaction $\\e$ simultaneously goes to $0,$ in the Boltzmann-Grad scaling $N \\e^{d-1} \\equiv 1.$ \nThe time of validity of the convergence is a fraction of the average time of first collision, due to a limitation of the time on which one can prove uniform estimates for the BBGKY and Boltzmann hierarchies. \nOur proof relies on the fundamental ideas of Lanford, and the important contributions of King, Cercignani, Illner and Pulvirenti, and Cercignani, Gerasimenko and Petrina. The main novelty here is the detailed study of pathological trajectories involving recollisions, which proves the term-by-term convergence for the correlation series expansion."
            },
            {
                "arxivId": "1002.3689",
                "title": "Explicit equilibria in a kinetic model of gambling.",
                "abstract": "We introduce and discuss a nonlinear kinetic equation of Boltzmann type which describes the evolution of wealth in a pure gambling process, where the entire sum of wealths of two agents is up for gambling, and randomly shared between the agents. For this equation the analytical form of the steady states is found for various realizations of the random fraction of the sum which is shared to the agents. Among others, the exponential distribution appears as steady state in case of a uniformly distributed random fraction, while Gamma distribution appears for a random fraction which is Beta distributed. The case in which the gambling game is only conservative-in-the-mean is shown to lead to an explicit heavy tailed distribution."
            },
            {
                "arxivId": "0710.3269",
                "title": "Differential equation approximations for Markov chains",
                "abstract": "We formulate some simple conditions under which a Markov chain may be approximated by the solution to a differential equation, with quantifiable error probabilities. The role of a choice of coordinate functions for the Markov chain is emphasised. The general theory is illustrated in three examples: the classical stochastic epidemic, a population process model with fast and slow variables, and core-finding algorithms for large random hypergraphs."
            },
            {
                "arxivId": "math-ph/0605052",
                "title": "Kinetic models of opinion formation",
                "abstract": "We introduce and discuss certain kinetic models of (continuous) opinion formation involving both exchange of opinion between individual agents and diffusion of information. We show conditions which ensure that the kinetic model reaches non trivial stationary states in case of lack of diffusion in correspondence of some opinion point. Analytical results are then obtained by considering a suitable asymptotic limit of the model yielding a Fokker-Planck equation for the distribution of opinion among individuals."
            },
            {
                "arxivId": "math/0412429",
                "title": "On a Kinetic Model for a Simple Market Economy",
                "abstract": null
            },
            {
                "arxivId": "math/9801145",
                "title": "Smoluchowski's coagulation equation: uniqueness, non-uniqueness and a hydrodynamic limit for the stochastic coalescent",
                "abstract": "Sufficient conditions are given for existence and uniqueness in Smoluchowski's coagulation equation, for a wide class of coagulation kernels and initial mass distributions. An example of non-uniqueness is constructed. The stochastic coalescent is shown to converge weakly to the solution of Smoluchowski's equation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2009.13961",
        "category": "econ",
        "title": "Online Action Learning in High Dimensions: A Conservative Perspective",
        "abstract": "Sequential learning problems are common in several fields of research and practical applications. Examples include dynamic pricing and assortment, design of auctions and incentives and permeate a large number of sequential treatment experiments. In this paper, we extend one of the most popular learning solutions, the $\\epsilon_t$-greedy heuristics, to high-dimensional contexts considering a conservative directive. We do this by allocating part of the time the original rule uses to adopt completely new actions to a more focused search in a restrictive set of promising actions. The resulting rule might be useful for practical applications that still values surprises, although at a decreasing rate, while also has restrictions on the adoption of unusual actions. With high probability, we find reasonable bounds for the cumulative regret of a conservative high-dimensional decaying $\\epsilon_t$-greedy rule. Also, we provide a lower bound for the cardinality of the set of viable actions that implies in an improved regret bound for the conservative version when compared to its non-conservative counterpart. Additionally, we show that end-users have sufficient flexibility when establishing how much safety they want, since it can be tuned without impacting theoretical properties. We illustrate our proposal both in a simulation exercise and using a real dataset.",
        "references": [
            {
                "arxivId": "1301.1722",
                "title": "Linear bandits in high dimension and recommendation systems",
                "abstract": "A large number of online services provide automated recommendations to help users to navigate through a large collection of items. New items (products, videos, songs, advertisements) are suggested on the basis of the user's past history and - when available - her demographic profile. Recommendations have to satisfy the dual goal of helping the user to explore the space of available items, while allowing the system to probe the user's preferences. We model this trade-off using linearly parametrized multi-armed bandits and prove upper and lower bounds that coincide up to constants in the data poor (high-dimensional) regime. We test (a variation of) the scheme used for estabilishing achievability on the Netflix dataset, and obtain results in agreement with the theory."
            },
            {
                "arxivId": "1003.0146",
                "title": "A contextual-bandit approach to personalized news article recommendation",
                "abstract": "Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.\n In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.\n The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2010.15864",
        "category": "econ",
        "title": "Identification and Estimation of Unconditional Policy Effects of an Endogenous Binary Treatment: An Unconditional MTE Approach",
        "abstract": "This paper studies the identification and estimation of policy effects when treatment status is binary and endogenous. We introduce a new class of marginal treatment effects (MTEs) based on the influence function of the functional underlying the policy target. We show that an unconditional policy effect can be represented as a weighted average of the newly defined MTEs over the individuals who are indifferent about their treatment status. We provide conditions for point identification of the unconditional policy effects. When a quantile is the functional of interest, we introduce the UNconditional Instrumental Quantile Estimator (UNIQUE) and establish its consistency and asymptotic distribution. In the empirical application, we estimate the effect of changing college enrollment status, induced by higher tuition subsidy, on the quantiles of the wage distribution.",
        "references": [
            {
                "arxivId": "2105.09445",
                "title": "Two Sample Unconditional Quantile Effect",
                "abstract": "This paper proposes a new framework to evaluate unconditional quantile effects (UQE) in a data combination model. The UQE measures the effect of a marginal counterfactual change in the unconditional distribution of a covariate on quantiles of the unconditional distribution of a target outcome. Under rank similarity and conditional independence assumptions, we provide a set of identification results for UQEs when the target covariate is continuously distributed and when it is discrete, respectively. Based on these identification results, we propose semiparametric estimators and establish their large sample properties under primitive conditions. Applying our method to a variant of Mincer\u2019s earnings function, we study the counterfactual quantile effect of actual work experience on income."
            },
            {
                "arxivId": "2007.13659",
                "title": "Unconditional quantile regression with high\u2010dimensional data",
                "abstract": "This paper considers estimation and inference for heterogeneous counterfactual effects with high\u2010dimensional data. We propose a novel robust score for debiased estimation of the unconditional quantile regression (Firpo, Fortin, and Lemieux (2009)) as a measure of heterogeneous counterfactual marginal effects. We propose a multiplier bootstrap inference and develop asymptotic theories to guarantee the size control in large sample. Simulation studies support our theories. Applying the proposed method to Job Corps survey data, we find that a policy, which counterfactually extends the duration of exposures to the Job Corps training program, will be effective especially for the targeted subpopulations of lower potential wage earners."
            },
            {
                "arxivId": "1805.11503",
                "title": "Estimation and inference for policy relevant treatment effects",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2112.04566",
        "category": "econ",
        "title": "Theoretical Economics and the Second-Order Economic Theory. What is it?",
        "abstract": "The economic and financial variables of economic agents determine macroeconomic variables. Current models consider agents' variables that are determined by the sums of values and volumes of agents' trades during some time interval {\\Delta}. We call them first-order economic variables. We describe how the volatilities and correlations of market trade values and volumes determine price volatility. We argue that such a link requests consideration of agents' economic variables of the second order that are composed of sums of squares of agents' transactions during {\\Delta}. Almost any variable of the first order should be complemented by its second-order pair. Respectively, the sums of agents' second-order variables introduce macroeconomic variables of the second order. The description of the first- and second-order macroeconomic variables establishes the subject of second-order economic theory. We highlight that the complexity of second-order economic theory essentially restricts any hopes for precise predictions of price probability and, at best, could provide estimates of price volatility. That limits the predictions of price probability to Gauss's approximations only.",
        "references": [
            {
                "arxivId": "2012.04506",
                "title": "Business Cycles as Collective Risk Fluctuations",
                "abstract": "We suggest use continuous numerical risk grades [0,1] of R for a single risk or the unit cube in Rn for n risks as the economic domain. We consider risk ratings of economic agents as their coordinates in the economic domain. Economic activity of agents, economic or other factors change agents risk ratings and that cause motion of agents in the economic domain. Aggregations of variables and transactions of individual agents in small volume of economic domain establish the continuous economic media approximation that describes collective variables, transactions and their flows in the economic domain as functions of risk coordinates. Any economic variable A(t,x) defines mean risk XA(t) as risk weighted by economic variable A(t,x). Collective flows of economic variables in bounded economic domain fluctuate from secure to risky area and back. These fluctuations of flows cause time oscillations of macroeconomic variables A(t) and their mean risks XA(t) in economic domain and are the origin of any business and credit cycles. We derive equations that describe evolution of collective variables, transactions and their flows in the economic domain. As illustration we present simple self-consistent equations of supply-demand cycles that describe fluctuations of supply, demand and their mean risks."
            },
            {
                "arxivId": "2009.14278",
                "title": "Price, Volatility and the Second-Order Economic Theory",
                "abstract": "We introduce the new price probability measure, which entirely depends on the probability measures of the value and the volume of the market trades. We define the nth statistical moment of the price as the ratio of the nth statistical moment of the value to the nth statistical moment of the volume of all trades performed during an averaging time interval \u0394. The set of the price statistical moments determines the price characteristic function and its Fourier transform defines the price probability measure. The price volatility depends on the 1st and the 2nd statistical moments of the value and the volume of the trades. The prediction of the price volatility requires a description of the sums of squares of the value and the volume of the market trades during the interval \u0394 and we call it the second-order economic theory. To develop that theory, we introduce numerical continuous risk ratings and distribute the agents by the risk ratings as coordinates. Based on distributions of the agents by the risk coordinates, we introduce a continuous economic media approximation that describes the collective trades. The agents perform the trades under the action of their expectations. We model the mutual impact of the expectations and the trades and derive equations that describe their evolution. To illustrate the benefits of our approach, in a linear approximation we describe perturbations of the mean price, the mean square price and the price volatility as functions of the first and the second-degree trades\u2019 disturbances."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2203.06860",
        "category": "econ",
        "title": "Cooperative networks and f-Shapley value",
        "abstract": "Lloyd Shapley's cooperative value allocation theory stands as a central concept in game theory, extensively utilized across various domains to distribute resources, evaluate individual contributions, and ensure fairness. The Shapley value formula and his four axioms that characterize it form the foundation of the theory. Traditionally, the Shapley value is assigned under the assumption that all players in a cooperative game will ultimately form the grand coalition. In this paper, we reinterpret the Shapley value as an expectation of a certain stochastic path integral, with each path representing a general coalition formation process. As a result, the value allocation is naturally extended to all partial coalition states. In addition, we provide a set of five properties that extend the Shapley axioms and characterize the stochastic path integral. Finally, by integrating Hodge calculus, stochastic processes, and path integration of edge flows on graphs, we expand the cooperative value allocation theory beyond the standard coalition game structure to encompass a broader range of cooperative network configurations.",
        "references": [
            {
                "arxivId": "2202.05594",
                "title": "The Shapley Value in Machine Learning",
                "abstract": "Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research."
            },
            {
                "arxivId": "1904.02868",
                "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
                "abstract": "As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor."
            },
            {
                "arxivId": "1709.08318",
                "title": "Hodge decomposition and the Shapley value of a cooperative game",
                "abstract": null
            },
            {
                "arxivId": "1507.05379",
                "title": "Hodge Laplacians on graphs",
                "abstract": "This is an elementary introduction to the Hodge Laplacian on a graph, a higher-order generalization of the graph Laplacian. We will discuss basic properties including cohomology and Hodge theory. The main feature of our approach is simplicity, requiring only knowledge of linear algebra and graph theory. We have also isolated the algebra from the topology to show that a large part of cohomology and Hodge theory is nothing more than the linear algebra of matrices satisfying $AB = 0$. For the remaining topological aspect, we cast our discussions entirely in terms of graphs as opposed to less-familiar topological objects like simplicial complexes."
            },
            {
                "arxivId": "1005.2405",
                "title": "Flows and Decompositions of Games: Harmonic and Potential Games",
                "abstract": "In this paper we introduce a novel flow representation for finite games in strategic form. This representation allows us to develop a canonical direct sum decomposition of an arbitrary game into three components, which we refer to as the potential, harmonic, and nonstrategic components. We analyze natural classes of games that are induced by this decomposition, and in particular, focus on games with no harmonic component and games with no potential component. We show that the first class corresponds to the well-known potential games. We refer to the second class of games as harmonic games, and demonstrate that this new class has interesting properties which contrast with properties of potential games. Exploiting the decomposition framework, we obtain explicit expressions for the projections of games onto the subspaces of potential and harmonic games. This enables an extension of the equilibrium properties of potential and harmonic games to \u201cnearby\u201d games."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2204.07506",
        "category": "econ",
        "title": "Price and Payoff Autocorrelations in a Multi-Period Consumption-Based Asset Pricing Model",
        "abstract": "This paper highlights the hidden dependence of the basic pricing equation of a multi-period consumption-based asset pricing model on price and payoff autocorrelations. We obtain the approximations of the basic pricing equation that describe the mean price\"to-day,\"mean payoff\"next-day,\"price and payoff volatilities, and price and payoff autocorrelations. The deep conjunction of the consumption-based model with other versions of asset pricing, such as ICAPM, APM, etc. (Cochrane, 2001), emphasizes that our results are valid for other pricing models.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2207.03816",
        "category": "econ",
        "title": "The welfare effects of nonlinear health dynamics",
        "abstract": "We generate a continuous measure of health to estimate a non-parametric model of health dynamics, showing that adverse health shocks are highly persistent when suffered by people in poor health. Canonical models cannot account for this pattern. We incorporate this health dynamic into a life-cycle model of consumption, savings, and labor force participation. After estimating the model parameters, we simulate the effects of health shocks on economic outcomes. We find that bad health shocks have long-term adverse economic effects that are more extreme for those in poor health. Furthermore, bad health shocks also increase the disparity of asset accumulation among this group of people. A canonical model of health dynamics would not reveal these effects.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2207.09036",
        "category": "econ",
        "title": "Schooling and Labor Market Consequences of School Construction in Indonesia: Comment",
        "abstract": "Duflo (2001) exploits a 1970s schooling expansion to estimate the returns to schooling in Indonesia. Under the study's difference-in-differences (DID) design, two patterns in the data--shallower pay scales for younger workers and negative selection in treatment--can violate the parallel trends assumption and upward-bias results. In response, I follow up later, test for trend breaks timed to the intervention, and perform changes-in-changes (CIC). I also correct data errors, cluster variance estimates, incorporate survey weights to correct for endogenous sampling, and test for (and detect) instrument weakness. Weak identification-robust inference yields positive but imprecise estimates. CIC estimates also tilt positive.",
        "references": [
            {
                "arxivId": "2103.13229",
                "title": "Simple Diagnostics for Two-Way Fixed Effects",
                "abstract": "Difference-in-differences estimation is a widely used method of program evaluation. When treatment is implemented in different places at different times, researchers often use two-way fixed effects to control for location-specific and period-specific shocks. Such estimates can be severely biased when treatment effects change over time within treated units. I review the sources of this bias and propose several simple diagnostics for assessing its likely severity. I illustrate these tools through a case study of free primary education in Sub-Saharan Africa. JEL codes: C21, O15"
            },
            {
                "arxivId": "1803.08807",
                "title": "Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects",
                "abstract": "Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)"
            },
            {
                "arxivId": "1510.01757",
                "title": "Fuzzy Differences-in-Differences",
                "abstract": "In many applications of the differences-in-differences (DID) method, the treatment increases more in the treatment group, but some units are also treated in the control group. In such fuzzy designs, a popular estimator of treatment effects is the DID of the outcome divided by the DID of the treatment, or OLS and 2SLS regressions with time and group fixed effects estimating weighted averages of this ratio across groups. We start by showing that when the treatment also increases in the control group, this ratio estimates a causal effect only if treatment effects are homogenous in the two groups. Even when the distribution of treatment is stable, it requires that the effect of time be the same on all counterfactual outcomes. As this assumption is not always applicable, we propose two alternative estimators. The first estimator relies on a generalization of common trends assumptions to fuzzy designs, while the second extends the changes-in-changes estimator of Athey & Imbens (2006). When the distribution of treatment changes in the control group, treatment effects are partially identified. Finally, we prove that our estimators are asymptotically normal and use them to revisit applied papers using fuzzy designs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2210.16113",
        "category": "econ",
        "title": "Intelligence and global bias in the stock market",
        "abstract": "Trade is one of the essential features of human intelligence. The securities market is the ultimate expression of it. The fundamental indicators of stocks include information about the effects of noise and bias on stock prices; however, distinguishing between them is generally hard. In this article, I present the fundamentals hypothesis based on rational expectations and detect the global bias components from the actual fundamental indicators using a log-normal distribution model based on the fundamentals hypothesis. The analysis results show that biases generally exhibit the same characteristics, strongly supporting our theory. Notably, the positive price-to-cash flows from the investing activities ratio are proxies for the fundamentals. The answer is simple: \"Cash is a fact, and profit is an opinion.\" Namely, opinions of management and accounting added noise to fundamentals. As a result, we obtain the Kesten process and the Pareto distribution. This result means the market knows this noise and shows a stable global bias in the stock market.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2211.00131",
        "category": "econ",
        "title": "New Concept for the Value Function of Prospect Theory",
        "abstract": "In the prospect theory, value function is typically concave for gains, commonly convex for losses, with losses usually having a steeper slope than gains. The neural system largely differs from the loss and gains sides. Five new studies on neurons related to this issue have examined neuronal responses to losses, gains, and reference points. This study investigates a new concept of the value function. A value function with a neuronal cusp may show variations and behavior cusps with catastrophe where a trader closes one's position.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2308.10138",
        "category": "econ",
        "title": "On the Inconsistency of Cluster-Robust Inference and How Subsampling Can Fix It",
        "abstract": "Conventional methods of cluster-robust inference are inconsistent in the presence of unignorably large clusters. We formalize this claim by establishing a necessary and sufficient condition for the consistency of the conventional methods. We find that this condition for the consistency is rejected for a majority of empirical research papers. In this light, we propose a novel score subsampling method that achieves uniform size control over a broad class of data generating processes, covering that fails the conventional method. Simulation studies support these claims. With real data used by an empirical paper, we showcase that the conventional methods conclude significance while our proposed method concludes insignificance.",
        "references": [
            {
                "arxivId": "2301.04527",
                "title": "Fast and Reliable Jackknife and Bootstrap Methods for Cluster\u2010Robust Inference",
                "abstract": "We provide computationally attractive methods to obtain jackknife-based cluster-robust variance matrix estimators (CRVEs) for linear regression models estimated by least squares. We also propose several new variants of the wild cluster bootstrap, which involve these CRVEs, jackknife-based bootstrap data-generating processes, or both. Extensive simulation experiments suggest that the new methods can provide much more reliable inferences than existing ones in cases where the latter are not trustworthy, such as when the number of clusters is small and/or cluster sizes vary substantially. Three empirical examples illustrate the new methods."
            },
            {
                "arxivId": "2211.14903",
                "title": "Inference in Cluster Randomized Trials with Matched Pairs",
                "abstract": "This paper considers the problem of inference in cluster randomized trials where treatment status is determined according to a\"matched pairs'' design. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by a\"matched pairs'' design we mean that a sample of clusters is paired according to baseline, cluster-level covariates and, within each pair, one cluster is selected at random for treatment. We study the large-sample behavior of a weighted difference-in-means estimator and derive two distinct sets of results depending on if the matching procedure does or does not match on cluster size. We then propose a single variance estimator which is consistent in either regime. Combining these results establishes the asymptotic exactness of tests based on these estimators. Next, we consider the properties of two common testing procedures based on t-tests constructed from linear regressions, and argue that both are generally conservative in our framework. We additionally study the behavior of a randomization test which permutes the treatment status for clusters within pairs, and establish its finite-sample and asymptotic validity for testing specific null hypotheses. Finally, we propose a covariate-adjusted estimator which adjusts for additional baseline covariates not used for treatment assignment, and establish conditions under which such an estimator leads to improvements in precision. A simulation study confirms the practical relevance of our theoretical results."
            },
            {
                "arxivId": "2210.16991",
                "title": "Non-Robustness of the Cluster-Robust Inference: with a Proposal of a New Robust Method",
                "abstract": "The conventional cluster-robust (CR) standard errors may not be robust. They are vulnerable to data that contain a small number of large clusters. When a researcher uses the 51 states in the U.S. as clusters, the largest cluster (California) consists of about 10% of the total sample. Such a case in fact violates the assumptions under which the widely used CR methods are guaranteed to work. We formally show that the conventional CR methods fail if the distribution of cluster sizes follows a power law with exponent less than two. Besides the example of 51 state clusters, some examples are drawn from a list of recent original research articles published in a top journal. In light of these negative results about the existing CR methods, we propose a weighted CR (WCR) method as a simple fix. Simulation studies support our arguments that the WCR method is robust while the conventional CR methods are not."
            },
            {
                "arxivId": "2205.03285",
                "title": "Cluster-robust inference: A guide to empirical practice",
                "abstract": null
            },
            {
                "arxivId": "2204.08356",
                "title": "Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes",
                "abstract": "This paper considers the problem of inference in cluster randomized experiments when cluster sizes are non-ignorable. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the cluster level. By non-ignorable cluster sizes, we refer to the possibility that the treatment effects may depend non-trivially on the cluster sizes. We frame our analysis in a super-population framework in which cluster sizes are random. In this way, our analysis departs from earlier analyses of cluster randomized experiments in which cluster sizes are treated as non-random. We distinguish between two different parameters of interest: the equally-weighted cluster-level average treatment effect, and the size-weighted cluster-level average treatment effect. For each parameter, we provide methods for inference in an asymptotic framework where the number of clusters tends to infinity and treatment is assigned using a covariate-adaptive stratified randomization procedure. We additionally permit the experimenter to sample only a subset of the units within each cluster rather than the entire cluster and demonstrate the implications of such sampling for some commonly used estimators. A small simulation study and empirical demonstration show the practical relevance of our theoretical results."
            },
            {
                "arxivId": "2109.03971",
                "title": "Some impossibility results for inference with cluster dependence with large clusters",
                "abstract": null
            },
            {
                "arxivId": "1902.01497",
                "title": "Asymptotic Theory for Clustered Samples",
                "abstract": "We provide a complete asymptotic distribution theory for clustered data with a large number of groups, generalizing the classic laws of large numbers, uniform laws, central limit theory, and clustered covariance matrix estimation. Our theory allows for clustered observations with heterogeneous and unbounded cluster sizes. Our conditions cleanly nest the classical results for i.n.i.d. observations, in the sense that our conditions specialize to the classical conditions under independent sampling. We use this theory to develop a full asymptotic distribution theory for estimation based on linear least-squares, 2SLS, nonlinear MLE, and nonlinear GMM."
            },
            {
                "arxivId": "1710.02926",
                "title": "When Should You Adjust Standard Errors for Clustering?",
                "abstract": "\n Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, e.g., by state but not by gender, and in observational studies, but not in completely randomized experiments? (ii) Why is conventional clustering an \u201call-or-nothing\u201d adjustment, while within-cluster correlations can be strong or extremely weak? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional cluster standard errors can be severely inflated, and propose new variance estimators that correct for this bias."
            },
            {
                "arxivId": "0706.1765",
                "title": "Extreme values of \u03b6\u2032(\u03c1)",
                "abstract": "Assuming the Riemann hypothesis, we exhibit large and small values of the derivative of the zeta function evaluated at the non\u2010trivial zeros of the zeta function. These results are proved by applying Soundararajan's resonance method."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2310.00231",
        "category": "econ",
        "title": "The Distributional Impact of Inflation in Pakistan: A Case Study of a New Price Focused Microsimulation Framework, PRICES",
        "abstract": "This paper develops a microsimulation model to simulate the distributional impact of price changes using Household Budget Survey data, income survey data and an Input Output Model. The primary purpose is to describe the model components. The secondary purpose is to demonstrate one component of the model by assessing the distributional and welfare impact of recent price changes in Pakistan. Over the period of November 2020 to November 2022, headline inflation 41.5 percent, with food and transportation prices increasing most. The analysis shows that despite large increases in energy prices, the importance of energy prices for the welfare losses due to inflation is limited because energy budget shares are small and inflation is relatively low. The overall distributional impact of recent price changes is mildly progressive, but household welfare is impacted significantly irrespective of households position along the income distribution. The biggest driver of welfare losses at the bottom of the income distribution was food price inflation, while inflation in other goods and services was the biggest driver at the top. To compensate households for increased living costs, transfers would need to be on average 40 percent of pre-inflation expenditure, assuming constant incomes. Behavioural responses to price changes have a negligible impact on the overall welfare cost to households.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2310.14138",
        "category": "econ",
        "title": "A prototype software framework for transferable computational health economic models and its early application in youth mental health",
        "abstract": "We are developing an economic model to explore multiple topics in Australian youth mental health policy. We want that model to be readily transferable to other jurisdictions. We developed a software framework for authoring transparent, reusable and updatable Computational Health Economic Models (CHEMs) (the software files that implement health economic models). We specified framework user requirements of a template CHEM module that facilitates modular model implementations, a simple programming syntax and tools for authoring new CHEM modules, supplying CHEMs with data, reporting reproducible CHEM analyses, searching for CHEM modules and maintaining a CHEM project website. We implemented the framework as six development version code libraries in the programming language R that integrate with online services for software development and research data archiving. We used the framework to author five development version R libraries of CHEM modules focused on utility mapping in youth mental health. These modules provide tools for variable validation, dataset description, multi-attribute instrument scoring, construction of mapping models, reporting of mapping studies and making out of sample predictions. We assessed these CHEM module libraries as mostly meeting transparency, reusability and updatability criteria that we have previously developed, but requiring more detailed documentation and unit testing of individual modules. Our software framework has potential value as a prototype for future tools to support the development of transferable CHEMs.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2401.00748",
        "category": "econ",
        "title": "Sequential choice functions and stability problems",
        "abstract": "The concept of sequential choice functions is introduced and studied. This concept applies to the reduction of the problem of stable matchings with sequential workers to a situation where the workers are linear.",
        "references": [
            {
                "arxivId": "2209.09293",
                "title": "Lexicographic Composition of Choice Functions",
                "abstract": "Lexicographic composition is a natural way to build an aggregate choice function from component choice functions. As the name suggests, the components are ordered and choose sequentially. The sets that subsequent components select from are constrained by the choices made by earlier choice functions. The speci\ufb01c constraints affect whether properties like path independence are preserved. For several domains of inputs, we characterize the constraints that ensure such preservation."
            },
            {
                "arxivId": "2202.13089",
                "title": "Stable and meta-stable contract networks",
                "abstract": null
            },
            {
                "arxivId": "math/0304171",
                "title": "Mathematics of Plott choice functions",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2403.15910",
        "category": "econ",
        "title": "Difference-in-Differences with Unpoolable Data",
        "abstract": "In this study, we identify and relax the assumption of data\"poolability\"in difference-in-differences (DID) estimation. Poolability, or the combination of observations from treated and control units into one dataset, is often not possible due to data privacy concerns. For instance, administrative health data stored in secure facilities is often not combinable across jurisdictions. We propose an innovative approach to estimate DID with unpoolable data: UN--DID. Our method incorporates adjustments for additional covariates, multiple groups, and staggered adoption. Without covariates, UN--DID and conventional DID give identical estimates of the average treatment effect on the treated (ATT). With covariates, we show mathematically and through simulations that UN--DID and conventional DID provide different, but equally informative, estimates of the ATT. An empirical example further underscores the utility of our methodology. The UN--DID method paves the way for more comprehensive analyses of policy impacts, even under data poolability constraints.",
        "references": [
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            },
            {
                "arxivId": "2107.11732",
                "title": "Federated causal inference in heterogeneous observational data",
                "abstract": "We are interested in estimating the effect of a treatment applied to individuals at multiple sites, where data is stored locally for each site. Due to privacy constraints, individual\u2010level data cannot be shared across sites; the sites may also have heterogeneous populations and treatment assignment mechanisms. Motivated by these considerations, we develop federated methods to draw inferences on the average treatment effects of combined data across sites. Our methods first compute summary statistics locally using propensity scores and then aggregate these statistics across sites to obtain point and variance estimators of average treatment effects. We show that these estimators are consistent and asymptotically normal. To achieve these asymptotic properties, we find that the aggregation schemes need to account for the heterogeneity in treatment assignments and in outcomes across sites. We demonstrate the validity of our federated methods through a comparative study of two large medical claims databases."
            },
            {
                "arxivId": "2102.02079",
                "title": "Federated Learning on Non-IID Data Silos: An Experimental Study",
                "abstract": "Due to the increasing privacy concerns and data regulations, training data have been increasingly fragmented, forming distributed databases of multiple \u201cdata silos\u201d (e.g., within different organizations and countries). To develop effective machine learning services, there is a must to exploit data from such distributed databases without exchanging the raw data. Recently, federated learning (FL) has been a solution with growing interests, which enables multiple parties to collaboratively train a machine learning model without exchanging their local data. A key and common challenge on distributed databases is the heterogeneity of the data distribution among the parties. The data of different parties are usually non-independently and identically distributed (i.e., non-IID). There have been many FL algorithms to address the learning effectiveness under non-IID data settings. However, there lacks an experimental study on systematically understanding their advantages and disadvantages, as previous studies have very rigid data partitioning strategies among parties, which are hardly representative and thorough. In this paper, to help researchers better understand and study the non-IID data setting in federated learning, we propose comprehensive data partitioning strategies to cover the typical non-IID data cases. Moreover, we conduct extensive experiments to evaluate state-of-the-art FL algorithms. We find that non-IID does bring significant challenges in learning accuracy of FL algorithms, and none of the existing state-of-the-art FL algorithms outperforms others in all cases. Our experiments provide insights for future studies of addressing the challenges in \u201cdata silos\u201d."
            },
            {
                "arxivId": "1812.01723",
                "title": "Doubly Robust Difference-in-Differences Estimators",
                "abstract": "Abstract This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying particular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available."
            },
            {
                "arxivId": "1707.09563",
                "title": "Identification of Treatment Effects under Conditional Partial Independence",
                "abstract": "Conditional independence of treatment assignment from potential outcomes is a commonly used but nonrefutable assumption. We derive identified sets for various treatment effect parameters under nonparametric deviations from this conditional independence assumption. These deviations are defined via a conditional treatment assignment probability, which makes it straightforward to interpret. Our results can be used to assess the robustness of empirical conclusions obtained under the baseline conditional independence assumption."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2403.16673",
        "category": "econ",
        "title": "Quasi-randomization tests for network interference",
        "abstract": "Many classical inferential approaches fail to hold when interference exists among the population units. This amounts to the treatment status of one unit affecting the potential outcome of other units in the population. Testing for such spillover effects in this setting makes the null hypothesis non-sharp. An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population. In randomized experiments, conditional randomized tests hold finite sample validity. Such approaches can pose computational challenges as finding these appropriate sub-populations based on experimental design can involve solving an NP-hard problem. In this paper, we view the network amongst the population as a random variable instead of being fixed. We propose a new approach that builds a conditional quasi-randomization test. Our main idea is to build the (non-sharp) null distribution of no spillover effects using random graph null models. We show that our method is exactly valid in finite-samples under mild assumptions. Our method displays enhanced power over other methods, with substantial improvement in complex experimental designs. We highlight that the method reduces to a simple permutation test, making it easy to implement in practice. We conduct a simulation study to verify the finite-sample validity of our approach and illustrate our methodology to test for interference in a weather insurance adoption experiment run in rural China.",
        "references": [
            {
                "arxivId": "2102.12604",
                "title": "Random Graphs with Prescribed K-Core Sequences: A New Null Model for Network Analysis",
                "abstract": "In the analysis of large-scale network data, a fundamental operation is the comparison of observed phenomena to the predictions provided by null models: when we find an interesting structure in a family of real networks, it is important to ask whether this structure is also likely to arise in random networks with similar characteristics to the real ones. A long-standing challenge in network analysis has been the relative scarcity of reasonable null models for networks; arguably the most common such model has been the configuration model, which starts with a graph G and produces a random graph with the same node degrees as G. This leads to a very weak form of null model, since fixing the node degrees does not preserve many of the crucial properties of the network, including the structure of its subgraphs. Guided by this challenge, we establish a new family of network null models that operate on the k-core decomposition. For a graph G, the k-core is its maximal subgraph of minimum degree k; and the core number of a node v in G is the largest k such that v belongs to the k-core of G. We provide the first efficient sampling algorithm to solve the following basic combinatorial problem: given a graph G, produce a random graph sampled nearly uniformly from among all graphs with the same sequence of core numbers as G. This opens the opportunity to compare observed networks G with random graphs that exhibit the same core numbers, a comparison that preserves aspects of the structure of G that are not captured by more local measures like the degree sequence. We illustrate the power of this core-based null model on some fundamental tasks in network analysis, including the enumeration of networks motifs."
            },
            {
                "arxivId": "2007.13302",
                "title": "Random graph asymptotics for treatment effect estimation under network interference",
                "abstract": "The network interference model for causal inference places all experimental units at the vertices of an undirected exposure graph, such that treatment assigned to one unit may affect the outcome of another unit if and only if these two units are connected by an edge. This model has recently gained popularity as means of incorporating interference effects into the Neyman--Rubin potential outcomes framework; and several authors have considered estimation of various causal targets, including the direct and indirect effects of treatment. In this paper, we consider large-sample asymptotics for treatment effect estimation under network interference in a setting where the exposure graph is a random draw from a graphon. When targeting the direct effect, we show that---in our setting---popular estimators are considerably more accurate than existing results suggest, and provide a central limit theorem in terms of moments of the graphon. Meanwhile, when targeting the indirect effect, we leverage our generative assumptions to propose a consistent estimator in a setting where no other consistent estimators are currently available. We also show how our results can be used to conduct a practical assessment of the sensitivity of randomized study inference to potential interference effects. Overall, our results highlight the promise of random graph asymptotics in understanding the practicality and limits of causal inference under network interference."
            },
            {
                "arxivId": "1910.10862",
                "title": "A\u00a0graph\u2010theoretic approach to randomization tests of causal effects under general\n\n\n interference",
                "abstract": "Interference exists when a unit's outcome depends on another unit's treatment assignment. For example, intensive policing on one street could have a spillover effect on neighbouring streets. Classical randomization tests typically break down in this setting because many null hypotheses of interest are no longer sharp under interference. A promising alternative is to instead construct a conditional randomization test on a subset of units and assignments for which a given null hypothesis is sharp. Finding these subsets is challenging, however, and existing methods are limited to special cases or have limited power. In this paper, we propose valid and easy\u2010to\u2010implement randomization tests for a general class of null hypotheses under arbitrary interference between units. Our key idea is to represent the hypothesis of interest as a bipartite graph between units and assignments, and to find an appropriate biclique of this graph. Importantly, the null hypothesis is sharp within this biclique, enabling conditional randomization\u2010based tests. We also connect the size of the biclique to statistical power. Moreover, we can apply off\u2010the\u2010shelf graph clustering methods to find such bicliques efficiently and at scale. We illustrate our approach in settings with clustered interference and show advantages over methods designed specifically for that setting. We then apply our method to a large\u2010scale policing experiment in Medell\u00edn, Colombia, where interference has a spatial structure."
            },
            {
                "arxivId": "1905.03446",
                "title": "Fast Uniform Generation of Random Graphs with Given Degree Sequences",
                "abstract": "In this paper we provide an algorithm that generates a graph with given degree sequence uniformly at random. Provided that \u0394^4=O(m), where \u0394 is the maximal degree and m is the number of edges, the algorithm runs in expected time O(m). Our algorithm significantly improves the previously most efficient uniform sampler, which runs in expected time O(m^2\u0394^2) for the same family of degree sequences. Our method uses a novel ingredient which progressively relaxes restrictions on an object being generated uniformly at random, and we use this to give fast algorithms for uniform sampling of graphs with other degree sequences as well. Using the same method, we also obtain algorithms with expected run time which is (i) linear for power-law degree sequences in cases where the previous best was O(n^4.081), and (ii) O(nd+d^4) for d-regular graphs when d=o(\u221a n), where the previous best was O(nd^3)."
            },
            {
                "arxivId": "1904.02308",
                "title": "Randomization Tests for Peer Effects in Group Formation Experiments",
                "abstract": "Measuring the effect of peers on individuals' outcomes is a challenging problem, in part because individuals often select peers who are similar in both observable and unobservable ways. Group formation experiments avoid this problem by randomly assigning individuals to groups and observing their responses; for example, do first\u2010year students have better grades when they are randomly assigned roommates who have stronger academic backgrounds? In this paper, we propose randomization\u2010based permutation tests for group formation experiments, extending classical Fisher Randomization Tests to this setting. The proposed tests are justified by the randomization itself, require relatively few assumptions, and are exact in finite samples. This approach can also complement existing strategies, such as linear\u2010in\u2010means models, by using a regression coefficient as the test statistic. We apply the proposed tests to two recent group formation experiments."
            },
            {
                "arxivId": "1704.01190",
                "title": "Testing for arbitrary interference on experimentation platforms",
                "abstract": "Experimentation platforms are essential to large modern technology companies, as they are used to carry out many randomized experiments daily. The classic assumption of no interference among users, under which the outcome for one user does not depend on the treatment assigned to other users, is rarely tenable on such platforms. Here, we introduce an experimental design strategy for testing whether this assumption holds. Our approach is in the spirit of the Durbin\u2013Wu\u2013Hausman test for endogeneity in econometrics, where multiple estimators return the same estimate if and only if the null hypothesis holds. The design that we introduce makes no assumptions on the interference model between units, nor on the network among the units, and has a sharp bound on the variance and an implied analytical bound on the Type I error rate. We discuss how to apply the proposed design strategy to large experimentation platforms, and we illustrate it in the context of an experiment on the LinkedIn platform."
            },
            {
                "arxivId": "1608.00607",
                "title": "Configuring Random Graph Models with Fixed Degree Sequences",
                "abstract": "Random graph null models have found widespread application in diverse research communities analyzing network datasets, including social, information, and economic networks, as well as food webs, protein-protein interactions, and neuronal networks. The most popular family of random graph null models, called configuration models, are defined as uniform distributions over a space of graphs with a fixed degree sequence. Commonly, properties of an empirical network are compared to properties of an ensemble of graphs from a configuration model in order to quantify whether empirical network properties are meaningful or whether they are instead a common consequence of the particular degree sequence. In this work we study the subtle but important decisions underlying the specification of a configuration model, and investigate the role these choices play in graph sampling procedures and a suite of applications. We place particular emphasis on the importance of specifying the appropriate graph labeling (stub-labeled or vertex-labeled) under which to consider a null model, a choice that closely connects the study of random graphs to the study of random contingency tables. We show that the choice of graph labeling is inconsequential for studies of simple graphs, but can have a significant impact on analyses of multigraphs or graphs with self-loops. The importance of these choices is demonstrated through a series of three vignettes, analyzing network datasets under many different configuration models and observing substantial differences in study conclusions under different models. We argue that in each case, only one of the possible configuration models is appropriate. While our work focuses on undirected static networks, it aims to guide the study of directed networks, dynamic networks, and all other network contexts that are suitably studied through the lens of random graph null models."
            },
            {
                "arxivId": "1506.02084",
                "title": "Exact p-Values for Network Interference",
                "abstract": "ABSTRACT We study the calculation of exact p-values for a large class of nonsharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit\u2019s treatment status on another according to the distance between units, for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (e.g., based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, e.g., only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment."
            },
            {
                "arxivId": "1404.7530",
                "title": "Design and Analysis of Experiments in Networks: Reducing Bias from Interference",
                "abstract": "Abstract Estimating the effects of interventions in networks is complicated due to interference, such that the outcomes for one experimental unit may depend on the treatment assignments of other units. Familiar statistical formalism, experimental designs, and analysis methods assume the absence of this interference, and result in biased estimates of causal effects when it exists. While some assumptions can lead to unbiased estimates, these assumptions are generally unrealistic in the context of a network and often amount to assuming away the interference. In this work, we evaluate methods for designing and analyzing randomized experiments under minimal, realistic assumptions compatible with broad interference, where the aim is to reduce bias and possibly overall error in estimates of average effects of a global treatment. In design, we consider the ability to perform random assignment to treatments that is correlated in the network, such as through graph cluster randomization. In analysis, we consider incorporating information about the treatment assignment of network neighbors. We prove sufficient conditions for bias reduction through both design and analysis in the presence of potentially global interference; these conditions also give lower bounds on treatment effects. Through simulations of the entire process of experimentation in networks, we measure the performance of these methods under varied network structure and varied social behaviors, finding substantial bias reductions and, despite a bias\u2013variance tradeoff, error reductions. These improvements are largest for networks with more clustering and data generating processes with both stronger direct effects of the treatment and stronger interactions between units."
            },
            {
                "arxivId": "1305.6979",
                "title": "Graph cluster randomization: network exposure to multiple universes",
                "abstract": "A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how graph cluster randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified. Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a restricted-growth condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference."
            },
            {
                "arxivId": "1005.1136",
                "title": "Random graphs with a given degree sequence",
                "abstract": "Large graphs are sometimes studied through their degree sequences (power law or regular graphs). We study graphs that are uniformly chosen with a given degree sequence. Under mild conditions, it is shown that sequences of such graphs have graph limits in the sense of Lov\\'{a}sz and Szegedy with identifiable limits. This allows simple determination of other features such as the number of triangles. The argument proceeds by studying a natural exponential model having the degree sequence as a sufficient statistic. The maximum likelihood estimate (MLE) of the parameters is shown to be unique and consistent with high probability. Thus $n$ parameters can be consistently estimated based on a sample of size one. A fast, provably convergent, algorithm for the MLE is derived. These ingredients combine to prove the graph limit theorem. Along the way, a continuous version of the Erd\\H{o}s--Gallai characterization of degree sequences is derived."
            },
            {
                "arxivId": "1003.0356",
                "title": "The number of graphs and a random graph with a given degree sequence",
                "abstract": "We consider the set of all graphs on n labeled vertices with prescribed degrees D = (d1,\u2026,dn). For a wide class of tame degree sequences D we obtain a computationally efficient asymptotic formula approximating the number of graphs within a relative error which approaches 0 as n grows. As a corollary, we prove that the structure of a random graph with a given tame degree sequence D is well described by a certain maximum entropy matrix computed from D. We also establish an asymptotic formula for the number of bipartite graphs with prescribed degrees of vertices, or, equivalently, for the number of 0\u20101 matrices with prescribed row and column sums. \u00a9 2012 Wiley Periodicals, Inc. Random Struct. Alg., 2013"
            },
            {
                "arxivId": "physics/0602124",
                "title": "Modularity and community structure in networks.",
                "abstract": "Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as \"modularity\" over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2403.16773",
        "category": "econ",
        "title": "Privacy-Protected Spatial Autoregressive Model",
        "abstract": "Spatial autoregressive (SAR) models are important tools for studying network effects. However, with an increasing emphasis on data privacy, data providers often implement privacy protection measures that make classical SAR models inapplicable. In this study, we introduce a privacy-protected SAR model with noise-added response and covariates to meet privacy-protection requirements. However, in this scenario, the traditional quasi-maximum likelihood estimator becomes infeasible because the likelihood function cannot be formulated. To address this issue, we first consider an explicit expression for the likelihood function with only noise-added responses. However, the derivatives are biased owing to the noise in the covariates. Therefore, we develop techniques that can correct the biases introduced by noise. Correspondingly, a Newton-Raphson-type algorithm is proposed to obtain the estimator, leading to a corrected likelihood estimator. To further enhance computational efficiency, we introduce a corrected least squares estimator based on the idea of bias correction. These two estimation methods ensure both data security and the attainment of statistically valid estimators. Theoretical analysis of both estimators is carefully conducted, and statistical inference methods are discussed. The finite sample performances of different methods are demonstrated through extensive simulations and the analysis of a real dataset.",
        "references": [
            {
                "arxivId": "0706.1062",
                "title": "Power-Law Distributions in Empirical Data",
                "abstract": "Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution\u2014the part of the distribution representing large but rare events\u2014and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov (KS) statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data, while in others the power law is ruled out."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2403.16844",
        "category": "econ",
        "title": "Resistant Inference in Instrumental Variable Models",
        "abstract": "The classical tests in the instrumental variable model can behave arbitrarily if the data is contaminated. For instance, one outlying observation can be enough to change the outcome of a test. We develop a framework to construct testing procedures that are robust to weak instruments, outliers and heavy-tailed errors in the instrumental variable model. The framework is constructed upon M-estimators. By deriving the influence functions of the classical weak instrument robust tests, such as the Anderson-Rubin test, K-test and the conditional likelihood ratio (CLR) test, we prove their unbounded sensitivity to infinitesimal contamination. Therefore, we construct contamination resistant/robust alternatives. In particular, we show how to construct a robust CLR statistic based on Mallows type M-estimators and show that its asymptotic distribution is the same as that of the (classical) CLR statistic. The theoretical results are corroborated by a simulation study. Finally, we revisit three empirical studies affected by outliers and demonstrate how the new robust tests can be used in practice.",
        "references": [
            {
                "arxivId": "2107.00975",
                "title": "A Robust Seemingly Unrelated Regressions For Row-Wise And Cell-Wise Contamination",
                "abstract": "The Seemingly Unrelated Regressions (SUR) model is a wide used estimation procedure in econometrics, insurance and finance, where very often, the regression model contains more than one equation. Unknown parameters, regression coefficients and covariances among the errors terms, are estimated using algorithms based on Generalized Least Squares or Maximum Likelihood, and the method, as a whole, is very sensitive to outliers. To overcome this problem M-estimators and S-estimators are proposed in the literature together with fast algorithms. However, these procedures are only able to cope with rowwise outliers in the error terms, while their performance becomes very poor in the presence of cell-wise outliers and as the number of equations increases. A new robust approach is proposed which is able to perform well under both contamination types as well as it is fast to compute. Illustrations based on Monte Carlo simulations and a real data example are provided."
            },
            {
                "arxivId": "1711.10635",
                "title": "Valid Inference Corrected for Outlier Removal",
                "abstract": "Abstract Ordinary least square (OLS) estimation of a linear regression model is well-known to be highly sensitive to outliers. It is common practice to (1) identify and remove outliers by looking at the data and (2) to fit OLS and form confidence intervals and p-values on the remaining data as if this were the original data collected. This standard \u201cdetect-and-forget\u201d approach has been shown to be problematic, and in this article we highlight the fact that it can lead to invalid inference and show how recently developed tools in selective inference can be used to properly account for outlier detection and removal. Our inferential procedures apply to a general class of outlier removal procedures that includes several of the most commonly used approaches. We conduct simulations to corroborate the theoretical results, and we apply our method to three real datasets to illustrate how our inferential results can differ from the traditional detect-and-forget strategy. A companion R package, outference, implements these new procedures with an interface that matches the functions commonly used for inference with lm in R. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2403.16934",
        "category": "econ",
        "title": "The Costs of Competition in Distributing Scarce Research Funds",
        "abstract": "Research funding systems are not isolated systems - they are embedded in a larger scientific system with an enormous influence on the system. This paper aims to analyze the allocation of competitive research funding from different perspectives: How reliable are decision processes for funding? What are the economic costs of competitive funding? How does competition for funds affect doing risky research? How do competitive funding environments affect scientists themselves, and which ethical issues must be considered? We attempt to identify gaps in our knowledge of research funding systems; we propose recommendations for policymakers and funding agencies, including empirical experiments of decision processes and the collection of data on these processes. With our recommendations we hope to contribute to developing improved ways of organizing research funding.",
        "references": [
            {
                "arxivId": "2212.05418",
                "title": "Is research funding always beneficial? A cross-disciplinary analysis of U.K. research 2014\u201320",
                "abstract": "Abstract Although funding is essential for some types of research and beneficial for others, it may constrain academic choice and creativity. Thus, it is important to check whether it ever seems unnecessary. Here we investigate whether funded U.K. research tends to be higher quality in all fields and for all major research funders. Based on peer review quality scores for 113,877 articles from all fields in the U.K.\u2019s Research Excellence Framework (REF) 2021, we estimate that there are substantial disciplinary differences in the proportion of funded journal articles, from Theology and Religious Studies (16%+) to Biological Sciences (91%+). The results suggest that funded research is likely to be of higher quality overall, for all the largest research funders, and for 30 out of 34 REF Units of Assessment (disciplines or sets of disciplines), even after factoring out research team size. There are differences between funders in the average quality of the research supported, however. Funding seems particularly associated with higher research quality in health-related fields. The results do not show cause and effect and do not take into account the amount of funding received but are consistent with funding either improving research quality or being won by high-quality researchers or projects."
            },
            {
                "arxivId": "1609.04793",
                "title": "Professional and citizen bibliometrics: complementarities and ambivalences in the development and use of indicators\u2014a state-of-the-art report",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-26.json",
        "arxivId": "2403.16980",
        "category": "econ",
        "title": "Economic DAO Governance: A Contestable Control Approach",
        "abstract": "In this article, we propose a new form of DAO governance that uses a sequential auction mechanism to overcome entrenched control issues that have emerged for DAOs by creating a regime of temporary contestable control. The mechanism avoids potential public choice problems inherent in voting approaches but at the same time provides a vehicle that can enhance and secure value than inheres to DAO voting and other DAO non-market governance procedures. It is robust to empty voting and is code feasible. It facilitates the ability of DAOs to meet their normative and operational goals in the face of diverse regulatory approaches. Designed to shift control to the party with the most promising business plan, at the same time it distributes surplus in a way that tends to promote investment by other parties.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2005.11500",
        "category": "econ",
        "title": "Quickest Detection of Ecological Regimes for Natural Resource Management",
        "abstract": null,
        "references": [
            {
                "arxivId": "1506.07134",
                "title": "Catastrophic Regime Shift in Water Reservoirs and S\u00e3o Paulo Water Supply Crisis",
                "abstract": "The relation between rainfall and water accumulated in reservoirs comprises nonlinear feedbacks. Here we show that they may generate alternative equilibrium regimes, one of high water-volume, the other of low water-volume. Reservoirs can be seen as socio-environmental systems at risk of regime shifts, characteristic of tipping point transitions. We analyze data from stored water, rainfall, and water inflow and outflow in the main reservoir serving the metropolitan area of S\u00e3o Paulo, Brazil, by means of indicators of critical regime shifts, and find a strong signal of a transition. We furthermore build a mathematical model that gives a mechanistic view of the dynamics and demonstrates that alternative stable states are an expected property of water reservoirs. We also build a stochastic version of this model that fits well to the data. These results highlight the broader aspect that reservoir management must account for their intrinsic bistability, and should benefit from dynamical systems theory. Our case study illustrates the catastrophic consequences of failing to do so."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2009.00553",
        "category": "econ",
        "title": "A vector monotonicity assumption for multiple instruments",
        "abstract": null,
        "references": [
            {
                "arxivId": "1109.0362",
                "title": "A triangular treatment effect model with random coefficients in the selection equation",
                "abstract": "This paper considers treatment effects under endogeneity with complex heterogeneity in the selection equation. We model the outcome of an endogenous treatment as a triangular system, where both the outcome and first-stage equations consist of a random coefficients model. The first-stage specifically allows for nonmonotone selection into treatment. We provide conditions under which marginal distributions of potential outcomes, average and quantile treatment effects, all conditional on first-stage random coefficients, are identified. Under the same conditions, we derive bounds on the (conditional) joint distributions of potential outcomes and gains from treatment, and provide additional conditions for their point identification. All conditional quantities yield unconditional effects (e.g., the average treatment effect) by weighted integration."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2301.08958",
        "category": "econ",
        "title": "A Practical Introduction to Regression Discontinuity Designs: Extensions",
        "abstract": "This monograph, together with its accompanying first part Cattaneo, Idrobo and Titiunik (2020), collects and expands the instructional materials we prepared for more than $50$ short courses and workshops on Regression Discontinuity (RD) methodology that we taught between 2014 and 2023. In this second monograph, we discuss several topics in RD methodology that build on and extend the analysis of RD designs introduced in Cattaneo, Idrobo and Titiunik (2020). Our first goal is to present an alternative RD conceptual framework based on local randomization ideas. This methodological approach can be useful in RD designs with discretely-valued scores, and can also be used more broadly as a complement to the continuity-based approach in other settings. Then, employing both continuity-based and local randomization approaches, we extend the canonical Sharp RD design in multiple directions: fuzzy RD designs, RD designs with discrete scores, and multi-dimensional RD designs. The goal of our two-part monograph is purposely practical and hence we focus on the empirical analysis of RD designs.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2305.16915",
        "category": "econ",
        "title": "When Is Cross Impact Relevant?",
        "abstract": "Trading pressure from one asset can move the price of another, a phenomenon referred to as cross impact. Using tick-by-tick data spanning 5 years for 500 assets listed in the United States, we identify the features that make cross-impact relevant to explain the variance of price returns. We show that price formation occurs endogenously within highly liquid assets. Then, trades in these assets influence the prices of less liquid correlated products, with an impact velocity constrained by their minimum trading frequency. We investigate the implications of such a multidimensional price formation mechanism on interest rate markets. We find that the 10-year bond future serves as the primary liquidity reservoir, influencing the prices of cash bonds and futures contracts within the interest rate curve. Such behaviour challenges the validity of the theory in Financial Economics that regards long-term rates as agents anticipations of future short term rates.",
        "references": [
            {
                "arxivId": "2112.13213",
                "title": "Cross-impact of order flow imbalance in equity markets",
                "abstract": "We investigate the impact of order flow imbalance (OFI) on price movements in equity markets in a multi-asset setting. First, we propose a systematic approach for combining OFIs at the top levels of the limit order book into an integrated OFI variable which better explains price impact, compared to the best-level OFI. We show that once the information from multiple levels is integrated into OFI, multi-asset models with cross-impact do not provide additional explanatory power for contemporaneous impact compared to a sparse model without cross-impact terms. On the other hand, we show that lagged cross-asset OFIs do improve the forecasting of future returns. We also establish that this lagged cross-impact mainly manifests at short-term horizons and decays rapidly in time."
            },
            {
                "arxivId": "2107.08684",
                "title": "A characterisation of cross-impact kernels",
                "abstract": "Trading a financial asset pushes its price as well as the prices of other assets, a phenomenon known as cross-impact. We consider a general class of kernel-based cross-impact models and investigate suitable parametrisations for trading purposes. We focus on kernels that guarantee that prices are martingales and anticipate future order flow (martingale-admissible kernels) and those that ensure there is no possible price manipulation (no-statistical-arbitrage-admissible kernels). We determine the overlap between these two classes and provide formulas for calibration of cross-impact kernels on data. We illustrate our results using SP500 futures data."
            },
            {
                "arxivId": "2102.02834",
                "title": "Cross Impact in Derivative Markets",
                "abstract": "We introduce a linear cross-impact framework in a setting in which the price of some given financial instruments (derivatives) is a deterministic function of one or more, possibly tradeable, stochastic factors (underlying). We show that a particular cross-impact model, the multivariate Kyle model, prevents arbitrage and aggregates (potentially non-stationary) traded order flows on derivatives into (roughly stationary) liquidity pools aggregating order flows traded on both derivatives and underlying. Using E-Mini futures and options along with VIX futures, we provide empirical evidence that the price formation process from order flows on derivatives is driven by cross-impact and confirm that the simple Kyle cross-impact model is successful at capturing parsimoniously such empirical phenomenology. Our framework may be used in practice for estimating execution costs, in particular hedging costs."
            },
            {
                "arxivId": "2011.10113",
                "title": "Price impact on term structure",
                "abstract": "We introduce a first theory of price impact in the presence of an interest rate term structure. We explain how one can formulate instantaneous and transient price impact on zero-coupon bonds with different maturities, including a cross price impact that is endogenous to the term structure. We connect the introduced impact to classic no-arbitrage theory for interest rate markets, showing that impact can be embedded in the pricing measure and that no arbitrage can be preserved. We extend the price impact setup to coupon-bearing bonds and further show how to implement price impact in an HJM framework. We present pricing examples in the presence of price impact and numerical examples of how impact changes the shape of the term structure. Finally, we show that our approach is applicable by solving an optimal execution problem in interest rate markets with the type of price impact we developed in the paper."
            },
            {
                "arxivId": "2004.01624",
                "title": "How to build a cross-impact model from first principles: theoretical requirements and empirical results",
                "abstract": "Which models of the cross-impacts of trading financial instruments make sense?"
            },
            {
                "arxivId": "1901.00834",
                "title": "The market nanostructure origin of asset price time reversal asymmetry",
                "abstract": "We introduce a method to infer lead-lag networks between the states of elements of complex systems, determined at different timescales. As such networks encode a causal structure of a system, inferring lead-lag networks for many pairs of timescales provides a global picture of the mutual influence between timescales. We apply our method to two trader-resolved FX data sets and document a strong and complex asymmetric influence of timescales on the structure of lead-lag networks. This asymmetry extends to the propagation of trader activity between timescales. For both retail and institutional traders, we find that historical activity over longer timescales has a greater correlation with future activity over shorter timescales (Zumbach effect), for sufficiently large timescales both in the past and future (about one hour for retail traders and two hours for institutional traders); remarkably the effect is opposite for smaller timescales, and much weaker for retail traders."
            },
            {
                "arxivId": "1806.07791",
                "title": "The Multivariate Kyle Model: More is Different",
                "abstract": "We reconsider the multivariate Kyle model in a risk-neutral setting with a single, perfectly informed rational insider and a rational competitive market maker, setting the price of n correlated securities. We prove the unicity of a symmetric, positive definite solution for the impact matrix and provide insights on its interpretation. We explore its implications from the perspective of empirical market microstructure, and argue that it provides a sensible inference procedure to cure some pathologies encountered in recent attempts to calibrate cross-impact matrices. As an illustration, we determine the empirical cross impact matrix of US. Treasuries, and compare the results with recent alternative calibration methods."
            },
            {
                "arxivId": "1708.02411",
                "title": "Nonlinear price impact from linear models",
                "abstract": "The impact of trades on asset prices is a crucial aspect of market dynamics for academics, regulators, and practitioners alike. Recently, universal and highly nonlinear master curves were observed for price impacts aggregated on all intra-day scales (Patzelt and Bouchaud 2017 arXiv:1706.04163). Here we investigate how well these curves, their scaling, and the underlying return dynamics are captured by linear \u2018propagator\u2019 models. We find that the classification of trades as price-changing versus non-price-changing can explain the price impact nonlinearities and short-term return dynamics to a very high degree. The explanatory power provided by the change indicator in addition to the order sign history increases with increasing tick size. To obtain these results, several long-standing technical issues for model calibration and testing are addressed. We present new spectral estimators for two- and three-point cross-correlations, removing the need for previously used approximations. We also show when calibration is unbiased and how to accurately reveal previously overlooked biases. Therefore, our results contribute significantly to understanding both recent empirical results and the properties of a popular class of impact models."
            },
            {
                "arxivId": "1705.00672",
                "title": "Portfolio choice with small temporary and transient price impact",
                "abstract": "We study portfolio selection in a model with both temporary and transient price impact introduced by Garleanu and Pedersen. In the large\u2010liquidity limit where both frictions are small, we derive explicit formulas for the asymptotically optimal trading rate and the corresponding minimal leading\u2010order performance loss. We find that the losses are governed by the volatility of the frictionless target strategy, like in models with only temporary price impact. In contrast, the corresponding optimal portfolio not only tracks the frictionless optimizer, but also exploits the displacement of the market price from its unaffected level."
            },
            {
                "arxivId": "1701.03098",
                "title": "Trading Strategies for Stock Pairs Regarding to the Cross-Impact Cost",
                "abstract": "We extend the framework of trading strategies of Gatheral [2010] from single stocks to a pair of stocks. Our trading strategy with the executions of two round-trip trades can be described by the trading rates of the paired stocks and the ratio of their trading periods. By minimizing the potential cost arising from cross-impacts, i.e., the price change of one stock due to the trades of another stock, we can find out an optimal strategy for executing a sequence of trades from different stocks. We further apply the model of the strategy to a specific case, where we quantify the cross-impacts of traded volumes and of time lag with empirical data for the computation of costs. We thus picture the influence of cross-impacts on the trading strategy."
            },
            {
                "arxivId": "1612.07742",
                "title": "Cross-impact and no-dynamic-arbitrage",
                "abstract": "We extend the \u2018No-dynamic-arbitrage and market impact\u2019-framework of Gatheral [Quant. Finance, 2010, 10(7), 749\u2013759] to the multi-dimensional case where trading in one asset has a cross-impact on the price of other assets. From the condition of absence of dynamical arbitrage we derive theoretical limits for the size and form of cross-impact that can be directly verified on data. For bounded decay kernels we find that cross-impact must be an odd and linear function of trading intensity and cross-impact from asset i to asset j must be equal to the one from j to i. To test these constraints we estimate cross-impact among sovereign bonds traded on the electronic platform MOT. While we find significant violations of the above symmetry condition of cross-impact, we show that these are not arbitrageable with simple strategies because of the presence of the bid-ask spread."
            },
            {
                "arxivId": "1609.02395",
                "title": "Dissecting cross-impact on stock markets: an empirical analysis",
                "abstract": "The vast majority of market impact studies assess each product individually, and the interactions between the different order flows are disregarded. This strong approximation may lead to an underestimation of trading costs and possible contagion effects. Transactions in fact mediate a significant part of the correlation between different instruments. In turn, liquidity shares the sectorial structure of market correlations, which can be encoded as a set of eigenvalues and eigenvectors. We introduce a multivariate linear propagator model that successfully describes such a structure, and accounts for a significant fraction of the covariance of stock returns. We dissect the various dynamical mechanisms that contribute to the joint dynamics of assets. We also define two simplified models with substantially less parameters in order to reduce overfitting, and show that they have superior out-of-sample performance."
            },
            {
                "arxivId": "1310.4471",
                "title": "Multivariate Transient Price Impact and Matrix-Valued Positive Definite Functions",
                "abstract": "We consider a model for linear transient price impact for multiple assets that takes cross-asset impact into account. Our main goal is to single out properties that need to be imposed on the decay kernel so that the model admits well-behaved optimal trade execution strategies. We first show that the existence of such strategies is guaranteed by assuming that the decay kernel corresponds to a matrix-valued positive definite function. An example illustrates, however, that positive definiteness alone does not guarantee that optimal strategies are well-behaved. Building on previous results from the one-dimensional case, we investigate a class of nonincreasing, nonnegative and convex decay kernels with values in the symmetric $K\\times K$ matrices. We show that these decay kernels are always positive definite and characterize when they are even strictly positive definite, a result that may be of independent interest. Optimal strategies for kernels from this class are well-behaved when one requires that the decay kernel is also commuting. We show how such decay kernels can be constructed by means of matrix functions and provide a number of examples. In particular we completely solve the case of matrix exponential decay."
            },
            {
                "arxivId": "0708.3198",
                "title": "Universal price impact functions of individual trades in an order-driven market",
                "abstract": "The trade size \u03c9 has a direct impact on the price formation of the stock traded. Econophysical analyses of transaction data for the US and Australian stock markets have uncovered market-specific scaling laws, where a master curve of price impact can be obtained in each market when stock capitalization C is included as an argument in the scaling relation. However, the rationale of introducing stock capitalization in the scaling is unclear and the anomalous negative correlation between price change r and trade size \u03c9 for small trades is unexplained. Here we show that these issues can be addressed by taking into account the aggressiveness of orders that result in trades together with a proper normalization technique. Using order book data from the Chinese market, we show that trades from filled and partially filled limit orders have very different price impacts. The price impact of trades from partially filled orders is constant when the volume is not too large, while that of filled orders shows power-law behavior r\u2009\u223c\u2009\u03c9\u03b1 with \u03b1\u2009\u2248\u20092/3. When returns and volumes are normalized by stock-dependent averages, capitalization-independent scaling laws emerge for both types of trades. However, no scaling relation in terms of stock capitalization can be constructed. In addition, the relation \u03b1\u2009=\u2009\u03b1\u03c9/\u03b1 r is verified for some individual stocks and for the whole data set containing all stocks using partially filled trades, where \u03b1\u03c9 and \u03b1 r are the tail exponents of trade sizes and returns. These observations also enable us to explain the anomalous negative correlation between r and \u03c9 for small-size trades."
            },
            {
                "arxivId": "0704.1099",
                "title": "The Epps effect revisited",
                "abstract": "We analyse the dependence of stock return cross-correlations on the data sampling frequency, known as the Epps effect: for high-resolution data the cross-correlations are significantly smaller than their asymptotic value as observed for daily data. The former description implies that a changing trading frequency should alter the characteristic time of the phenomenon. This is not true for empirical data: the Epps curves do not scale with market activity. The latter result indicates that the time scale of the phenomenon is related to the reaction time of market participants (this we denote as the human time scale), independent of market activity. In this paper we give a new description of the Epps effect through the decomposition of cross-correlations. After testing our method on a model of generated random walk price changes we justify our analytical results by fitting the Epps curves of real-world data."
            },
            {
                "arxivId": "cond-mat/0406224",
                "title": "Random walks, liquidity molasses and critical response in financial markets",
                "abstract": "Stock prices are observed to be random walks in time despite a strong, long-term memory in the signs of trades (buys or sells). Lillo and Farmer have recently suggested that these correlations are compensated by opposite long-ranged fluctuations in liquidity, with an otherwise permanent market impact, challenging the scenario proposed in Quantitative Finance, 2004, 4, 176, where the impact is instead transient, with a power-law decay in time. The exponent of this decay is precisely tuned to a critical value, ensuring simultaneously that prices are diffusive on long time scales and that the impact function is nearly lag independent. We provide new analysis of empirical data that confirm and make more precise our previous claims. We show that the power-law decay of the bare impact function comes both from an excess flow of limit order opposite to the market order flow, and to a systematic anti-correlation of the bid\u2013ask motion between trades, two effects that create a \u2018liquidity molasses\u2019 which dampens market volatility."
            },
            {
                "arxivId": "cond-mat/0311053",
                "title": "The Long Memory of the Efficient Market",
                "abstract": "For the London Stock Exchange we demonstrate that the signs of orders obey a long-memory process. The autocorrelation function decays roughly as a power law with an exponent of 0.6, corresponding to a Hurst exponent H = 0.7. This implies that the signs of future orders are quite predictable from the signs of past orders; all else being equal, this would suggest a very strong market inefficiency. We demonstrate, however, that fluctuations in order signs are compensated for by anti-correlated fluctuations in transaction size and liquidity, which are also long-memory processes that act to make the returns whiter. We show that some institutions display long-range memory and others don\u0092t."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2306.02987",
        "category": "econ",
        "title": "Frequency Regulation with Storage: On Losses and Profits",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2311.16570",
        "category": "econ",
        "title": "Epistemic Limits of Empirical Finance: Causal Reductionism and Self-Reference",
        "abstract": "The clarion call for causal reduction in the study of capital markets is intensifying. However, in self-referencing and open systems such as capital markets, the idea of unidirectional causation (if applicable) may be limiting at best, and unstable or fallacious at worst. In this research, we critically assess the use of scientific deduction and causal inference within the study of empirical finance and econometrics. We then demonstrate the idea of competing causal chains using a toy model adapted from ecological predator/prey relationships. From this, we develop the alternative view that the study of empirical finance, and the risks contained therein, may be better appreciated once we admit that our current arsenal of quantitative finance tools may be limited to ex post causal inference under popular assumptions. Where these assumptions are challenged, for example in a recognizable reflexive context, the prescription of unidirectional causation proves deeply problematic.",
        "references": [
            {
                "arxivId": "1310.4067",
                "title": "On pricing kernels, information and risk",
                "abstract": "This paper compares out-of-sample, ex-ante risk and returns of arbitrage pricing theory (APT) risk-factor based, zero-cost portfolios with characteristic-based, zero-cost portfolios. In particular the Haugen and Baker characteristic-based model framework is used in a comparison with the capital asset pricing model (CAPM) (Haugen, R. A., & Baker, L. N. (1996). Commonality in the determinants of expected stocks returns. Journal of Financial Economics, 41, 401\u2013439), and three-factor Fama and French APT model portfolios to analyse returns of stocks listed on the Johannesburg Stock Exchange (Fama, E., & French, K. (1993). Common risk factors in the returns on stocks and bonds. Journal of Financial Economics, 33, 3\u201356). The finding that cross-sectional characteristic-based models have yielded portfolios with higher excess monthly returns but lower risk than their arbitrage pricing theory counterparts is discussed. Under the assumption of general no arbitrage conditions, it is argued that evidence in favour of characteristic-based pricing provides insight into the non-linear nature in which information is assimilated into pricing kernels for the market considered."
            },
            {
                "arxivId": "0710.4235",
                "title": "Top-down causation by information control: from a philosophical problem to a scientific research programme",
                "abstract": "It has been claimed that different types of causes must be considered in biological systems, including top-down as well as same-level and bottom-up causation, thus enabling the top levels to be causally efficacious in their own right. To clarify this issue, the important distinctions between information and signs are introduced here and the concepts of information control and functional equivalence classes in those systems are rigorously defined and used to characterize when top-down causation by feedback control happens, in a way that is testable. The causally significant elements we consider are equivalence classes of lower level processes, realized in biological systems through different operations having the same outcome within the context of information control and networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17112",
        "category": "econ",
        "title": "The Impact of Pradhan Mantri Ujjwala Yojana on Indian Households",
        "abstract": "This study critically evaluates the impact of the Pradhan Mantri Ujjwala Yojana (PMUY) on LPG accessibility among poor households in India. Using Propensity Score Matching and Difference-in-Differences estimators and the National Family Health Survey (NFHS) dataset, the Average Treatment Effect on the interdedly Treated is a modest 2.1 percentage point increase in LPG consumption due to PMUY, with a parallel decrease in firewood consumption. Regional analysis reveals differential impacts, with significant progress in the North, West, and South but less pronounced effects in the East and North East. The study also underscores variance across social groups, with Schedule Caste households showing the most substantial benefits, while Scheduled Tribes households are hardly affected. Despite the PMUY's initial success in facilitating LPG access, sustaining its usage remains challenging. Policy should emphasise targeted interventions, income support, and address regional and community-specific disparities for the sustained usage of LPG.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17162",
        "category": "econ",
        "title": "Design Insights for Industrial CO2 Capture, Transport, and Storage Systems",
        "abstract": "We present design methods and insights for CO2 capture, transport, and storage systems for clusters of industrial facilities, with a case-study focus on the state of Louisiana. Our analytical framework includes: (1) evaluating the scale and concentration of capturable CO2 emissions at individual facilities for the purpose of estimating the cost of CO2 capture retrofits, (2) a screening method to identify potential CO2 storage sites and estimate their storage capacities, injectivities, and costs; and (3) an approach for cost-minimized design of pipeline infrastructure connecting CO2 capture plants with storage sites that considers land use patterns, existing rights-of-way, demographics, and a variety of social and environmental justice factors. In applying our framework to Louisiana, we estimate up to 50 million tCO2/y of industrial emissions (out of today's total emissions of 130 MtCO2/y) can be captured at under 100 USD/tCO2, and up to 100 MtCO2/y at under 120 USD/tCO2. We identified 98 potential storage sites with estimated aggregate total injectivity between 330 and 730 MtCO2/yr and storage costs ranging from 8 to 17 USD/tCO2. We find dramatic reductions in the aggregate pipeline length and CO2 transport cost per tonne when groups of capture plants share pipeline infrastructure rather than build dedicated single-user pipelines. Smaller facilities (emitting less than 1 MtCO2/y), which account for a quarter of Louisiana's industrial emissions, see the largest transport cost benefits from sharing of infrastructure. Pipeline routes designed to avoid disadvantaged communities (social and environmental justice) so as not to reinforce historical practices of disenfranchisement involve only modestly higher pipeline lengths and costs.",
        "references": [
            {
                "arxivId": "2005.13688",
                "title": "Great SCOT! Rapid tool for carbon sequestration science, engineering, and economics",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17515",
        "category": "econ",
        "title": "Prediction-sharing During Training and Inference",
        "abstract": "Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data.",
        "references": [
            {
                "arxivId": "2211.02091",
                "title": "Fairness in Federated Learning via Core-Stability",
                "abstract": "Federated learning provides an effective paradigm to jointly optimize a model benefited from rich distributed data while protecting data privacy. Nonetheless, the heterogeneity nature of distributed data makes it challenging to define and ensure fairness among local agents. For instance, it is intuitively\"unfair\"for agents with data of high quality to sacrifice their performance due to other agents with low quality data. Currently popular egalitarian and weighted equity-based fairness measures suffer from the aforementioned pitfall. In this work, we aim to formally represent this problem and address these fairness issues using concepts from co-operative game theory and social choice theory. We model the task of learning a shared predictor in the federated setting as a fair public decision making problem, and then define the notion of core-stable fairness: Given $N$ agents, there is no subset of agents $S$ that can benefit significantly by forming a coalition among themselves based on their utilities $U_N$ and $U_S$ (i.e., $\\frac{|S|}{N} U_S \\geq U_N$). Core-stable predictors are robust to low quality local data from some agents, and additionally they satisfy Proportionality and Pareto-optimality, two well sought-after fairness and efficiency notions within social choice. We then propose an efficient federated learning protocol CoreFed to optimize a core stable predictor. CoreFed determines a core-stable predictor when the loss functions of the agents are convex. CoreFed also determines approximate core-stable predictors when the loss functions are not convex, like smooth neural networks. We further show the existence of core-stable predictors in more general settings using Kakutani's fixed point theorem. Finally, we empirically validate our analysis on two real-world datasets, and we show that CoreFed achieves higher core-stability fairness than FedAvg while having similar accuracy."
            },
            {
                "arxivId": "2205.11295",
                "title": "Pareto-Improving Data-Sharing\u2731",
                "abstract": "We study the effects of data sharing between firms on prices, profits, and consumer welfare. Although indiscriminate sharing of consumer data decreases firm profits due to the subsequent increase in competition, selective sharing can be beneficial. We show that there are data-sharing mechanisms that are strictly Pareto-improving, simultaneously increasing firm profits and consumer welfare. Within the class of Pareto-improving mechanisms, we identify one that maximizes firm profits and one that maximizes consumer welfare."
            },
            {
                "arxivId": "2201.09137",
                "title": "Long-term Data Sharing under Exclusivity Attacks",
                "abstract": "The quality of learning generally improves with the scale and diversity of data. Companies and institutions can therefore benefit from building models over shared data. Many cloud and blockchain platforms, as well as government initiatives, are interested in providing this type of service. These cooperative efforts face a challenge, which we call \"exclusivity attacks\". A firm can share distorted data, so that it learns the best model fit, but is also able to mislead others. We study protocols for long-term interactions and their vulnerability to these attacks, in particular for regression and clustering tasks. We find that the choice of communication protocol is essential for vulnerability: The protocol is much more vulnerable if firms can continuously initiate communication, instead of periodically asked for their inputs. Vulnerability may also depend on the number of Sybil identities a firm can control."
            },
            {
                "arxivId": "2010.00753",
                "title": "Model-sharing Games: Analyzing Federated Learning Under Voluntary Participation",
                "abstract": "Federated learning is a setting where agents, each with access to their own data source, combine models learned from local data to create a global model. If agents are drawing their data from different distributions, though, federated learning might produce a biased global model that is not optimal for each agent. This means that agents face\u00a0a fundamental question: should they join the global model or stay with their local model? In this work, we show how this situation can be naturally analyzed through the framework of coalitional game theory.\u00a0\n\nMotivated by these considerations, we propose the following game:\u00a0there are heterogeneous players with\u00a0different model parameters\u00a0governing their data distribution and different amounts of data they have noisily drawn from their own distribution. Each player's goal is to obtain a model with minimal expected mean squared error (MSE) on their own distribution. They have a choice of fitting a model based solely on their own data, or combining their learned parameters with those of some subset of the other players. Combining models reduces the variance component of their error through access to more data, but increases the bias because of the heterogeneity of distributions. In this work, we derive exact expected MSE values for problems in linear regression and mean estimation. We use these values to analyze the resulting game in the framework of hedonic game theory; we study how players might divide into coalitions, where each set of players within a coalition jointly constructs a single model.\u00a0 In a case with arbitrarily many players that each have either a \"small\" or \"large\" amount of data, we constructively show that there always exists a stable partition of players into coalitions.\u00a0"
            },
            {
                "arxivId": "2006.11901",
                "title": "Free-rider Attacks on Model Aggregation in Federated Learning",
                "abstract": "Free-rider attacks on federated learning consist in dissimulating participation to the federated learning process with the goal of obtaining the final aggregated model without actually contributing with any data. We introduce here the first theoretical and experimental analysis of free-rider attacks on federated learning schemes based on iterative parameters aggregation, such as FedAvg or FedProx, and provide formal guarantees for these attacks to converge to the aggregated models of the fair participants. We first show that a straightforward implementation of this attack can be simply achieved by not updating the local parameters during the iterative federated optimization. As this attack can be detected by adopting simple countermeasures at the server level, we subsequently study more complex disguising schemes based on stochastic updates of the free-rider parameters. We demonstrate the proposed strategies on a number of experimental scenarios, in both iid and non-iid settings. We conclude by providing recommendations to avoid free-rider attacks in real world applications of federated learning, especially in sensitive domains where security of data and models is critical."
            },
            {
                "arxivId": "2005.10038",
                "title": "Coopetition Against an Amazon",
                "abstract": null
            },
            {
                "arxivId": "1909.03618",
                "title": "Bias-Variance Games",
                "abstract": "Firms engaged in electronic commerce increasingly rely on predictive analytics via machine-learning algorithms to drive a wide array of managerial decisions. The tuning of many standard machine learning algorithms can be understood as trading off bias (i.e., accuracy) with variance (i.e., precision) in the algorithm's predictions. The goal of this paper is to understand how competition between firms affects their strategic choice of such algorithms. To this end, we model the interaction of two firms choosing learning algorithms as a game and analyze its equilibria. Absent competition, players care only about the magnitude of predictive error and not its source. In contrast, our main result is that with competition, players prefer to incur error due to variance rather than due to bias, even at the cost of higher total error. In addition, we show that competition can have counterintuitive implications---for example, reducing the error incurred by a firm's algorithm can be harmful to that firm---but we provide conditions under which such phenomena do not occur. In addition to our theoretical analysis, we also validate our insights by applying our metrics to a publicly available data set."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17546",
        "category": "econ",
        "title": "Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining",
        "abstract": "The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools of text mining and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, text mining techniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances. Finally, topic modeling is used to examine key components and their relative significance in job descriptions. By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals. This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17605",
        "category": "econ",
        "title": "On the Pettis Integral Approach to Large Population Games",
        "abstract": "The analysis of large population economies with incomplete information often entails the integration of a continuum of random variables. We showcase the usefulness of the integral notion \\`a la Pettis (1938) to study such models. We present several results on Pettis integrals, including convenient sufficient conditions for Pettis integrability and Fubini-like exchangeability formulae, illustrated through a running example. Building on these foundations, we conduct a unified analysis of Bayesian games with arbitrarily many heterogeneous agents. We provide a sufficient condition on payoff structures, under which the equilibrium uniqueness is guaranteed across all signal structures. Our condition is parsimonious, as it turns out necessary when strategic interactions are undirected. We further identify the moment restrictions, imposed on the equilibrium action-state joint distribution, which have crucial implications for information designer's problem of persuading a population of strategically interacting agents. To attain these results, we introduce and develop novel mathematical tools, built on the theory of integral kernels and reproducing kernel Hilbert spaces in functional analysis.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17624",
        "category": "econ",
        "title": "The Inclusive Synthetic Control Method",
        "abstract": "The Synthetic Control Method (SCM) estimates the causal effect of a policy intervention in a panel data setting with only a few treated units and control units. The treated outcome in the absence of the intervention is recovered by a weighted average of the control units. The latter cannot be affected by the intervention, neither directly nor indirectly. We introduce the inclusive synthetic control method (iSCM), a novel and intuitive synthetic control modification that allows including units potentially affected directly or indirectly by an intervention in the donor pool. Our method is well suited for applications with multiple treated units where including treated units in the donor pool substantially improves the pre-intervention fit and/or for applications where some of the units in the donor pool might be affected by spillover effects. Our iSCM is very easy to implement, and any synthetic control type estimation and inference procedure can be used. Finally, as an illustrative empirical example, we re-estimate the causal effect of German reunification on GDP per capita allowing for spillover effects from West Germany to Austria.",
        "references": [
            {
                "arxivId": "2211.12095",
                "title": "Asymptotic Properties of the Synthetic Control Method",
                "abstract": "This paper provides new insights into the asymptotic properties of the synthetic control method (SCM). We show that the synthetic control (SC) weight converges to a limiting weight that minimizes the mean squared prediction risk of the treatment-effect estimator when the number of pretreatment periods goes to infinity, and we also quantify the rate of convergence. Observing the link between the SCM and model averaging, we further establish the asymptotic optimality of the SC estimator under imperfect pretreatment fit, in the sense that it achieves the lowest possible squared prediction error among all possible treatment effect estimators that are based on an average of control units, such as matching, inverse probability weighting and difference-in-differences. The asymptotic optimality holds regardless of whether the number of control units is fixed or divergent. Thus, our results provide justifications for the SCM in a wide range of applications. The theoretical results are verified via simulations. JEL classification: C13, C21, C23"
            },
            {
                "arxivId": "1912.03290",
                "title": "Synthetic Controls and Weighted Event Studies with Staggered Adoption",
                "abstract": "Staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. The synthetic control method (SCM) is a recent addition to the evaluation toolkit but is designed to study a single treated unit and does not easily accommodate staggered adoption. In this paper, we generalize SCM to the staggered adoption setting. Current practice involves fitting SCM separately for each treated unit and then averaging. We show that the average of separate SCM fits does not necessarily achieve good balance for the average of the treated units, leading to possible bias in the estimated effect. We propose \"partially pooled\" SCM weights that instead minimize both average and state-specific imbalance, and show that the resulting estimator controls bias under a linear factor model. We also combine our partially pooled SCM weights with traditional fixed effects methods to obtain an augmented estimator that improves over both SCM weighting and fixed effects estimation alone. We assess the performance of the proposed method via extensive simulations and apply our results to the question of whether teacher collective bargaining leads to higher school spending, finding minimal impacts. We implement the proposed method in the augsynth R package."
            },
            {
                "arxivId": "1902.07343",
                "title": "Estimation and Inference for Synthetic Control Methods with Spillover Effects",
                "abstract": "The synthetic control method is often used in treatment effect estimation with panel data where only a few units are treated and a small number of post-treatment periods are available. Current estimation and inference procedures for synthetic control methods do not allow for the existence of spillover effects, which are plausible in many applications. In this paper, we consider estimation and inference for synthetic control methods, allowing for spillover effects. We propose estimators for both direct treatment effects and spillover effects and show they are asymptotically unbiased. In addition, we propose an inferential procedure and show it is asymptotically unbiased. Our estimation and inference procedure applies to cases with multiple treated units or periods, and where the underlying factor model is either stationary or cointegrated. In simulations, we confirm that the presence of spillovers renders current methods biased and have distorted sizes, whereas our methods yield properly sized tests and retain reasonable power. We apply our method to a classic empirical example that investigates the effect of California's tobacco control program as in Abadie et al. (2010) and find evidence of spillovers."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1811.04170",
                "title": "The Augmented Synthetic Control Method",
                "abstract": "Abstract The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The \u201csynthetic control\u201d is a weighted average of control units that balances the treated unit\u2019s pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package."
            },
            {
                "arxivId": "1712.09089",
                "title": "An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls",
                "abstract": "Abstract We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available."
            },
            {
                "arxivId": "1711.06940",
                "title": "Robust Synthetic Control",
                "abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. To begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. We provide the first finite sample analysis for a broader class of models, the Latent Variable Model, in contrast to Factor Models previously considered in the literature. Further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same setting, we prove that the mean-squared-error (MSE) in our prediction estimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the noise variance. Using a data aggregation method, we show that the MSE can be made as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to a consistent estimator. We also introduce a Bayesian framework to quantify the model uncertainty through posterior probabilities. Our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method."
            },
            {
                "arxivId": "1711.02745",
                "title": "Identification and Estimation of Spillover Effects in Randomized Experiments",
                "abstract": "I study identification, estimation and inference for spillover effects in experiments where units' outcomes may depend on the treatment assignments of other units within a group. I show that the commonly-used linear-in-means (LIM) regression identifies a weighted sum of spillover effects with some negative weights, and that the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. I propose nonparametric estimators for average direct and spillover effects that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. These findings are illustrated using data from a conditional cash transfer program and with simulations. The empirical results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation: the estimated difference in means and the LIM coefficients are all close to zero and statistically insignificant, whereas the nonparametric estimators I propose reveal large, nonlinear and significant spillover effects."
            },
            {
                "arxivId": "1710.10251",
                "title": "Matrix Completion Methods for Causal Panel Data Models",
                "abstract": "Abstract In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \u201cmissing\u201d elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data."
            },
            {
                "arxivId": "1911.08521",
                "title": "Synthetic controls with imperfect pretreatment fit",
                "abstract": "We analyze the properties of the Synthetic Control (SC) and related estimators when the pre\u2010treatment fit is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre\u2010treatment periods goes to infinity. Still, we show that a demeaned version of the SC method can improve in terms of bias and variance relative to the difference\u2010in\u2010difference estimator. We also derive a specification test for the demeaned SC estimator in this setting with imperfect pre\u2010treatment fit. Given our theoretical results, we provide practical guidance for applied researchers on how to justify the use of such estimators in empirical applications."
            },
            {
                "arxivId": "1610.07748",
                "title": "Balancing, Regression, Difference-in-Differences and Synthetic Control Methods: A Synthesis",
                "abstract": "In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units."
            },
            {
                "arxivId": "1609.06245",
                "title": "Identification and Estimation of Treatment and Interference Effects in Observational Studies on Networks",
                "abstract": "Abstract\u2013Causal inference on a population of units connected through a network often presents technical challenges, including how to account for interference. In the presence of interference, for instance, potential outcomes of a unit depend on their treatment as well as on the treatments of other units, such as their neighbors in the network. In observational studies, a further complication is that the typical unconfoundedness assumption must be extended\u2014say, to include the treatment of neighbors, and individual and neighborhood covariates\u2014to guarantee identification and valid inference. Here, we propose new estimands that define treatment and interference effects. We then derive analytical expressions for the bias of a naive estimator that wrongly assumes away interference. The bias depends on the level of interference but also on the degree of association between individual and neighborhood treatments. We propose an extended unconfoundedness assumption that accounts for interference, and we develop new covariate-adjustment methods that lead to valid estimates of treatment and interference effects in observational studies on networks. Estimation is based on a generalized propensity score that balances individual and neighborhood covariates across units under different levels of individual treatment and of exposure to neighbors\u2019 treatment. We carry out simulations, calibrated using friendship networks and covariates in a nationally representative longitudinal study of adolescents in grades 7\u201312 in the United States, to explore finite-sample performance in different realistic settings. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17777",
        "category": "econ",
        "title": "Deconvolution from two order statistics",
        "abstract": "Economic data are often contaminated by measurement errors and truncated by ranking. This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements. The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity. Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models. We adapt an existing simulated sieve estimator and illustrate its performance in finite samples.",
        "references": [
            {
                "arxivId": "2210.03547",
                "title": "Order Statistics Approaches to Unobserved Heterogeneity in Auctions",
                "abstract": "We establish nonparametric identification of auction models with continuous unobserved heterogeneity using either three consecutive order statistics of bids or two with an instrument. We then propose sieve maximum likelihood estimators for the joint distribution of unobserved heterogeneity and private value, as well as their conditional and marginal distributions. Lastly, we apply our methodology to a novel dataset from judicial auctions in China. Our estimates suggest substantial gains from accounting for unobserved heterogeneity when setting reserve prices. We propose a simple scheme that achieves nearly optimal revenue by using the appraisal value as the reserve price."
            },
            {
                "arxivId": "2205.12917",
                "title": "Identification of Auction Models Using Order Statistics",
                "abstract": "Auction data often fail to record all bids or all relevant factors that shift bidder values. In this paper, we study the identification of auction models with unobserved heterogeneity (UH) using multiple order statistics of bids. Classical measurement error approaches require multiple independent measurements. Order statistics, by definition, are dependent, rendering classical approaches inapplicable. First, we show that models with nonseparable finite UH is identifiable using three consecutive order statistics or two consecutive ones with an instrument. Second, two arbitrary order statistics identify the models if UH provides support variations. Third, models with separable continuous UH are identifiable using two consecutive order statistics under a weak restrictive stochastic dominance condition. Lastly, we apply our methods to U.S. Forest Service timber auctions and find evidence of UH."
            },
            {
                "arxivId": "2106.15035",
                "title": "Empirical Framework for Cournot Oligopoly with Private Information",
                "abstract": "We propose an empirical framework for Cournot oligopoly with private information about costs. First, considering a linear demand with a random intercept, we characterize the Bayesian Cournot-Nash equilibrium and determine its testable implications. Then we establish nonparametric identification of the joint distribution of demand and market-specific technology shock, and then firm-specific cost distributions. Following the identification steps, we propose a likelihood-based estimation method, and for illustration, apply it to the global upstream market for crude oil. We also extend the baseline model to include either conduct parameters, nonlinear demand, or selective entry."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-27.json",
        "arxivId": "2403.17798",
        "category": "econ",
        "title": "Ethical considerations when planning, implementing and releasing health economic model software: a new proposal",
        "abstract": "Most health economic analyses are undertaken with the aid of computers. However, the research ethics of implementing health economic models as software (or computational health economic models (CHEMs)) are poorly understood. We propose that developers and funders of CHEMs should adhere to research ethics principles and pursue the goals of: (i) socially acceptable user requirements and design specifications; (ii) fit for purpose implementations; and (iii) socially beneficial post-release use. We further propose that a transparent (T), reusable (R) and updatable (U) CHEM is suggestive of a project team that has largely met these goals. We propose six criteria for assessing TRU CHEMs: (T1) software files are publicly available; (T2) developer contributions and judgments on appropriate use are easily identified; (R1) programming practices facilitate independent reuse of model components; (R2) licenses permit reuse and derivative works; (U1) maintenance infrastructure is in place; and (U2) releases are systematically retested and deprecated. Few existing CHEMs would meet all TRU criteria. Addressing these limitations will require the development of new and updated good practice guidelines and investments by governments and other research funders in enabling infrastructure and human capital.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2103.14351",
        "category": "econ",
        "title": "A Natural Adaptive Process for Collective Decision-Making",
        "abstract": "Consider an urn filled with balls, each labeled with one of several possible collective decisions. Now, let a random voter draw two balls from the urn and pick her more preferred as the collective decision. Relabel the losing ball with the collective decision, put both balls back into the urn, and repeat. Once in a while, relabel a randomly drawn ball with a random collective decision. We prove that the empirical distribution of collective decisions produced by this process approximates a maximal lottery, a celebrated probabilistic voting rule proposed by Peter C. Fishburn (Rev. Econ. Stud., 51(4), 1984). In fact, the probability that the collective decision in round $n$ is made according to a maximal lottery increases exponentially in $n$. The proposed procedure is more flexible than traditional voting rules and bears strong similarities to natural processes studied in biology, physics, and chemistry as well as algorithms proposed in machine learning.",
        "references": [
            {
                "arxivId": "q-bio/0605042",
                "title": "Coexistence versus extinction in the stochastic cyclic Lotka-Volterra model.",
                "abstract": "Cyclic dominance of species has been identified as a potential mechanism to maintain biodiversity, see, e.g., B. Kerr, M. A. Riley, M. W. Feldman and B. J. M. Bohannan [Nature 418, 171 (2002)] and B. Kirkup and M. A. Riley [Nature 428, 412 (2004)]. Through analytical methods supported by numerical simulations, we address this issue by studying the properties of a paradigmatic non-spatial three-species stochastic system, namely, the \"rock-paper-scissors\" or cyclic Lotka-Volterra model. While the deterministic approach (rate equations) predicts the coexistence of the species resulting in regular (yet neutrally stable) oscillations of the population densities, we demonstrate that fluctuations arising in the system with a finite number of agents drastically alter this picture and are responsible for extinction: After long enough time, two of the three species die out. As main findings we provide analytic estimates and numerical computation of the extinction probability at a given time. We also discuss the implications of our results for a broad class of competing population systems."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2206.01114",
        "category": "econ",
        "title": "Coarse Wage-Setting and Behavioral Firms",
        "abstract": "This paper shows that the bunching of wages at round numbers is partly driven by firm coarse wage-setting. Using data on 280 million new hires from Brazil, I first establish that salaries tend to cluster at round numbers. Then, I show that firms that tend to hire workers at round-numbered salaries are less sophisticated and have worse market outcomes. Next, I develop a wage-posting model in which optimization costs lead to the adoption of coarse rounded wages and provide evidence supporting three model predictions using two research designs. Finally, I examine some consequences of coarse wage-setting for relevant economic outcomes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2210.09426",
        "category": "econ",
        "title": "Party On: The Labor Market Returns to Social Networks in Adolescence",
        "abstract": "We investigate the returns to adolescent friendships on earnings in adulthood using data from the National Longitudinal Study of Adolescent to Adult Health. Because both education and friendships are jointly determined in adolescence, OLS estimates of their returns are likely biased. We implement a novel procedure to obtain bounds on the causal returns to friendships: we assume that the returns to schooling range from 5 to 15% (based on prior literature), and instrument for friendships using similarity in age among peers. Having one more friend in adolescence increases earnings between 7 and 14%, substantially more than OLS estimates would suggest.",
        "references": [
            {
                "arxivId": "1703.04157",
                "title": "Using Aggregated Relational Data to Feasibly Identify Network Structure Without Network Data",
                "abstract": "Social network data are often prohibitively expensive to collect, limiting empirical network research. We propose an inexpensive and feasible strategy for network elicitation using Aggregated Relational Data (ARD): responses to questions of the form \"how many of your links have trait k ?\" Our method uses ARD to recover parameters of a network formation model, which permits sampling from a distribution over node- or graph-level statistics. We replicate the results of two field experiments that used network data and draw similar conclusions with ARD alone."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2402.19380",
        "category": "econ",
        "title": "Not flexible enough? Impacts of electric carsharing on a power sector with variable renewables",
        "abstract": "Electrifying the car fleet is a major strategy for mitigating greenhouse gas emissions in the transport sector. However, electrification alone will not solve all the negative externalities associated with cars. In light of other problems such as street space as well as concerns about the use of mineral resources for battery electric cars, reducing the car fleet size would be beneficial, particularly in cities. Carsharing could offer a way to reconcile current car usage habits with a reduction in the car fleet size. However, it could also reduce the potential of electric cars to align their grid interactions with variable renewable electricity generation. We investigate how electric carsharing may impact the power sector in the future. We combine three open-source quantitative methods, including sequence clustering of car travel diaries, a probabilistic tool to generate synthetic electric vehicle time series, and an optimization model of the power sector. For 2030 scenarios of Germany with a renewable share of at least 80%, we show that switching to electric carsharing only moderately increases power sector costs. In our main setting, carsharing increases yearly power sector costs by less than 100 euros per substituted private electric car. This cost effect is largest under the assumption of bidirectional charging. It is mitigated when other sources of flexibility for the power sector are considered. Carsharing further causes a shift from wind power to solar PV in the optimal capacity mix, and may also trigger additional investments in stationary electricity storage. Overall, we find that shared electric cars still have the potential to be operated largely in line with variable renewable electricity generation. We conclude that electric carsharing is unlikely to cause much damage to the power sector, but could bring various other benefits, which may outweigh power sector cost increases.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2403.15198",
        "category": "econ",
        "title": "On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation",
        "abstract": "We study a family of distance functions on rankings that allow for asymmetric treatments of alternatives and consider the distinct relevance of the top and bottom positions for ordered lists. We provide a full axiomatic characterization of our distance. In doing so, we retrieve new characterizations of existing axioms and show how to effectively weaken them for our purposes. This analysis highlights the generality of our distance as it embeds many (semi)metrics previously proposed in the literature. Subsequently, we show that, notwithstanding its level of generality, our distance is still readily applicable. We apply it to preference aggregation, studying the features of the associated median voting rule. It is shown how the derived preference function satisfies many desirable features in the context of voting rules, ranging from fairness to majority and Pareto-related properties. We show how to compute consensus rankings exactly, and provide generalized Diaconis-Graham inequalities that can be leveraged to obtain approximation algorithms. Finally, we propose some truncation ideas for our distances inspired by Lu and Boutilier (2010). These can be leveraged to devise a Polynomial-Time-Approximation Scheme for the corresponding rank aggregation problem.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2403.18086",
        "category": "econ",
        "title": "Generalizing Better Response Paths and Weakly Acyclic Games",
        "abstract": "Weakly acyclic games generalize potential games and are fundamental to the study of game theoretic control. In this paper, we present a generalization of weakly acyclic games, and we observe its importance in multi-agent learning when agents employ experimental strategy updates in periods where they fail to best respond. While weak acyclicity is defined in terms of path connectivity properties of a game's better response graph, our generalization is defined using a generalized better response graph. We provide sufficient conditions for this notion of generalized weak acyclicity in both two-player games and $n$-player games. To demonstrate that our generalization is not trivial, we provide examples of games admitting a pure Nash equilibrium that are not generalized weakly acyclic. The generalization presented in this work is closely related to the recent theory of satisficing paths, and the counterexamples presented here constitute the first negative results in that theory.",
        "references": [
            {
                "arxivId": "1903.05812",
                "title": "Decentralized Learning for Optimality in Stochastic Dynamic Teams and Games With Local Control and Global State Information",
                "abstract": "Stochastic dynamic teams and games are rich models for decentralized systems and challenging testing grounds for multiagent learning. Previous work that guaranteed team optimality assumed stateless dynamics, or an explicit coordination mechanism, or joint-control sharing. In this article, we present an algorithm with guarantees of convergence to team optimal policies in teams and common interest games. The algorithm is a two-timescale method that uses a variant of Q-learning on the finer timescale to perform policy evaluation while exploring the policy space on the coarser timescale. Agents following this algorithm are \u201cindependent learners\u201d: they use only local controls, local cost realizations, and global state information, without access to controls of other agents. The results presented here are the first, to the best of our knowledge, to give formal guarantees of convergence to team optimality using independent learners in stochastic dynamic teams and common interest games."
            },
            {
                "arxivId": "1705.02424",
                "title": "A Passivity-Based Approach to Nash Equilibrium Seeking Over Networks",
                "abstract": "In this paper, we consider the problem of distributed Nash equilibrium (NE) seeking over networks, a setting in which players have limited local information on the others\u2019 decisions. We start from a continuous-time gradient-play dynamics that converges to an NE under strict monotonicity of the pseudogradient and assumes perfect information. We consider how to modify it in the case of partial, or networked information between players. We propose an augmented gradient-play dynamics with correction, in which players communicate locally only with their neighbors to compute an estimate of the other players\u2019 actions. We derive the new dynamics based on the reformulation as a multiagent coordination problem over an undirected graph. We exploit incremental passivity properties and show that a synchronizing, distributed Laplacian feedback can be designed using relative estimates of the neighbors. Under a strict monotonicity property of the pseudogradient, we show that the augmented gradient-play dynamics converges to consensus on the NE of the game. We further discuss two cases that highlight the tradeoff between properties of the game and the communication graph."
            },
            {
                "arxivId": "1612.04724",
                "title": "On convergence rates of game theoretic reinforcement learning algorithms",
                "abstract": null
            },
            {
                "arxivId": "1110.4412",
                "title": "Aspiration learning in coordination games",
                "abstract": "We consider the problem of distributed convergence to efficient outcomes in coordination games through payoff-based learning dynamics, namely aspiration learning. The proposed learning scheme assumes that players reinforce well performed actions, by successively playing these actions, otherwise they randomize among alternative actions. Our first contribution is the characterization of the asymptotic behavior of the induced Markov chain of the iterated process by an equivalent finite-state Markov chain, which simplifies previously introduced analysis on aspiration learning. We then characterize explicitly the behavior of the proposed aspiration learning in a generalized version of so-called coordination games, an example of which is network formation games. In particular, we show that in coordination games the expected percentage of time that the efficient action profile is played can become arbitrarily large."
            },
            {
                "arxivId": "1108.2092",
                "title": "On the Structure of Weakly Acyclic Games",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-28.json",
        "arxivId": "2403.18185",
        "category": "econ",
        "title": "Learning Optimal Behavior Through Reasoning and Experiences",
        "abstract": "We develop a novel framework of bounded rationality under cognitive frictions that studies learning over optimal behavior through both deliberative reasoning and accumulated experiences. Using both types of information, agents engage in Bayesian non-parametric estimation of the unknown action value function. Reasoning signals are produced internally through mental deliberation, subject to a cognitive cost. Experience signals are the observed utility outcomes at previous actions. Agents' subjective estimation uncertainty, which evolves through information accumulation, modulates the two modes of learning in a state- and history-dependent way. We discuss how the model draws on and bridges conceptual, methodological and empirical insights from both economics and the cognitive sciences literature on reinforcement learning.",
        "references": [
            {
                "arxivId": "1910.01913",
                "title": "If MaxEnt RL is the Answer, What is the Question?",
                "abstract": "Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods."
            },
            {
                "arxivId": "cs/9605103",
                "title": "Reinforcement Learning: A Survey",
                "abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-29.json",
        "arxivId": "2203.09001",
        "category": "econ",
        "title": "Selection and Parallel Trends",
        "abstract": "We study the role of selection into treatment in difference-in-differences (DiD) designs. We derive necessary and sufficient conditions for parallel trends assumptions under general classes of selection mechanisms. These conditions characterize the empirical content of parallel trends. For settings where the necessary conditions are questionable, we propose tools for selection-based sensitivity analysis. We also provide templates for justifying DiD in applications with and without covariates. A reanalysis of the causal effect of NSW training programs demonstrates the usefulness of our selection-based approach to sensitivity analysis.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-29.json",
        "arxivId": "2203.12740",
        "category": "econ",
        "title": "Correcting Attrition Bias using Changes-in-Changes",
        "abstract": "Attrition is a common and potentially important threat to internal validity in treatment effect studies. We extend the changes-in-changes approach to identify the average treatment effect for respondents and the entire study population in the presence of attrition. Our method, which exploits baseline outcome data, can be applied to randomized experiments as well as quasi-experimental difference-in-difference designs. A formal comparison highlights that while widely used corrections typically impose restrictions on whether or how response depends on treatment, our proposed attrition correction exploits restrictions on the outcome model. We further show that the conditions required for our correction can accommodate a broad class of response models that depend on treatment in an arbitrary way. We illustrate the implementation of the proposed corrections in an application to a large-scale randomized experiment.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-29.json",
        "arxivId": "2304.04506",
        "category": "econ",
        "title": "A micro\u2010founded comparison of fiscal policies between indirect and direct job creation",
        "abstract": "The purpose of this paper is to provide a micro\u2010economic foundation for an argument that the direct employment by the government is more desirable than the government purchase of private goods to eliminate unemployment. A general equilibrium model with monopolistic competition is devised, and the effects of policies (government purchase, tax rate operation, and government employment) on macroeconomic variables (consumption, price, and profit) are investigated. It is shown that (1) the government purchase is inflationary in the sense that additional effective demand by the government not only increases private employment but also raises prices; (2) the government employment can achieve full employment without causing a rise in prices.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-29.json",
        "arxivId": "2403.19051",
        "category": "econ",
        "title": "TOWARDS STANDARDIZED REGULATIONS FOR BLOCK CHAIN SMART CONTRACTS: INSIGHTS FROM DELPHI AND SWARA ANALYSIS",
        "abstract": "The rise of digital currency and the public ledger Block Chain has led to the development of a new type of electronic contract known as \"smart contracts.\" For these contracts to be considered valid, they must adhere to traditional contract rules and be concluded without any impediments. Once written, encrypted, and signed, smart contracts are recorded in the Block Chain Ledger, providing transparent and secure record-keeping. Smart contracts offer several benefits, including their ability to execute automatically without requiring human intervention, their provision of public visibility of contract provisions on the Block Chain, their avoidance of financial crimes like Money Laundering, and their prevention of contract abuses. However, disputes arising from smart contracts still\nrequire human intervention, presenting unique challenges in enforcing these contracts, such as evidentiary issues, enforceability of waivers of defenses, and jurisdictional and choice-of-law considerations. Due to the novel nature of smart contracts, there are currently no standardized regulations that apply to them. Countries that have approved them have turned to customary law to legitimize their use. The Delphi method was used to identify critical success factors for applying blockchain transactions in a manufacturing company. Stepwise Weight Assessment Ratio Analysis (SWARA) was then utilized to determine the most influential factors. The proposed methodology was implemented, and results show that the most influential factors for the successful application of blockchain transactions as smart contracts in a manufacturing company are: turnover, the counter argument, vision, components for building, and system outcome quality. Conversely, connections with government entities and subcontractors, and the guarantee of quality have the least influence on successful implementation. These findings can contribute to the development of a legal framework for smart contracts in a manufacturing company.",
        "references": [
            {
                "arxivId": "2308.10090",
                "title": "Minimizing Turns in Watchman Robot Navigation: Strategies and Solutions",
                "abstract": "The Orthogonal Watchman Route Problem (OWRP) entails the search for the shortest path, known as the watchman route, that a robot must follow within a polygonal environment. The primary objective is to ensure that every point in the environment remains visible from at least one point on the route, allowing the robot to survey the entire area in a single, continuous sweep. This research places particular emphasis on reducing the number of turns in the route, as it is crucial for optimizing navigation in watchman routes within the field of robotics. The cost associated with changing direction is of significant importance, especially for specific types of robots. This paper introduces an efficient linear-time algorithm for solving the OWRP under the assumption that the environment is monotone. The findings of this study contribute to the progress of robotic systems by enabling the design of more streamlined patrol robots. These robots are capable of efficiently navigating complex environments while minimizing the number of turns. This advancement enhances their coverage and surveillance capabilities, making them highly effective in various real-world applications."
            },
            {
                "arxivId": "2304.02094",
                "title": "TM-vector: A Novel Forecasting Approach for Market stock movement with a Rich Representation of Twitter and Market data",
                "abstract": "Stock market forecasting has been a challenging part for many analysts and researchers. Trend analysis, statistical techniques, and movement indicators have traditionally been used to predict stock price movements, but text extraction has emerged as a promising method in recent years. The use of neural networks, especially recurrent neural networks, is abundant in the literature. In most studies, the impact of different users was considered equal or ignored, whereas users can have other effects. In the current study, we will introduce TM-vector and then use this vector to train an IndRNN and ultimately model the market users' behaviour. In the proposed model, TM-vector is simultaneously trained with both the extracted Twitter features and market information. Various factors have been used for the effectiveness of the proposed forecasting approach, including the characteristics of each individual user, their impact on each other, and their impact on the market, to predict market direction more accurately. Dow Jones 30 index has been used in current work. The accuracy obtained for predicting daily stock changes of Apple is based on various models, closed to over 95\\% and for the other stocks is significant. Our results indicate the effectiveness of TM-vector in predicting stock market direction."
            },
            {
                "arxivId": "2105.05192",
                "title": "Digital Building Twins and Blockchain for Performance-Based (Smart) Contracts",
                "abstract": null
            },
            {
                "arxivId": "1902.07986",
                "title": "Probabilistic Smart Contracts: Secure Randomness on the Blockchain",
                "abstract": "In today\u2019s programmable blockchains, smart contracts are limited to being deterministic and non-probabilistic. This lack of randomness is a consequential limitation, given that a wide variety of real-world financial contracts, such as casino games and lotteries, depend entirely on randomness. As a result, several ad-hoc random number generation approaches have been developed to be used in smart contracts. These include ideas such as using an oracle or relying on the block hash. However, these approaches are manipulatable, i.e. their output can be tampered with by parties who might not be neutral, such as the owner of the oracle or the miners.We propose a novel game-theoretic approach for generating provably unmanipulatable pseudorandom numbers on the blockchain. Our approach allows smart contracts to access a trustworthy source of randomness that does not rely on potentially compromised miners or oracles, hence enabling the creation of a new generation of smart contracts that are not limited to being non-probabilistic and can be drawn from the much more general class of probabilistic programs."
            },
            {
                "arxivId": "1901.07807",
                "title": "Interacting with the Internet of Things Using Smart Contracts and Blockchain Technologies",
                "abstract": null
            },
            {
                "arxivId": "1808.00093",
                "title": "Implementation of Smart Contracts Using Hybrid Architectures with On and Off\u2013Blockchain Components",
                "abstract": "Decentralised (on-blockchain) and centralised (off\u2013blockchain) platforms are available for the implementation of smart contracts. However, none of the two alternatives can individually provide the services and quality of services (QoS) imposed on smart contracts involved in a large class of applications. The reason is that blockchain platforms suffer from scalability, performance, transaction costs and other limitations. Likewise, off\u2013blockchain platforms are afflicted by drawbacks emerging from their dependence on single trusted third parties. We argue that in several applications, hybrid platforms composed from the integration of on and off\u2013blockchain platforms are more adequate. Developers that informatively choose between the three alternatives are likely to implement smart contracts that deliver the expected QoS. Hybrid architectures are largely unexplored. To help cover the gap and as a proof of concept, in this paper we discuss the implementation of smart contracts on hybrid architectures. We show how a smart contract can be split and executed partially on an off\u2013blockchain contract compliance checker and partially on the rinkeby ethereum network. To test the solution, we expose it to sequences of contractual operations generated mechanically by a contract validator tool."
            },
            {
                "arxivId": "1805.00626",
                "title": "On and Off-Blockchain Enforcement Of Smart Contracts",
                "abstract": null
            },
            {
                "arxivId": "1709.10000",
                "title": "Using Blockchain and smart contracts for secure data provenance management",
                "abstract": "Blockchain technology has evolved from being an immutable ledger of transactions for cryptocurrencies to a programmable interactive the environment for building distributed reliable applications. Although, blockchain technology has been used to address various challenges, to our knowledge none of the previous work focused on using blockchain to develop a secure and immutable scientific data provenance management framework that automatically verifies the provenance records. In this work, we leverage blockchain as a platform to facilitate trustworthy data provenance collection, verification, and management. The developed system utilizes smart contracts and open provenance model (OPM) to record immutable data trails. We show that our proposed framework can efficiently and securely capture and validate provenance data, and prevent any malicious modification to the captured data as long as the majority of the participants are honest."
            },
            {
                "arxivId": "1801.02029",
                "title": "A Perspective on Blockchain Smart Contracts: Reducing Uncertainty and Complexity in Value Exchange",
                "abstract": "The blockchain constitutes a technology-based, rather than social or regulation based, means to lower uncertainty about one another in order to exchange value. However, its use may very well also lead to increased complexity resulting from having to subsume work that displaced intermediary institutions had performed. We present our perspective that smart contracts may be used to mitigate this increased complexity. We further posit that smart contracts can be delineated according to complexity: Smart contracts that can be verified objectively without much uncertainty belong in an inter- organizational context; those that cannot be objectively verified belong in an intra- organizational context. We state that smart contracts that implement a formal (e.g. mathematical or simulation) model are especially beneficial for both contexts: They can be used to express and enforce inter-organizational agreements, and their basis in a common formalism may ensure effective evaluation and comparison between different intra-organizational contracts. Finally, we present a case study of our perspective by describing Intellichain, which implements formal, agent-based simulation model as a smart contract to provide epidemiological decision support."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-03-29.json",
        "arxivId": "2403.19077",
        "category": "econ",
        "title": "Blockchains, MEV and the knapsack problem: a primer",
        "abstract": "In this paper, we take a close look at a problem labeled maximal extractable value (MEV), which arises in a blockchain due to the ability of a block producer to manipulate the order of transactions within a block. Indeed, blockchains such as Ethereum have spent considerable resources addressing this issue and have redesigned the block production process to account for MEV. This paper provides an overview of the MEV problem and tracks how Ethereum has adapted to its presence. A vital aspect of the block building exercise is that it is a variant of the knapsack problem. Consequently, this paper highlights the role of designing auctions to fill a knapsack--or knapsack auctions--in alleviating the MEV problem. Overall, this paper presents a survey of the main issues and an accessible primer for researchers and students wishing to explore the economics of block building and MEV further.",
        "references": [
            {
                "arxivId": "cs/0202017",
                "title": "Truth revelation in approximately efficient combinatorial auctions",
                "abstract": "Some important classical mechanisms considered in Microeconomics and Game Theory require the solution of a difficult optimization problem. This is true of mechanisms for combinatorial auctions, which have in recent years assumed practical importance, and in particular of the gold standard for combinatorial auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these mechanisms---in particular, their truth revelation properties---assumes that the optimization problems are solved precisely. In reality, these optimization problems can usually be solved only in an approximate fashion. We investigate the impact on such mechanisms of replacing exact solutions by approximate ones. Specifically, we look at a particular greedy optimization method. We show that the GVA payment scheme does not provide for a truth revealing mechanism. We introduce another scheme that does guarantee truthfulness for a restricted class of players. We demonstrate the latter property by identifying natural properties for combinatorial auctions and showing that, for our restricted class of players, they imply that truthful strategies are dominant. Those properties have applicability beyond the specific auction studied."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-01.json",
        "arxivId": "2403.19847",
        "category": "econ",
        "title": "Strategic complementarities as stochastic control under sticky price",
        "abstract": "We examine how monetary shocks spread throughout an economic model characterized by sticky prices and general equilibrium, where the pricing strategies of firms are interlinked, fostering a mutually beneficial relationship. In this dynamic equilibrium, pricing choices of firms are influenced by overall economic factors, which are themselves affected by these decisions. We approach this situation using a path integral control method, yielding several important insights. We confirm the presence and uniqueness of the equilibrium and scrutinize the impulse response function (IRF) of output subsequent to a shock affecting the entire economy.",
        "references": [
            {
                "arxivId": "2401.03649",
                "title": "Bayes Factor of Zero Inflated Models under Jeffereys Prior",
                "abstract": "Microbiome omics data including 16S rRNA reveal intriguing dynamic associations between the human microbiome and various disease states. Drastic changes in microbiota can be associated with factors like diet, hormonal cycles, diseases, and medical interventions. Along with the identification of specific bacteria taxa associated with diseases, recent advancements give evidence that metabolism, genetics, and environmental factors can model these microbial effects. However, the current analytic methods for integrating microbiome data are fully developed to address the main challenges of longitudinal metagenomics data, such as high-dimensionality, intra-sample dependence, and zero-inflation of observed counts. Hence, we propose the Bayes factor approach for model selection based on negative binomial, Poisson, zero-inflated negative binomial, and zero-inflated Poisson models with non-informative Jeffreys prior. We find that both in simulation studies and real data analysis, our Bayes factor remarkably outperform traditional Akaike information criterion and Vuong's test. A new R package BFZINBZIP has been introduced to do simulation study and real data analysis to facilitate Bayesian model selection based on the Bayes factor."
            },
            {
                "arxivId": "2311.02113",
                "title": "Path Integral Control in Infectious Disease Modeling (preprint)",
                "abstract": "COVID-19, a global pandemic of unprecedented scale, has had a profound impact on nations worldwide, resulting in the tragic loss of nearly 1.1 million lives in the United States and a staggering 7 million worldwide. In the absence of effective vaccines, governments across the globe resorted to the implementation of lockdown measures as a vital strategy to mitigate the virus's relentless spread. While these restrictions were widely enforced, crucial sectors like public health and safety remained operational, ensuring the continuity of essential services. The timing and stringency of lockdown measures in various U.S. states were intricately tailored to the severity of the outbreak within their respective regions. Lockdowns effectively curtailed social interactions, thereby significantly reducing virus transmission. However, it is essential to strike a balance, as prolonged lockdowns can sow apprehension among the populace, impeding the resumption of normal social activities due to the persistent fear of contracting COVID-19. These prolonged restrictions have reverberated throughout the business landscape, resulting in reduced consumer and employee participation, ultimately denting long-term profitability sustainability. Businesses that lacked the resilience of adequate inventory faced the dire prospect of permanent closure. Regrettably, the absence of substantial government financial support has made business closure an all too common outcome, with the arduous task of revival to former employment levels."
            },
            {
                "arxivId": "2206.04248",
                "title": "On Lock-down Control of a Pandemic Model (preprint)",
                "abstract": "In this paper a Feynman-type path integral control approach is used for a recursive formulation of a health objective function subject to a fatigue dynamics, a forward-looking stochastic multi-risk susceptible-infective-recovered (SIR) model with risk-group's Bayesian opinion dynamics towards vaccination against COVID-19. My main interest lies in solving a minimization of a policy-maker's social cost which depends on some deterministic weight. I obtain an optimal lock-down intensity from a Wick-rotated Schrodinger-type equation which is analogous to a Hamiltonian-Jacobi-Bellman (HJB) equation. My formulation is based on path integral control and dynamic programming tools facilitates the analysis and permits the application of algorithm to obtain numerical solution for pandemic control model. Feynman path integral is a quantization method which uses the quantum Lagrangian function, while Schrodinger's quantization uses the Hamiltonian function. These two methods are believed to be equivalent but, this equivalence has not fully proved mathematically. As the complexity and memory requirements of grid-based partial differential equation (PDE) solvers increase exponentially as the dimension of the system increases, this method becomes impractical in the case with high dimensions. As an alternative path integral control solves a class a stochastic control problems with a Monte Carlo method for a HJB equation and this approach avoids the need of a global grid of the domain of the HJB equation."
            },
            {
                "arxivId": "2108.00845",
                "title": "Scoring a Goal Optimally in a Soccer Game Under Liouville-Like Quantum Gravity Action",
                "abstract": null
            },
            {
                "arxivId": "2107.05183",
                "title": "Consensus as a Nash Equilibrium of a Stochastic Differential Game",
                "abstract": "In this paper a consensus has been constructed in a social network which is modeled by a stochastic differential game played by agents of that network. Each agent independently minimizes a cost function which represents their motives. A conditionally expected integral cost function has been considered under an agent\u2019s opinion filtration. The dynamic cost functional is minimized subject to a stochastic differential opinion dynamics. As opinion dynamics represents an agent\u2019s differences of opinion from the others as well as from their previous opinions, random influences and stubbornness make it more volatile. An agent uses their rate of change of opinion at certain time point as a control input. This turns out to be a non-cooperative stochastic differential game which have a feedback Nash equilibrium. A Feynman-type path integral approach has been used to determine an optimal feedback opinion and control. This is a new approach in this literature. Later in this paper an explicit solution of a feedback Nash equilibrium opinion is determined."
            },
            {
                "arxivId": "2107.02291",
                "title": "Optimal Estimation of Brownian Penalized Regression Coefficients",
                "abstract": "In this paper we introduce a new methodology to determine an optimal coefficient of penalized functional regression. We assume the dependent, independent variables and the regression coefficients are functions of time and error dynamics follow a stochastic differential equation. First we construct our objective function as a time dependent residual sum of square and then minimize it with respect to regression coefficients subject to different error dynamics such as LASSO, group LASSO, fused LASSO and cubic smoothing spline. Then we use Feynman-type path integral approach to determine a Schr\u00a8odinger-type equation which have the entire information of the system. Using first order conditions with respect to these coefficients give us a closed form solution of them."
            },
            {
                "arxivId": "2106.14318",
                "title": "Effects of water currents on fish migration through a Feynman-type path integral approach under $\\sqrt{8/3}$ Liouville-like quantum gravity surfaces",
                "abstract": null
            },
            {
                "arxivId": "2001.11099",
                "title": "Motivation to Run in One-Day Cricket",
                "abstract": "In this paper we introduce a new methodology to determine an optimal coefficient for a positive finite measure of batting average, strike rate, and bowling average of a player in order to get an optimal score of a team under dynamic modeling using a path integral method. We also introduce new run dynamics modeled as a stochastic differential equation in order to incorporate the average weather conditions at the cricket ground, the weather condition on the day of the match including sudden deterioration which leads to a partial or complete stop of the game, total attendance, and home field advantage."
            },
            {
                "arxivId": "1704.06553",
                "title": "Optimal stopping in mean field games, an obstacle problem approach",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-01.json",
        "arxivId": "2403.19945",
        "category": "econ",
        "title": "Optimal Auction Design with Flexible Royalty Payments",
        "abstract": "We study the design of an auction for a license. Each agent has a signal about his future profit from winning the license. If the license is allocated, the winner can be charged a flexible royalty based on the profits he reports. The principal can audit the winner, at a cost, and charge limited penalties. We solve for the auction that maximizes revenue, net auditing costs. In this auction, the winner pays linear royalties up to a cap, beyond which there is no auditing. A more optimistic bidder pays more upfront in exchange for a lower royalty cap.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-01.json",
        "arxivId": "2403.20310",
        "category": "econ",
        "title": "Predicting the impact of e-commerce indices on international trade in Iran and other selected members of the Organization for Economic Co-operation and Development (OECD) by using the artificial intelligence and P-VAR model",
        "abstract": "This study aims at predicting the impact of e-commerce indicators on international trade of the selected OECD countries and Iran, by using the artificial intelligence approach and P-VAR. According to the nature of export, import, GDP, and ICT functions, and the characteristics of nonlinearity, this analysis is performed by using the MPL neural network. The export, import, GDP, and ICT findings were examined with 99 percent accuracy. Using the P-VAR model in the Eviews software, the initial database and predicted data were applied to estimate the impact of e-commerce on international trade. The findings from analyzing the data show that there is a bilateral correlation between e-commerce which means that ICT and international trade affect each other and the Goodness of fit of the studied model is confirmed.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2004.12369",
        "category": "econ",
        "title": "Maximum likelihood estimation of stochastic frontier models with endogeneity",
        "abstract": null,
        "references": [
            {
                "arxivId": "1605.00499",
                "title": "Monte Carlo Confidence Sets for Identified Sets",
                "abstract": "In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows."
            },
            {
                "arxivId": "0911.2093",
                "title": "Statistical applications of the multivariate skew normal distribution",
                "abstract": "Azzalini and Dalla Valle have recently discussed the multivariate skew normal distribution which extends the class of normal distributions by the addition of a shape parameter. The first part of the present paper examines further probabilistic properties of the distribution, with special emphasis on aspects of statistical relevance. Inferential and other statistical issues are discussed in the following part, with applications to some multivariate statistics problems, illustrated by numerical examples. Finally, a further extension is described which introduces a skewing factor of an elliptical density."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2010.09227",
        "category": "econ",
        "title": "Influencing Competition Through Shelf Design",
        "abstract": "Shelf design decisions strongly influence product demand. In particular, placing products in desirable locations increases demand. This primary effect on shelf position is clear, but there is a secondary effect based on the relative positioning of nearby products. Intuitively, products located next to each other are more likely to be compared having positive and negative effects. On the one hand, locations closer to relatively strong products will be undesirable, as these strong products will draw demand from others -- an effect that is stronger for those in close proximity. On the other hand, because strong products tend to attract more traffic, locations closer to them elicit high consumer attention by increased visibility. Modifying the GEV class of models to allow demand to be moderated by competitors' proximity, these two effects emerge naturally. We found that although the competition effect is usually stronger, it is not always the dominating effect. Shelf displays can achieve higher profits by exploiting the relative influence on competition from shelf design to shift demand to higher profitability products. In the paper towel category, we found profitability differences of up to 7\\% and displays with 3\\% higher gross profits over the best shelf design present in our data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2101.00399",
        "category": "econ",
        "title": "The law of large numbers for large stable matchings",
        "abstract": null,
        "references": [
            {
                "arxivId": "2104.02009",
                "title": "Identification and Estimation in Many-to-one Two-sided Matching without Transfers",
                "abstract": "In a setting of many-to-one two-sided matching with non-transferable utilities, e.g., college admissions, we study conditions under which preferences of both sides are identified with data on one single market. Regardless of whether the market is centralized or decentralized, assuming that the observed matching is stable, we show nonparametric identification of preferences of both sides under certain exclusion restrictions. To take our results to the data, we use Monte Carlo simulations to evaluate different estimators, including the ones that are directly constructed from the identification. We find that a parametric Bayesian approach with a Gibbs sampler works well in realistically sized problems. Finally, we illustrate our methodology in decentralized admissions to public and private schools in Chile and conduct a counterfactual analysis of an affirmative action policy."
            },
            {
                "arxivId": "1812.04195",
                "title": "Measuring Diffusion over a Large Network",
                "abstract": "\n This paper introduces a measure of the diffusion of binary outcomes over a large, sparse network, when the diffusion is observed in two time periods. The measure captures the aggregated spillover effect of the state-switches in the initial period on their neighbors\u2019 outcomes in the second period. This paper introduces a causal network that captures the causal connections among the cross-sectional units over the two periods. It shows that when the researcher's observed network contains the causal network as a subgraph, the measure of diffusion is identified as a simple, spatio-temporal dependence measure of observed outcomes. When the observed network does not satisfy this condition, but the spillover effect is nonnegative, the spatio-temporal dependence measure serves as a lower bound for diffusion. Using this, a lower confidence bound for diffusion is proposed and its asymptotic validity is established. The Monte Carlo simulation studies demonstrate the finite sample stability of the inference across a range of network configurations. The paper applies the method to data on Indian villages to measure the diffusion of microfinancing decisions over households\u2019 social networks."
            },
            {
                "arxivId": "1704.02999",
                "title": "Estimating local interactions among many agents who observe their neighbors",
                "abstract": "In various economic environments, people observe other people with whom they strategically interact. We can model such information\u2010sharing relations as an information network, and the strategic interactions as a game on the network. When any two agents in the network are connected either directly or indirectly in a large network, empirical modeling using an equilibrium approach can be cumbersome, since the testable implications from an equilibrium generally involve all the players of the game, whereas a researcher's data set may contain only a fraction of these players in practice. This paper develops a tractable empirical model of linear interactions where each agent, after observing part of his neighbors' types, not knowing the full information network, uses best responses that are linear in his and other players' types that he observes, based on simple beliefs about the other players' strategies. We provide conditions on information networks and beliefs such that the best responses take an explicit form with multiple intuitive features. Furthermore, the best responses reveal how local payoff interdependence among agents is translated into local stochastic dependence of their actions, allowing the econometrician to perform asymptotic inference without having to observe all the players in the game or having to know the precise sampling process."
            },
            {
                "arxivId": "2106.02371",
                "title": "Cupid\u2019s Invisible Hand: Social Surplus and Identification in Matching Models",
                "abstract": "We investigate a model of one-to-one matching with transferable utility when some of the characteristics of the players are unobservable to the analyst. We allow for a wide class of distributions of unobserved heterogeneity, subject only to a separability assumption that generalizes Choo and Siow (2006). We first show that the stable matching maximizes a social gain function that trades off the average surplus due to the observable characteristics and a generalized entropy term that reflects the impact of matching on unobserved characteristics. We use this result to derive simple closed-form formulae that identify the joint surplus in every possible match and the equilibrium utilities of all participants, given any known distribution of unobserved heterogeneity. If transfers are observed, then the pre-transfer utilities of both partners are also identified. We also present a very fast algorithm that computes the optimal matching for any specification of the joint surplus. We conclude by discussing some empirical approaches suggested by these results."
            },
            {
                "arxivId": "1305.6156",
                "title": "Estimating Average Causal Effects Under Interference Between Units",
                "abstract": "This paper presents a randomization-based framework for estimating causal effects under interference between units. The framework integrates three components: (i) an experimental design that defines the probability distribution of treatment assignments, (ii) a mapping that relates experimental treatment assignments to exposures received by units in the experiment, and (iii) estimands that make use of the experiment to answer questions of substantive interest. Using this framework, we develop the case of estimating average unit-level causal effects from a randomized experiment with interference of arbitrary but known form. The resulting estimators are based on inverse probability weighting. We provide randomization-based variance estimators that account for the complex clustering that can occur when interference is present. We also establish consistency and asymptotic normality under local dependence assumptions. We discuss refinements including covariate-adjusted effect estimators and ratio estimation. We illustrate and assess empirical performance with a naturalistic simulation using network data from American high schools."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2103.09164",
        "category": "econ",
        "title": "Screening p-Hackers: Dissemination Noise as Bait",
        "abstract": "We show that adding noise to data before making data public is effective at screening p-hacked findings: spurious explanations of the outcome variable produced by attempting multiple econometric specifications. Noise creates \"baits'' that affect two types of researchers differently. Uninformed p-hackers who engage in data mining with no prior information about the true causal mechanism often fall for baits and report verifiably wrong results when evaluated with the original data. But informed researchers who start with an ex-ante hypothesis about the causal mechanism before seeing any data are minimally affected by noise. We characterize the optimal level of dissemination noise and highlight the relevant trade-offs in a simple theoretical model. Dissemination noise is a tool that statistical agencies (e.g., the US Census Bureau) currently use to protect privacy, and we show this existing practice can be repurposed to improve research credibility. The full paper can be found at https://arxiv.org/abs/2103.09164.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2108.03110",
        "category": "econ",
        "title": "A Pomeranzian Growth Theory of the Great Divergence",
        "abstract": "This study constructs a growth model of the Great Divergence that formalizes Pomeranz's (2000) hypothesis that the relief of land constraints in Europe has caused divergence in economic growth between Europe and China since the 19th century. The model consists of the agricultural and manufacturing sectors. The agricultural sector produces subsistence goods from land, intermediate goods from the manufacturing sector, and labor. The manufacturing sector produces goods from labor, and its productivity grows through the learning-by-doing of full-time manufacturing workers. Households make fertility decisions. In the model, a large exogenous positive shock in land supply causes the transition of the economy from the Malthusian state, in which all workers are engaged in agricultural production and per capita income is constant, to the non-Malthusian state, in which the share of workers engaged in agricultural production gradually decreases and per capita income grows at a roughly constant growth rate. The quantitative predictions of the model provide several insights into the causes of the Great Divergence.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2203.04770",
        "category": "econ",
        "title": "Non-smooth integrability theory",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2212.08709",
        "category": "econ",
        "title": "Structural Complexities of Matching Mechanisms",
        "abstract": "We study various novel complexity measures for two-sided matching mechanisms, applied to the two canonical strategyproof matching mechanisms, Deferred Acceptance (DA) and Top Trading Cycles (TTC). Our metrics are designed to capture the complexity of various structural (rather than computational) concerns, in particular ones of recent interest within economics. We consider a unified, flexible approach to formalizing our questions: Define a protocol or data structure performing some task, and bound the number of bits that it requires. Our main results apply this approach to four questions of general interest; for mechanisms matching applicants to institutions, our questions are: (1) How can one applicant affect the outcome matching? (2) How can one applicant affect another applicant's set of options? (3) How can the outcome matching be represented / communicated? (4) How can the outcome matching be verified? Holistically, our results show that TTC is more complex than DA, formalizing previous intuitions that DA has a simpler structure than TTC. For question (2), our result gives a new combinatorial characterization of which institutions are removed from each applicant's set of options when a new applicant is added in DA; this characterization may be of independent interest. For question (3), our result gives new tight lower bounds proving that the relationship between the matching and the priorities is more complex in TTC than in DA. We nonetheless showcase that this higher complexity of TTC is nuanced: By constructing new tight lower-bound instances and new verification protocols, we prove that DA and TTC are comparable in complexity under questions (1) and (4). This more precisely delineates the ways in which TTC is more complex than DA, and emphasizes that diverse considerations must factor into gauging the complexity of matching mechanisms.",
        "references": [
            {
                "arxivId": "2101.10367",
                "title": "Modeling Assumptions Clash with the Real World: Transparency, Equity, and Community Challenges for Student Assignment Algorithms",
                "abstract": "Across the United States, a growing number of school districts are turning to matching algorithms to assign students to public schools. The designers of these algorithms aimed to promote values such as transparency, equity, and community in the process. However, school districts have encountered practical challenges in their deployment. In fact, San Francisco Unified School District voted to stop using and completely redesign their student assignment algorithm because it was frustrating for families and it was not promoting educational equity in practice. We analyze this system using a Value Sensitive Design approach and find that one reason values are not met in practice is that the system relies on modeling assumptions about families\u2019 priorities, constraints, and goals that clash with the real world. These assumptions overlook the complex barriers to ideal participation that many families face, particularly because of socioeconomic inequalities. We argue that direct, ongoing engagement with stakeholders is central to aligning algorithmic values with real world conditions. In doing so we must broaden how we evaluate algorithms while recognizing the limitations of purely algorithmic solutions in addressing complex socio-political problems."
            },
            {
                "arxivId": "2101.05149",
                "title": "On the Computational Properties of Obviously Strategy-Proof Mechanisms",
                "abstract": "We present a polynomial-time algorithm that determines, given some choice rule, whether there exists an obviously strategy-proof mechanism for that choice rule."
            },
            {
                "arxivId": "2012.14898",
                "title": "Exponential communication separations between notions of selfishness",
                "abstract": "We consider the problem of implementing a fixed social choice function between multiple players (which takes as input a type ti from each player i and outputs an outcome f(t1,\u2026, tn)), in which each player must be incentivized to follow the protocol. In particular, we study the communication requirements of a protocol which: (a) implements f, (b) implements f and computes payments that make it ex-post incentive compatible (EPIC) to follow the protocol, and (c) implements f and computes payments in a way that makes it dominant-strategy incentive compatible (DSIC) to follow the protocol. We show exponential separations between all three of these quantities, already for just two players. That is, we first construct an f such that f can be implemented in communication c, but any EPIC implementation of f (with any choice of payments) requires communication exp(c). This answers an open question of [Fadel and Segal, 2009; Babaioff et. al., 2013]. Second, we construct an f such that an EPIC protocol implements f with communication C, but all DSIC implementations of f require communication exp(C)."
            },
            {
                "arxivId": "2012.14623",
                "title": "The communication complexity of payment computation",
                "abstract": "Let (f,P) be an incentive compatible mechanism where f is the social choice function and P is the payment function. In many important settings, f uniquely determines P (up to a constant) and therefore a common approach is to focus on the design of f and neglect the role of the payment function. Fadel and Segal [JET, 2009] question this approach by taking the lenses of communication complexity: can it be that the communication complexity of an incentive compatible mechanism that implements f (that is, computes both the output and the payments) is much larger than the communication complexity of computing the output? I.e., can it be that ccIC(f)>>cc(f)? Fadel and Segal show that for every f, ccIC(f)\u2264 exp(cc(f)). They also show that fully computing the incentive compatible mechanism is strictly harder than computing only the output: there exists a social choice function f such that ccIC(f)=cc(f)+1. In a follow-up work, Babaioff, Blumrosen, Naor, and Schapira [EC\u201908] provide a social choice function f such that ccIC(f)=\u0398(n\u00b7 cc(f)), where n is the number of players. The question of whether the exponential upper bound of Fadel and Segal is tight remained wide open. In this paper we solve this question by explicitly providing a function f such that ccIC(f)= exp(cc(f)). In fact, we establish this via two very different proofs. In contrast, we show that if the players are risk-neutral and we can compromise on a randomized truthful-in-expectation implementation (and not on deterministic ex-post implementation) gives that ccTIE(f)=poly(n,cc(f)) for every function f, as long as the domain of f is single parameter or a convex multi-parameter domain. We also provide efficient algorithms for deterministic computation of payments in several important domains."
            },
            {
                "arxivId": "1910.04401",
                "title": "Representing All Stable Matchings by Walking a Maximal Chain",
                "abstract": "The seminal book of Gusfield and Irving [GI89] provides a compact and algorithmically useful way to represent the collection of stable matches corresponding to a given set of preferences. In this paper, we reinterpret the main results of [GI89], giving a new proof of the characterization which is able to bypass a lot of the \"theory building\" of the original works. We also provide a streamlined and efficient way to compute this representation. Our proofs and algorithms emphasize the connection to well-known properties of the deferred acceptance algorithm."
            },
            {
                "arxivId": "1903.06696",
                "title": "Bulow-Klemperer-Style Results for Welfare Maximization in Two-Sided Markets",
                "abstract": "We consider the problem of welfare maximization in two-sided markets using simple mechanisms that are prior-independent. The Myerson-Satterthwaite impossibility theorem shows that even for bilateral trade, there is no feasible (IR, truthful, budget balanced) mechanism that has welfare as high as the optimal-yet-infeasible VCG mechanism, which attains maximal welfare but runs a deficit. On the other hand, the optimal feasible mechanism needs to be carefully tailored to the Bayesian prior, and is extremely complex, eluding a precise description. \nWe present Bulow-Klemperer-style results to circumvent these hurdles in double-auction markets. We suggest using the Buyer Trade Reduction (BTR) mechanism, a variant of McAfee's mechanism, which is feasible and simple (in particular, deterministic, truthful, prior-independent, anonymous). First, in the setting where buyers' and sellers' values are sampled i.i.d. from the same distribution, we show that for any such market of any size, BTR with one additional buyer whose value is sampled from the same distribution has expected welfare at least as high as the optimal in the original market. \nWe then move to a more general setting where buyers' values are sampled from one distribution and sellers' from another, focusing on the case where the buyers' distribution first-order stochastically dominates the sellers'. We present bounds on the number of buyers that, when added, guarantees that BTR in the augmented market have welfare at least as high as the optimal in the original market. Our lower bounds extend to a large class of mechanisms, and all of our results extend to adding sellers instead of buyers. In addition, we present positive results about the usefulness of pricing at a sample for welfare maximization in two-sided markets under the above two settings, which to the best of our knowledge are the first sampling results in this context."
            },
            {
                "arxivId": "1804.05537",
                "title": "A Structural and Algorithmic Study of Stable Matching Lattices of \"Nearby\" Instances, with Applications",
                "abstract": "Recently MV18 identified and initiated work on the new problem of understanding structural relationships between the lattices of solutions of two\"nearby\"instances of stable matching. They also gave an application of their work to finding a robust stable matching. However, the types of changes they allowed in going from instance $A$ to $B$ were very restricted, namely any one agent executes an upward shift. In this paper, we allow any one agent to permute its preference list arbitrarily. Let $M_A$ and $M_B$ be the sets of stable matchings of the resulting pair of instances $A$ and $B$, and let $\\mathcal{L}_A$ and $\\mathcal{L}_B$ be the corresponding lattices of stable matchings. We prove that the matchings in $M_A \\cap M_B$ form a sublattice of both $\\mathcal{L}_A$ and $\\mathcal{L}_B$ and those in $M_A \\setminus M_B$ form a join semi-sublattice of $\\mathcal{L}_A$. These properties enable us to obtain a polynomial time algorithm for not only finding a stable matching in $M_A \\cap M_B$, but also for obtaining the partial order, as promised by Birkhoff's Representation Theorem, thereby enabling us to generate all matchings in this sublattice. Our algorithm also helps solve a version of the robust stable matching problem. We discuss another potential application, namely obtaining new insights into the incentive compatibility properties of the Gale-Shapley Deferred Acceptance Algorithm."
            },
            {
                "arxivId": "1804.00553",
                "title": "Finding Stable Matchings that are Robust to Errors in the Input",
                "abstract": "We study the problem of finding solutions to the stable matching problem that are robust to errors in the input and we obtain a polynomial time algorithm for a special class of errors. In the process, we also initiate work on a new structural question concerning the stable matching problem, namely finding relationships between the lattices of solutions of two \"nearby\" instances. \nOur main algorithmic result is the following: We identify a polynomially large class of errors, $D$, that can be introduced in a stable matching instance. Given an instance $A$ of stable matching, let $B$ be the random variable that represents the instance that results after introducing {\\em one} error from $D$, chosen via a given discrete probability distribution. The problem is to find a stable matching for $A$ that maximizes the probability of being stable for $B$ as well. Via new structural properties of the type described in the question stated above, we give a combinatorial polynomial time algorithm for this problem. \nWe also show that the set of robust stable matchings for instance $A$, under probability distribution $p$, forms a sublattice of the lattice of stable matchings for $A$. We give an efficient algorithm for finding a succinct representation for this set; this representation has the property that any member of the set can be efficiently retrieved from it."
            },
            {
                "arxivId": "1711.02165",
                "title": "The menu complexity of \"one-and-a-half-dimensional\" mechanism design",
                "abstract": "We study the menu complexity of optimal and approximately-optimal auctions in the context of the \"FedEx\" problem, a so-called \"one-and-a-half-dimensional\" setting where a single bidder has both a value and a deadline for receiving an [FGKK16]. The menu complexity of an auction is equal to the number of distinct (allocation, price) pairs that a bidder might receive [HN13]. We show the following when the bidder has $n$ possible deadlines: \n- Exponential menu complexity is necessary to be exactly optimal: There exist instances where the optimal mechanism has menu complexity is $2^n-1$. This matches exactly the upper bound provided by Fiat et al.'s algorithm, and resolves one of their open questions [FGKK16]. \n- Fully polynomial menu complexity is necessary and sufficient for approximation: For all instances, there exists a mechanism guaranteeing a multiplicative (1-\\epsilon)-approximation to the optimal revenue with menu complexity $O(n^{3/2}\\sqrt{\\frac{\\min\\{n/\\epsilon,\\ln(v_{\\max})\\}}{\\epsilon}}) = O(n^2/\\epsilon)$, where $v_{\\max}$ denotes the largest value in the support of integral distributions. \n- There exist instances where any mechanism guaranteeing a multiplicative $(1-O(1/n^2))$-approximation to the optimal revenue requires menu complexity $\\Omega(n^2)$. \nOur main technique is the polygon approximation of concave functions [Rote19], and our results here should be of independent interest. We further show how our techniques can be used to resolve an open question of [DW17] on the menu complexity of optimal auctions for a budget-constrained buyer."
            },
            {
                "arxivId": "1711.01032",
                "title": "A simply exponential upper bound on the maximum number of stable matchings",
                "abstract": "Stable matching is a classical combinatorial problem that has been the subject of intense theoretical and empirical study since its introduction in 1962 in a seminal paper by Gale and Shapley. In this paper, we provide a new upper bound on f(n), the maximum number of stable matchings that a stable matching instance with n men and n women can have. It has been a long-standing open problem to understand the asymptotic behavior of f(n) as n\u2192\u221e, first posed by Donald Knuth in the 1970s. Until now the best lower bound was approximately 2.28n, and the best upper bound was 2nlogn\u2212 O(n). In this paper, we show that for all n, f(n) \u2264 cn for some universal constant c. This matches the lower bound up to the base of the exponent. Our proof is based on a reduction to counting the number of downsets of a family of posets that we call \u201cmixing\u201d. The latter might be of independent interest."
            },
            {
                "arxivId": "1708.08907",
                "title": "Bounds on the Menu-Size of Approximately Optimal Auctions via Optimal-Transport Duality",
                "abstract": "The question of the minimum menu-size for approximate (i.e., up-to-$\\varepsilon$) Bayesian revenue maximization when selling two goods to an additive risk-neutral quasilinear buyer was introduced by Hart and Nisan (2013), who give an upper bound of $O(\\frac{1}{\\varepsilon^4})$ for this problem. Using the optimal-transport duality framework of Daskalakis et al. (2013, 2015), we derive the first lower bound for this problem - of $\\Omega(\\frac{1}{\\sqrt[4]{\\varepsilon}})$, even when the values for the two goods are drawn i.i.d. from \"nice\" distributions, establishing how to reason about approximately optimal mechanisms via this duality framework. This bound implies, for any fixed number of goods, a tight bound of $\\Theta(\\log\\frac{1}{\\varepsilon})$ on the minimum deterministic communication complexity guaranteed to suffice for running some approximately revenue-maximizing mechanism, thereby completely resolving this problem. As a secondary result, we show that under standard economic assumptions on distributions, the above upper bound of Hart and Nisan (2013) can be strengthened to $O(\\frac{1}{\\varepsilon^2})$."
            },
            {
                "arxivId": "1612.08821",
                "title": "The Competition Complexity of Auctions: A Bulow-Klemperer Result for Multi-Dimensional Bidders",
                "abstract": "A seminal result of Bulow and Klemperer [1989] demonstrates the power of competition for extracting revenue: when selling a single item to n bidders whose values are drawn i.i.d. from a regular distribution, the simple welfare-maximizing VCG mechanism (in this case, a second price-auction) with one additional bidder extracts at least as much revenue in expectation as the optimal mechanism. The beauty of this theorem stems from the fact that VCG is a prior-independent mechanism, where the seller possesses no information about the distribution, and yet, by recruiting one additional bidder it performs better than any prior-dependent mechanism tailored exactly to the distribution at hand (without the additional bidder). In this work, we establish the first full Bulow-Klemperer results in multi-dimensional environments, proving that by recruiting additional bidders, the revenue of the VCG mechanism surpasses that of the optimal (possibly randomized, Bayesian incentive compatible) mechanism. For a given environment with i.i.d. bidders, we term the number of additional bidders needed to achieve this guarantee the environment's competition complexity. Using the recent duality-based framework of Cai et al. [2016] for reasoning about optimal revenue, we show that the competition complexity of n bidders with additive valuations over m independent, regular items is at most n+2m-2 and at least log(m). We extend our results to bidders with additive valuations subject to downward-closed constraints, showing that these significantly more general valuations increase the competition complexity by at most an additive m-1 factor. We further improve this bound for the special case of matroid constraints, and provide additional extensions as well."
            },
            {
                "arxivId": "1612.04746",
                "title": "A Simple and Approximately Optimal Mechanism for a Buyer with Complements",
                "abstract": "Recent literature on approximately optimal revenue maximization has shown that in settings where agent valuations for items are complement free, the better of selling the items separately and bundling them together guarantees a constant fraction of the optimal revenue. However, most real-world settings involve some degree of complementarity among items. The role that complementarity plays in the trade-off of simplicity versus optimality has been an obvious missing piece of the puzzle. In \u201cA Simple and Approximately Optimal Mechanism for a Buyer with Complements,\u201d the authors show that the same simple selling mechanism\u2014the better of selling separately and as a grand bundle\u2014guarantees a $\\Theta(d)$ fraction of the optimal revenue, where $d$ is a measure of the degree of complementarity. One key modeling contribution is a tractable notion of \u201cdegree of complementarity\u201d that admits meaningful results and insights\u2014they demonstrate that previous definitions fall short in this regard."
            },
            {
                "arxivId": "1610.04873",
                "title": "Gibbard-Satterthwaite Success Stories and Obvious Strategyproofness",
                "abstract": "The Gibbard-Satterthwaite Impossibility Theorem [Gibbard, 1973, Satterthwaite, 1975] holds that dictatorship is the only Pareto optimal and strategyproof social choice function on the full domain of preferences. Much of the work in mechanism design aims at getting around this impossibility theorem. Three grand success stories stand out. On the domains of single-peaked preferences, of object assignment, and of quasilinear preferences, there are appealing Pareto optimal and strategyproof social choice functions. We investigate whether these success stories are robust to strengthening strategyproofness to obvious strategyproofness, a stronger incentive property that was recently introduced by Li [2015] and has since garnered considerable attention. For single-peaked preferences, we characterize the class of OSP-implementable and unanimous social choice functions as dictatorships with safeguards against extremism -- mechanisms (which turn out to also be Pareto optimal) in which the dictator can choose the outcome, but other agents may prevent the dictator from choosing an outcome that is too extreme. Median voting is consequently not OSP-implementable. Moreover, even when there are only two possible outcomes, majority voting is not OSP-implementable, and unanimity is the only OSP-implementable supermajority rule. For object assignment, we characterize the class of OSP-implementable and Pareto optimal matching rules as sequential barter with lurkers -- a significant generalization over bossy variants of bipolar serially dictatorial rules. While Li [2015] shows that second-price auctions are OSP-implementable when only one good is sold, we show that this positive result does not extend to the case of multiple goods. Even when all agents' preferences over goods are quasilinear and additive, no welfare-maximizing auction where losers pay nothing is OSP-implementable when more than one good is sold. Our analysis makes use of a gradual revelation principle, an analog of the (direct) revelation principle for OSP mechanisms that we present and prove, and believe to be of independent interest. An integrated examination, of all of these negative and positive results, on the one hand reveals that the various mechanics that come into play within obviously strategyproof mechanisms are considerably richer and more diverse than previously demonstrated and can give rise to rather exotic and quite intricate mechanisms in some domains, however on the other hand suggests that the boundaries of obvious strategyproofness are significantly less far-reaching than one may hope in other domains. We thus observe that in a natural sense, obvious strategyproofness is neither \"too strong\" nor \"too weak\" a definition for capturing \"strategyproofness that is easy to see,\" but in fact while it performs as intuitively expected on some domains, it \"overshoots\" on some other domains, and \"undershoots\" on yet other domains."
            },
            {
                "arxivId": "1604.06580",
                "title": "The menu-size complexity of revenue approximation",
                "abstract": "We consider a monopolist that is selling n items to a single additive buyer, where the buyer's values for the items are drawn according to independent distributions F1,F2,\u2026,Fn that possibly have unbounded support. It is well known that - unlike in the single item case - the revenue-optimal auction (a pricing scheme) may be complex, sometimes requiring a continuum of menu entries. It is also known that simple auctions with a finite bounded number of menu entries can extract a constant fraction of the optimal revenue. Nonetheless, the question of the possibility of extracting an arbitrarily high fraction of the optimal revenue via a finite menu size remained open. In this paper, we give an affirmative answer to this open question, showing that for every n and for every \u03b5>0, there exists a complexity bound C=C(n,\u03b5) such that auctions of menu size at most C suffice for obtaining a (1-\u03b5) fraction of the optimal revenue from any F1,\u2026,Fn. We prove upper and lower bounds on the revenue approximation complexity C(n,\u03b5), as well as on the deterministic communication complexity required to run an auction that achieves such an approximation."
            },
            {
                "arxivId": "1604.01971",
                "title": "Computational Efficiency Requires Simple Taxation",
                "abstract": "We characterize the communication complexity of truthful mechanisms. Our departure point is the well known taxation principle. The taxation principle asserts that every truthful mechanism can be interpreted as follows: every player is presented with a menu that consists of a price for each bundle (the prices depend only on the valuations of the other players). Each player is allocated a bundle that maximizes his profit according to this menu. We define the taxation complexity of a truthful mechanism to be the logarithm of the maximum number of menus that may be presented to a player. Our main finding is that in general the taxation complexity essentially equals the communication complexity. The proof consists of two main steps. First, we prove that for rich enough domains the taxation complexity is at most the communication complexity. We then show that the taxation complexity is much smaller than the communication complexity only in \"pathological\" cases and provide a formal description of these extreme cases. Next, we study mechanisms that access the valuations via value queries only. In this setting we establish that the menu complexity - a notion that was already studied in several different contexts - characterizes the number of value queries that the mechanism makes in exactly the same way that the taxation complexity characterizes the communication complexity. Our approach yields several applications, including strengthening the solution concept with low communication overhead, fast computation of prices, and hardness of approximation by computationally efficient truthful mechanisms."
            },
            {
                "arxivId": "1511.00452",
                "title": "Stable matching mechanisms are not obviously strategy-proof",
                "abstract": null
            },
            {
                "arxivId": "1409.4150",
                "title": "Strong Duality for a Multiple-Good Monopolist",
                "abstract": "We provide a duality-based framework for revenue maximization in a multiple-good monopoly. Our framework shows that every optimal mechanism has a certificate of optimality, taking the form of an optimal transportation map between measures. Using our framework, we prove that grand-bundling mechanisms are optimal if and only if two stochastic dominance conditions hold between specific measures induced by the buyer's type distribution. This result strengthens several results in the literature, where only sufficient conditions for grand-bundling optimality have been provided. As a corollary of our tight characterization of grand-bundling optimality, we show that the optimal mechanism for n independent uniform items each supported on [c; c + 1] is a grand-bundling mechanism, as long as c is sufficiently large, extending Pavlov's result for 2 items [Pavlov 2011]. Surprisingly, our characterization also implies that, for all c and for all sufficiently large n, the optimal mechanism for n independent uniform items supported on [c; c + 1] is not a grand bundling mechanism. The necessary and sufficient condition for grand bundling optimality is a special case of our more general characterization result that provides necessary and sufficient conditions for the optimality of an arbitrary mechanism for an arbitrary type distribution."
            },
            {
                "arxivId": "1405.7709",
                "title": "A Stable Marriage Requires Communication",
                "abstract": "The Gale-Shapley algorithm for the Stable Marriage Problem is known to take $\\Theta(n^2)$ steps to find a stable marriage in the worst case, but only $\\Theta(n \\log n)$ steps in the average case (with $n$ women and $n$ men). In 1976, Knuth asked whether the worst-case running time can be improved in a model of computation that does not require sequential access to the whole input. A partial negative answer was given by Ng and Hirschberg, who showed that $\\Theta(n^2)$ queries are required in a model that allows certain natural random-access queries to the participants' preferences. A significantly more general - albeit slightly weaker - lower bound follows from Segal's general analysis of communication complexity, namely that $\\Omega(n^2)$ Boolean queries are required in order to find a stable marriage, regardless of the set of allowed Boolean queries. \nUsing a reduction to the communication complexity of the disjointness problem, we give a far simpler, yet significantly more powerful argument showing that $\\Omega(n^2)$ Boolean queries of any type are indeed required for finding a stable - or even an approximately stable - marriage. Notably, unlike Segal's lower bound, our lower bound generalizes also to (A) randomized algorithms, (B) allowing arbitrary separate preprocessing of the women's preferences profile and of the men's preferences profile, (C) several variants of the basic problem, such as whether a given pair is married in every/some stable marriage, and (D) determining whether a proposed marriage is stable or far from stable. In order to analyze \"approximately stable\" marriages, we introduce the notion of \"distance to stability\" and provide an efficient algorithm for its computation."
            },
            {
                "arxivId": "1405.6146",
                "title": "A Simple and Approximately Optimal Mechanism for an Additive Buyer",
                "abstract": "We consider a monopolist seller with n heterogeneous items, facing a single buyer. The buyer hasa value for each item drawn independently according to(non-identical) distributions, and his value for a set ofitems is additive. The seller aims to maximize his revenue.It is known that an optimal mechanism in this setting maybe quite complex, requiring randomization [19] and menusof infinite size [15]. Hart and Nisan [17] have initiated astudy of two very simple pricing schemes for this setting:item pricing, in which each item is priced at its monopolyreserve; and bundle pricing, in which the entire set ofitems is priced and sold as one bundle. Hart and Nisan [17]have shown that neither scheme can guarantee more thana vanishingly small fraction of the optimal revenue. Insharp contrast, we show that for any distributions, thebetter of item and bundle pricing is a constant-factorapproximation to the optimal revenue. We further discussextensions to multiple buyers and to valuations that arecorrelated across items."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2302.04201",
        "category": "econ",
        "title": "Labor Market Effects of the Venezuelan Refugee Crisis in Brazil",
        "abstract": "We use administrative panel data on the universe of Brazilian formal workers to investigate the labor market effects of the Venezuelan crisis in Brazil, focusing on the border state of Roraima. The results using difference-in-differences show that the monthly wages of Brazilians in Roraima increased by around 2 percent, which was mostly driven by those working in sectors and occupations with no refugee involvement. The study finds negligible job displacement for Brazilians but finds evidence of native workers moving to occupations without immigrants. We also find that immigrants in the informal market offset the substitution effects in the formal market.",
        "references": [
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1812.01723",
                "title": "Doubly Robust Difference-in-Differences Estimators",
                "abstract": "Abstract This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying particular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available."
            },
            {
                "arxivId": "1710.02926",
                "title": "When Should You Adjust Standard Errors for Clustering?",
                "abstract": "\n Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, e.g., by state but not by gender, and in observational studies, but not in completely randomized experiments? (ii) Why is conventional clustering an \u201call-or-nothing\u201d adjustment, while within-cluster correlations can be strong or extremely weak? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional cluster standard errors can be severely inflated, and propose new variance estimators that correct for this bias."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2303.05781",
        "category": "econ",
        "title": "Strategy-proofness with single-peaked and single-dipped preferences",
        "abstract": "We analyze the problem of locating a public facility in a domain of single-peaked and single-dipped preferences when the social planner knows the type of preference (single-peaked or single-dipped) of each agent. Our main result characterizes all strategy-proof rules and shows that they can be decomposed into two steps. In the first step, the agents with single-peaked preferences are asked about their peaks and, for each profile of reported peaks, at most two alternatives are preselected. In the second step, the agents with single-dipped preferences are asked to reveal their dips to complete the decision between the preselected alternatives. Our result generalizes the findings of Moulin (1980) and Barber\\`a and Jackson (1994) for single-peaked and of Manjunath (2014) for single-dipped preferences. Finally, we show that all strategy-proof rules are also group strategy-proof and analyze the implications of Pareto efficiency.",
        "references": [
            {
                "arxivId": "2102.11686",
                "title": "New Characterizations of Strategy-Proofness under Single-Peakedness",
                "abstract": null
            },
            {
                "arxivId": "1412.3414",
                "title": "Strategyproof Mechanisms for One-Dimensional Hybrid and Obnoxious Facility Location Models",
                "abstract": "We consider a strategic variant of the facility location problem. We would like to locate a facility on a closed interval. There are n agents located on that interval, divided into two types: type 1 agents, who wish for the facility to be as far from them as possible, and type 2 agents, who wish for the facility to be as close to them as possible. Our goal is to maximize a form of aggregated social benefit: maxisum- the sum of the agents' utilities, or the egalitarian objective- the minimal agent utility. The strategic aspect of the problem is that the agents' locations are not known to us, but rather reported to us by the agents- an agent might misreport his location in an attempt to move the facility away from or towards to his true location. We therefore require the facility-locating mechanism to be strategyproof, namely that reporting truthfully is a dominant strategy for each agent. As simply maximizing the social benefit is generally not strategyproof, our goal is to design strategyproof mechanisms with good approximation ratios. \nFor the maxisum objective, in the deterministic setting, we provide a best-possible 3- approximate strategyproof mechanism; in the randomized setting, we provide a 23/13- approximate strategyproof mechanism and a lower bound of \\frac{2}{\\sqrt{3}}. For the egalitarian objective, we provide a lower bound of 3/2 in the randomized setting, and show that no bounded approximation ratio is attainable in the deterministic setting. To obtain our deterministic lower bounds, we characterize all deterministic strategyproof mechanisms when all agents are of type 1. Finally, we consider a generalized model that allows an agent to control more than one location, and provide best-possible 3- and 3/2- approximate strategyproof mechanisms for maxisum, in the deterministic and randomized settings respectively, when only type 1 agents are present."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2311.14032",
        "category": "econ",
        "title": "Counterfactual Sensitivity in Quantitative Spatial Models",
        "abstract": "Counterfactuals in quantitative spatial models are functions of the current state of the world and the model parameters. Current practice treats the current state of the world as perfectly observed, but there is good reason to believe that it is measured with error. This paper provides tools for quantifying uncertainty about counterfactuals when the current state of the world is measured with error. I recommend an empirical Bayes approach to uncertainty quantification, which is both practical and theoretically justified. I apply the proposed method to the applications in Adao, Costinot, and Donaldson (2017) and Allen and Arkolakis (2022) and find non-trivial uncertainty about counterfactuals.",
        "references": [
            {
                "arxivId": "1610.08401",
                "title": "Universal Adversarial Perturbations",
                "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2312.13939",
        "category": "econ",
        "title": "Binary endogenous treatment in stochastic frontier models with an application to soil conservation in El Salvador",
        "abstract": "Numerous programs exist to promote productivity, alleviate poverty, and enhance food security in developing countries. Stochastic frontier analysis can be helpful to assess their effectiveness. However, challenges can arise when accounting for treatment endogeneity, often intrinsic to these interventions. We study maximum likelihood estimation of stochastic frontier models when both the frontier and inefficiency depend on a potentially endogenous binary treatment. We use instrumental variables to define an assignment mechanism and explicitly model the density of the first and second\u2010stage error terms. We provide empirical evidence using data from a soil conservation program in El Salvador.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2402.10982",
        "category": "econ",
        "title": "mshw, a forecasting library to predict short-term electricity demand based on multiple seasonal Holt-Winters",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2403.06344",
        "category": "econ",
        "title": "Can One Hear the Shape of a Decision Problem?",
        "abstract": "We explore the connection between an agent's decision problem and her ranking of information structures. We find that a finite amount of ordinal data on the agent's ranking of experiments is enough to identify her (finite) set of undominated actions (up to relabeling and duplication) and the beliefs rendering each such action optimal. An additional smattering of cardinal data, comparing the relative value to the agent of finitely many pairs of experiments, identifies her utility function up to an action-independent payoff.",
        "references": [
            {
                "arxivId": "2404.01190",
                "title": "Call the Dentist! A (Con-)Cavity in the Value of Information",
                "abstract": "A natural way of quantifying the ``amount of information'' in decision problems yields a globally concave value for information. Another (in contrast, adversarial) way almost never does."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00028",
        "category": "econ",
        "title": "Antinetwork among China A-shares",
        "abstract": "The correlation-based financial networks, constructed with the correlation relationships among the time series of fluctuations of daily logarithmic prices of stocks, are intensively studied. However, these studies ignore the importance of negative correlations. This paper is the first time to consider the negative and positive correlations separately, and accordingly to construct weighted temporal antinetwork and network among stocks listed in the Shanghai and Shenzhen stock exchanges. For (anti)networks during the first 24 years of the 21st century, the node's degree and strength, the assortativity coefficient, the average local clustering coefficient, and the average shortest path length are analyzed systematically. This paper unveils some essential differences in these topological measurements between antinetwork and network. The findings of the differences between antinetwork and network have an important role in understanding the dynamics of a financial complex system. The observation of antinetwork is of great importance in optimizing investment portfolios and risk management. More importantly, this paper proposes a new direction for studying complex systems, namely the correlation-based antinetwork.",
        "references": [
            {
                "arxivId": "1012.0206",
                "title": "Catastrophic cascade of failures in interdependent networks",
                "abstract": null
            },
            {
                "arxivId": "physics/0605251",
                "title": "Correlation based networks of equity returns sampled at different time horizons",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0312682",
                "title": "Clustering and information in correlation based financial networks",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0302546",
                "title": "Dynamics of market correlations: taxonomy and portfolio analysis.",
                "abstract": "The time dependence of the recently introduced minimum spanning tree description of correlations between stocks, called the \"asset tree\" has been studied in order to reflect the financial market taxonomy. The nodes of the tree are identified with stocks and the distance between them is a unique function of the corresponding element of the correlation matrix. By using the concept of a central vertex, chosen as the most strongly connected node of the tree, an important characteristic is defined by the mean occupation layer. During crashes, due to the strong global correlation in the market, the tree shrinks topologically, and this is shown by a low value of the mean occupation layer. The tree seems to have a scale-free structure where the scaling exponent of the degree distribution is different for \"business as usual\" and \"crash\" periods. The basic structure of the tree topology is very robust with respect to time. We also point out that the diversification aspect of portfolio optimization results in the fact that the assets of the classic Markowitz portfolio are always located on the outer leaves of the tree. Technical aspects such as the window size dependence of the investigated quantities are also discussed."
            },
            {
                "arxivId": "cond-mat/0212037",
                "title": "Dynamic asset trees and Black Monday",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0205405",
                "title": "Assortative mixing in networks.",
                "abstract": "A network is said to show assortative mixing if the nodes in the network that have many connections tend to be connected to other nodes with many connections. Here we measure mixing patterns in a variety of networks and find that social networks are mostly assortatively mixed, but that technological and biological networks tend to be disassortative. We propose a model of an assortatively mixed network, which we study both analytically and numerically. Within this model we find that networks percolate more easily if they are assortative and that they are also more robust to vertex removal."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00164",
        "category": "econ",
        "title": "Sequential Synthetic Difference in Differences",
        "abstract": "We study the estimation of treatment effects of a binary policy in environments with a staggered treatment rollout. We propose a new estimator -- Sequential Synthetic Difference in Difference (Sequential SDiD) -- and establish its theoretical properties in a linear model with interactive fixed effects. Our estimator is based on sequentially applying the original SDiD estimator proposed in Arkhangelsky et al. (2021) to appropriately aggregated data. To establish the theoretical properties of our method, we compare it to an infeasible OLS estimator based on the knowledge of the subspaces spanned by the interactive fixed effects. We show that this OLS estimator has a sequential representation and use this result to show that it is asymptotically equivalent to the Sequential SDiD estimator. This result implies the asymptotic normality of our estimator along with corresponding efficiency guarantees. The method developed in this paper presents a natural alternative to the conventional DiD strategies in staggered adoption designs.",
        "references": [
            {
                "arxivId": "1710.10251",
                "title": "Matrix Completion Methods for Causal Panel Data Models",
                "abstract": "Abstract In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \u201cmissing\u201d elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00183",
        "category": "econ",
        "title": "Shared Hardships Strengthen Bonds: Negative Shocks, Embeddedness and Employee Retention",
        "abstract": "Unexpected events --\"shocks\"-- are the motive force in explaining changes in embeddedness and retention within the unfolding model of labor turnover. Substantial research effort has examined strategies for insulating valued employees from adverse shocks. However, this paper provides empirical evidence that unambiguously negative shocks can increase employee retention when underlying firm and employee incentives with respect to these shocks are aligned. Using survival analysis on a unique data set of 466,236 communication records and 45,873 employment spells from 21 trucking companies, we show how equipment-related shocks tend to increase the duration of employment. Equipment shocks also generate paradoxically positive sentiments that demonstrate an increase in employees' affective commitment to the firm. Our results highlight the important moderating role aligned incentives have in how shocks ultimately translate into retention. Shared hardships strengthen bonds in employment as in other areas.",
        "references": [
            {
                "arxivId": "2007.15584",
                "title": "The effects of remote work on collaboration among information workers",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00221",
        "category": "econ",
        "title": "Robust Learning for Optimal Dynamic Treatment Regimes with Observational Data",
        "abstract": "Many public policies and medical interventions involve dynamics in their treatment assignments, where treatments are sequentially assigned to the same individuals across multiple stages, and the effect of treatment at each stage is usually heterogeneous with respect to the history of prior treatments and associated characteristics. We study statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's history. We propose a step-wise doubly-robust approach to learn the optimal DTR using observational data under the assumption of sequential ignorability. The approach solves the sequential treatment assignment problem through backward induction, where, at each step, we combine estimators of propensity scores and action-value functions (Q-functions) to construct augmented inverse probability weighting estimators of values of policies for each stage. The approach consistently estimates the optimal DTR if either a propensity score or Q-function for each stage is consistently estimated. Furthermore, the resulting DTR can achieve the optimal convergence rate $n^{-1/2}$ of regret under mild conditions on the convergence rate for estimators of the nuisance parameters.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00784",
        "category": "econ",
        "title": "Estimating sample paths of Gauss-Markov processes from noisy data",
        "abstract": "I derive the pointwise conditional means and variances of an arbitrary Gauss-Markov process, given noisy observations of points on a sample path. These moments depend on the process's mean and covariance functions, and on the conditional moments of the sampled points. I study the Brownian motion and bridge as special cases.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00806",
        "category": "econ",
        "title": "Algorithmic Collusion by Large Language Models",
        "abstract": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions (\"prompts\") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.",
        "references": [
            {
                "arxivId": "2401.15794",
                "title": "Regulation of Algorithmic Collusion",
                "abstract": "Consider sellers in a competitive market that use algorithms to adapt their prices from data that they collect. In such a context it is plausible that algorithms could arrive at prices that are higher than the competitive prices and this may benefit sellers at the expense of consumers (i.e., the buyers in the market). This paper gives a definition of plausible algorithmic non-collusion for pricing algorithms. The definition allows a regulator to empirically audit algorithms by applying a statistical test to the data that they collect. Algorithms that are good, i.e., approximately optimize prices to market conditions, can be augmented to contain the data sufficient to pass the audit. Algorithms that have colluded on, e.g., supra-competitive prices cannot pass the audit. The definition allows sellers to possess useful side information that may be correlated with supply and demand and could affect the prices used by good algorithms. The paper provides an analysis of the statistical complexity of such an audit, i.e., how much data is sufficient for the test of non-collusion to be accurate."
            },
            {
                "arxivId": "2307.03172",
                "title": "Lost in the Middle: How Language Models Use Long Contexts",
                "abstract": "Abstract While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models."
            },
            {
                "arxivId": "2305.16291",
                "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
                "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2301.07543",
                "title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?",
                "abstract": "Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution."
            },
            {
                "arxivId": "2212.08073",
                "title": "Constitutional AI: Harmlessness from AI Feedback",
                "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels."
            },
            {
                "arxivId": "2207.13243",
                "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
                "abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, \u201cinner\u201d interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications."
            },
            {
                "arxivId": "2205.04661",
                "title": "Pricing with algorithms",
                "abstract": "This paper studies Markov perfect equilibria in a repeated duopoly model where sellers choose algorithms. An algorithm is a mapping from the competitor's price to own price. Once set, algorithms respond quickly. Customers arrive randomly and so do opportunities to revise the algorithm. In the simple game with two possible prices, monopoly outcome is the unique equilibrium for standard functional forms of the profit function. More generally, with multiple prices, exercise of market power is the rule -- in all equilibria, the expected payoff of both sellers is above the competitive outcome, and that of at least one seller is close to or above the monopoly outcome. Sustenance of such collusion seems outside the scope of standard antitrust laws for it does not involve any direct communication."
            },
            {
                "arxivId": "2202.05947",
                "title": "Artificial Intelligence and Auction Design",
                "abstract": "Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about the lowest bid to win, as introduced by Google at the time of the switch to first-price auctions, increases competitiveness of auctions."
            },
            {
                "arxivId": "2201.11903",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
            },
            {
                "arxivId": "2110.11855",
                "title": "Auctions between Regret-Minimizing Agents",
                "abstract": "We analyze a scenario in which software agents implemented as regret-minimizing algorithms engage in a repeated auction on behalf of their users. We study first-price and second-price auctions, as well as their generalized versions (e.g., as those used for ad auctions). Using both theoretical analysis and simulations, we show that, surprisingly, in second-price auctions the players have incentives to misreport their true valuations to their own learning agents, while in first-price auctions it is a dominant strategy for all players to truthfully report their valuations to their agents."
            },
            {
                "arxivId": "2009.03300",
                "title": "Measuring Massive Multitask Language Understanding",
                "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
            },
            {
                "arxivId": "2005.14165",
                "title": "Language Models are Few-Shot Learners",
                "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.00864",
        "category": "econ",
        "title": "Convolution-t Distributions",
        "abstract": "We introduce a new class of multivariate heavy-tailed distributions that are convolutions of heterogeneous multivariate t-distributions. Unlike commonly used heavy-tailed distributions, the multivariate convolution-t distributions embody cluster structures with flexible nonlinear dependencies and heterogeneous marginal distributions. Importantly, convolution-t distributions have simple density functions that facilitate estimation and likelihood-based inference. The characteristic features of convolution-t distributions are found to be important in an empirical analysis of realized volatility measures and help identify their underlying factor structure.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-02.json",
        "arxivId": "2404.01190",
        "category": "econ",
        "title": "Call the Dentist! A (Con-)Cavity in the Value of Information",
        "abstract": "A natural way of quantifying the ``amount of information'' in decision problems yields a globally concave value for information. Another (in contrast, adversarial) way almost never does.",
        "references": [
            {
                "arxivId": "1908.01633",
                "title": "Payoffs-Beliefs Duality and the Value of Information",
                "abstract": "In decision problems under incomplete information, payoff vectors (indexed by states of nature) and beliefs are naturally paired by bilinear duality. We exploit this duality to analyze the value of information using convex analysis. We then derive global estimates of the value of information of any information structure from local properties of the value function and of the set of optimal actions taken at the prior belief only, and apply our results to the marginal value of information."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2202.06191",
        "category": "econ",
        "title": "Exploration and Incentivizing Participation in Clinical Trials",
        "abstract": "Participation incentives a well-known issue inhibiting clinical trials. We frame this issue as a non-standard exploration-exploitation tradeoff: the trial would like to explore as uniformly as possible, whereas each patient prefers\"exploitation\", i.e., treatments that seem best. We incentivize participation by leveraging information asymmetry between the trial and the patients. We measure statistical performance via worst-case estimation error under adversarially generated outcomes, a standard objective for clinical trials. We obtain a near-optimal solution in terms of this objective: an incentive-compatible mechanism with a particular guarantee, and a nearly matching impossibility result for any incentive-compatible mechanism. Our results extend to heterogeneous agents.",
        "references": [
            {
                "arxivId": "1904.08551",
                "title": "Asymptotic behavior of Bayesian learners with misspecified models",
                "abstract": null
            },
            {
                "arxivId": "1904.07272",
                "title": "Introduction to Multi-Armed Bandits",
                "abstract": "Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments. The chapters are as follows: stochastic bandits, lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full Feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual Bandits; bandits and games; bandits with knapsacks; bandits and incentives."
            },
            {
                "arxivId": "1811.06026",
                "title": "Incentivizing Exploration with Selective Data Disclosure",
                "abstract": "We study the design of rating systems that incentivize (more) efficient social learning among self-interested agents. Agents arrive sequentially and are presented with a set of possible actions, each of which yields a positive reward with an unknown probability. A disclosure policy sends messages about the rewards of previously-chosen actions to arriving agents. These messages can alter agents' incentives towards exploration, taking potentially sub-optimal actions for the sake of learning more about their rewards. Prior work achieves much progress with disclosure policies that merely recommend an action to each user, without any other supporting information, and sometimes recommend exploratory actions. All this work relies heavily on standard, yet very strong rationality assumptions. However, these assumptions are quite problematic in the context of the motivating applications: recommendation systems such as Yelp, Amazon, or Netflix, and macthing markets such as AirBnB. It is very unclear whether users would know and understand a complicated disclosure policy announced by the principal, let alone trust the principal to faithfully implement it. (The principal may deviate from the announced policy either intentionally, or due to insufficient information about the users, or because of bugs in implementation.) Even if the users understand the policy and trust that it was implemented as claimed, they might not react to it rationally, particularly given the lack of supporting information and the possibility of being singled out for exploration. For example, users may find such disclosure policies unacceptable and leave the system. We study a particular class of disclosure policies that use messages, called unbiased subhistories, consisting of the actions and rewards from a subsequence of past agents. Each subsequence is chosen ahead of time, according to a predetermined partial order on the rounds. We posit a flexible model of frequentist agent response, which we argue is plausible for this class of \"order-based\" disclosure policies. We measure the performance of a policy by its regret, i.e., the difference in expected total reward between the best action and the policy. A disclosure policy that reveals full history in each round risks inducing herding behavior among the agents, and typically has regret linear in the time horizon T. Our main result is an order-based disclosure policy that obtains regret ~O (\u221aT). This regret is known to be optimal in the worst case over reward distributions, even absent incentives. We also exhibit simpler order-based policies with higher, but still sublinear, regret. These policies can be interpreted as dividing a sublinear number of agents into constant-sized focus groups, whose histories are then revealed to future agents. Helping market participants find whatever they are looking for, and coordinating their search and exploration behavior in a globally optimal way, is an essential part of market design. This paper continues the line of work on \"incentivized exploration\": essentially, exploration-exploitation learning in the presence of self-interested users whose incentives are skewed in favor of exploitation. Conceptually, we study the interplay of information design, social learning, and multi-armed bandit algorithms. To the best of our knowledge, this is the first paper in the literature on incentivized exploration (and possibly in the broader literature on \"learning and incentives\") which attempts to mitigate the limitations of standard economic assumptions. Full version: https://arxiv.org/abs/1811.06026."
            },
            {
                "arxivId": "1612.01205",
                "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits",
                "abstract": "We study the off-policy evaluation problem---estimating the value of a target policy using data collected by another policy---under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse propensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is suboptimal. We then propose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of data sets, often outperforming prior work by orders of magnitude."
            },
            {
                "arxivId": "1512.08427",
                "title": "Incentivizing Exploration with Heterogeneous Value of Money",
                "abstract": null
            },
            {
                "arxivId": "1507.07191",
                "title": "Economic Recommendation Systems",
                "abstract": "In the on-line Explore and Exploit literature, central to Machine Learning, a central planner is faced with a set of alternatives, each yielding some unknown reward. The planner's goal is to learn the optimal alternative as soon as possible, via experimentation. A typical assumption in this model is that the planner has full control over the experiment design and implementation. When experiments are implemented by a society of self-motivated agents the planner can only recommend experimentation but has no power to enforce it. Kremer et al (JPE, 2014) introduce the first study of explore and exploit schemes that account for agents' incentives. In their model it is implicitly assumed that agents do not see nor communicate with each other. Their main result is a characterization of an optimal explore and exploit scheme. In this work we extend Kremer et al (JPE, 2014) by adding a layer of a social network according to which agents can observe each other. It turns out that when observability is factored in the scheme proposed by Kremer et al (JPE, 2014) is no longer incentive compatible. In our main result we provide a tight bound on how many other agents can each agent observe and still have an incentive-compatible algorithm and asymptotically optimal outcome. More technically, for a setting with N agents where the number of nodes with degree greater than N^alpha is bounded by N^beta and 2*alpha+beta < 1 we construct incentive-compatible asymptotically optimal mechanism. The bound 2*alpha+beta < 1 is shown to be tight."
            },
            {
                "arxivId": "1502.06901",
                "title": "Equilibrium in Misspecified Markov Decision Processes",
                "abstract": "We provide an equilibrium framework for modeling the behavior of an agent who holds a simplified view of a dynamic optimization problem. The agent faces a Markov decision process, where a transition probability function determines the evolution of a state variable as a function of the previous state and the agent's action. The agent is uncertain about the true transition function and has a prior over a set of possible transition functions; this set reflects the agent's (possibly simplified) view of her environment and may not contain the true function. We define an equilibrium concept and provide conditions under which it characterizes steady\u2010state behavior when the agent updates her beliefs using Bayes' rule."
            },
            {
                "arxivId": "1502.04147",
                "title": "Bayesian Incentive-Compatible Bandit Exploration",
                "abstract": "Individual decision-makers consume information revealed by the previous decision makers, and produce information that may help in future decision makers. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as elsewhere, such as medical decisions. Each decision maker when required to select an action, would individually prefer to exploit, select the highest expected reward action conditional on her information. At the same time, each decision maker would prefer previous decision makers to explore, producing information about the rewards of various actions. A social planner, by means of carefully designed information disclosure, can incentivize the agents to balance the exploration and exploitation, and maximize social welfare. We formulate this problem as a multi-arm bandit problem (and various generalizations thereof) under incentive-compatibility constraints induced by agents' Bayesian priors. We design an incentive-compatible bandit algorithm for the social planner with asymptotically optimal regret. Further, we provide a black-box reduction from an arbitrary multi-arm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit settings, even ones that incorporate contexts and arbitrary partial feedback."
            },
            {
                "arxivId": "1411.1152",
                "title": "Berk-Nash Equilibrium: A Framework for Modeling Agents with Misspecified Models",
                "abstract": "We develop an equilibrium framework that relaxes the standard assumption that people have a correctly-specified view of their environment. Each player is characterized by a (possibly misspecified) subjective model, which describes the set of feasible beliefs over payoff-relevant consequences as a function of actions. We introduce the notion of a Berk-Nash equilibrium: Each player follows a strategy that is optimal given her belief, and her belief is restricted to be the best fit among the set of beliefs she considers possible. The notion of best fit is formalized in terms of minimizing the Kullback-Leibler divergence, which is endogenous and depends on the equilibrium strategy profile. Standard solution concepts such as Nash equilibrium and self-confirming equilibrium constitute special cases where players have correctly-specified models. We provide a learning foundation for Berk-Nash equilibrium by extending and combining results from the statistics literature on misspecified learning and the economics literature on learning in games."
            },
            {
                "arxivId": "1503.02834",
                "title": "Doubly Robust Policy Evaluation and Optimization",
                "abstract": "We study sequential decision making in environments where rewards are only partially observed, but can be modeled as a function of observed contexts and the chosen action by the decision maker. This setting, known as contextual bandits, encompasses a wide variety of applications such as health care, content recommendation and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. In this work, we leverage the strengths and overcome the weaknesses of the two approaches by applying the doubly robust estimation technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust estimation uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice in policy evaluation and optimization."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2203.03744",
        "category": "econ",
        "title": "Identifying the Deviator",
        "abstract": "A group of players are supposed to follow a prescribed profile of strategies. If they follow this profile, they will reach a given target. We show that if the target is not reached because some player deviates, then an outside observer can identify the deviator. We also construct identification methods in two nontrivial cases.",
        "references": [
            {
                "arxivId": "2201.05148",
                "title": "Regularity of the minmax value and equilibria in multiplayer Blackwell games",
                "abstract": "A real-valued function \u03c6 that is defined over all Borel sets of a topological space is regular if for every Borel set W, \u03c6(W) is the supremum of \u03c6(C), over all closed sets C that are contained in W, and the infimum of \u03c6(O), over all open sets O that contain W. We study Blackwell games with finitely many players. We show that when each player has a countable set of actions and the objective of a certain player is represented by a Borel winning set, that player\u2019s minmax value is regular. We then use the regularity of the minmax value to establish the existence of \u03b5-equilibria in two distinct classes of Blackwell games. One is the class of n-player Blackwell games where each player has a finite action space and an analytic winning set, and the sum of the minmax values over the players exceeds n \u2212 1. The other class is that of Blackwell games with bounded upper semi-analytic payoff functions, history-independent finite action spaces, and history-independent minmax values. For the latter class, we obtain a characterization of the set of equilibrium payoffs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2302.10562",
        "category": "econ",
        "title": "Renewable energy expansion under taxes and subsidies: A transmission operator\u2019s perspective",
        "abstract": null,
        "references": [
            {
                "arxivId": "1411.1607",
                "title": "Julia: A Fresh Approach to Numerical Computing",
                "abstract": "Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be \u201claws of nature\" by practitioners of numerical computing: \\beginlist \\item High-level dynamic programs have to be slow. \\item One must prototype in one language and then rewrite in another language for speed or deployment. \\item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \\endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after dif..."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2304.09078",
        "category": "econ",
        "title": "Club coefficients in the UEFA Champions League: Time for shift to an Elo-based formula",
        "abstract": "One of the most popular club football tournaments, the UEFA Champions League, will see a fundamental reform from the 2024/25 season: the traditional group stage will be replaced by one league where each of the 36 teams plays eight matches. To guarantee that the opponents of the clubs are of the same strength in the new design, it is crucial to forecast the performance of the teams before the tournament as well as possible. This paper investigates whether the currently used rating of the teams, the UEFA club coefficient, can be improved by taking the games played in the national leagues into account. According to our logistic regression models, a variant of the Elo method provides a higher accuracy in terms of explanatory power in the Champions League matches. The Union of European Football Associations (UEFA) is encouraged to follow the example of the FIFA World Ranking and reform the calculation of the club coefficients in order to avoid unbalanced schedules in the novel tournament format of the Champions League.",
        "references": [
            {
                "arxivId": "2201.00691",
                "title": "FIFA ranking: Evaluation and path forward",
                "abstract": "In this work, we study the ranking algorithm used by F\u00e9d\u00e9ration Internationale de Football Association (FIFA); we analyze the parameters that it currently uses, show the formal probabilistic model from which it can be derived, and optimize the latter. In particular, analyzing games since the introduction of the algorithm in 2018, we conclude that game\u2019s \u201cimportance\u201d (defined by FIFA and used by the algorithm) is counterproductive from the point of view of the predictive capacity of the algorithm. We also postulate that the algorithm should be rooted in the formal modeling principle, where the Davidson model proposed in 1970 seems to be an excellent candidate, preserving the form of the algorithm currently used. The results indicate that the predictive capacity of the algorithm is considerably improved by using the home-field advantage (HFA), as well as the explicit model for the draws in the game. Moderate but notable improvement may be achieved by introducing the weighting of the results with the goal differential, which, although not rooted in a formal modeling principle, is compatible with the current algorithm and can be tuned to the characteristics of the football competition."
            },
            {
                "arxivId": "1912.02076",
                "title": "UEFA Against the Champions? An Evaluation of the Recent Reform of the Champions League Qualification",
                "abstract": "The paper evaluates the impact of the only reform in the Champions Path of UEFA Champions League qualifying system, effective from the 2018/19 season. In contrast to previous studies, our methodology considers five seasons instead of only one to filter out any possible season-specific attributes. The chances of some national champions decrease much stronger than suggested by the reduction in the number of available slots. Since the negative effects depend to a large extent on the arbitrary cutoffs in the access list, we propose to introduce some randomness into the determination of entry stages."
            },
            {
                "arxivId": "1801.06644",
                "title": "UEFA Champions League Entry Has Not Satisfied Strategyproofness in Three Seasons",
                "abstract": "This article investigates the qualification for the Union of European Football Association (UEFA) Champions League (CL), the most prestigious club competition in European football, with respect to the theoretical property of strategyproofness. We find that in three seasons (2015-2016, 2016-2017, and 2017-2018), the UEFA Europa League titleholder might have been better off by losing its match against the CL titleholder in their domestic championship. A straightforward solution is suggested in order to avoid the occurrence of this paradox. The use of an incentive compatible rule would have a real effect on the qualification in these three seasons of the UEFA CL."
            },
            {
                "arxivId": "1705.09575",
                "title": "Ranking soccer teams on basis of their current strength: a comparison of maximum likelihood approaches",
                "abstract": "We present ten different strength-based statistical models that we use to model soccer match outcomes with the aim of producing a new ranking. The models are of four main types: Thurstone-Mosteller, Bradley-Terry, Independent Poisson and Bivariate Poisson, and their common aspect is that the parameters are estimated via weighted maximum likelihood, the weights being a match importance factor and a time depreciation factor giving less weight to matches that are played a long time ago. Since our goal is to build a ranking reflecting the teams' current strengths, we compare the 10 models on basis of their predictive performance via the Rank Probability Score at the level of both domestic leagues and national teams. We find that the best models are the Bivariate and Independent Poisson models. We then illustrate the versatility and usefulness of our new rankings by means of three examples where the existing rankings fail to provide enough information or lead to peculiar results."
            },
            {
                "arxivId": "1507.05045",
                "title": "On the ranking of a Swiss system chess team tournament",
                "abstract": null
            },
            {
                "arxivId": "1508.06773",
                "title": "Ranking by pairwise comparisons for Swiss-system tournaments",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2311.02789",
        "category": "econ",
        "title": "Estimation and Inference for a Class of Generalized Hierarchical Models",
        "abstract": "In this paper, we consider estimation and inference for the unknown parameters and function involved in a class of generalized hierarchical models. Such models are of great interest in the literature of neural networks (such as Bauer and Kohler, 2019). We propose a rectified linear unit (ReLU) based deep neural network (DNN) approach, and contribute to the design of DNN by i) providing more transparency for practical implementation, ii) defining different types of sparsity, iii) showing the differentiability, iv) pointing out the set of effective parameters, and v) offering a new variant of rectified linear activation function (ReLU), etc. Asymptotic properties are established accordingly, and a feasible procedure for the purpose of inference is also proposed. We conduct extensive numerical studies to examine the finite-sample performance of the estimation methods, and we also evaluate the empirical relevance and applicability of the proposed models and estimation methods to real data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2402.13604",
        "category": "econ",
        "title": "Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE",
        "abstract": "This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We finetune a preexisting language model (CANINE) to do this automatically, thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall, and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history, and various related disciplines.",
        "references": [
            {
                "arxivId": "1912.01703",
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.00475",
        "category": "econ",
        "title": "Shill-Proof Auctions",
        "abstract": "In a single-item auction, a duplicitous seller may masquerade as one or more bidders in order to manipulate the clearing price. This paper characterizes auction formats that are shill-proof: a profit-maximizing seller has no incentive to submit any shill bids. We distinguish between strong shill-proofness, in which a seller with full knowledge of bidders' valuations can never profit from shilling, and weak shill-proofness, which requires only that the expected equilibrium profit from shilling is nonpositive. The Dutch auction (with suitable reserve) is the unique optimal and strongly shill-proof auction. Moreover, the Dutch auction (with no reserve) is the unique prior-independent auction that is both efficient and weakly shill-proof. While there are a multiplicity of strategy-proof, weakly shill-proof, and optimal auctions; any optimal auction can satisfy only two properties in the set {static, strategy-proof, weakly shill-proof}.",
        "references": [
            {
                "arxivId": "2004.01598",
                "title": "Credible, Truthful, and Two-Round (Optimal) Auctions via Cryptographic Commitments",
                "abstract": "We consider the sale of a single item to multiple buyers by a revenue-maximizing seller. Recent work of Akbarpour and Li formalizes credibility as an auction desideratum, and prove that the only optimal, credible, strategyproof auction is the ascending price auction with reserves. In contrast, when buyers' valuations are MHR, we show that the mild additional assumption of a cryptographically secure commitment scheme suffices for a simple two-round auction which is optimal, strategyproof, and credible (even when the number of bidders is only known by the auctioneer). We extend our analysis to the case when buyer valuations are \u03b1-strongly regular for any \u03b1 > 0, up to arbitrary \u03b5 in credibility. Interestingly, we also prove that this construction cannot be extended to regular distributions, nor can the \u03b5 be removed with multiple bidders."
            },
            {
                "arxivId": "1901.06830",
                "title": "StableFees: A Predictable Fee Market for Cryptocurrencie",
                "abstract": "Blockchain-based cryptocurrencies must solve the problem of assigning priorities to competing transactions. The most widely used mechanism involves each transaction offering a fee to be paid once the transaction is processed, but this discriminatory price mechanism fails to yield stable equilibria with predictable prices. We propose an alternate fee setting mechanism, StableFees, that is based on uniform price auctions. We prove that our proposed protocol is free from manipulation by users and miners as the number of users and miners increases and show empirically that gains from manipulation are small in practice. We show that StableFees reduces the fees paid by users and reduces the variance of fee income to miners. Data from December 2017 show that, if implemented, StableFees could have saved Bitcoin users $272,528,000 USD in transaction fees while reducing the variance of miner\u2019s fee income, on average, by a factor of 7.4. We argue that our fee protocol also has important social welfare and environmental benefits. This paper was accepted by Agostino Capponi, Special Issue of Management Science: Blockchains and Crypto Economics. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2023.4735 ."
            },
            {
                "arxivId": "1709.08881",
                "title": "Redesigning Bitcoin\u2019s Fee Market",
                "abstract": "The Bitcoin payment system involves two agent types: users that transact with the currency and pay fees and miners in charge of authorizing transactions and securing the system in return for these fees. Two of Bitcoin\u2019s challenges are (i) securing sufficient miner revenues as block rewards decrease, and (ii) alleviating the throughput limitation due to a small maximal block size cap. These issues are strongly related as increasing the maximal block size may decrease revenue due to Bitcoin\u2019s pay-your-bid approach. To decouple them, we analyze the \u201cmonopolistic auction\u201d [16], showing (i) its revenue does not decrease as the maximal block size increases, (ii) it is resilient to an untrusted auctioneer (the miner), and (iii) simplicity for transaction issuers (bidders), as the average gain from strategic bid shading (relative to bidding one\u2019s value) diminishes as the number of bids increases."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01333",
        "category": "econ",
        "title": "Methods of Stochastic Field Theory in Non-Equilibrium Systems --- Spontaneous Symmetry Breaking of Ergodicity",
        "abstract": "Recently, a couple of investigations related to symmetry breaking phenomena, 'spontaneous stochasticity' and 'ergodicity breaking' have led to significant impacts in a variety of fields related to the stochastic processes such as economics and finance. We investigate on the origins and effects of those original symmetries in the action from the mathematical and the effective field theory points of view. It is naturally expected that whenever the system respects any symmetry, it would be spontaneously broken once the system falls into a vacuum state which minimizes an effective action of the dynamical system.",
        "references": [
            {
                "arxivId": "2310.03526",
                "title": "Multifractal dimensions for orthogonal-to-unitary crossover ensemble.",
                "abstract": "Multifractal analysis is a powerful approach for characterizing ergodic or localized nature of eigenstates in complex quantum systems. In this context, the eigenvectors of random matrices belonging to invariant ensembles naturally serve as models for ergodic states. However, it has been found that the finite-size versions of multifractal dimensions for these eigenvectors converge to unity logarithmically slowly with increasing system size N. In fact, this strong finite-size effect is capable of distinguishing the ergodicity behavior of orthogonal and unitary invariant classes. Motivated by this observation, in this work, we provide semi-analytical expressions for the ensemble-averaged multifractal dimensions associated with eigenvectors in the orthogonal-to-unitary crossover ensemble. Additionally, we explore shifted and scaled variants of multifractal dimensions, which, in contrast to the multifractal dimensions themselves, yield distinct values in the orthogonal and unitary limits as N\u2192\u221e and, therefore, may serve as a convenient measure for studying the crossover. We substantiate our results using Monte Carlo simulations of the underlying crossover random matrix model. We then apply our results to analyze the multifractal dimensions in a quantum kicked rotor, a Sinai billiard system, and a correlated spin-chain model in a random field. The orthogonal-to-unitary crossover in these systems is realized by tuning relevant system parameters, and we find that in the crossover regime, the observed finite-dimension multifractal dimensions can be captured very well with our results."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01404",
        "category": "econ",
        "title": "Symmetric mechanisms for two-sided matching problems",
        "abstract": "We focus on the basic one-to-one two-sided matching model, where there are two disjoint sets of agents of equal size, and each agent in a set has preferences on the agents in the other set, modelled by linear orders. The goal is to find a matching that associates each agent in one set with one and only one agent in the other set based on the agents' preferences. A mechanism is a rule that associates a set of matchings to each preference profile. Stability, which refers to the capability to select only stable matchings, is an important property a mechanism should fulfill. Another crucial property, especially useful for applications, is resoluteness, which requires that the mechanism always selects a unique matching. The two versions of the deferred acceptance algorithm are examples of stable and resolute mechanisms. However, these mechanisms are severely unfair since they strongly favor one of the two sides of the market. In this paper, we introduce a property that mechanisms may meet which relates to fairness. Such property, called symmetry, is formulated in a way able to capture different levels of fairness within and across the two sets of agents and generalize existing notions. We prove several possibility and impossibility results, mainly involving the most general notion of symmetry, known as gender fairness: among others, a resolute and gender fair mechanism exists if and only if each side of the market consists of an odd number of agents; there exists no resolute, stable and gender fair mechanism.",
        "references": [
            {
                "arxivId": "2001.10875",
                "title": "Algorithms for new types of fair stable matchings",
                "abstract": "We study the problem of finding \"fair\" stable matchings in the Stable Marriage problem with Incomplete lists (SMI). For an instance $I$ of SMI there may be many stable matchings, providing significantly different outcomes for the sets of men and women. We introduce two new notions of fairness in SMI. Firstly, a regret-equal stable matching minimises the difference in ranks of a worst-off man and a worst-off woman, among all stable matchings. Secondly, a min-regret sum stable matching minimises the sum of ranks of a worst-off man and a worst-off woman, among all stable matchings. We present two new efficient algorithms to find stable matchings of these types. Firstly, the Regret-Equal Degree Iteration Algorithm finds a regret-equal stable matching in $O(d_0 nm)$ time, where $d_0$ is the absolute difference in ranks between a worst-off man and a worst-off woman in the man-optimal stable matching, $n$ is the number of men or women, and $m$ is the total length of all preference lists. Secondly, the Min-Regret Sum Algorithm finds a min-regret sum stable matching in $O(d_s m)$ time, where $d_s$ is the difference in the ranks between a worst-off man in each of the woman-optimal and man-optimal stable matchings. Experiments to compare several types of fair optimal stable matchings were conducted and show that the Regret-Equal Degree Iteration Algorithm produces matchings that are competitive with respect to other fairness objectives. On the other hand, existing types of \"fair\" stable matchings did not provide as close an approximation to regret-equal stable matchings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01495",
        "category": "econ",
        "title": "Estimating Heterogeneous Effects: Applications to Labor Economics",
        "abstract": "A growing number of applications involve settings where, in order to infer heterogeneous effects, a researcher compares various units. Examples of research designs include children moving between different neighborhoods, workers moving between firms, patients migrating from one city to another, and banks offering loans to different firms. We present a unified framework for these settings, based on a linear model with normal random coefficients and normal errors. Using the model, we discuss how to recover the mean and dispersion of effects, other features of their distribution, and to construct predictors of the effects. We provide moment conditions on the model's parameters, and outline various estimation strategies. A main objective of the paper is to clarify some of the underlying assumptions by highlighting their economic content, and to discuss and inform some of the key practical choices.",
        "references": [
            {
                "arxivId": "1806.01494",
                "title": "Leave-Out Estimation of Variance Components",
                "abstract": "We propose leave-out estimators of quadratic forms designed for the study of linear models with unrestricted heteroscedasticity. Applications include analysis of variance and tests of linear restrictions in models with many regressors. An approximation algorithm is provided that enables accurate computation of the estimator in very large datasets. We study the large sample properties of our estimator allowing the number of regressors to grow in proportion to the number of observations. Consistency is established in a variety of settings where plug-in methods and estimators predicated on homoscedasticity exhibit first-order biases. For quadratic forms of increasing rank, the limiting distribution can be represented by a linear combination of normal and non-central $\\chi^2$ random variables, with normality ensuing under strong identification. Standard error estimators are proposed that enable tests of linear restrictions and the construction of uniformly valid confidence intervals for quadratic forms of interest. We find in Italian social security records that leave-out estimates of a variance decomposition in a two-way fixed effects model of wage determination yield substantially different conclusions regarding the relative contribution of workers, firms, and worker-firm sorting to wage inequality than conventional methods. Monte Carlo exercises corroborate the accuracy of our asymptotic approximations, with clear evidence of non-normality emerging when worker mobility between blocks of firms is limited."
            },
            {
                "arxivId": "2102.02124",
                "title": "Discretizing Unobserved Heterogeneity",
                "abstract": "We study discrete panel data methods where unobserved heterogeneity is revealed in a first step, in environments where population heterogeneity is not discrete. We focus on \n two\u2010step grouped fixed\u2010effects (GFE) estimators, where individuals are first classified into groups using \n kmeans clustering, and the model is then estimated allowing for group\u2010specific heterogeneity. Our framework relies on two key properties: heterogeneity is a function\u2014possibly nonlinear and time\u2010varying\u2014of a low\u2010dimensional continuous latent type, and informative moments are available for classification. We illustrate the method in a model of wages and labor market participation, and in a probit model with time\u2010varying heterogeneity. We derive asymptotic expansions of two\u2010step GFE estimators as the number of groups grows with the two dimensions of the panel. We propose a data\u2010driven rule for the number of groups, and discuss bias reduction and inference.\n"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01498",
        "category": "econ",
        "title": "Existence, uniqueness, and regularity of solutions to nonlinear and non-smooth parabolic obstacle problems",
        "abstract": "We establish the existence, uniqueness, and $W^{1,2,p}$-regularity of solutions to fully nonlinear parabolic obstacle problems when the obstacle is the pointwise supremum of arbitrary functions in $W^{1,2,p}$ and the operator is only assumed to be measurable in the state and time variables. The results hold for a large class of non-smooth obstacles, including all convex obstacles. Applied to stopping problems, they imply that the decision maker never stops at a convex kink of the stopping payoff. The proof relies on new $W^{1,2,p}$-estimates for obstacle problems where the obstacle is the maximum of finitely many functions in $W^{1,2,p}$.",
        "references": [
            {
                "arxivId": "1705.02400",
                "title": "On the existence of $W^{1,2}_{p}$ solutions for fully nonlinear parabolic equations under either relaxed or no convexity assumptions",
                "abstract": "We establish the existence of solutions of fully nonlinear parabolic second-order equations like $\\partial_{t}u+H(v,Dv,D^{2}v,t,x)=0$ in smooth cylinders without requiring $H$ to be convex or concave with respect to the second-order derivatives. Apart from ellipticity nothing is required of $H$ at points at which $|D^{2}v|\\leq K$, where $K$ is any fixed constant. For large $|D^{2}v|$ some kind of relaxed convexity assumption with respect to $D^{2}v$ mixed with a VMO condition with respect to $t,x$ are still imposed. The solutions are sought in Sobolev classes. We also establish the solvability without almost any conditions on $H$, apart from ellipticity, but of a \"cut-off\" version of the equation $\\partial_{t}u+H(v,Dv,D^{2}v,t,x)=0$."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01566",
        "category": "econ",
        "title": "Heterogeneous Treatment Effects and Causal Mechanisms",
        "abstract": "The credibility revolution advances the use of research designs that permit identification and estimation of causal effects. However, understanding which mechanisms produce measured causal effects remains a challenge. A dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects with respect to pre-treatment covariates. This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism. We show first that this design cannot provide evidence of mechanism activation without additional, generally implicit, assumptions. Further, even when these assumptions are satisfied, if a measured outcome is produced by a non-linear transformation of a directly-affected outcome of theoretical interest, heterogeneous treatment effects are not informative of mechanism activation. We provide novel guidance for interpretation and research design in light of these findings.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01641",
        "category": "econ",
        "title": "The impact of geopolitical risk on the international agricultural market: Empirical analysis based on the GJR-GARCH-MIDAS model",
        "abstract": "The current international landscape is turbulent and unstable, with frequent outbreaks of geopolitical conflicts worldwide. Geopolitical risk has emerged as a significant threat to regional and global peace, stability, and economic prosperity, causing serious disruptions to the global food system and food security. Focusing on the international food market, this paper builds different dimensions of geopolitical risk measures based on the random matrix theory and constructs single- and two-factor GJR-GARCH-MIDAS models with fixed time span and rolling window, respectively, to investigate the impact of geopolitical risk on food market volatility. The findings indicate that modeling based on rolling window performs better in describing the overall volatility of the wheat, maize, soybean, and rice markets, and the two-factor models generally exhibit stronger explanatory power in most cases. In terms of short-term fluctuations, all four staple food markets demonstrate obvious volatility clustering and high volatility persistence, without significant asymmetry. Regarding long-term volatility, the realized volatility of wheat, maize, and soybean significantly exacerbates their long-run market volatility. Additionally, geopolitical risks of different dimensions show varying directions and degrees of effects in explaining the long-term market volatility of the four staple food commodities. This study contributes to the understanding of the macro-drivers of food market fluctuations, provides useful information for investment using agricultural futures, and offers valuable insights into maintaining the stable operation of food markets and safeguarding global food security.",
        "references": [
            {
                "arxivId": "1405.5000",
                "title": "Correlation structure and principal components in the global crude oil market",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.01782",
        "category": "econ",
        "title": "Multicriteria Analysis Model in Sustainable Corn Farming Area Planning",
        "abstract": "This study aims to develop a framework for multicriteria analysis to evaluate alternatives for sustainable corn agricultural area planning, considering the integration of ecological, economic, and social aspects as pillars of sustainability. The research method uses qualitative and quantitative approaches to integrate ecological, economic, and social aspects in the multicriteria analysis. The analysis involves land evaluation, subcriteria identification, and data integration using Multidimensional Scaling and Analytical Hierarchy Process methods to prioritize developing sustainable corn agricultural areas. Based on the results of the RAP-Corn analysis, it indicates that the ecological dimension depicts less sustainability. The AHP results yield weight distribution and highly relevant scores that describe tangible preferences. Priority directions are grouped as strategic steps toward achieving the goals of sustainable corn agricultural area planning.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.02141",
        "category": "econ",
        "title": "Robustly estimating heterogeneity in factorial data using Rashomon Partitions",
        "abstract": "Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like geometry. RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-03.json",
        "arxivId": "2404.02142",
        "category": "econ",
        "title": "Priority-Neutral Matching Lattices Are Not Distributive",
        "abstract": "Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure. However, in school-choice problems, stable matchings are not Pareto optimal for the students. Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching. Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice. We study the structure of the priority-neutral lattice. Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice. In particular, we show that the priority-neutral lattice need not be distributive. Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answering an open question. This show that many widely-used properties of stable matchings fail for priority-neutral matchings; in particular, the set of priority-neutral matchings cannot be represented by via a partial ordering on a set of rotations. However, by proving a novel structural property of the set of priority-neutral matchings, we also show that not every lattice arises as a priority-neutral lattice, which suggests that the exact nature of the family of priority-neutral lattices may be subtle.",
        "references": [
            {
                "arxivId": "2212.07108",
                "title": "School Choice with Farsighted Students",
                "abstract": "We consider priority-based school choice problems with farsighted students. We show that a singleton set consisting of the matching obtained from the Top Trading Cycles (TTC) mechanism is a farsighted stable set. However, the matching obtained from the Deferred Acceptance (DA) mechanism may not belong to any farsighted stable set. Hence, the TTC mechanism provides an assignment that is not only Pareto efficient but also farsightedly stable. Moreover, looking forward three steps ahead is already sufficient for stabilizing the matching obtained from the TTC."
            },
            {
                "arxivId": "1910.04401",
                "title": "Representing All Stable Matchings by Walking a Maximal Chain",
                "abstract": "The seminal book of Gusfield and Irving [GI89] provides a compact and algorithmically useful way to represent the collection of stable matches corresponding to a given set of preferences. In this paper, we reinterpret the main results of [GI89], giving a new proof of the characterization which is able to bypass a lot of the \"theory building\" of the original works. We also provide a streamlined and efficient way to compute this representation. Our proofs and algorithms emphasize the connection to well-known properties of the deferred acceptance algorithm."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-04.json",
        "arxivId": "2403.19439",
        "category": "econ",
        "title": "Dynamic Analyses of Contagion Risk and Module Evolution on the SSE A-Shares Market Based on Minimum Information Entropy",
        "abstract": "The interactive effect is significant in the Chinese stock market, exacerbating the abnormal market volatilities and risk contagion. Based on daily stock returns in the Shanghai Stock Exchange (SSE) A-shares, this paper divides the period between 2005 and 2018 into eight bull and bear market stages to investigate interactive patterns in the Chinese financial market. We employ the Least Absolute Shrinkage and Selection Operator (LASSO) method to construct the stock network, compare the heterogeneity of bull and bear markets, and further use the Map Equation method to analyse the evolution of modules in the SSE A-shares market. Empirical results show that (1) the connected effect is more significant in bear markets than bull markets and gives rise to abnormal volatilities in the stock market; (2) a system module can be found in the network during the first four stages, and the industry aggregation effect leads to module differentiation in the last four stages; (3) some stocks have leading effects on others throughout eight periods, and medium- and small-cap stocks with poor financial conditions are more likely to become risk sources, especially in bear markets. Our conclusions are beneficial to improving investment strategies and making regulatory policies.",
        "references": [
            {
                "arxivId": "2002.07100",
                "title": "Crisis contagion in the world trade network",
                "abstract": null
            },
            {
                "arxivId": "1903.01820",
                "title": "Influence of petroleum and gas trade on EU economies from the reduced Google matrix analysis of UN COMTRADE data",
                "abstract": null
            },
            {
                "arxivId": "1202.0409",
                "title": "Correlation and network analysis of global financial indices.",
                "abstract": "Random matrix theory (RMT) and network methods are applied to investigate the correlation and network properties of 20 financial indices. The results are compared before and during the financial crisis of 2008. In the RMT method, the components of eigenvectors corresponding to the second largest eigenvalue form two clusters of indices in the positive and negative directions. The components of these two clusters switch in opposite directions during the crisis. The network analysis uses the Fruchterman-Reingold layout to find clusters in the network of indices at different thresholds. At a threshold of 0.6, before the crisis, financial indices corresponding to the Americas, Europe, and Asia-Pacific form separate clusters. On the other hand, during the crisis at the same threshold, the American and European indices combine together to form a strongly linked cluster while the Asia-Pacific indices form a separate weakly linked cluster. If the value of the threshold is further increased to 0.9 then the European indices (France, Germany, and the United Kingdom) are found to be the most tightly linked indices. The structure of the minimum spanning tree of financial indices is more starlike before the crisis and it changes to become more chainlike during the crisis. The average linkage hierarchical clustering algorithm is used to find a clearer cluster structure in the network of financial indices. The cophenetic correlation coefficients are calculated and found to increase significantly, which indicates that the hierarchy increases during the financial crisis. These results show that there is substantial change in the structure of the organization of financial indices during a financial crisis."
            },
            {
                "arxivId": "1105.0257",
                "title": "Map equation for link communities.",
                "abstract": "Community structure exists in many real-world networks and has been reported being related to several functional properties of the networks. The conventional approach was partitioning nodes into communities, while some recent studies start partitioning links instead of nodes to find overlapping communities of nodes efficiently. We extended the map equation method, which was originally developed for node communities, to find link communities in networks. This method is tested on various kinds of networks and compared with the metadata of the networks, and the results show that our method can identify the overlapping role of nodes effectively. The advantage of this method is that the node community scheme and link community scheme can be compared quantitatively by measuring the unknown information left in the networks besides the community structure. It can be used to decide quantitatively whether or not the link community scheme should be used instead of the node community scheme. Furthermore, this method can be easily extended to the directed and weighted networks since it is based on the random walk."
            },
            {
                "arxivId": "0906.1405",
                "title": "The map equation",
                "abstract": null
            },
            {
                "arxivId": "0707.0609",
                "title": "Maps of random walks on complex networks reveal community structure",
                "abstract": "To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of >6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the network\u2014including physics, chemistry, molecular biology, and medicine\u2014information flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences."
            },
            {
                "arxivId": "physics/0612035",
                "title": "An information-theoretic framework for resolving community structure in complex networks",
                "abstract": "To understand the structure of a large-scale biological, social, or technological network, it can be helpful to decompose the network into smaller subunits or modules. In this article, we develop an information-theoretic foundation for the concept of modularity in networks. We identify the modules of which the network is composed by finding an optimal compression of its topology, capitalizing on regularities in its structure. We explain the advantages of this approach and illustrate them by partitioning a number of real-world and model networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-04.json",
        "arxivId": "2404.02273",
        "category": "econ",
        "title": "Beyond Social Media Analogues",
        "abstract": "The steady flow of social-media cases toward the Supreme Court shows a nation reworking its fundamental relationship with technology. The cases raise a host of questions ranging from difficult to impossible: how to nurture a vibrant public square when a few tech giants dominate the flow of information, how social media can be at the same time free from conformist groupthink and also protected against harmful disinformation campaigns, and how government and industry can cooperate on such problems without devolving toward censorship. To such profound questions, this Essay offers a comparatively modest contribution -- what not to do. Always the lawyer's instinct is toward analogy, considering what has come before and how it reveals what should come next. Almost invariably, that is the right choice. The law's cautious evolution protects society from disruptive change. But almost is not always, and, with social media, disruptive change is already upon us. Using social-media laws from Texas and Florida as a case study, this Essay shows how social-media's distinct features render it poorly suited to analysis by analogy and argues that courts should instead shift their attention toward crafting legal doctrines targeted to address social media's unique ills.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-04.json",
        "arxivId": "2404.02498",
        "category": "econ",
        "title": "From Time-inconsistency to Time-consistency for Optimal Stopping Problems",
        "abstract": "For optimal stopping problems with time-inconsistent preference, we measure the inherent level of time-inconsistency by taking the time needed to turn the naive strategies into the sophisticated ones. In particular, when in a repeated experiment the naive agent can observe her actual sequence of actions which are inconsistent with what she has planned at the initial time, she then chooses her immediate action based on the observations on her later actual behavior. The procedure is repeated until her actual sequence of actions are consistent with her plan at any time. We show that for the preference value of cumulative prospect theory, in which the time-inconsistency is due to the probability distortion, the higher the degree of probability distortion, the more severe the level of time-inconsistency, and the more time required to turn the naive strategies into the sophisticated ones.",
        "references": [
            {
                "arxivId": "1709.03535",
                "title": "General stopping behaviors of na\u00efve and noncommitted sophisticated agents, with application to probability distortion",
                "abstract": "We consider the problem of stopping a diffusion process with a payoff functional that renders the problem time\u2010inconsistent. We study stopping decisions of na\u00efve agents who reoptimize continuously in time, as well as equilibrium strategies of sophisticated agents who anticipate but lack control over their future selves' behaviors. When the state process is one dimensional and the payoff functional satisfies some regularity conditions, we prove that any equilibrium can be obtained as a fixed point of an operator. This operator represents strategic reasoning that takes the future selves' behaviors into account. We then apply the general results to the case when the agents distort probability and the diffusion process is a geometric Brownian motion. The problem is inherently time\u2010inconsistent as the level of distortion of a same event changes over time. We show how the strategic reasoning may turn a na\u00efve agent into a sophisticated one. Moreover, we derive stopping strategies of the two types of agent for various parameter specifications of the problem, illustrating rich behaviors beyond the extreme ones such as \u201cnever\u2010stopping\u201d or \u201cnever\u2010starting.\u201d"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-04.json",
        "arxivId": "2404.02671",
        "category": "econ",
        "title": "Bayesian Bi-level Sparse Group Regressions for Macroeconomic Forecasting",
        "abstract": "We propose a Machine Learning approach for optimal macroeconomic forecasting in a high-dimensional setting with covariates presenting a known group structure. Our model encompasses forecasting settings with many series, mixed frequencies, and unknown nonlinearities. We introduce in time-series econometrics the concept of bi-level sparsity, i.e. sparsity holds at both the group level and within groups, and we assume the true model satisfies this assumption. We propose a prior that induces bi-level sparsity, and the corresponding posterior distribution is demonstrated to contract at the minimax-optimal rate, recover the model parameters, and have a support that includes the support of the model asymptotically. Our theory allows for correlation between groups, while predictors in the same group can be characterized by strong covariation as well as common characteristics and patterns. Finite sample performance is illustrated through comprehensive Monte Carlo experiments and a real-data nowcasting exercise of the US GDP growth rate.",
        "references": [
            {
                "arxivId": "2005.14057",
                "title": "Machine Learning Time Series Regressions With an Application to Nowcasting",
                "abstract": "Abstract This article introduces structured machine learning regressions for high-dimensional time series data potentially sampled at different frequencies. The sparse-group LASSO estimator can take advantage of such time series data structures and outperforms the unstructured LASSO. We establish oracle inequalities for the sparse-group LASSO estimator within a framework that allows for the mixing processes and recognizes that the financial and the macroeconomic data may have heavier than exponential tails. An empirical application to nowcasting US GDP growth indicates that the estimator performs favorably compared to other alternatives and that text data can be a useful addition to more traditional numerical data. Our methodology is implemented in the R package midasml, available from CRAN."
            },
            {
                "arxivId": "1903.08025",
                "title": "Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction",
                "abstract": "We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. In particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We establish good frequentist asymptotic properties of the posterior of the in-sample and out-of-sample prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. Simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. When applied to forecasting U.S. GDP, our penalized regressions can outperform many strong competitors. Results suggest that financial variables may have some, although very limited, short-term predictive content."
            },
            {
                "arxivId": "1807.03439",
                "title": "Bayesian linear regression for multivariate responses under group sparsity",
                "abstract": "We study frequentist properties of a Bayesian high-dimensional multivariate linear regression model with correlated responses. The predictors are separated into many groups and the group structure is pre-determined. Two features of the model are unique: (i) group sparsity is imposed on the predictors. (ii) the covariance matrix is unknown and its dimensions can also be high. We choose a product of independent spike-and-slab priors on the regression coefficients and a new prior on the covariance matrix based on its eigendecomposition. Each spike-and-slab prior is a mixture of a point mass at zero and a multivariate density involving a $\\ell_{2,1}$-norm. We first obtain the posterior contraction rate, the bounds on the effective dimension of the model with high posterior probabilities. We then show that the multivariate regression coefficients can be recovered under certain compatibility conditions. Finally, we quantify the uncertainty for the regression coefficients with frequentist validity through a Bernstein-von Mises type theorem. The result leads to selection consistency for the Bayesian method. We derive the posterior contraction rate using the general theory by constructing a suitable test from the first principle using moment bounds for certain likelihood ratios. This leads to posterior concentration around the truth with respect to the average Renyi divergence of order 1/2. This technique of obtaining the required tests for posterior contraction rate could be useful in many other problems."
            },
            {
                "arxivId": "1611.01310",
                "title": "Achieving shrinkage in a time-varying parameter model framework",
                "abstract": null
            },
            {
                "arxivId": "1512.01013",
                "title": "Bayesian Variable Selection and Estimation for Group Lasso",
                "abstract": "The paper revisits the Bayesian group lasso and uses spike and slab priors for group variable selection. In the process, the connection of our model with penalized regression is demonstrated, and the role of posterior median for thresholding is pointed out. We show that the posterior median estimator has the oracle property for group variable selection and estimation under orthogonal designs, while the group lasso has suboptimal asymptotic estimation rate when variable selection consistency is achieved. Next we consider bi-level selection problem and propose the Bayesian sparse group selection again with spike and slab priors to select variables both at the group level and also within a group. We demonstrate via simulation that the posterior median estimator of our spike and slab models has excellent performance for both variable selection and estimation."
            },
            {
                "arxivId": "1403.0735",
                "title": "BAYESIAN LINEAR REGRESSION WITH SPARSE PRIORS",
                "abstract": "We study full Bayesian procedures for high-dimensional linear regression under sparsity constraints. The prior is a mixture of point masses at zero and continuous distributions. Under compatibility conditions on the design matrix, the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector, and to give optimal prediction of the response vector. It is also shown to select the correct sparse model, or at least the coefficients that are significantly different from zero. The asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification."
            },
            {
                "arxivId": "1305.5270",
                "title": "On adaptive posterior concentration rates",
                "abstract": "tion of Holder balls and that moreover achieve our lower bound. We analyse the consequences in terms of asymptotic behaviour of poste- rior credible balls as well as frequentist minimax adaptive estimation. Our results are appended with an upper bound for the contraction rate under an arbitrary loss in a generic regular experiment. The up- per bound is attained for certain sieve priors and enables to extend our results to density estimation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-04.json",
        "arxivId": "2404.02687",
        "category": "econ",
        "title": "Karma: An Experimental Study",
        "abstract": "A system of non-tradable credits that flow between individuals like karma, hence proposed under that name, is a mechanism for repeated resource allocation that comes with attractive efficiency and fairness properties, in theory. In this study, we test karma in an online experiment in which human subjects repeatedly compete for a resource with time-varying and stochastic individual preferences or urgency to acquire the resource. We confirm that karma has significant and sustained welfare benefits even in a population with no prior training. We identify mechanism usage in contexts with sporadic high urgency, more so than with frequent moderate urgency, and implemented as an easy (binary) karma bidding scheme as particularly effective for welfare improvements: relatively larger aggregate efficiency gains are realized that are (almost) Pareto superior. These findings provide guidance for further testing and for future implementation plans of such mechanisms in the real world.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-04.json",
        "arxivId": "2404.02756",
        "category": "econ",
        "title": "Chasing Contests",
        "abstract": "This paper proposes a dynamic research contest, namely the chasing contest, in which two asymmetric contestants exert costly effort to accomplish two breakthroughs in a continuous-time framework. The contestants are asymmetric in that one of them is present-biased and has already achieved one breakthrough (the leader), while the other is time-consistent and needs to achieve two breakthroughs to win (the chaser). The principal can choose between two disclosure policies, which either immediately announces the first breakthrough of the chaser (public chasing contest), or only announces the contest results until its terminus (hidden chasing contest). We fully characterize the unique x-start and y-stop equilibrium of the chasing contest with the two disclosure policies, in which the leader starts working from an instant x to the end while the chaser stops exerting effort by the instant y. We further compare the asymmetric incentives of the two disclosure policies. Our results imply that the chaser will never stop earlier in the hidden chasing contest compared to its public chasing counterpart, whereas the leader works longer in the public chasing contest only if the contest permits a late deadline.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-05.json",
        "arxivId": "2305.17615",
        "category": "econ",
        "title": "Estimating overidentified linear models with heteroskedasticity and outliers",
        "abstract": "Overidentified two-stage least square (TSLS) is commonly adopted by applied economists to address endogeneity. Though it potentially gives more efficient or informative estimate, overidentification comes with a cost. The bias of TSLS is severe when the number of instruments is large. Hence, Jackknife Instrumental Variable Estimator (JIVE) has been proposed to reduce bias of overidentified TSLS. A conventional heuristic rule to assess the performance of TSLS and JIVE is approximate bias. This paper formalizes this concept and applies the new definition of approximate bias to three classes of estimators that bridge between OLS, TSLS and a variant of JIVE, namely, JIVE1. Three new approximately unbiased estimators are proposed. They are called AUK, TSJI1 and UOJIVE. Interestingly, a previously proposed approximately unbiased estimator UIJIVE can be viewed as a special case of UOJIVE. While UIJIVE is approximately unbiased asymptotically, UOJIVE is approximately unbiased even in finite sample. Moreover, UOJIVE estimates parameters for both endogenous and control variables whereas UIJIVE only estimates the parameter of the endogenous variables. TSJI1 and UOJIVE are consistent and asymptotically normal under fixed number of instruments. They are also consistent under many-instrument asymptotics. This paper characterizes a series of moment existence conditions to establish all asymptotic results. In addition, the new estimators demonstrate good performances with simulated and empirical datasets.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-05.json",
        "arxivId": "2306.05299",
        "category": "econ",
        "title": "Heterogeneous Autoregressions in Short T Panel Data Models",
        "abstract": "This paper considers a first-order autoregressive panel data model with individual-specific effects and heterogeneous autoregressive coefficients defined on the interval (-1,1], thus allowing for some of the individual processes to have unit roots. It proposes estimators for the moments of the cross-sectional distribution of the autoregressive (AR) coefficients, assuming a random coefficient model for the autoregressive coefficients without imposing any restrictions on the fixed effects. It is shown the standard generalized method of moments estimators obtained under homogeneous slopes are biased. Small sample properties of the proposed estimators are investigated by Monte Carlo experiments and compared with a number of alternatives, both under homogeneous and heterogeneous slopes. It is found that a simple moment estimator of the mean of heterogeneous AR coefficients performs very well even for moderate sample sizes, but to reliably estimate the variance of AR coefficients much larger samples are required. It is also required that the true value of this variance is not too close to zero. The utility of the heterogeneous approach is illustrated in the case of earnings dynamics.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-05.json",
        "arxivId": "2401.14582",
        "category": "econ",
        "title": "High-Dimensional Forecasting with Known Knowns and Known Unknowns",
        "abstract": "Forecasts play a central role in decision making under uncertainty. After a brief review of the general issues, this paper considers ways of using high-dimensional data in forecasting. We consider selecting variables from a known active set, known knowns, using Lasso and OCMT, and approximating unobserved latent factors, known unknowns, by various means. This combines both sparse and dense approaches. We demonstrate the various issues involved in variable selection in a high-dimensional setting with an application to forecasting UK inflation at different horizons over the period 2020q1-2023q1. This application shows both the power of parsimonious models and the importance of allowing for global variables.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-05.json",
        "arxivId": "2404.03235",
        "category": "econ",
        "title": "Marginal Treatment Effects and Monotonicity",
        "abstract": "How robust are analyses based on marginal treatment effects (MTE) to violations of Imbens and Angrist (1994) monotonicity? In this note, I present weaker forms of monotonicity under which popular MTE-based estimands still identify the parameters of interest.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-05.json",
        "arxivId": "2404.03319",
        "category": "econ",
        "title": "Early warning systems for financial markets of emerging economies",
        "abstract": "We develop and apply a new online early warning system (EWS) for what is known in machine learning as concept drift, in economics as a regime shift and in statistics as a change point. The system goes beyond linearity assumed in many conventional methods, and is robust to heavy tails and tail-dependence in the data, making it particularly suitable for emerging markets. The key component is an effective change-point detection mechanism for conditional entropy of the data, rather than for a particular indicator of interest. Combined with recent advances in machine learning methods for high-dimensional random forests, the mechanism is capable of finding significant shifts in information transfer between interdependent time series when traditional methods fail. We explore when this happens using simulations and we provide illustrations by applying the method to Uzbekistan's commodity and equity markets as well as to Russia's equity market in 2021-2023.",
        "references": [
            {
                "arxivId": "2109.09692",
                "title": "Modeling Regime Shifts in Multiple Time Series",
                "abstract": "We investigate the problem of discovering and modeling regime shifts in an ecosystem comprising multiple time series known as co-evolving time series. Regime shifts refer to the changing behaviors exhibited by series at different time intervals. Learning these changing behaviors is a key step toward time series forecasting. While advances have been made, existing methods suffer from one or more of the following shortcomings: (1) failure to take relationships between time series into consideration for discovering regimes in multiple time series; (2) lack of an effective approach that models time-dependent behaviors exhibited by series; (3) difficulties in handling data discontinuities which may be informative. Most of the existing methods are unable to handle all of these three issues in a unified framework. This, therefore, motivates our effort to devise a principled approach for modeling interactions and time-dependency in co-evolving time series. Specifically, we model an ecosystem of multiple time series by summarizing the heavy ensemble of time series into a lighter and more meaningful structure called a mapping grid. By using the mapping grid, our model first learns time series behavioral dependencies through a dynamic network representation, then learns the regime transition mechanism via a full time-dependent Cox regression model. The originality of our approach lies in modeling interactions between time series in regime identification and in modeling time-dependent regime transition probabilities, usually assumed to be static in existing work."
            },
            {
                "arxivId": "2007.05432",
                "title": "Reactive Soft Prototype Computing for Concept Drift Streams",
                "abstract": null
            },
            {
                "arxivId": "1807.11408",
                "title": "Local Linear Forests",
                "abstract": "Abstract Random forests are a powerful method for nonparametric regression, but are limited in their ability to fit smooth signals. Taking the perspective of random forests as an adaptive kernel method, we pair the forest kernel with a local linear regression adjustment to better capture smoothness. The resulting procedure, local linear forests, enables us to improve on asymptotic rates of convergence for random forests with smooth signals, and provides substantial gains in accuracy on both real and simulated data. We prove a central limit theorem valid under regularity conditions on the forest and smoothness constraints, and propose a computationally efficient construction for confidence intervals. Moving to a causal inference application, we discuss the merits of local regression adjustments for heterogeneous treatment effect estimation, and give an example on a dataset exploring the effect word choice has on attitudes to the social safety net. Last, we include simulation results on real and generated data. A software implementation is available in the R package grf. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1804.05753",
                "title": "RFCDE: Random Forests for Conditional Density Estimation",
                "abstract": "Random forests is a common non-parametric regression technique which performs well for mixed-type data and irrelevant covariates, while being robust to monotonic variable transformations. Existing random forest implementations target regression or classification. We introduce the RFCDE package for fitting random forest models optimized for nonparametric conditional density estimation, including joint densities for multiple responses. This enables analysis of conditional probability distributions which is useful for propagating uncertainty and of joint distributions that describe relationships between multiple responses and covariates. RFCDE is released under the MIT open-source license and can be accessed at this https URL . Both R and Python versions, which call a common C++ library, are available."
            },
            {
                "arxivId": "1804.03515",
                "title": "Hyperparameters and tuning strategies for random forest",
                "abstract": "The random forest (RF) algorithm has several hyperparameters that have to be set by the user, for example, the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain, and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a presenting brief overview of tuning strategies, we demonstrate the application of one of the most established tuning strategies, model\u2010based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters."
            },
            {
                "arxivId": "1707.00167",
                "title": "Asymptotic distribution-free change-point detection for multivariate and non-Euclidean data",
                "abstract": "We consider the testing and estimation of change-points, locations where the distribution abruptly changes, in a sequence of multivariate or non-Euclidean observations. We study a nonparametric framework that utilizes similarity information among observations, which can be applied to various data types as long as an informative similarity measure on the sample space can be defined. The existing approach along this line has low power and/or biased estimates for change-points under some common scenarios. We address these problems by considering new tests based on similarity information. Simulation studies show that the new approaches exhibit substantial improvements in detecting and estimating change-points. In addition, under some mild conditions, the new test statistics are asymptotically distribution free under the null hypothesis of no change. Analytic p-value approximations to the significance of the new test statistics for the single change-point alternative and changed interval alternative are derived, making the new approaches easy off-the-shelf tools for large datasets. The new approaches are illustrated in an analysis of New York taxi data."
            },
            {
                "arxivId": "1604.03611",
                "title": "Sequential change-point detection based on nearest neighbors",
                "abstract": "We propose a new framework for the detection of change-points in online, sequential data analysis. The approach utilizes nearest neighbor information and can be applied to sequences of multivariate observations or non-Euclidean data objects, such as network data. Different stopping rules are explored, and one specific rule is recommended due to its desirable properties. An accurate analytic approximation of the average run length is derived for the recommended rule, making it an easy off-the-shelf approach for real multivariate/object sequential data monitoring applications. Simulations reveal that the new approach has better performance than likelihood-based approaches for high dimensional data. The new approach is illustrated through a real dataset in detecting global structural changes in social networks."
            },
            {
                "arxivId": "1510.03827",
                "title": "On Asymptotic Optimality in Sequential Changepoint Detection: Non-iid Case",
                "abstract": "We consider a sequential Bayesian changepoint detection problem for a general stochastic model, assuming that the observed data may be dependent and non-identically distributed and the prior distribution of the change point is arbitrary, not necessarily geometric. Tartakovsky and Veeravalli (2005) developed a general asymptotic theory of changepoint detection in the case of non-identically distributed and dependent observations and discrete time, and Baron and Tartakovsky (2006) in continuous time assuming the certain stability of the log-likelihood ratio process. This stability property was formulated in terms of the <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>-quick convergence of the normalized log-likelihood ratio process to a positive and finite number, which can be interpreted as the limiting Kullback\u2013Leibler information between the \u201cchange\u201d and \u201cno change\u201d hypotheses. In these papers, it was conjectured that the <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>-quick convergence can be relaxed in the <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>-complete convergence, which is typically much easier to verify in particular examples. In the present paper, we justify this conjecture by showing that the Shiryaev change detection procedure is nearly optimal, minimizing asymptotically to first order (as the probability of false alarm vanishes) moments of the delay to detection up to order <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula> whenever <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>-complete convergence holds. We also study asymptotic properties of the Shiryaev\u2013Roberts detection procedure in the Bayesian context."
            },
            {
                "arxivId": "1510.02903",
                "title": "Asymptotically optimal pointwise and minimax quickest change-point detection for dependent data",
                "abstract": null
            },
            {
                "arxivId": "1509.01570",
                "title": "Real-time financial surveillance via quickest change-point detection methods",
                "abstract": "We consider the problem of efficient financial surveillance aimed at \"on-the-go\" detection of structural breaks (anomalies) in \"live\"-monitored financial time series. With the problem approached statistically, viz. as that of multi-cyclic sequential (quickest) change-point detection, we propose a semi-parametric multi-cyclic change-point detection procedure to promptly spot anomalies as they occur in the time series under surveillance. The proposed procedure is a derivative of the likelihood ratio-based Shiryaev-Roberts (SR) procedure; the latter is a quasi-Bayesian surveillance method known to deliver the fastest (in the multi-cyclic sense) speed of detection, whatever be the false alarm frequency. We offer a case study where we first carry out, step by step, statistical analysis of a set of real-world financial data, and then set up and devise (a) the proposed SR-based anomaly-detection procedure and (b) the celebrated Cumulative Sum (CUSUM) chart to detect structural breaks in the data. While both procedures performed well, the proposed SR-derivative, conforming to the intuition, seemed slightly better."
            },
            {
                "arxivId": "0904.3370",
                "title": "On optimality of the Shiryaev-Roberts procedure for detecting a change in distribution",
                "abstract": "In 1985, for detecting a change in distribution, Pollak introduced a specific minimax performance metric and a randomized version of the Shiryaev-Roberts procedure where the zero initial condition is replaced by a random variable sampled from the quasi-stationary distribution of the Shiryaev-Roberts statistic. Pollak proved that this procedure is third-order asymptotically optimal as the mean time to false alarm becomes large. The question of whether Pollak's procedure is strictly minimax for any false alarm rate has been open for more than two decades, and there were several attempts to prove this strict optimality. In this paper, we provide a counterexample which shows that Pollak's procedure is not optimal and that there is a strictly optimal procedure which is nothing but the Shiryaev-Roberts procedure that starts with a specially designed deterministic point."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-05.json",
        "arxivId": "2404.03581",
        "category": "econ",
        "title": "Consumer Behavior under Benevolent Price Discrimination",
        "abstract": "Extensive research shows that consumers are generally averse to price discrimination. However, instruments of differential pricing can benefit consumer surplus and alleviate inequity through targeted price discounts. This paper examines how these outcome considerations influence consumer reactions to price discrimination. Six studies with 3951 participants show that a large share of consumers is willing to costly switch away from a store that introduces a discount for low-income consumers. This happens irrespective of whether income differences are due to luck or merit. While the price-discriminating store does attract some new high-income consumers, it cannot compensate the loss of existing consumers. Allowing for altruistic preferences by simulating a market mechanism increases costly support for price discounts, but does not alleviate consumer aversions. Finally, we provide evidence that warm glow drives costly support for price discounts.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2303.16629",
        "category": "econ",
        "title": "Power sector effects of alternative options for de-fossilizing heavy-duty vehicles: go electric, and charge smartly",
        "abstract": "In the passenger car segment, battery-electric vehicles (BEV) have emerged as the most promising option to de-fossilize transportation. For heavy-duty vehicles (HDV), the technology space still appears to be more open. Aside from BEV, electric road systems (ERS) for dynamic power transfer are discussed, as well as indirect electrification with trucks that use hydrogen fuel cells or e-fuels. Here we investigate the power sector implications of these alternative options. We apply an open-source capacity expansion model to future scenarios of Germany with high renewable energy shares, drawing on detailed route-based truck traffic data. Results show that power sector costs are lowest for flexibly charged BEV that also carry out vehicle-to-grid operations, and highest for HDV using e-fuels. If BEV and ERS-BEV are not charged in an optimized way, power sector costs increase, but are still substantially lower than in scenarios with hydrogen or e-fuels. This is a consequence of the poor energy efficiency of indirect HDV electrification, which outweighs its temporal flexibility benefits. We further find a higher use of solar photovoltaic energy for BEV and ERS-BEV, while hydrogen and e-fuel HDV lead to a higher use of wind power and fossil electricity generation. Results are qualitatively robust in sensitivity analyses without the European interconnection or with capacity limits for wind power expansion.",
        "references": [
            {
                "arxivId": "2211.16419",
                "title": "Geographical balancing of wind power decreases storage needs in a 100% renewable European power sector",
                "abstract": null
            },
            {
                "arxivId": "2005.02765",
                "title": "An open tool for creating battery-electric vehicle time series from empirical data, emobpy",
                "abstract": null
            },
            {
                "arxivId": "2005.03464",
                "title": "Optimal supply chains and power sector benefits of green hydrogen",
                "abstract": null
            },
            {
                "arxivId": "2002.05209",
                "title": "Decreasing market value of variable renewables can be avoided by policy action",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2306.05568",
        "category": "econ",
        "title": "Maximally Machine-Learnable Portfolios",
        "abstract": "When it comes to stock returns, any form of predictability can bolster risk-adjusted profitability. We develop a collaborative machine learning algorithm that optimizes portfolio weights so that the resulting synthetic security is maximally predictable. Precisely, we introduce MACE, a multivariate extension of Alternating Conditional Expectations that achieves the aforementioned goal by wielding a Random Forest on one side of the equation, and a constrained Ridge Regression on the other. There are two key improvements with respect to Lo and MacKinlay's original maximally predictable portfolio approach. First, it accommodates for any (nonlinear) forecasting algorithm and predictor set. Second, it handles large portfolios. We conduct exercises at the daily and monthly frequency and report significant increases in predictability and profitability using very little conditioning information. Interestingly, predictability is found in bad as well as good times, and MACE successfully navigates the debacle of 2022.",
        "references": [
            {
                "arxivId": "2207.08815",
                "title": "Why do tree-based models still outperform deep learning on tabular data?",
                "abstract": "While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner."
            },
            {
                "arxivId": "2206.01825",
                "title": "Debiased Machine Learning without Sample-Splitting for Stable Estimators",
                "abstract": "Estimation and inference on causal parameters is typically reduced to a generalized method of moments problem, which involves auxiliary functions that correspond to solutions to a regression or classification problem. Recent line of work on debiased machine learning shows how one can use generic machine learning estimators for these auxiliary problems, while maintaining asymptotic normality and root-$n$ consistency of the target parameter of interest, while only requiring mean-squared-error guarantees from the auxiliary estimation algorithms. The literature typically requires that these auxiliary problems are fitted on a separate sample or in a cross-fitting manner. We show that when these auxiliary estimation algorithms satisfy natural leave-one-out stability properties, then sample splitting is not required. This allows for sample re-use, which can be beneficial in moderately sized sample regimes. For instance, we show that the stability properties that we propose are satisfied for ensemble bagged estimators, built via sub-sampling without replacement, a popular technique in machine learning practice."
            },
            {
                "arxivId": "2202.10817",
                "title": "Canonical Portfolios: Optimal Asset and Signal Combination",
                "abstract": "This paper presents a novel framework for analyzing the optimal asset and signal combination problem. Our approach builds upon the dynamic portfolio selection problem introduced by Brandt and Santa-Clara (2006) and consists of two stages. First, we reformulate their original investment problem into a tractable one that allows us to derive a closed-form expression for the optimal portfolio policy that is scalable to large cross-sectional financial applications. Second, we recast the problem of selecting a portfolio of correlated assets and signals into selecting a set of uncorrelated managed portfolios through the lens of Canonical Correlation Analysis of Hotelling (1936). The new investment environment of uncorrelated managed portfolios offers unique economic insights into the joint correlation structure of our optimal portfolio policy. We also operationalize our theoretical framework to bridge the gap between theory and practice, showcasing the improved performance of our proposed method over natural competing benchmarks."
            },
            {
                "arxivId": "2202.04146",
                "title": "A Neural Phillips Curve and a Deep Output Gap",
                "abstract": "Many problems plague the estimation of Phillips curves. Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include creating reasonable proxies for the notable absentees or extracting them via some form of assumptions-heavy filtering procedure. I propose an alternative route: a Hemisphere Neural Network (HNN) whose peculiar architecture yields a final layer where components can be interpreted as latent states within a Neural Phillips Curve. There are benefits. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, computations are fast. Third, forecasts are economically interpretable. Fourth, inflation volatility can also be predicted by merely adding a hemisphere to the model. Among other findings, the contribution of real activity to inflation appears severely underestimated in traditional econometric specifications. Also, HNN captures out-of-sample the 2021 upswing in inflation and attributes it first to an abrupt and sizable disanchoring of the expectations component, followed by a wildly positive gap starting from late 2020. HNN\u2019s gap unique path comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators \u2013 some of which are skyrocketing as of early 2022. *Departement des Sciences \u00c9conomiques, goulet_coulombe.philippe@uqam.ca. First and foremost, I am grateful to Hugo Couture for outstanding research assistance. For helpful discussions and/or comments on an earlier draft, I would like to thank Frank Diebold, Mikael Frenette, Alain Guay, Maximilian Goebel, Guillaume Poulin-Bellisle, Josefine Quast, Dalibor Stevanovic, David Wigglesworth, Boyuan Zhang, as well as participants at the UQAM weekly seminar, at at the CIRANO Montreal Macro Workshop, the SKEMA/CERGY Inflation Forecasting Workshop 2021, at the Bank of Canada Macro Brownbag Seminar, at the CFE 2021. ar X iv :2 20 2. 04 14 6v 1 [ ec on .E M ] 8 F eb 2 02 2"
            },
            {
                "arxivId": "2101.02118",
                "title": "Do We Really Need Deep Learning Models for Time Series Forecasting?",
                "abstract": "Time series forecasting is a crucial task in machine learning, as it has a wide range of applications including but not limited to forecasting electricity consumption, traffic, and air quality. Traditional forecasting models rely on rolling averages, vector auto-regression and auto-regressive integrated moving averages. On the other hand, deep learning and matrix factorization models have been recently proposed to tackle the same problem with more competitive performance. However, one major drawback of such models is that they tend to be overly complex in comparison to traditional techniques. In this paper, we report the results of prominent deep learning models with respect to a well-known machine learning baseline, a Gradient Boosting Regression Tree (GBRT) model. Similar to the deep neural network (DNN) models, we transform the time series forecasting task into a window-based regression problem. Furthermore, we feature-engineered the input and output structure of the GBRT model, such that, for each training window, the target values are concatenated with external features, and then flattened to form one input instance for a multi-output GBRT model. We conducted a comparative study on nine datasets for eight state-of-the-art deep-learning models that were presented at top-level conferences in the last years. The results demonstrate that the window-based input transformation boosts the performance of a simple GBRT model to levels that outperform all state-of-the-art DNN models evaluated in this paper."
            },
            {
                "arxivId": "2009.03394",
                "title": "Deep Learning, Predictability, and Optimal Portfolio Returns",
                "abstract": "We study optimal dynamic portfolio choice of a long-horizon investor who uses deep learning methods to predict equity returns when forming optimal portfolios. The results show statistically and economically significant out-of-sample portfolio benefits of deep learning as measured by high certainty equivalent returns and Sharpe ratios. Return predictability via deep learning generates substantially improved portfolio performance across different subsamples, particularly the recession periods. These gains are robust to including transaction costs, short-selling and borrowing constraints."
            },
            {
                "arxivId": "2008.07063",
                "title": "To Bag is to Prune",
                "abstract": "It is notoriously hard to build a bad Random Forest (RF). Concurrently, RF is perhaps the only standard ML algorithm that blatantly overfits in-sample without any consequence out-of-sample. Standard arguments cannot rationalize this paradox. I propose a new explanation: bootstrap aggregation and model perturbation as implemented by RF automatically prune a (latent) true underlying tree. More generally, there is no need to tune the stopping point of a properly randomized ensemble of greedily optimized base learners. Thus, Boosting and MARS are eligible for automatic (implicit) tuning. I empirically demonstrate the property, with simulated and real data, by reporting that these new completely overfitting ensembles yield an out-of-sample performance equivalent to that of their tuned counterparts -- or better."
            },
            {
                "arxivId": "2008.01714",
                "title": "Macroeconomic data transformations matter",
                "abstract": null
            },
            {
                "arxivId": "2006.12724",
                "title": "The Macroeconomy as a Random Forest",
                "abstract": "I develop the macroeconomic random forest (MRF), an algorithm adapting the canonical machine learning (ML) tool, to flexibly model evolving parameters in a linear macro equation. Its main output, generalized time\u2010varying parameters (GTVPs), is a versatile device nesting many popular nonlinearities (threshold/switching, smooth transition, and structural breaks/change) and allowing for sophisticated new ones. The approach delivers clear forecasting gains over numerous alternatives, predicts the 2008 drastic rise in unemployment, and performs well for inflation. Unlike most ML\u2010based methods, MRF is directly interpretable\u2014via its GTVPs. For instance, the successful unemployment forecast is due to the influence of forward\u2010looking variables (e.g., term spreads and housing starts) nearly doubling before every recession. Interestingly, the Phillips curve has indeed flattened, and its might is highly cyclical."
            },
            {
                "arxivId": "2008.12477",
                "title": "How is Machine Learning Useful for Macroeconomic Forecasting?",
                "abstract": "We move beyond Is Machine Learning Useful for Macroeconomic Forecasting? by adding the how. The current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. To the contrary, we study the usefulness of the underlying features driving ML gains over standard macroeconometric methods. We distinguish four so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. To do so, we design experiments that allow to identify the \u201ctreatment\u201d effects of interest. We conclude that (i) nonlinearity is the true game changer for macroeconomic prediction, (ii) the standard factor model remains the best regularization, (iii) K-fold cross-validation is the best practice and (iv) the L2 is preferred to the e-insensitive in-sample loss. The forecasting gains of nonlinear techniques are associated with high macroeconomic uncertainty, financial stress and housing bubble bursts. This suggests that Machine Learning is useful for macroeconomic forecasting by mostly capturing important nonlinearities that arise in the context of uncertainty and financial frictions."
            },
            {
                "arxivId": "1906.11300",
                "title": "Benign overfitting in linear regression",
                "abstract": "The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lie in an infinite-dimensional space vs. when the data lie in a finite-dimensional space with dimension that grows faster than the sample size."
            },
            {
                "arxivId": "1905.05841",
                "title": "Efficient computation of mean reverting portfolios using cyclical coordinate descent",
                "abstract": "The econometric challenge of finding sparse mean reverting portfolios based on a subset of a large number of assets is well known. Many current state-of-the-art approaches fall into the field of co-integration theory, where the problem is phrased in terms of an eigenvector problem with sparsity constraint. Although a number of approximate solutions have been proposed to solve this NP-hard problem, all are based on relatively simple models and are limited in their scalability. In this paper, we leverage information obtained from a heterogeneous simultaneous graphical dynamic linear model (H-SGDLM) and propose a novel formulation of the mean reversion problem, which is phrased in terms of a quasi-convex minimisation with a normalisation constraint. This new formulation allows us to employ a cyclical coordinate descent algorithm for efficiently computing an exact sparse solution, even in a large universe of assets, while the use of H-SGDLM data allows us to easily control the required level of sparsity. We demonstrate the flexibility, speed and scalability of the proposed approach on S&P500, FX and ETF futures data."
            },
            {
                "arxivId": "1903.08560",
                "title": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation",
                "abstract": "Interpolators-estimators that achieve zero training error-have attracted growing attention in machine learning, mainly because state-of-the art neural networks appear to be models of this type. In this paper, we study minimum \u2113 2 norm (\"ridgeless\") interpolation least squares regression, focusing on the high-dimensional regime in which the number of unknown parameters p is of the same order as the number of samples n. We consider two different models for the feature distribution: a linear model, where the feature vectors x i \u2208 \u211d p are obtained by applying a linear transform to a vector of i.i.d. entries, x i = \u03a31/2 z i (with z i \u2208 \u211d p ); and a nonlinear model, where the feature vectors are obtained by passing the input through a random one-layer neural network, xi = \u03c6(Wz i ) (with z i \u2208 \u211d d , W \u2208 \u211d p \u00d7 d a matrix of i.i.d. entries, and \u03c6 an activation function acting componentwise on Wz i ). We recover-in a precise quantitative way-several phenomena that have been observed in large-scale neural networks and kernel machines, including the \"double descent\" behavior of the prediction risk, and the potential benefits of overparametrization."
            },
            {
                "arxivId": "1904.00745",
                "title": "Deep Learning in Asset Pricing",
                "abstract": "We use deep neural networks to estimate an asset pricing model for individual stock returns that takes advantage of the vast amount of conditioning information, keeps a fully flexible form, and accounts for time variation. The key innovations are to use the fundamental no-arbitrage condition as criterion function to construct the most informative test assets with an adversarial approach and to extract the states of the economy from many macroeconomic time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in terms of Sharpe ratio, explained variation, and pricing errors and identifies the key factors that drive asset prices. This paper was accepted by Agostino Capponi, finance. Supplemental Material: The online appendix and data are available at https://doi.org/10.1287/mnsc.2023.4695 ."
            },
            {
                "arxivId": "1812.11118",
                "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off",
                "abstract": "Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias\u2013variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias\u2013variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This \u201cdouble-descent\u201d curve subsumes the textbook U-shaped bias\u2013variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning."
            },
            {
                "arxivId": "1812.05792",
                "title": "Making Sense of Random Forest Probabilities: a Kernel Perspective",
                "abstract": "A random forest is a popular tool for estimating probabilities in machine learning classification tasks. However, the means by which this is accomplished is unprincipled: one simply counts the fraction of trees in a forest that vote for a certain class. In this paper, we forge a connection between random forests and kernel regression. This places random forest probability estimation on more sound statistical footing. As part of our investigation, we develop a model for the proximity kernel and relate it to the geometry and sparsity of the estimation problem. We also provide intuition and recommendations for tuning a random forest to improve its probability estimates."
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1701.05016",
                "title": "Mean-Reverting Portfolio With Budget Constraint",
                "abstract": "This paper considers the mean-reverting portfolio (MRP) design problem arising from statistical arbitrage (a.k.a. pairs trading) in the financial markets. It aims at designing a portfolio of underlying assets by optimizing the mean reversion strength of the portfolio, while taking into consideration the portfolio variance and an investment budget constraint. Several specific design problems are considered based on different mean reversion criteria. Efficient algorithms are proposed to solve the problems. Numerical results on both synthetic and market data show that the proposed MRP design methods can generate consistent profits and outperform the traditional design methods and the benchmark methods in the literature."
            },
            {
                "arxivId": "1611.08393",
                "title": "Mean-reverting portfolio design via majorization-minimization method",
                "abstract": "This paper considers the mean-reverting portfolio design problem arising from statistical arbitrage in the financial markets. The problem is formulated by optimizing a criterion characterizing the mean-reversion strength of the portfolio and taking into consideration the variance of the portfolio and an investment budget constraint at the same time. An efficient algorithm based on the majorization-minimization (MM) method is proposed to solve the problem. Numerical results show that our proposed mean-reverting portfolio design method can significantly outperform every underlying single spread and the benchmark method in the literature."
            },
            {
                "arxivId": "1610.01271",
                "title": "Generalized random forests",
                "abstract": "We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN."
            },
            {
                "arxivId": "1511.04839",
                "title": "Nonparametric Canonical Correlation Analysis",
                "abstract": "Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA."
            },
            {
                "arxivId": "1502.02312",
                "title": "Bayesian and Empirical Bayesian Forests",
                "abstract": "We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view random forests as samples from a posterior distribution. This insight provides large gains in interpretability, and motivates a class of Bayesian forest (BF) algorithms that yield small but reliable performance gains. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This leads to an empirical Bayesian forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform subsampling based alternatives by a large margin."
            },
            {
                "arxivId": "0708.3048",
                "title": "Identifying small mean-reverting portfolios",
                "abstract": "Given multivariate time series, we study the problem of forming portfolios with maximum mean reversion while constraining the number of assets in these portfolios. We show that it can be formulated as a sparse canonical correlation analysis and study various algorithms to solve the corresponding sparse generalized eigenvalue problems. After discussing penalized parameter estimation procedures, we study the sparsity versus predictability trade-off and the significance of predictability in various markets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2310.17496",
        "category": "econ",
        "title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach",
        "abstract": "In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators that do not cause shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.",
        "references": [
            {
                "arxivId": "2402.07322",
                "title": "Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis",
                "abstract": "Online A/B testing is widely used in the internet industry to inform decisions on new feature roll-outs. For online marketplaces (such as advertising markets), standard approaches to A/B testing may lead to biased results when buyers operate under a budget constraint, as budget consumption in one arm of the experiment impacts performance of the other arm. To counteract this interference, one can use a budget-split design where the budget constraint operates on a per-arm basis and each arm receives an equal fraction of the budget, leading to ``budget-controlled A/B testing.'' Despite clear advantages of budget-controlled A/B testing, performance degrades when budget are split too small, limiting the overall throughput of such systems. In this paper, we propose a parallel budget-controlled A/B testing design where we use market segmentation to identify submarkets in the larger market, and we run parallel experiments on each submarket. Our contributions are as follows: First, we introduce and demonstrate the effectiveness of the parallel budget-controlled A/B test design with submarkets in a large online marketplace environment. Second, we formally define market interference in first-price auction markets using the first price pacing equilibrium (FPPE) framework. Third, we propose a debiased surrogate that eliminates the first-order bias of FPPE, drawing upon the principles of sensitivity analysis in mathematical programs. Fourth, we derive a plug-in estimator for the surrogate and establish its asymptotic normality. Fifth, we provide an estimation procedure for submarket parallel budget-controlled A/B tests. Finally, we present numerical examples on semi-synthetic data, confirming that the debiasing technique achieves the desired coverage properties."
            },
            {
                "arxivId": "2311.18274",
                "title": "Semiparametric Efficient Inference in Adaptive Experiments",
                "abstract": "We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting the asymptotic variance. Empirical results demonstrate that our methods yield narrower confidence sequences than those previously developed in the literature while maintaining time-uniform error control."
            },
            {
                "arxivId": "2310.16294",
                "title": "Producer-Side Experiments Based on Counterfactual Interleaving Designs for Online Recommender Systems",
                "abstract": "Recommender systems have become an integral part of online platforms, providing personalized suggestions for purchasing items, consuming contents, and connecting with individuals. An online recommender system consists of two sides of components: the producer side comprises product sellers, content creators, or service providers, etc., and the consumer side includes buyers, viewers, or guests, etc. To optimize an online recommender system, A/B tests serve as the golden standard for comparing different ranking models and evaluating their impact on both the consumers and producers. While consumer-side experiments are relatively straightforward to design and commonly used to gauge the impact of ranking changes on the behavior of consumers (buyers, viewers, etc.), designing producer-side experiments presents a considerable challenge because producer items in the treatment and control groups need to be ranked by different models and then merged into a single ranking for the recommender to show to each consumer. In this paper, we review issues with the existing methods, propose new design principles for producer-side experiments, and develop a rigorous solution based on counterfactual interleaving designs for accurately measuring the effects of ranking changes on the producers (sellers, creators, etc.)."
            },
            {
                "arxivId": "2309.07107",
                "title": "A Study of Symbiosis Bias in A/B Tests of Recommendation Algorithms",
                "abstract": "One assumption underlying the unbiasedness of global treatment effect estimates from randomized experiments is the stable unit treatment value assumption (SUTVA). Many experiments that compare the efficacy of different recommendation algorithms violate SUTVA, because each algorithm is trained on a pool of shared data, often coming from a mixture of recommendation algorithms in the experiment. We explore, through simulation, cluster randomized and data-diverted solutions to mitigating this bias, which we call\"symbiosis bias.\""
            },
            {
                "arxivId": "2305.02542",
                "title": "Correcting for Interference in Experiments: A Case Study at Douyin",
                "abstract": "Interference is a ubiquitous problem in experiments conducted on two-sided content marketplaces, such as Douyin (China\u2019s analog of TikTok). In many cases, creators are the natural unit of experimentation, but creators interfere with each other through competition for viewers\u2019 limited time and attention. \u201cNaive\u201d estimators currently used in practice simply ignore the interference, but in doing so incur bias on the order of the treatment effect. We formalize the problem of inference in such experiments as one of policy evaluation. Off-policy estimators, while unbiased, are impractically high variance. We introduce a novel Monte-Carlo estimator, based on \u201cDifferences-in-Qs\u201d (DQ) techniques, which achieves bias that is second-order in the treatment effect, while remaining sample-efficient to estimate. On the theoretical side, our contribution is to develop a generalized theory of Taylor expansions for policy evaluation, which extends DQ theory to all major MDP formulations. On the practical side, we implement our estimator on Douyin\u2019s experimentation platform, and in the process develop DQ into a truly \u201cplug-and-play\u201d estimator for interference in real-world settings: one which provides robust, low-bias, low-variance treatment effect estimates; admits computationally cheap, asymptotically exact uncertainty quantification; and reduces MSE by 99% compared to the best existing alternatives in our applications."
            },
            {
                "arxivId": "2304.02572",
                "title": "Evaluating Online Bandit Exploration In Large-Scale Recommender System",
                "abstract": "Bandit learning has been an increasingly popular design choice for recommender system. Despite the strong interest in bandit learning from the community, there remains multiple bottlenecks that prevent many bandit learning approaches from productionalization. One major bottleneck is how to test the effectiveness of bandit algorithm with fairness and without data leakage. Different from supervised learning algorithms, bandit learning algorithms emphasize greatly on the data collection process through their explorative nature. Such explorative behavior may induce unfair evaluation in a classic A/B test setting. In this work, we apply upper confidence bound (UCB) to our large scale short video recommender system and present a test framework for the production bandit learning life-cycle with a new set of metrics. Extensive experiment results show that our experiment design is able to fairly evaluate the performance of bandit learning in the recommender system."
            },
            {
                "arxivId": "2209.00197",
                "title": "Switchback Experiments under Geometric Mixing",
                "abstract": "The switchback is an experimental design that measures treatment effects by repeatedly turning an intervention on and off for a whole system. Switchback experiments are a robust way to overcome cross-unit spillover effects; however, they are vulnerable to bias from temporal carryovers. In this paper, we consider properties of switchback experiments in Markovian systems that mix at a geometric rate. We find that, in this setting, standard switchback designs suffer considerably from carryover bias: Their estimation error decays as $T^{-1/3}$ in terms of the experiment horizon $T$, whereas in the absence of carryovers a faster rate of $T^{-1/2}$ would have been possible. We also show, however, that judicious use of burn-in periods can considerably improve the situation, and enables errors that decay as $\\log(T)^{1/2}T^{-1/2}$. Our formal results are mirrored in an empirical evaluation."
            },
            {
                "arxivId": "2207.01616",
                "title": "Breaking Feedback Loops in Recommender Systems with Causal Inference",
                "abstract": "Recommender systems play a key role in shaping modern web ecosystems. These systems alternate between (1) making recommendations (2) collecting user responses to these recommendations, and (3) retraining the recommendation algorithm based on this feedback. During this process the recommender system influences the user behavioral data that is subsequently used to update it, thus creating a feedback loop. Recent work has shown that feedback loops may compromise recommendation quality and homogenize user behavior, raising ethical and performance concerns when deploying recommender systems. To address these issues, we propose the Causal Adjustment for Feedback Loops (CAFL), an algorithm that provably breaks feedback loops using causal inference and can be applied to any recommendation algorithm that optimizes a training loss. Our main observation is that a recommender system does not suffer from feedback loops if it reasons about causal quantities, namely the intervention distributions of recommendations on user ratings. Moreover, we can calculate this intervention distribution from observational data by adjusting for the recommender system's predictions of user preferences. Using simulated environments, we demonstrate that CAFL improves recommendation quality when compared to prior correction methods."
            },
            {
                "arxivId": "2206.02371",
                "title": "Markovian Interference in Experiments",
                "abstract": "We consider experiments in dynamical systems where interventions on some experimental units impact other units through a limiting constraint (such as a limited inventory). Despite outsize practical importance, the best estimators for this `Markovian' interference problem are largely heuristic in nature, and their bias is not well understood. We formalize the problem of inference in such experiments as one of policy evaluation. Off-policy estimators, while unbiased, apparently incur a large penalty in variance relative to state-of-the-art heuristics. We introduce an on-policy estimator: the Differences-In-Q's (DQ) estimator. We show that the DQ estimator can in general have exponentially smaller variance than off-policy evaluation. At the same time, its bias is second order in the impact of the intervention. This yields a striking bias-variance tradeoff so that the DQ estimator effectively dominates state-of-the-art alternatives. From a theoretical perspective, we introduce three separate novel techniques that are of independent interest in the theory of Reinforcement Learning (RL). Our empirical evaluation includes a set of experiments on a city-scale ride-hailing simulator."
            },
            {
                "arxivId": "2205.12803",
                "title": "Estimating the total treatment effect in randomized experiments with unknown network structure",
                "abstract": "Significance In many domains, we want to estimate the total treatment effect (TTE) in situations where we suspect network interference is present. However, we often cannot measure the network or the implied dependency structure. Surprisingly, we are able to develop principles for designing randomized experiments without knowledge of the network, showing that under reasonable conditions one can nonetheless estimate the TTE, accounting for interference on the unknown network. The proposed design principles, and related estimator, work with a broad class of outcome models. Our estimator has low variance under simple randomized designs, resulting in an efficient and practical solution for estimating total treatment effect in the presence of complex network effects. We detail the assumptions under which the proposed methods work and discuss situations when they may fail."
            },
            {
                "arxivId": "2112.13495",
                "title": "Multiple Randomization Designs",
                "abstract": "In this study we introduce a new class of experimental designs. In a classical randomized controlled trial (RCT), or A/B test, a randomly selected subset of a population of units (e.g., individuals, plots of land, or experiences) is assigned to a treatment (treatment A), and the remainder of the population is assigned to the control treatment (treatment B). The difference in average outcome by treatment group is an estimate of the average effect of the treatment. However, motivating our study, the setting for modern experiments is often different, with the outcomes and treatment assignments indexed by multiple populations. For example, outcomes may be indexed by buyers and sellers, by content creators and subscribers, by drivers and riders, or by travelers and airlines and travel agents, with treatments potentially varying across these indices. Spillovers or interference can arise from interactions between units across populations. For example, sellers' behavior may depend on buyers' treatment assignment, or vice versa. This can invalidate the simple comparison of means as an estimator for the average effect of the treatment in classical RCTs. We propose new experiment designs for settings in which multiple populations interact. We show how these designs allow us to study questions about interference that cannot be answered by classical randomized experiments. Finally, we develop new statistical methods for analyzing these Multiple Randomization Designs."
            },
            {
                "arxivId": "2106.00762",
                "title": "A/B Testing for Recommender Systems in a Two-sided Marketplace",
                "abstract": "Two-sided marketplaces are standard business models of many online platforms (e.g., Amazon, Facebook, LinkedIn), wherein the platforms have consumers, buyers or content viewers on one side and producers, sellers or content-creators on the other. Consumer side measurement of the impact of a treatment variant can be done via simple online A/B testing. Producer side measurement is more challenging because the producer experience depends on the treatment assignment of the consumers. Existing approaches for producer side measurement are either based on graph cluster-based randomization or on certain treatment propagation assumptions. The former approach results in low-powered experiments as the producer-consumer network density increases and the latter approach lacks a strict notion of error control. In this paper, we propose (i) a quantification of the quality of a producer side experiment design, and (ii) a new experiment design mechanism that generates high-quality experiments based on this quantification. Our approach, called UniCoRn (Unifying Counterfactual Rankings), provides explicit control over the quality of the experiment and its computation cost. Further, we prove that our experiment design is optimal to the proposed design quality measure. Our approach is agnostic to the density of the producer-consumer network and does not rely on any treatment propagation assumption. Moreover, unlike the existing approaches, we do not need to know the underlying network in advance, making this widely applicable to the industrial setting where the underlying network is unknown and challenging to predict a priori due to its dynamic nature. We use simulations to validate our approach and compare it against existing methods. We also deployed UniCoRn in an edge recommendation application that serves tens of millions of members and billions of edge recommendations daily."
            },
            {
                "arxivId": "2104.12222",
                "title": "Interference, Bias, and Variance in Two-Sided Marketplace Experimentation: Guidance for Platforms",
                "abstract": "Two-sided marketplace platforms often run experiments (or A/B tests) to test the effect of an intervention before launching it platform-wide. A typical approach is to randomize users into a treatment group, which receives the intervention, and a control group, which does not. The platform then compares the performance in the two groups to estimate the effect if the intervention were launched to everyone. We focus on two common experiment types, where the platform randomizes users either on the supply side or on the demand side. For these experiments, it is known that the resulting estimates of the treatment effect are typically biased: individuals in the market compete with each other, which creates interference and leads to a biased estimate. Here, we observe that economic interactions (competition among demand and supply) lead to statistical phenomenon (biased estimates). We develop a simple, tractable market model to study bias and variance in these experiments with interference. We focus on two choices available to the platform: (1) Which side of the platform should it randomize on (supply or demand)? (2) What proportion of individuals should be allocated to treatment? We find that both choices affect the bias and variance of the resulting estimators, but in different ways. The bias-optimal choice of experiment type depends on the relative amounts of supply and demand in the market, and we discuss how a platform can use market data to select the experiment type. Importantly, we find that in many circumstances choosing the bias-optimal experiment type has little effect on variance, and in some cases coincide with the variance-optimal type. On the other hand, we find that the choice of treatment proportion can induce a bias-variance tradeoff, where the bias-minimizing proportion increases variance. We discuss how a platform can navigate this tradeoff and best choose the proportion, using a combination of modeling as well as contextual knowledge about the market, the risk of the intervention, and reasonable effect sizes of the intervention."
            },
            {
                "arxivId": "2103.06392",
                "title": "Design and Analysis of Bipartite Experiments Under a Linear Exposure-response Model",
                "abstract": "A bipartite experiment consists of one set of units being assigned treatments and another set of units for which we measure outcomes. The two sets of units are connected by a bipartite graph, governing how the treated units can affect the outcome units. The bipartite framework naturally arises in marketplace experiments where, for example, experimenters may seek to investigate the effect of discounting goods on buyer behavior. In this paper, we consider estimation of the average total treatment effect in the bipartite experimental framework under a linear exposure-response model. We introduce the Exposure Reweighted Linear (ERL) estimator, and show that the estimator is unbiased, consistent and asymptotically normal, provided that the bipartite graph is sufficiently sparse. To facilitate inference, we introduce an unbiased and consistent estimator of the variance of the ERL point estimator. In addition, we introduce a cluster-based design, Exposure-Design, that uses heuristics to increase the precision of the ERL estimator by realizing a desirable exposure distribution. Finally, we demonstrate the application of the described methodology to marketplace experiments using a publicly available Amazon user-item review dataset. The full version of the paper is available at: https://arxiv.org/abs/2103.06392."
            },
            {
                "arxivId": "2101.09855",
                "title": "Weak Signal Asymptotics for Sequentially Randomized Experiments",
                "abstract": "We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multiarmed bandit problems. In an experiment with n time steps, we let the mean reward gaps between actions scale to the order [Formula: see text] to preserve the difficulty of the learning task as n grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments\u2014adapted to this scaling regime and with arm selection probabilities that vary continuously with state\u2014converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not upper-confidence bound, which does not satisfy our continuity assumption). We show that all sequential experiments whose randomization probabilities have a Lipschitz-continuous dependence on the observed data suffer from suboptimal regret performance when the reward gaps are relatively large. Conversely, we find that a version of Thompson sampling with an asymptotically uninformative prior variance achieves near-optimal instance-specific regret scaling, including with large reward gaps, but these good regret properties come at the cost of highly unstable posterior beliefs. This paper was accepted by Baris Ata, stochastic models and simulation. Supplemental Material: The data and online appendix are available at https://doi.org/10.1287/mnsc.2023.4964 ."
            },
            {
                "arxivId": "2009.02297",
                "title": "Randomized graph cluster randomization",
                "abstract": "Abstract The global average treatment effect (GATE) is a primary quantity of interest in the study of causal inference under network interference. With a correctly specified exposure model of the interference, the Horvitz\u2013Thompson (HT) and H\u00e1jek estimators of the GATE are unbiased and consistent, respectively, yet known to exhibit extreme variance under many designs and in many settings of interest. With a fixed clustering of the interference graph, graph cluster randomization (GCR) designs have been shown to greatly reduce variance compared to node-level random assignment, but even so the variance is still often prohibitively large. In this work, we propose a randomized version of the GCR design, descriptively named randomized graph cluster randomization (RGCR), which uses a random clustering rather than a single fixed clustering. By considering an ensemble of many different clustering assignments, this design avoids a key problem with GCR where the network exposure probability of a given node can be exponentially small in a single clustering. We propose two inherently randomized graph decomposition algorithms for use with RGCR designs, randomized 3-net and 1-hop-max, adapted from the prior work on multiway graph cut problems and the probabilistic approximation of (graph) metrics. We also propose weighted extensions of these two algorithms with slight additional advantages. All these algorithms result in network exposure probabilities that can be estimated efficiently. We derive structure-dependent upper bounds on the variance of the HT estimator of the GATE, depending on the metric structure of the graph driving the interference. Where the best-known such upper bound for the HT estimator under a GCR design is exponential in the parameters of the metric structure, we give a comparable upper bound under RGCR that is instead polynomial in the same parameters. We provide extensive simulations comparing RGCR and GCR designs, observing substantial improvements in GATE estimation in a variety of settings."
            },
            {
                "arxivId": "2009.00148",
                "title": "Design and Analysis of Switchback Experiments",
                "abstract": "Switchback experiments, where a firm sequentially exposes an experimental unit to random treatments, are among the most prevalent designs used in the technology sector, with applications ranging from ride-hailing platforms to online marketplaces. Although practitioners have widely adopted this technique, the derivation of the optimal design has been elusive, hindering practitioners from drawing valid causal conclusions with enough statistical power. We address this limitation by deriving the optimal design of switchback experiments under a range of different assumptions on the order of the carryover effect\u2014the length of time a treatment persists in impacting the outcome. We cast the optimal experimental design problem as a minimax discrete optimization problem, identify the worst-case adversarial strategy, establish structural results, and solve the reduced problem via a continuous relaxation. For switchback experiments conducted under the optimal design, we provide two approaches for performing inference. The first provides exact randomization-based p-values, and the second uses a new finite population central limit theorem to conduct conservative hypothesis tests and build confidence intervals. We further provide theoretical results when the order of the carryover effect is misspecified and provide a data-driven procedure to identify the order of the carryover effect. We conduct extensive simulations to study the numerical performance and empirical properties of our results and conclude with practical suggestions. This paper was accepted by George Shanthikumar, big data analytics."
            },
            {
                "arxivId": "2007.13302",
                "title": "Random graph asymptotics for treatment effect estimation under network interference",
                "abstract": "The network interference model for causal inference places all experimental units at the vertices of an undirected exposure graph, such that treatment assigned to one unit may affect the outcome of another unit if and only if these two units are connected by an edge. This model has recently gained popularity as means of incorporating interference effects into the Neyman--Rubin potential outcomes framework; and several authors have considered estimation of various causal targets, including the direct and indirect effects of treatment. In this paper, we consider large-sample asymptotics for treatment effect estimation under network interference in a setting where the exposure graph is a random draw from a graphon. When targeting the direct effect, we show that---in our setting---popular estimators are considerably more accurate than existing results suggest, and provide a central limit theorem in terms of moments of the graphon. Meanwhile, when targeting the indirect effect, we leverage our generative assumptions to propose a consistent estimator in a setting where no other consistent estimators are currently available. We also show how our results can be used to conduct a practical assessment of the sensitivity of randomized study inference to potential interference effects. Overall, our results highlight the promise of random graph asymptotics in understanding the practicality and limits of causal inference under network interference."
            },
            {
                "arxivId": "2007.13019",
                "title": "Feedback Loop and Bias Amplification in Recommender Systems",
                "abstract": "Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users' taste over time and also homogenization of the users. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group."
            },
            {
                "arxivId": "2006.05591",
                "title": "Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach",
                "abstract": "Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard randomized controlled trials are typically not feasible, since the goal is to estimate policy performance on the entire system. Instead, the typical current practice involves dynamically alternating between the two policies for fixed lengths of time, and comparing the average performance of each over the intervals in which they were run as an estimate of the treatment effect. However, this approach suffers from *temporal interference*: one algorithm alters the state of the system as seen by the second algorithm, biasing estimates of the treatment effect. Further, the simple non-adaptive nature of such designs implies they are not sample efficient. \nWe develop a benchmark theoretical model in which to study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies). We assume estimation of the steady state reward for each chain proceeds via nonparametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efficient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martingale analysis of Markov chains via Poisson's equation, we characterize efficient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efficient online experimental design that adaptively samples the two Markov chains."
            },
            {
                "arxivId": "2004.12489",
                "title": "Reducing Interference Bias in Online Marketplace Pricing Experiments",
                "abstract": "Online marketplace designers frequently run A/B tests to measure the impact of proposed product changes. However, given that marketplaces are inherently connected, total average treatment effect estimates obtained through Bernoulli randomized experiments are often biased due to violations of the stable unit treatment value assumption. This can be particularly problematic for experiments that impact sellers' strategic choices, affect buyers' preferences over items in their consideration set, or change buyers' consideration sets altogether. In this work, we measure and reduce bias due to interference in online marketplace experiments by using observational data to creating clusters of similar listings, and then using those clusters to conduct cluster-randomized field experiments. We provide a lower bound on the magnitude of bias due to interference by conducting a meta-experiment that randomizes over two experiment designs: one Bernoulli randomized, one cluster randomized. In both meta-experiment arms, treatment sellers are subject to a different platform fee policy than control sellers, resulting in different prices for buyers. By conducting a joint analysis of the two meta-experiment arms, we find a large and statistically significant difference between the total average treatment effect estimates obtained with the two designs, and estimate that 32.60% of the Bernoulli-randomized treatment effect estimate is due to interference bias. We also find weak evidence that the magnitude and/or direction of interference bias depends on extent to which a marketplace is supply- or demand-constrained, and analyze a second meta-experiment to highlight the difficulty of detecting interference bias when treatment interventions require intention-to-treat analysis."
            },
            {
                "arxivId": "2004.12162",
                "title": "Limiting Bias from Test-Control Interference in Online Marketplace Experiments",
                "abstract": "In an A/B test, the typical objective is to measure the total average treatment effect (TATE), which measures the difference between the average outcome if all users were treated and the average outcome if all users were untreated. However, a simple difference-in-means estimator will give a biased estimate of the TATE when outcomes of control units depend on the outcomes of treatment units, an issue we refer to as test-control interference. Using a simulation built on top of data from Airbnb, this paper considers the use of methods from the network interference literature for online marketplace experimentation. We model the marketplace as a network in which an edge exists between two sellers if their goods substitute for one another. We then simulate seller outcomes, specifically considering a \"status quo\" context and \"treatment\" context that forces all sellers to lower their prices. We use the same simulation framework to approximate TATE distributions produced by using blocked graph cluster randomization, exposure modeling, and the Hajek estimator for the difference in means. We find that while blocked graph cluster randomization reduces the bias of the naive difference-in-means estimator by as much as 62%, it also significantly increases the variance of the estimator. On the other hand, the use of more sophisticated estimators produces mixed results. While some provide (small) additional reductions in bias and small reductions in variance, others lead to increased bias and variance. Overall, our results suggest that experiment design and analysis techniques from the network experimentation literature are promising tools for reducing bias due to test-control interference in marketplace experiments."
            },
            {
                "arxivId": "2002.05670",
                "title": "Experimental Design in Two-Sided Platforms: An Analysis of Bias",
                "abstract": "We develop an analytical framework to study experimental design in two-sided platforms. In the settings we consider, customers rent listings; rented listings are occupied for some amount of time, then become available. Platforms typically use two common designs to study interventions in such settings: customer-side randomization (CR), and listing-side randomization (LR), along with associated estimators. We develop a stochastic model and associated mean field limit to capture dynamics in such systems, and use our model to investigate how the performance of these estimators is affected by interference effects between listings and between customers. Good experimental design depends on market balance: we show that in highly demand-constrained markets, CR is unbiased, while LR is biased; conversely, in highly supply-constrained markets, LR is unbiased, while CR is biased. We also study a design based on two-sided randomization (TSR) where both customers and listings are randomized to treatment and control, and show that appropriate choices of such designs can be unbiased in both extremes of market balance, and also yield low bias in intermediate regimes of market balance. Full paper available at: https://arxiv.org/abs/2002.05670."
            },
            {
                "arxivId": "1911.03764",
                "title": "Optimal Experimental Design for Staggered Rollouts",
                "abstract": "In this paper, we study the design and analysis of experiments conducted on a set of units over multiple time periods in which the starting time of the treatment may vary by unit. The design problem involves selecting an initial treatment time for each unit in order to most precisely estimate both the instantaneous and cumulative effects of the treatment. We first consider nonadaptive experiments, in which all treatment assignment decisions are made prior to the start of the experiment. For this case, we show that the optimization problem is generally NP-hard, and we propose a near-optimal solution. Under this solution, the fraction entering treatment each period is initially low, then high, and finally low again. Next, we study an adaptive experimental design problem, in which both the decision to continue the experiment and treatment assignment decisions are updated after each period\u2019s data are collected. For the adaptive case, we propose a new algorithm, the precision-guided adaptive experiment algorithm, which addresses the challenges at both the design stage and the stage of estimating treatment effects, ensuring valid post-experiment inference, accounting for the adaptive nature of the design. Using realistic settings, we demonstrate that our proposed solutions can reduce the opportunity cost of the experiments by more than 50%, compared with static design benchmarks. This paper was accepted by George Shanthikumar, data science. Funding: S. Athey and G. Imbens were supported by the Office of Naval Research [Grant N00014-19-1-2468]. M. Bayati was supported by the National Science Foundation [Grant CMMI 1554140]. Supplemental Material: The e-companion and data are available at https://doi.org/10.1287/mnsc.2023.4928 ."
            },
            {
                "arxivId": "1903.02124",
                "title": "Experimenting in Equilibrium",
                "abstract": "Classical approaches to experimental design assume that intervening on one unit does not affect other units. There are many important settings, however, where this noninterference assumption does not hold, as when running experiments on supply-side incentives on a ride-sharing platform or subsidies in an energy marketplace. In this paper, we introduce a new approach to experimental design in large-scale stochastic systems with considerable cross-unit interference, under an assumption that the interference is structured enough that it can be captured via mean-field modeling. Our approach enables us to accurately estimate the effect of small changes to system parameters by combining unobtrusive randomization with lightweight modeling, all while remaining in equilibrium. We can then use these estimates to optimize the system by gradient descent. Concretely, we focus on the problem of a platform that seeks to optimize supply-side payments [Formula: see text] in a centralized marketplace where different suppliers interact via their effects on the overall supply-demand equilibrium, and we show that our approach enables the platform to optimize [Formula: see text] in large systems using vanishingly small perturbations. This paper was accepted by Hamid Nazerzadeh, big data analytics."
            },
            {
                "arxivId": "1710.11214",
                "title": "How algorithmic confounding in recommendation systems increases homogeneity and decreases utility",
                "abstract": "Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals' perceptions of the world, and life-altering decisions. These systems are often evaluated or trained with data from users already exposed to algorithmic recommendations; this creates a pernicious feedback loop. Using simulations, we demonstrate how using data confounded in this way homogenizes user behavior without increasing utility."
            },
            {
                "arxivId": "1606.00908",
                "title": "A/B Testing of Auctions",
                "abstract": "A common method in the practice of large scale auction design, e.g., in auctions placing advertisements on online media and Internet search engines, is A/B testing. In A/B testing, the auction house is running an incumbent mechanism A, and would like to determine if a novel mechanism B obtains higher revenue. This is done by splitting the traffic so that most of it goes to A and some of it, e.g., five to ten percent, goes to B. An issue with this approach is that if the bidders are unaware of which mechanism their bid will be considered in, the bid equilibrium is neither for A nor B but for a mechanism C that is a convex combination of A and B. A miscalculation sometimes performed in practice is to consider and compare average revenue from A (resp. B) from the times when A (resp. B) is run. This miscalculation is equivalent to simulating A on the bids in C and can often give the opposite conclusion; e.g., if A and B are one- and two-unit highest-bids-win winner-pays-bid auctions, respectively, then B will always appear in this miscalculation to have higher revenue. For a fixed set of bids, a winner-pays-bid mechanism's revenue is monotone in its allocation probabilities. Of course, in equilibrium, increased allocation probabilities can cause reduced revenue as bidders may lower their bids. We present an A/B testing method that applies generally to the position auction model popularized by the Varian [2007] and Edelman et al. [2007] analyses of auctions for sponsored search and now a fundamental model for the study of auction theory; e.g., see Hartline [2013]. A position auction is defined by a decreasing sequence of weights, bidders are assigned to positions in decreasing order of bids, and payments are charged. Typical payment rules are \"generalized first price\" and \"generalized second price\"; the former requires bidders to pay their weighted bid, whereas the latter requires bidders to pay the weighted bid of the next highest bidder."
            },
            {
                "arxivId": "1605.09171",
                "title": "Randomization and The Pernicious Effects of Limited Budgets on Auction Experiments",
                "abstract": "Buyers (e.g., advertisers) often have limited financial and processing resources, and so their participation in auctions is throttled. Changes to auctions may affect bids or throttling and any change may affect what winners pay. This paper shows that if an A/B experiment affects only bids, then the observed treatment effect is unbiased when all the bidders in an auction are randomly assigned to A or B but it can be severely biased otherwise, even in the absence of throttling. Experiments that affect throttling algorithms can also be badly biased, but the bias can be substantially reduced if the budget for each advertiser in the experiment is allocated to separate pots for the A and B arms of the experiment."
            },
            {
                "arxivId": "1404.7530",
                "title": "Design and Analysis of Experiments in Networks: Reducing Bias from Interference",
                "abstract": "Abstract Estimating the effects of interventions in networks is complicated due to interference, such that the outcomes for one experimental unit may depend on the treatment assignments of other units. Familiar statistical formalism, experimental designs, and analysis methods assume the absence of this interference, and result in biased estimates of causal effects when it exists. While some assumptions can lead to unbiased estimates, these assumptions are generally unrealistic in the context of a network and often amount to assuming away the interference. In this work, we evaluate methods for designing and analyzing randomized experiments under minimal, realistic assumptions compatible with broad interference, where the aim is to reduce bias and possibly overall error in estimates of average effects of a global treatment. In design, we consider the ability to perform random assignment to treatments that is correlated in the network, such as through graph cluster randomization. In analysis, we consider incorporating information about the treatment assignment of network neighbors. We prove sufficient conditions for bias reduction through both design and analysis in the presence of potentially global interference; these conditions also give lower bounds on treatment effects. Through simulations of the entire process of experimentation in networks, we measure the performance of these methods under varied network structure and varied social behaviors, finding substantial bias reductions and, despite a bias\u2013variance tradeoff, error reductions. These improvements are largest for networks with more clustering and data generating processes with both stronger direct effects of the treatment and stronger interactions between units."
            },
            {
                "arxivId": "1305.6979",
                "title": "Graph cluster randomization: network exposure to multiple universes",
                "abstract": "A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how graph cluster randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified. Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a restricted-growth condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2404.03794",
        "category": "econ",
        "title": "Effect of State and Local Sexual Orientation Anti-Discrimination Laws on Labor Market Differentials",
        "abstract": "This paper presents the first quasi-experimental research examining the effect of both local and state anti-discrimination laws on sexual orientation on the labor supply and wages of lesbian, gay, and bisexual (LGB) workers. To do so, we use the American Community Survey data on household composition to infer sexual orientation and combine this with a unique panel dataset on local anti-discrimination laws. Using variation in law implementation across localities over time and between same-sex and different-sex couples, we find that anti-discrimination laws significantly reduce gaps in labor force participation rate, employment, and the wage gap for gay men relative to straight men. These laws also significantly reduce the labor force participation rate, employment, and wage premium for lesbian women relative to straight women. One explanation for the reduced labor supply and wage premium is that lesbian couples begin to have more children in response to the laws. Finally, we present evidence that state anti-discrimination laws significantly and persistently increased support for same-sex marriage. This research shows that anti-discrimination laws can be an effective policy tool for reducing labor market inequalities across sexual orientation and improving sentiment toward LGB Americans.",
        "references": [
            {
                "arxivId": "2108.12419",
                "title": "Revisiting event study designs: robust and efficient estimation",
                "abstract": "We develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Our method applies with time-varying controls, in triple-difference designs, and with certain non-binary treatments. We show the practical relevance of our results in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter - about half as large as benchmark estimates used to calibrate macroeconomic models - and predominantly occurs in the first month after the rebate."
            },
            {
                "arxivId": "1807.06698",
                "title": "Pink Work: Same-Sex Marriage, Employment and Discrimination",
                "abstract": "Abstract This paper analyzes how the legalization of same-sex marriage in the U.S. affected same-sex couples in the labor market by using data from the American Community Survey. Access to marriage led to amendments in tax, health insurance, and adoption laws that could have encouraged some same-sex partners to specialize in household production and decrease their labor supply. Nevertheless, estimates from a difference-in-difference model show that the individual and joint probabilities of being employed increased among same-sex couples. Additional evidence suggests that these changes in employment were driven by improvements in attitudes and lower discrimination against sexual minorities following the introduction of marriage equality."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2404.03989",
        "category": "econ",
        "title": "Instruments And Effects Of Monetary And Fiscal Policy: The Relationship Between Inflation, Vat, And Deposit Interest Rate",
        "abstract": "In this study, we aimed to examine the effect of VAT revenues and Deposit Interest Rates on Inflation in Turkey between 1985-2022. Within the framework of econometric analysis of the obtained data, the analysis was carried out using ADF unit root test, Johansen Co-Integration Test, Error Terms and VECM (Vector Error Correction Model) models. According to the analysis results, it was understood that the data were stationary at the I(I) level, it was determined that there was a cointegrated relationship between them in the long term, and by estimating the error term, causality findings were determined within the framework of VECM analysis. According to the causality results of the Wald Test; causality is found from Deposit Interest Rate to VAT and Inflation, and from Inflation to VAT and Deposit Interest Rate (bidirectional), while causality is also found from VAT to Inflation and Deposit Interest Rates.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2404.04097",
        "category": "econ",
        "title": "Subscription-Based Inventory Planning for E-Grocery Retailing",
        "abstract": "The growing e-grocery sector faces challenges in becoming profitable due to heightened customer expectations and logistical complexities. This paper addresses the impact of uncertainty in customer demand on inventory planning for online grocery retailers. Given the perishable nature of grocery products and intense market competition, retailers must ensure product availability while minimising overstocking costs. We propose introducing subscription offers as a solution to mitigate these inventory challenges. Unlike existing literature focusing on uniform subscription models that may harm profitability, our approach considers the synergy between implementing product subscriptions and cost savings from improved inventory planning. We present a three-step procedure enabling retailers to understand uncertainty costs, quantify the value of gathering additional planning information, and implement profitability-enhancing subscription offers. This holistic approach ensures the development of sustainable subscription models in the e-grocery domain.",
        "references": [
            {
                "arxivId": "2205.06572",
                "title": "Dynamic Stochastic Inventory Management in E-Grocery Retailing",
                "abstract": "E-grocery retailing enables ordering products online to be delivered at a future time slot chosen by the customer. This emerging field of business provides retailers with large and comprehensive new data sets, yet creates several challenges for the inventory management process. For example, the risk of a single item's stock-out leading to a complete cancellation of the shopping process is higher in e-grocery than in traditional store retailing. As a consequence, retailers aim at very high service level targets to provide satisfactory customer service and to ensure long-term business growth. When determining replenishment order quantities, it is of crucial importance to precisely account for the full uncertainty in the inventory process. This requires predictive and prescriptive analytics to (1) estimate suitable underlying probability distributions to represent the uncertainty caused by non-stationary customer demand, shelf lives, and supply, and to (2) integrate those forecasts into a comprehensive multi-period optimisation framework. In this paper, we model this stochastic dynamic problem by a sequential decision process that allows us to avoid simplifying assumptions commonly made in the literature, such as the focus on a single demand period. As the resulting problem will typically be analytically intractable, we propose a stochastic lookahead policy incorporating Monte Carlo techniques to fully propagate the associated uncertainties in order to derive replenishment order quantities. This policy naturally integrates probabilistic forecasts and allows us to explicitly derive the value of accounting for probabilistic information compared to myopic or deterministic approaches in a simulation-based setting. In addition, we evaluate our policy in a case study based on real-world data where underlying probability distributions are estimated from historical data and explanatory variables."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2404.04105",
        "category": "econ",
        "title": "Judgment in macroeconomic output growth predictions: Efficiency, accuracy and persistence",
        "abstract": "The present study applies observations of individual predictions of the first three releases of the US output growth rate to evaluate how the applied judgment affects prediction efficiency and accuracy as well as if judgment is persistent. While the first two issues have been assessed in other studies, there is little evidence on the formation of judgment in macroeconomic projections. Most of the forecasters produce unbiased predictions, but employing the median Bloomberg projection as baseline, it turns out that judgment generally does not improve accuracy. There seems to be persistence in the judgment applied by forecasters in the sense that the sign of the adjustment in the first release prediction carries over to the projections of the two following revisions. One possible explanation is that forecasters use some kind of anchor-and-adjustment heuristic.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-08.json",
        "arxivId": "2404.04121",
        "category": "econ",
        "title": "Productivity and quality-adjusted life years: QALYs, PALYs and beyond",
        "abstract": "We develop a unified framework for the measurement and valuation of health and productivity. Within this framework, we characterize evaluation functions allowing for compromises between the classical quality-adjusted life years (QALYs) and its polar productivity-adjusted life years (PALYs). Our framework and characterization results provide a new normative basis for the economic evaluation of health care interventions, as well as occupational health and safety policies, aimed to impact both health and productivity of individuals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "1906.10258",
        "category": "econ",
        "title": "Policy Targeting under Network Interference",
        "abstract": "\n This paper studies the problem of optimally allocating treatments in the presence of spillover effects, using information from a (quasi-)experiment. I introduce a method that maximizes the sample analog of average social welfare when spillovers occur. I construct semi-parametric welfare estimators with known and unknown propensity scores and cast the optimization problem into a mixed-integer linear program, which can be solved using off-the-shelf algorithms. I derive a strong set of guarantees on regret, i.e., the difference between the maximum attainable welfare and the welfare evaluated at the estimated policy. The proposed method presents attractive features for applications: (i) it does not require network information of the target population; (ii) it exploits heterogeneity in treatment effects for targeting individuals; (iii) it does not rely on the correct specification of a particular structural model; and (iv) it accommodates constraints on the policy function. An application for targeting information on social networks illustrates the advantages of the method.",
        "references": [
            {
                "arxivId": "2103.01470",
                "title": "Network Cluster-Robust Inference",
                "abstract": "Since network data commonly consists of observations from a single large network, researchers often partition the network into clusters in order to apply cluster\u2010robust inference methods. Existing such methods require clusters to be asymptotically independent. Under mild conditions, we prove that, for this requirement to hold for network\u2010dependent data, it is necessary and sufficient that clusters have low conductance, the ratio of edge boundary size to volume. This yields a simple measure of cluster quality. We find in simulations that when clusters have low conductance, cluster\u2010robust methods control size better than HAC estimators. However, for important classes of networks lacking low\u2010conductance clusters, the former can exhibit substantial size distortion. To determine the number of low\u2010conductance clusters and construct them, we draw on results in spectral graph theory that connect conductance to the spectrum of the graph Laplacian. Based on these results, we propose to use the spectrum to determine the number of low\u2010conductance clusters and spectral clustering to construct them."
            },
            {
                "arxivId": "2012.13710",
                "title": "Analysis of Randomized Experiments with Network Interference and Noncompliance",
                "abstract": "Randomized experiments have become a standard tool in economics. In analyzing randomized experiments, the traditional approach has been based on the Stable Unit Treatment Value (SUTVA: Rubin (1990)) assumption which dictates that there is no interference between individuals. However, the SUTVA assumption fails to hold in many applications due to social interaction, general equilibrium, and/or externality effects. While much progress has been made in relaxing the SUTVA assumption, most of this literature has only considered a setting with perfect compliance to treatment assignment. In practice, however, noncompliance occurs frequently where the actual treatment receipt is different from the assignment to the treatment. In this paper, we study causal effects in randomized experiments with network interference and noncompliance. Spillovers are allowed to occur at both treatment choice stage and outcome realization stage. In particular, we explicitly model treatment choices of agents as a binary game of incomplete information where resulting equilibrium treatment choice probabilities affect outcomes of interest. Outcomes are further characterized by a random coefficient model to allow for general unobserved heterogeneity in the causal effects. After defining our causal parameters of interest, we propose a simple control function estimator and derive its asymptotic properties under large-network asymptotics. We apply our methods to the randomized subsidy program of Dupas (2014) where we find evidence of spillover effects on both short-run and long-run adoption of insecticide-treated bed nets. Finally, we illustrate the usefulness of our methods by analyzing the impact of counterfactual subsidy policies."
            },
            {
                "arxivId": "2012.04055",
                "title": "Who should get vaccinated? Individualized allocation of vaccines over SIR network",
                "abstract": null
            },
            {
                "arxivId": "2007.13302",
                "title": "Random graph asymptotics for treatment effect estimation under network interference",
                "abstract": "The network interference model for causal inference places all experimental units at the vertices of an undirected exposure graph, such that treatment assigned to one unit may affect the outcome of another unit if and only if these two units are connected by an edge. This model has recently gained popularity as means of incorporating interference effects into the Neyman--Rubin potential outcomes framework; and several authors have considered estimation of various causal targets, including the direct and indirect effects of treatment. In this paper, we consider large-sample asymptotics for treatment effect estimation under network interference in a setting where the exposure graph is a random draw from a graphon. When targeting the direct effect, we show that---in our setting---popular estimators are considerably more accurate than existing results suggest, and provide a central limit theorem in terms of moments of the graphon. Meanwhile, when targeting the indirect effect, we leverage our generative assumptions to propose a consistent estimator in a setting where no other consistent estimators are currently available. We also show how our results can be used to conduct a practical assessment of the sensitivity of randomized study inference to potential interference effects. Overall, our results highlight the promise of random graph asymptotics in understanding the practicality and limits of causal inference under network interference."
            },
            {
                "arxivId": "2003.06023",
                "title": "Causal Spillover Effects Using Instrumental Variables",
                "abstract": "Abstract I set up a potential outcomes framework to analyze spillover effects using instrumental variables. I characterize the population compliance types in a setting in which spillovers can occur on both treatment take-up and outcomes, and provide conditions for identification of the marginal distribution of compliance types. I show that intention-to-treat (ITT) parameters aggregate multiple direct and spillover effects for different compliance types, and hence do not have a clear link to causally interpretable parameters. Moreover, rescaling ITT parameters by first-stage estimands generally recovers a weighted combination of average effects where the sum of weights is larger than one. I then analyze identification of causal direct and spillover effects under one-sided noncompliance, and show that causal effects can be estimated by 2SLS in this case. I illustrate the proposed methods using data from an experiment on social interactions and voting behavior. I also introduce an alternative assumption, independence of the peers\u2019 types, that identifies parameters of interest under two-sided noncompliance by restricting the amount of heterogeneity in average potential outcomes. Supplementary material of this article will be available in online."
            },
            {
                "arxivId": "2001.06052",
                "title": "Recovering Network Structure from Aggregated Relational Data using Penalized Regression",
                "abstract": "Social network data can be expensive to collect. Breza et al. (2017) propose aggregated relational data (ARD) as a low-cost substitute that can be used to recover the structure of a latent social network when it is generated by a specific parametric random effects model. Our main observation is that many economic network formation models produce networks that are effectively low-rank. As a consequence, network recovery from ARD is generally possible without parametric assumptions using a nuclear-norm penalized regression. We demonstrate how to implement this method and provide finite-sample bounds on the mean squared error of the resulting estimator for the distribution of network links. Computation takes seconds for samples with hundreds of observations. Easy-to-use code in R and Python can be found at https://github.com/mpleung/ARD."
            },
            {
                "arxivId": "1911.07085",
                "title": "Causal Inference Under Approximate Neighborhood Interference",
                "abstract": "This paper studies causal inference in randomized experiments under network interference. Commonly used models of interference posit that treatments assigned to alters beyond a certain network distance from the ego have no effect on the ego's response. However, this assumption is violated in common models of social interactions. We propose a substantially weaker model of \u201capproximate neighborhood interference\u201d (ANI) under which treatments assigned to alters further from the ego have a smaller, but potentially nonzero, effect on the ego's response. We formally verify that ANI holds for well\u2010known models of social interactions. Under ANI, restrictions on the network topology, and asymptotics under which the network size increases, we prove that standard inverse\u2010probability weighting estimators consistently estimate useful exposure effects and are approximately normal. For inference, we consider a network HAC variance estimator. Under a finite population model, we show that the estimator is biased but that the bias can be interpreted as the variance of unit\u2010level exposure effects. This generalizes Neyman's well\u2010known result on conservative variance estimation to settings with interference."
            },
            {
                "arxivId": "1909.03489",
                "title": "Multiway Cluster Robust Double/Debiased Machine Learning",
                "abstract": "Abstract This article investigates double/debiased machine learning (DML) under multiway clustered sampling environments. We propose a novel multiway cross-fitting algorithm and a multiway DML estimator based on this algorithm. We also develop a multiway cluster robust standard error formula. Simulations indicate that the proposed procedure has favorable finite sample performance. Applying the proposed method to market share data for demand analysis, we obtain larger two-way cluster robust standard errors for the price coefficient than nonrobust ones in the demand model."
            },
            {
                "arxivId": "1905.09751",
                "title": "Learning When-to-Treat Policies",
                "abstract": "Abstract Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an \u201cadvantage doubly robust\u201d estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1905.04325",
                "title": "Seeding with Costly Network Information",
                "abstract": "Seeding the most influential individuals based on the contact structure can substantially enhance the extent of a spread over the social network. Most of the influence maximization literature assumes the knowledge of the entire network graph. However, in practice, obtaining full knowledge of the network structure is very costly. We propose polynomial-time algorithms that provide almost tight approximation guarantees using a bounded number of queries to the graph structure. We also provide impossibility results to lower bound the query complexity and show tightness of our guarantees."
            },
            {
                "arxivId": "1905.00744",
                "title": "Sparsity Double Robust Inference of Average Treatment Effects",
                "abstract": "Many popular methods for building confidence intervals on causal effects under high-dimensional confounding require strong \"ultra-sparsity\" assumptions that may be difficult to validate in practice. To alleviate this difficulty, we here study a new method for average treatment effect estimation that yields asymptotically exact confidence intervals assuming that either the conditional response surface or the conditional probability of treatment allows for an ultra-sparse representation (but not necessarily both). This guarantee allows us to provide valid inference for average treatment effect in high dimensions under considerably more generality than available baselines. In addition, we showcase that our results are semi-parametrically efficient."
            },
            {
                "arxivId": "1905.04028",
                "title": "Demand and Welfare Analysis in Discrete Choice Models with Social Interactions",
                "abstract": "\n Many real-life settings of individual choice involve social interactions, causing targeted policies to have spillover effects. This paper develops novel empirical tools for analyzing demand and welfare effects of policy interventions in binary choice settings with social interactions. Examples include subsidies for health product adoption and vouchers for attending a high-achieving school. We show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand prediction under interactions, are insufficient for welfare calculations. This is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare effects and deadweight-loss from a policy intervention. Standard index restrictions imply distribution-free bounds on welfare. We propose ways to identify and consistently estimate the structural parameters and welfare bounds allowing for unobserved group effects that are potentially correlated with observables and are possibly unbounded. We illustrate our results using experimental data on mosquito-net adoption in rural Kenya."
            },
            {
                "arxivId": "1904.01047",
                "title": "Dynamically optimal treatment allocation using Reinforcement Learning",
                "abstract": "Devising guidance on how to assign individuals to treatment is an important goal of empirical research. In practice individuals often arrive sequentially, and the planner faces various constraints such as limited budget/capacity, or borrowing constraints, or the need to place people in a queue. For instance, a governmental body may receive a budget outlay at the beginning of an year, and it may need to decide how best to allocate resources within the year to individuals who arrive sequentially. In this and other examples involving inter-temporal trade-offs, previous work on devising optimal policy rules in a static context is either not applicable, or is sub-optimal. Here we show how one can use offline observational data to estimate an optimal policy rule that maximizes ex-ante expected welfare in this dynamic context. We allow the class of policy rules to be restricted for computational, legal or incentive compatibility reasons. The problem is equivalent to one of optimal control under a constrained policy class, and we exploit recent developments in Reinforcement Learning (RL) to propose an algorithm to solve this. The algorithm is easily implementable and computationally efficient, with speedups achieved through multiple RL agents learning in parallel processes. We also characterize the statistical regret from using our estimated policy rule. To do this, we show that a Partial Differential Equation (PDE) characterizes the evolution of the value function under each policy. The data enables us to obtain a sample version of the PDE that provides estimates of these value functions. The estimated policy rule is the one with the maximal estimated value function. Using the theory of viscosity solutions to PDEs we show that the policy regret decays at a $n^{-1/2}$ rate in most examples; this is the same rate as that obtained in the static case."
            },
            {
                "arxivId": "1903.09679",
                "title": "Identification and Estimation of a Partially Linear Regression Model Using Network Data",
                "abstract": "I study a regression model in which one covariate is an unknown function of a latent driver of link formation in a network. Rather than specify and fit a parametric network formation model, I introduce a new method based on matching pairs of agents with similar columns of the squared adjacency matrix, the \n ijth entry of which contains the number of other agents linked to both agents \n i and \n j. The intuition behind this approach is that for a large class of network formation models the columns of the squared adjacency matrix characterize all of the identifiable information about individual linking behavior. In this paper, I describe the model, formalize this intuition, and provide consistent estimators for the parameters of the regression model.\n"
            },
            {
                "arxivId": "1903.02124",
                "title": "Experimenting in Equilibrium",
                "abstract": "Classical approaches to experimental design assume that intervening on one unit does not affect other units. There are many important settings, however, where this noninterference assumption does not hold, as when running experiments on supply-side incentives on a ride-sharing platform or subsidies in an energy marketplace. In this paper, we introduce a new approach to experimental design in large-scale stochastic systems with considerable cross-unit interference, under an assumption that the interference is structured enough that it can be captured via mean-field modeling. Our approach enables us to accurately estimate the effect of small changes to system parameters by combining unobtrusive randomization with lightweight modeling, all while remaining in equilibrium. We can then use these estimates to optimize the system by gradient descent. Concretely, we focus on the problem of a platform that seeks to optimize supply-side payments [Formula: see text] in a centralized marketplace where different suppliers interact via their effects on the overall supply-demand equilibrium, and we show that our approach enables the platform to optimize [Formula: see text] in large systems using vanishingly small perturbations. This paper was accepted by Hamid Nazerzadeh, big data analytics."
            },
            {
                "arxivId": "1903.01059",
                "title": "Limit theorems for network dependent random variables",
                "abstract": null
            },
            {
                "arxivId": "1812.04195",
                "title": "Measuring Diffusion over a Large Network",
                "abstract": "\n This paper introduces a measure of the diffusion of binary outcomes over a large, sparse network, when the diffusion is observed in two time periods. The measure captures the aggregated spillover effect of the state-switches in the initial period on their neighbors\u2019 outcomes in the second period. This paper introduces a causal network that captures the causal connections among the cross-sectional units over the two periods. It shows that when the researcher's observed network contains the causal network as a subgraph, the measure of diffusion is identified as a simple, spatio-temporal dependence measure of observed outcomes. When the observed network does not satisfy this condition, but the spillover effect is nonnegative, the spatio-temporal dependence measure serves as a lower bound for diffusion. Using this, a lower confidence bound for diffusion is proposed and its asymptotic validity is established. The Monte Carlo simulation studies demonstrate the finite sample stability of the inference across a range of network configurations. The paper applies the method to data on Indian villages to measure the diffusion of microfinancing decisions over households\u2019 social networks."
            },
            {
                "arxivId": "1810.04778",
                "title": "Offline Multi-Action Policy Learning: Generalization and Optimization",
                "abstract": "As a result of digitization of the economy, more and more decision makers from a wide range of domains have gained the ability to target products, services, and information provision based on individual characteristics. Examples include selecting offers, prices, advertisements, or emails to send to consumers, choosing a bid to submit in a contextual first-price auctions, and determining which medication to prescribe to a patient. The key to enabling this is to learn a treatment policy from historical observational data in a sample-efficient way, hence uncovering the best personalized treatment choice recommendation. In \u201cOffline Policy Learning: Generalization and Optimization,\u201d Z. Zhou, S. Athey, and S. Wager provide a sample-optimal policy learning algorithm that is computationally efficient and that learns a tree-based treatment policy from observational data. In our quest toward fully automated personalization, the work provides a theoretically sound and practically implementable approach."
            },
            {
                "arxivId": "1812.10576",
                "title": "Deconfounding Reinforcement Learning in Observational Settings",
                "abstract": "We propose a general formulation for addressing reinforcement learning (RL) problems in settings with observational data. That is, we consider the problem of learning good policies solely from historical data in which unobserved factors (confounders) affect both observed actions and rewards. Our formulation allows us to extend a representative RL algorithm, the Actor-Critic method, to its deconfounding variant, with the methodology for this extension being easily applied to other RL algorithms. In addition to this, we develop a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Using this benchmark, we demonstrate that the proposed algorithms are superior to traditional RL methods in confounded environments with observational data. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full RL problems with observational data. Code is available at this https URL."
            },
            {
                "arxivId": "1809.09953",
                "title": "Deep Neural Networks for Estimation and Inference: Application to Causal Effects and Other Semiparametric Estimands",
                "abstract": "We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second\u2010step inference after first\u2010step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now\u2010common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed\u2010width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression\u2010type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing."
            },
            {
                "arxivId": "1809.09561",
                "title": "Evaluating stochastic seeding strategies in networks",
                "abstract": "When trying to maximize the adoption of a behavior in a population connected by a social network, it is common to strategize about where in the network to seed the behavior, often with an element of randomness. Selecting seeds uniformly at random is a basic but compelling strategy in that it distributes seeds broadly throughout the network. A more sophisticated stochastic strategy, one-hop targeting, is to select random network neighbors of random individuals; this exploits a version of the friendship paradox, whereby the friend of a random individual is expected to have more friends than a random individual, with the hope that seeding a behavior at more connected individuals leads to more adoption. Many seeding strategies have been proposed, but empirical evaluations have demanded large field experiments designed specifically for this purpose and have yielded relatively imprecise comparisons of strategies. Here we show how stochastic seeding strategies can be evaluated more efficiently in such experiments, how they can be evaluated \u201coff-policy\u201d using existing data arising from experiments designed for other purposes, and how to design more efficient experiments. In particular, we consider contrasts between stochastic seeding strategies and analyze nonparametric estimators adapted from policy evaluation and importance sampling. We use simulations on real networks to show that the proposed estimators and designs can substantially increase precision while yielding valid inference. We then apply our proposed estimators to two field experiments, one that assigned households to an intensive marketing intervention and one that assigned students to an antibullying intervention. This paper was accepted by Gui Liberali, Special Issue on Data-Driven Prescriptive Analytics."
            },
            {
                "arxivId": "1808.05293",
                "title": "Design-Based Analysis in Difference-in-Differences Settings with Staggered Adoption",
                "abstract": "In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is is an unbiased estimator of a particular weighted average causal effect. We characterize the proeperties of this estimand, and show that the standard variance estimator is conservative."
            },
            {
                "arxivId": "1807.01635",
                "title": "Randomization Inference for Peer Effects",
                "abstract": "Abstract Many previous causal inference studies require no interference, that is, the potential outcomes of a unit do not depend on the treatments of other units. However, this no-interference assumption becomes unreasonable when a unit interacts with other units in the same group or cluster. In a motivating application, a top Chinese university admits students through two channels: the college entrance exam (also known as Gaokao) and recommendation (often based on Olympiads in various subjects). The university randomly assigns students to dorms, each of which hosts four students. Students within the same dorm live together and have extensive interactions. Therefore, it is likely that peer effects exist and the no-interference assumption does not hold. It is important to understand peer effects, because they give useful guidance for future roommate assignment to improve the performance of students. We define peer effects using potential outcomes. We then propose a randomization-based inference framework to study peer effects with arbitrary numbers of peers and peer types. Our inferential procedure does not assume any parametric model on the outcome distribution. Our analysis gives useful practical guidance for policy makers of the university. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1806.07422",
                "title": "Doubly robust estimation in observational studies with partial interference",
                "abstract": "Interference occurs when the treatment (or exposure) of one individual affects the outcomes of others. In some settings, it may be reasonable to assume that individuals can be partitioned into clusters such that there is no interference between individuals in different clusters, that is, there is partial interference. In observational studies with partial interference, inverse probability weighted (IPW) estimators have been something else different possible treatment effects. However, the validity of IPW estimators depends on the propensity score being known or correctly modelled. Alternatively, one can estimate the treatment effect using an outcome regression model. In this paper, we propose doubly robust (DR) estimators that utilize both models and are consistent and asymptotically normal if either model, but not necessarily both, is correctly specified. Empirical results are presented to demonstrate the DR property of the proposed estimators and the efficiency gain of DR over IPW estimators when both models are correctly specified. The different estimators are illustrated using data from a study examining the effects of cholera vaccination in Bangladesh."
            },
            {
                "arxivId": "1806.05081",
                "title": "LASSO-Driven Inference in Time and Space",
                "abstract": "We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak dependence. A sequence of large-scale regressions with LASSO is applied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure. Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors."
            },
            {
                "arxivId": "1711.07077",
                "title": "Estimation Considerations in Contextual Bandits",
                "abstract": "Contextual bandit algorithms seek to learn a personalized treatment assignment policy, balancing exploration against exploitation. Although a number of algorithms have been proposed, there is little guidance available for applied researchers to select among various approaches. Motivated by the econometrics and statistics literatures on causal effects estimation, we study a new consideration to the exploration vs. exploitation framework, which is that the way exploration is conducted in the present may contribute to the bias and variance in the potential outcome model estimation in subsequent stages of learning. We leverage parametric and non-parametric statistical estimation methods and causal effect estimation methods in order to propose new contextual bandit designs. Through a variety of simulations, we show how alternative design choices impact the learning performance and provide insights on why we observe these effects."
            },
            {
                "arxivId": "1711.06399",
                "title": "AVERAGE TREATMENT EFFECTS IN THE PRESENCE OF UNKNOWN INTERFERENCE.",
                "abstract": "We investigate large-sample properties of treatment effect estimators under unknown interference in randomized experiments. The inferential target is a generalization of the average treatment effect estimand that marginalizes over potential spillover effects. We show that estimators commonly used to estimate treatment effects under no interference are consistent for the generalized estimand for several common experimental designs under limited but otherwise arbitrary and unknown interference. The rates of convergence depend on the rate at which the amount of interference grows and the degree to which it aligns with dependencies in treatment assignment. Importantly for practitioners, the results imply that if one erroneously assumes that units do not interfere in a setting with limited, or even moderate, interference, standard estimators are nevertheless likely to be close to an average treatment effect if the sample is sufficiently large. Conventional confidence statements may, however, not be accurate."
            },
            {
                "arxivId": "1711.02745",
                "title": "Identification and Estimation of Spillover Effects in Randomized Experiments",
                "abstract": "I study identification, estimation and inference for spillover effects in experiments where units' outcomes may depend on the treatment assignments of other units within a group. I show that the commonly-used linear-in-means (LIM) regression identifies a weighted sum of spillover effects with some negative weights, and that the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. I propose nonparametric estimators for average direct and spillover effects that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. These findings are illustrated using data from a conditional cash transfer program and with simulations. The empirical results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation: the estimated difference in means and the LIM coefficients are all close to zero and statistically insignificant, whereas the nonparametric estimators I propose reveal large, nonlinear and significant spillover effects."
            },
            {
                "arxivId": "1710.06026",
                "title": "Targeting Interventions in Networks",
                "abstract": "We study games in which a network mediates strategic spillovers and externalities among the players. How does a planner optimally target interventions that change individuals' private returns to investment? We analyze this question by decomposing any intervention into orthogonal \n principal components, which are determined by the network and are ordered according to their associated eigenvalues. There is a close connection between the nature of spillovers and the representation of various principal components in the optimal intervention. In games of strategic complements (substitutes), interventions place more weight on the top (bottom) principal components, which reflect more global (local) network structure. For large budgets, optimal interventions are simple\u2014they essentially involve only a single principal component.\n"
            },
            {
                "arxivId": "1710.04656",
                "title": "Behavioral Communities and the Atomic Structure of Networks",
                "abstract": "We develop a method of detecting the `behavioral communities' of a social network based on how people act when they choose their behaviors in coordination with their friends' behaviors. There can be multiple different `conventions' (equilibria) in which people in some parts of the network adopt a behavior while people in other parts of the network do not. We define atoms/communities to be groups of people who behave the same as each other in every convention. This provides a microfoundation for a method of detecting communities in social and economic networks. We characterize such behavioral communities in some random graphs as a function of how strongly the benefits of adopting the behavior depend on others' behaviors. We also discuss applications including: optimally seeding the diffusion of behaviors involving peer influence, detecting which demographics or nodal characteristics define a society's communities, estimating the strength of peer influence on behavior, as well as identifying missing network data by observing a series of conventions."
            },
            {
                "arxivId": "1706.01778",
                "title": "Sampling\u2010Based versus Design\u2010Based Uncertainty in Regression Analysis",
                "abstract": "Consider a researcher estimating the parameters of a regression function based on data for all 50 states in the United States or on data for all visits to a website. What is the interpretation of the estimated parameters and the standard errors? In practice, researchers typically assume that the sample is randomly drawn from a large population of interest and report standard errors that are designed to capture sampling variation. This is common even in applications where it is difficult to articulate what that population of interest is, and how it differs from the sample. In this article, we explore an alternative approach to inference, which is partly design\u2010based. In a design\u2010based setting, the values of some of the regressors can be manipulated, perhaps through a policy intervention. Design\u2010based uncertainty emanates from lack of knowledge about the values that the regression outcome would have taken under alternative interventions. We derive standard errors that account for design\u2010based uncertainty instead of, or in addition to, sampling\u2010based uncertainty. We show that our standard errors in general are smaller than the usual infinite\u2010population sampling\u2010based standard errors and provide conditions under which they coincide."
            },
            {
                "arxivId": "1705.08527",
                "title": "Causal Inference for Social Network Data",
                "abstract": "We extend recent work by van der Laan (2014) on causal inference for causally connected units to more general social network settings. Our asymptotic results allow for dependence of each observation on a growing number of other units as sample size increases. We are not aware of any previous methods for inference about network members in observational settings that allow the number of ties per node to increase as the network grows. While previous methods have generally implicitly focused on one of two possible sources of dependence among social network observations, we allow for both dependence due to contagion, or transmission of information across network ties, and for dependence due to latent similarities among nodes sharing ties. We describe estimation and inference for causal effects that are specifically of interest in social network settings."
            },
            {
                "arxivId": "1703.04157",
                "title": "Using Aggregated Relational Data to Feasibly Identify Network Structure Without Network Data",
                "abstract": "Social network data are often prohibitively expensive to collect, limiting empirical network research. We propose an inexpensive and feasible strategy for network elicitation using Aggregated Relational Data (ARD): responses to questions of the form \"how many of your links have trait k ?\" Our method uses ARD to recover parameters of a network formation model, which permits sampling from a distribution over node- or graph-level statistics. We replicate the results of two field experiments that used network data and draw similar conclusions with ARD alone."
            },
            {
                "arxivId": "1702.02896",
                "title": "Efficient Policy Learning",
                "abstract": "We consider the problem of using observational data to learn treatment assignment policies that satisfy certain constraints specified by a practitioner, such as budget, fairness, or functional form constraints. This problem has previously been studied in economics, statistics, and computer science, and several regret-consistent methods have been proposed. However, several key analytical components are missing, including a characterization of optimal methods for policy learning, and sharp bounds for minimax regret. In this paper, we derive lower bounds for the minimax regret of policy learning under constraints, and propose a method that attains this bound asymptotically up to a constant factor. Whenever the class of policies under consideration has a bounded Vapnik-Chervonenkis dimension, we show that the problem of minimax-regret policy learning can be asymptotically reduced to first efficiently evaluating how much each candidate policy improves over a randomized baseline, and then maximizing this value estimate. Our analysis relies on uniform generalizations of classical semiparametric efficiency results for average treatment effect estimation, paired with sharp concentration bounds for weighted empirical risk minimization that may be of independent interest."
            },
            {
                "arxivId": "1610.02738",
                "title": "Best subset binary prediction",
                "abstract": "We consider a variable selection problem for the prediction of binary outcomes. We study the best subset selection procedure by which the explanatory variables are chosen by maximizing Manski (1975, 1985)'s maximum score type objective function subject to a constraint on the maximal number of selected variables. We show that this procedure can be equivalently reformulated as solving a mixed integer optimization (MIO) problem, which enables computation of the exact or an approximate solution with a de finite approximation error bound. In terms of theoretical results, we obtain non-asymptotic upper and lower risk bounds when the dimension of potential covariates is possibly much larger than the sample size. Our upper and lower risk bounds are minimax rate-optimal when the maximal number of selected variables is fi xed and does not increase with the sample size. We illustrate usefulness of the best subset binary prediction approach via Monte Carlo simulations and an empirical application of the work-trip transportation mode choice."
            },
            {
                "arxivId": "1609.06245",
                "title": "Identification and Estimation of Treatment and Interference Effects in Observational Studies on Networks",
                "abstract": "Abstract\u2013Causal inference on a population of units connected through a network often presents technical challenges, including how to account for interference. In the presence of interference, for instance, potential outcomes of a unit depend on their treatment as well as on the treatments of other units, such as their neighbors in the network. In observational studies, a further complication is that the typical unconfoundedness assumption must be extended\u2014say, to include the treatment of neighbors, and individual and neighborhood covariates\u2014to guarantee identification and valid inference. Here, we propose new estimands that define treatment and interference effects. We then derive analytical expressions for the bias of a naive estimator that wrongly assumes away interference. The bias depends on the level of interference but also on the degree of association between individual and neighborhood treatments. We propose an extended unconfoundedness assumption that accounts for interference, and we develop new covariate-adjustment methods that lead to valid estimates of treatment and interference effects in observational studies on networks. Estimation is based on a generalized propensity score that balances individual and neighborhood covariates across units under different levels of individual treatment and of exposure to neighbors\u2019 treatment. We carry out simulations, calibrated using friendship networks and covariates in a nationally representative longitudinal study of adolescents in grades 7\u201312 in the United States, to explore finite-sample performance in different realistic settings. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1609.04464",
                "title": "Peer Encouragement Designs in Causal Inference with Partial Interference and Identification of Local Average Network Effects",
                "abstract": "In non-network settings, encouragement designs have been widely used to analyze causal effects of a treatment, policy, or intervention on an outcome of interest when randomizing the treatment was considered impractical or when compliance to treatment cannot be perfectly enforced. Unfortunately, such questions related to treatment compliance have received less attention in network settings and the most well-studied experimental design in networks, the two-stage randomization design, requires perfect compliance with treatment. The paper proposes a new experimental design called peer encouragement design to study network treatment effects when enforcing treatment randomization is not feasible. The key idea in peer encouragement design is the idea of personalized encouragement, which allows point-identification of familiar estimands in the encouragement design literature. The paper also defines new causal estimands, local average network effects, that can be identified under the new design and analyzes the effect of non-compliance behavior in randomized experiments on networks."
            },
            {
                "arxivId": "1609.03167",
                "title": "Model Selection for Treatment Choice: Penalized Welfare Maximization",
                "abstract": "This paper studies a new statistical decision rule for the treatment assignment problem. Consider a utilitarian policy maker who must use sample data to allocate one of two treatments to members of a population, based on their observable characteristics. In practice, it is often the case that policy makers do not have full discretion on how these covariates can be used, for legal, ethical or political reasons. We treat this constrained problem as a statistical decision problem, where we evaluate the performance of decision rules by their maximum regret. We focus on settings in which the policy maker may want to select amongst a collection of such constrained classes: examples we consider include choosing the number of covariates over which to perform best-subset selection, and model selection when approximating a complicated class via a sieve. We adapt and extend results from statistical learning to develop a decision rule which we call the Penalized Welfare Maximization (PWM) rule. We establish an oracle inequality for the regret of the PWM rule which shows that it is able to perform model selection over the collection of available classes. We then use this oracle inequality to derive relevant bounds on maximum regret for PWM. We illustrate the model-selection capabilities of our method with a small simulation exercise, and conclude by applying our rule to data from the Job Training Partnership Act (JTPA) study."
            },
            {
                "arxivId": "1608.08925",
                "title": "Recursive Partitioning for Personalization using Observational Data",
                "abstract": "We study the problem of learning to choose from m discrete treatment options (e.g., news item or medical drug) the one with best causal effect for a particular instance (e.g., user or patient) where the training data consists of passive observations of covariates, treatment, and the outcome of the treatment. The standard approach to this problem is regress and compare: split the training data by treatment, fit a regression model in each split, and, for a new instance, predict all m outcomes and pick the best. By reformulating the problem as a single learning task rather than m separate ones, we propose a new approach based on recursively partitioning the data into regimes where different treatments are optimal. We extend this approach to an optimal partitioning approach that finds a globally optimal partition, achieving a compact, interpretable, and impactful personalization model. We develop new tools for validating and evaluating personalization models on observational data and use these to demonstrate the power of our novel approaches in a personalized medicine and a job training application."
            },
            {
                "arxivId": "1608.05845",
                "title": "Centrality measures in networks",
                "abstract": null
            },
            {
                "arxivId": "1603.07573",
                "title": "STATISTICAL INFERENCE FOR THE MEAN OUTCOME UNDER A POSSIBLY NON-UNIQUE OPTIMAL TREATMENT STRATEGY.",
                "abstract": "We consider challenges that arise in the estimation of the mean outcome under an optimal individualized treatment strategy defined as the treatment rule that maximizes the population mean outcome, where the candidate treatment rules are restricted to depend on baseline covariates. We prove a necessary and sufficient condition for the pathwise differentiability of the optimal value, a key condition needed to develop a regular and asymptotically linear (RAL) estimator of the optimal value. The stated condition is slightly more general than the previous condition implied in the literature. We then describe an approach to obtain root-n rate confidence intervals for the optimal value even when the parameter is not pathwise differentiable. We provide conditions under which our estimator is RAL and asymptotically efficient when the mean outcome is pathwise differentiable. We also outline an extension of our approach to a multiple time point problem. All of our results are supported by simulations."
            },
            {
                "arxivId": "1510.04342",
                "title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests",
                "abstract": "ABSTRACT Many scientific and engineering challenges\u2014ranging from personalized medicine to customized marketing recommendations\u2014require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman\u2019s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates."
            },
            {
                "arxivId": "1508.03179",
                "title": "Residual Weighted Learning for Estimating Individualized Treatment Rules",
                "abstract": "ABSTRACT Personalized medicine has received increasing attention among statisticians, computer scientists, and clinical practitioners. A major component of personalized medicine is the estimation of individualized treatment rules (ITRs). Recently, Zhao et al. proposed outcome weighted learning (OWL) to construct ITRs that directly optimize the clinical outcome. Although OWL opens the door to introducing machine learning techniques to optimal treatment regimes, it still has some problems in performance. (1) The estimated ITR of OWL is affected by a simple shift of the outcome. (2) The rule from OWL tries to keep treatment assignments that subjects actually received. (3) There is no variable selection mechanism with OWL. All of them weaken the finite sample performance of OWL. In this article, we propose a general framework, called residual weighted learning (RWL), to alleviate these problems, and hence to improve finite sample performance. Unlike OWL which weights misclassification errors by clinical outcomes, RWL weights these errors by residuals of the outcome from a regression fit on clinical covariates excluding treatment assignment. We use the smoothed ramp loss function in RWL and provide a difference of convex (d.c.) algorithm to solve the corresponding nonconvex optimization problem. By estimating residuals with linear models or generalized linear models, RWL can effectively deal with different types of outcomes, such as continuous, binary, and count outcomes. We also propose variable selection methods for linear and nonlinear rules, respectively, to further improve the performance. We show that the resulting estimator of the treatment rule is consistent. We further obtain a rate of convergence for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed RWL methods is illustrated in simulation studies and in an analysis of cystic fibrosis clinical trial data. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1506.02084",
                "title": "Exact p-Values for Network Interference",
                "abstract": "ABSTRACT We study the calculation of exact p-values for a large class of nonsharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit\u2019s treatment status on another according to the distance between units, for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (e.g., based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, e.g., only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment."
            },
            {
                "arxivId": "1504.01132",
                "title": "Recursive partitioning for heterogeneous causal effects",
                "abstract": "In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without \u201csparsity\u201d assumptions. We propose an \u201chonest\u201d approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the \u201cground truth\u201d for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90% confidence intervals, whereas coverage ranges between 74% and 84% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7\u201322%."
            },
            {
                "arxivId": "1406.2293",
                "title": "Gossip: Identifying Central Individuals in a Social Network",
                "abstract": "Can we identify the members of a community who are best- placed to diffuse information simply by asking a random sample of individuals? We show that boundedly-rational individuals can, simply by tracking sources of gossip, identify those who are most central in a network according to \"diffusion centrality,\" which nests other standard centrality measures. Testing this prediction with data from 35 Indian villages, we find that respondents accurately nominate those who are diffusion central (not just those with many friends). Moreover, these nominees are more central in the network than traditional village leaders and geographically central individuals."
            },
            {
                "arxivId": "1212.6885",
                "title": "Gaussian approximation of suprema of empirical processes",
                "abstract": "We develop a new direct approach to approximating suprema of general empirical processes by a sequence of suprema of Gaussian processes, without taking the route of approximating empirical processes themselves in the sup-norm. We prove an abstract approximation theorem that is applicable to a wide variety of problems, primarily in statistics. Especially, the bound in the main approximation theorem is non-asymptotic and the theorem does not require uniform boundedness of the class of functions. The proof of the approximation theorem builds on a new coupling inequality for maxima of sums of random vectors, the proof of which depends on an effective use of Stein's method for normal approximation, and some new empirical processes techniques. We study applications of this approximation theorem to local empirical processes and series estimation in nonparametric regression where the classes of functions change with the sample size and are not Donsker-type. Importantly, our new technique is able to prove the Gaussian approximation for the supremum type statistics under considerably weak regularity conditions, especially concerning the bandwidth and the number of series functions, in those examples."
            },
            {
                "arxivId": "1212.2013",
                "title": "Concentration Inequalities in Locally Dependent Spaces",
                "abstract": "This paper studies concentration inequalities for functions of locally dependent random variables. We show that the usual definition of local dependence does not imply concentration for general Hamming Lipschitz functions. We define hypergraph dependence, which is a special case of local dependence, and show that it implies concentration if the maximal neighborhood size is small. We prove concentration in Hamming distance, Talagrand distance, and for self-bounding functions of a particular type under this dependence structure."
            },
            {
                "arxivId": "1206.1874",
                "title": "Multivariate Bernoulli distribution",
                "abstract": "In this paper, we consider the multivariate Bernoulli distribution as a model to estimate the structure of graphs with binary nodes. This distribution is discussed in the framework of the exponential family, and its statistical properties regarding independence of the nodes are demonstrated. Importantly the model can estimate not only the main effects and pairwise interactions among the nodes but also is capable of modeling higher order interactions, allowing for the existence of complex clique effects. We compare the multivariate Bernoulli model with existing graphical inference models - the Ising model and the multivariate Gaussian model, where only the pairwise interactions are considered. On the other hand, the multivariate Bernoulli distribution has an interesting property in that independence and uncorrelatedness of the component random variables are equivalent. Both the marginal and conditional distributions of a subset of variables in the multivariate Bernoulli distribution still follow the multivariate Bernoulli distribution. Furthermore, the multivariate Bernoulli logistic model is developed under generalized linear model theory by utilizing the canonical link function in order to include covariate information on the nodes, edges and cliques. We also consider variable selection techniques such as LASSO in the logistic model to impose sparsity structure on the graph. Finally, we discuss extending the smoothing spline ANOVA approach to the multivariate Bernoulli logistic model to enable estimation of non-linear effects of the predictor variables."
            },
            {
                "arxivId": "1105.3369",
                "title": "PERFORMANCE GUARANTEES FOR INDIVIDUALIZED TREATMENT RULES.",
                "abstract": "Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [11]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is non-concave in the parameters. Furthermore there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges we consider estimation based on l(1) penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule."
            },
            {
                "arxivId": "1103.4601",
                "title": "Doubly Robust Policy Evaluation and Learning",
                "abstract": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. \nIn this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice."
            },
            {
                "arxivId": "1004.4792",
                "title": "Social Network Sensors for Early Detection of Contagious Outbreaks",
                "abstract": "Current methods for the detection of contagious outbreaks give contemporaneous information about the course of an epidemic at best. It is known that individuals near the center of a social network are likely to be infected sooner during the course of an outbreak, on average, than those at the periphery. Unfortunately, mapping a whole network to identify central individuals who might be monitored for infection is typically very difficult. We propose an alternative strategy that does not require ascertainment of global network structure, namely, simply monitoring the friends of randomly selected individuals. Such individuals are known to be more central. To evaluate whether such a friend group could indeed provide early detection, we studied a flu outbreak at Harvard College in late 2009. We followed 744 students who were either members of a group of randomly chosen individuals or a group of their friends. Based on clinical diagnoses, the progression of the epidemic in the friend group occurred 13.9 days (95% C.I. 9.9\u201316.6) in advance of the randomly chosen group (i.e., the population as a whole). The friend group also showed a significant lead time (p<0.05) on day 16 of the epidemic, a full 46 days before the peak in daily incidence in the population as a whole. This sensor method could provide significant additional time to react to epidemics in small or large populations under surveillance. The amount of lead time will depend on features of the outbreak and the network at hand. The method could in principle be generalized to other biological, psychological, informational, or behavioral contagions that spread in networks."
            },
            {
                "arxivId": "1010.2731",
                "title": "A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers",
                "abstract": "High-dimensional statistical inference deals with models in which the the number of parameters p is comparable to or larger than the sample size n. Since it is usually impossible to obtain consistent procedures unless p/n \u2192 0, a line of recent work has studied models with various types of structure (e.g., sparse vectors; block-structured matrices; low-rank matrices; Markov assumptions). In such settings, a general approach to estimation is to solve a regularized convex program (known as a regularized M-estimator) which combines a loss function (measuring how well the model fits the data) with some regularization function that encourages the assumed structure. The goal of this paper is to provide a unified framework for establishing consistency and convergence rates for such regularized M-estimators under high-dimensional scaling. We state one main theorem and show how it can be used to re-derive several existing results, and also to obtain several new results on consistency and convergence rates. Our analysis also identifies two key properties of loss and regularization functions, referred to as restricted strong convexity and decomposability, that ensure the corresponding regularized M-estimators have fast convergence rates."
            },
            {
                "arxivId": "physics/0509039",
                "title": "The dynamics of viral marketing",
                "abstract": "We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We then establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies product and pricing categories for which viral marketing seems to be very effective."
            },
            {
                "arxivId": "math/0410104",
                "title": "Normal approximation under local dependence",
                "abstract": "We establish both uniform and nonuniform error bounds of the Berry\u2013Esseen type in normal approximation under local dependence. These results are of an order close to the best possible if not best possible. They are more general or sharper than many existing ones in the literature. The proofs couple Stein\u2019s method with the concentration inequality approach."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2204.10275",
        "category": "econ",
        "title": "Do t-Statistic Hurdles Need to be Raised?",
        "abstract": "Many scholars have called for raising statistical hurdles to guard against false discoveries in academic publications. I show these calls may be difficult to justify empirically. Published data exhibit bias: results that fail to meet existing hurdles are often unobserved. These unobserved results must be extrapolated, which can lead to weak identification of revised hurdles. In contrast, statistics that can target only published findings (e.g. empirical Bayes shrinkage and the FDR) can be strongly identified, as data on published findings is plentiful. I demonstrate these results theoretically and in an empirical analysis of the cross-sectional return predictability literature.",
        "references": [
            {
                "arxivId": "2209.13623",
                "title": "Publication Bias in Asset Pricing Research",
                "abstract": "Researchers are more likely to share notable findings. As a result, published findings tend to overstate the magnitude of real-world phenomena. This bias is a natural concern for asset pricing research, which has found hundreds of return predictors and little consensus on their origins. Empirical evidence on publication bias comes from large scale meta-studies. Meta-studies of cross-sectional return predictability have settled on four stylized facts that demonstrate publication bias is not a dominant factor: (1) almost all findings can be replicated, (2) predictability persists out-of-sample, (3) empirical $t$-statistics are much larger than 2.0, and (4) predictors are weakly correlated. Each of these facts has been demonstrated in at least three meta-studies. Empirical Bayes statistics turn these facts into publication bias corrections. Estimates from three meta-studies find that the average correction (shrinkage) accounts for only 10 to 15 percent of in-sample mean returns and that the risk of inference going in the wrong direction (the false discovery rate) is less than 10%. Meta-studies also find that $t$-statistic hurdles exceed 3.0 in multiple testing algorithms and that returns are 30 to 50 percent weaker in alternative portfolio tests. These facts are easily misinterpreted as evidence of publication bias effects. We clarify these misinterpretations and others, including the conflating of ``mostly false findings'' with ``many insignificant findings,'' ``data snooping'' with ``liquidity effects,'' and ``failed replications'' with ``insignificant ad-hoc trading strategies.'' Meta-studies outside of the cross-sectional literature are rare. The four facts from cross-sectional meta-studies provide a framework for future research. We illustrate with a preliminary re-examination of equity premium predictability."
            },
            {
                "arxivId": "2208.09638",
                "title": "Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability",
                "abstract": "What is the purpose of pre-analysis plans, and how should they be designed? We propose a principal-agent model where a decision-maker relies on selective but truthful reports by an analyst. The analyst has data access, and non-aligned objectives. In this model, the implementation of statistical decision rules (tests, estimators) requires an incentive-compatible mechanism. We first characterize which decision rules can be implemented. We then characterize optimal statistical decision rules subject to implementability. We show that implementation requires pre-analysis plans. Focussing specifically on hypothesis tests, we show that optimal rejection rules pre-register a valid test for the case when all data is reported, and make worst-case assumptions about unreported data. Optimal tests can be found as a solution to a linear-programming problem."
            },
            {
                "arxivId": "2206.15365",
                "title": "Most Claimed Statistical Findings in Cross-Sectional Return Predictability Are Likely True",
                "abstract": "Harvey, Liu, and Zhu (2016) \u201cargue that most claimed research findings in financial economics are likely false.\u201d Surprisingly, their false discovery rate (FDR) estimates suggest most are true. I revisit their results by developing non- and semi-parametric FDR estimators that account for publication bias and empirical correlations. These estimators provide simple closed-form expressions and reliably produce an upper bound on the FDR in simulations that cluster-bootstrap from empirical predictor returns. Applying these estimators to the Chen-Zimmermann dataset of 205 predictors, I find that most claimed statistical findings in the cross-sectional predictability literature are likely true."
            },
            {
                "arxivId": "1709.07588",
                "title": "Abandon Statistical Significance",
                "abstract": "ABSTRACT We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm\u2014and the p-value thresholds intrinsic to it\u2014as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to \u201cban\u201d p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly."
            },
            {
                "arxivId": "1711.10527",
                "title": "Identification of and Correction for Publication Bias",
                "abstract": "Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study\u2019s results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.(JEL C13, C90, I23, J23, J38, L82)"
            },
            {
                "arxivId": "1709.10193",
                "title": "Forecasting with Dynamic Panel Data Models",
                "abstract": "This paper considers the problem of forecasting a collection of short time series using cross\u2010sectional information in panel data. We construct point predictors using Tweedie's formula for the posterior mean of heterogeneous coefficients under a correlated random effects distribution. This formula utilizes cross\u2010sectional information to transform the unit\u2010specific (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coefficients. We show that the risk of a predictor based on a nonparametric kernel estimate of the Tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated random effects distribution as known (ratio optimality). Our empirical Bayes predictor performs well compared to various competitors in a Monte Carlo study. In an empirical application, we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions."
            },
            {
                "arxivId": "0808.0597",
                "title": "Comment: Microarrays, Empirical Bayes and the Two-Groups Model",
                "abstract": "Abstract. Brad Efron\u2019s paper has inspired a return to the ideas be-hind Bayes, frequency and empirical Bayes. The latter preferably wouldnot be limited to exchangeable models for the data and hyperparam-eters. Parallels are revealed between microarray analyses and pro\ufb01lingof hospitals, with advances suggesting more decision modeling for geneidenti\ufb01cation also. Then good multilevel and empirical Bayes modelsfor random e\ufb00ects should be sought when regression toward the meanis anticipated. Key words and phrases: Bayes, frequency, interval estimation, ex-changeable, general model, random e\ufb00ects.1. FREQUENCY, BAYES, EMPIRICAL BAYESAND A GENERAL MODELBrad Efron\u2019s two-groups approach and the empir-ical null (\u201cnull\u201d refers to a distribution, not to ahypothesis) extension of his local fdr addresses test-ing many hypotheses simultaneously, with model-ing enabled by the repeated presence of many simi-lar problems. He assumes two-level models for ran-dom e\ufb00ects, developing theory by drawing on andcombining ideas from frequency, Bayesian and em-pirical Bayesian perspectives. The last half-centuryin statistics has seen exciting developments frommany perspectives for simultaneous estimation ofrandom e\ufb00ects, but there has been little explicit par-allel work on the complementary problem of hypoth-esis testing. That changes in Brad\u2019s paper,especiallyfor testing many hypotheses when exchangeabilityrestrictions are plausible.\u201cEmpirical Bayes\u201d is in the paper\u2019s title, said inSection 3 to be a \u201cbipolar\u201d methodology that draws"
            },
            {
                "arxivId": "math/0406519",
                "title": "A stochastic process approach to false discovery control",
                "abstract": "This paper extends the theory of false discovery rates (FDR) pioneered by Benjamini and Hochberg [J. Roy. Statist. Soc. Ser B 57 (1995) 289-300]. We develop a framework in which the False Discovery Proportion (FDP)-the number of false rejections divided by the number of rejections-is treated as a stochastic process. After obtaining the limiting distribution of the process, we demonstrate the validity of a class of procedures for controlling the False Discovery Rate (the expected FDP). We construct a confidence envelope for the whole FDP process. From these envelopes we derive confidence thresholds, for controlling the quantiles of the distribution of the FDP as well as controlling the number of false discoveries. We also investigate methods for estimating the p-value distribution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2205.09922",
        "category": "econ",
        "title": "Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models",
        "abstract": "We introduce closed-form formulas of out-of-sample predictive densities for forecasting and backcasting of mixed causal-noncausal (Structural) Vector Autoregressive VAR models. These nonlinear and time irreversible non-Gaussian VAR processes are shown to satisfy the Markov property in both calendar and reverse time. A post-estimation inference method for assessing the forecast interval uncertainty due to the preliminary estimation step is introduced too. The nonlinear past-dependent innovations of a mixed causal-noncausal VAR model are defined and their filtering and identification methods are discussed. Our approach is illustrated by a simulation study, and an application to cryptocurrency prices.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2210.10585",
        "category": "econ",
        "title": "The Minimum Wage as an Anchor: Effects on Determinations of Fairness by Humans and AI",
        "abstract": "I study the role of minimum wage as an anchor for judgements of the fairness of wages by both human subjects and artificial intelligence (AI). Through surveys of human subjects enrolled in the crowdsourcing platform Prolific.co and queries submitted to the OpenAI's language model GPT-3, I test whether the numerical response for what wage is deemed fair for a particular job description changes when respondents and GPT-3 are prompted with additional information that includes a numerical minimum wage, whether realistic or unrealistic, relative to a control where no minimum wage is stated. I find that the minimum wage influences the distribution of responses for the wage considered fair by shifting the mean response toward the minimum wage, thus establishing the minimum wage's role as an anchor for judgements of fairness. However, for unrealistically high minimum wages, namely $50 and $100, the distribution of responses splits into two distinct modes, one that approximately follows the anchor and one that remains close to the control, albeit with an overall upward shift towards the anchor. The anchor exerts a similar effect on the AI bot; however, the wage that the AI bot perceives as fair exhibits a systematic downward shift compared to human subjects' responses. For unrealistic values of the anchor, the responses of the bot also split into two modes but with a smaller proportion of the responses adhering to the anchor compared to human subjects. As with human subjects, the remaining responses are close to the control group for the AI bot but also exhibit a systematic shift towards the anchor. During experimentation, I noted some variability in the bot responses depending on small perturbations of the prompt, so I also test variability in the bot's responses with respect to more meaningful differences in gender and race cues in the prompt, finding anomalies in the distribution of responses.",
        "references": [
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1412.1897",
                "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
                "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call \u201cfooling images\u201d (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2211.16643",
        "category": "econ",
        "title": "Security Issuance, Institutional Investors and Quid Pro Quo: Insights from SPACs",
        "abstract": "Security issuance through intermediaries is subject to informational and agency-related frictions. However, separating their effects on securities has been difficult. We estimate those effects separately using SPAC data. To that end, we identify\"premium\"investors who produce value-relevant information. Their participation is associated with lower liquidation risk and higher returns. In contrast,\"non-premium\"investors engage only in quid pro quo arrangements. They receive high returns from an intermediary (quid) for participating in weaker future deals initiated by that intermediary (quo). Thus, quid pro quo is not pure agency cost; it includes transfers enabling more firms to go public.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2309.02447",
        "category": "econ",
        "title": "Economic Complexity Limits Accuracy of Price Probability Predictions by Gaussian Distributions",
        "abstract": "We discuss the economic reasons why the predictions of price and return statistical moments in the coming decades, in the best case, will be limited by their averages and volatilities. That limits the accuracy of the forecasts of price and return probabilities by Gaussian distributions. The economic origin of these restrictions lies in the fact that the predictions of the market-based n-th statistical moments of price and return for n=1,2,.., require the description of the economic variables of the n-th order that are determined by sums of the n-th degrees of values or volumes of market trades. The lack of existing models that describe the evolution of the economic variables determined by the sums of the 2nd degrees of market trades results in the fact that even predictions of the volatilities of price and return are very uncertain. One can ignore existing economic barriers that we highlight but cannot overcome or resolve them. The accuracy of predictions of price and return probabilities substantially determines the reliability of asset pricing models and portfolio theories. The restrictions on the accuracy of predictions of price and return statistical moments reduce the reliability and veracity of modern asset pricing and portfolio theories.",
        "references": [
            {
                "arxivId": "2302.07935",
                "title": "Market-Based Probability of Stock Returns",
                "abstract": "Markets possess all available information on stock returns. The randomness of market trade determines the statistics of stock returns. This paper describes the dependence of the first four market-based statistical moments of stock returns on statistical moments and correlations of current and past trade values. The mean return of trades during the averaging period coincides with Markowitz's definition of portfolio value weighted return. We derive the market-based volatility of return and return-value correlations. We present approximations of the characteristic functions and probability measures of stock return by a finite number of market-based statistical moments. To forecast market-based average return or volatility of return, one should predict the statistical moments and correlations of current and past market trade values at the same time horizon."
            },
            {
                "arxivId": "2208.07839",
                "title": "Why Economic Theories and Policies Fail? Unnoticed Variables and Overlooked Economics",
                "abstract": "Accuracy of economic theories and efficiency of economic policy strictly depend on the choice of the economic variables and processes mostly liable for description of economic reality. That states the general problem of assessment of any possible economic variables and processes chargeable for economic evolution. We show that economic variables and processes described by current economic theories constitute only a negligible fraction of factors responsible for economic dynamics. We consider numerous unnoted economic variables and overlooked economic processes those determine the states and predictions of the real economics. We regard collective economic variables, collective transactions and expectations, mean risks of economic variables and transactions, collective velocities and flows of economic variables, transactions and expectations as overlooked factors of economic evolution. We introduce market-based probability of the asset price and consider unnoticed influence of market stochasticity on randomness of macroeconomic variables. We introduce economic domain composed by continuous numeric risk grades and outline that the bounds of the economic domain result in unnoticed inherent cyclical motion of collective variables, transactions and expectations those are responsible for observed business cycles. Our treatment of unnoticed and overlooked factors of theoretical economics and policy decisions preserves a wide field of studies for many decades for academic researchers, economic authorities and high-level politicians."
            },
            {
                "arxivId": "2205.07256",
                "title": "Market-Based Asset Price Probability",
                "abstract": "We consider the randomness of market trade values and volumes as the origin of asset price stochasticity. We define the first four market-based price statistical moments that depend on statistical moments and correlations of market trade values and volumes. Market-based price statistical moments coincide with conventional frequency-based ones if all trade volumes are constant during the time averaging interval. We present approximations of market-based price probability by a finite number of price statistical moments. We consider the consequences of the use of market-based price statistical moments for asset-pricing models and Value-at-Risk. We show that the use of volume weighted average price results in zero price-volume correlations. We derive market-based correlations between price and squares of volume and between squares of price and volume. To forecast market-based price volatility at horizon T one should predict the first two statistical moments of market trade values and volumes and their correlations at the same horizon T."
            },
            {
                "arxivId": "2112.04566",
                "title": "Theoretical Economics and the Second-Order Economic Theory. What is it?",
                "abstract": "The economic and financial variables of economic agents determine macroeconomic variables. Current models consider agents' variables that are determined by the sums of values and volumes of agents' trades during some time interval {\\Delta}. We call them first-order economic variables. We describe how the volatilities and correlations of market trade values and volumes determine price volatility. We argue that such a link requests consideration of agents' economic variables of the second order that are composed of sums of squares of agents' transactions during {\\Delta}. Almost any variable of the first order should be complemented by its second-order pair. Respectively, the sums of agents' second-order variables introduce macroeconomic variables of the second order. The description of the first- and second-order macroeconomic variables establishes the subject of second-order economic theory. We highlight that the complexity of second-order economic theory essentially restricts any hopes for precise predictions of price probability and, at best, could provide estimates of price volatility. That limits the predictions of price probability to Gauss's approximations only."
            },
            {
                "arxivId": "2105.13903",
                "title": "Three Remarks On Asset Pricing",
                "abstract": "We consider the time interval \u0394 during which the market trade time-series are averaged as the key factor of the consumption-based asset-pricing model that causes modification of the basic pricing equation. The duration of \u0394 determines Taylor series of investor\u2019s utility over current and future values of consumption. We present consumption at current and future moments as sums of their mean values and perturbations during \u0394 of the price at current moment t and perturbations of the payoff at day t+1. For linear and quadratic Taylor series approximations of the basic equation we obtain new relations on mean price, mean payoff, their volatilities, skewness and amount of asset \u03bemax that delivers max to investor\u2019s utility. The stochasticity of market trade time-series defines random properties of the asset price time-series during \u0394. We introduce new market-based price probability measure entirely determined by frequency-based probability measures of the market trade value and volume. The conventional frequency-based price probability is a very special case of the market-based price probability measure when all trade volumes during \u0394 equal unit. Prediction of the market-based price probability at horizon T equals forecast of the market trade value and volume probabilities at same horizon. The similar Taylor series and probability measures alike to market-based price probability can be used as approximations of different versions of asset pricing, financial and economic models that describe relations between economic and financial variables averaged during some time interval \u0394."
            },
            {
                "arxivId": "2012.04506",
                "title": "Business Cycles as Collective Risk Fluctuations",
                "abstract": "We suggest use continuous numerical risk grades [0,1] of R for a single risk or the unit cube in Rn for n risks as the economic domain. We consider risk ratings of economic agents as their coordinates in the economic domain. Economic activity of agents, economic or other factors change agents risk ratings and that cause motion of agents in the economic domain. Aggregations of variables and transactions of individual agents in small volume of economic domain establish the continuous economic media approximation that describes collective variables, transactions and their flows in the economic domain as functions of risk coordinates. Any economic variable A(t,x) defines mean risk XA(t) as risk weighted by economic variable A(t,x). Collective flows of economic variables in bounded economic domain fluctuate from secure to risky area and back. These fluctuations of flows cause time oscillations of macroeconomic variables A(t) and their mean risks XA(t) in economic domain and are the origin of any business and credit cycles. We derive equations that describe evolution of collective variables, transactions and their flows in the economic domain. As illustration we present simple self-consistent equations of supply-demand cycles that describe fluctuations of supply, demand and their mean risks."
            },
            {
                "arxivId": "2009.14278",
                "title": "Price, Volatility and the Second-Order Economic Theory",
                "abstract": "We introduce the new price probability measure, which entirely depends on the probability measures of the value and the volume of the market trades. We define the nth statistical moment of the price as the ratio of the nth statistical moment of the value to the nth statistical moment of the volume of all trades performed during an averaging time interval \u0394. The set of the price statistical moments determines the price characteristic function and its Fourier transform defines the price probability measure. The price volatility depends on the 1st and the 2nd statistical moments of the value and the volume of the trades. The prediction of the price volatility requires a description of the sums of squares of the value and the volume of the market trades during the interval \u0394 and we call it the second-order economic theory. To develop that theory, we introduce numerical continuous risk ratings and distribute the agents by the risk ratings as coordinates. Based on distributions of the agents by the risk coordinates, we introduce a continuous economic media approximation that describes the collective trades. The agents perform the trades under the action of their expectations. We model the mutual impact of the expectations and the trades and derive equations that describe their evolution. To illustrate the benefits of our approach, in a linear approximation we describe perturbations of the mean price, the mean square price and the price volatility as functions of the first and the second-degree trades\u2019 disturbances."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2311.06718",
        "category": "econ",
        "title": "Sustainable Development Goal (SDG) 8: New Zealand\u2019s Prospects while Yield Curve Inverts in Central Bank Digital Currency (CBDC) Era",
        "abstract": "In the inverted yield curve environment, I intend to assess the feasibility of fulfilling Sustainable Development Goal (SDG) 8, decent work and economic growth, of the United Nations by 2030 in New Zealand. Central Bank Digital Currency (CBDC) issuance supports SDG 8, based on the Cobb-Douglas production function, the growth accounting relation, and the Theory of Aggregate Demand. Bright prospects exist for New Zealand.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2403.09265",
        "category": "econ",
        "title": "Zonal vs. Nodal Pricing: An Analysis of Different Pricing Rules in the German Day-Ahead Market",
        "abstract": "The European electricity market is based on large pricing zones with a uniform day-ahead price. The energy transition leads to shifts in supply and demand and increasing redispatch costs. In an attempt to ensure efficient market clearing and congestion management, the EU Commission has mandated the Bidding Zone Review (BZR) to reevaluate the configuration of European bidding zones. Based on a unique data set published in the context of the BZR, we compare various pricing rules for the German power market. We compare market clearing and pricing for national, zonal, and nodal models, including their generation costs and associated redispatch costs. Moreover, we investigate different non-uniform pricing rules and their economic implications for the German electricity market. Our results indicate that the differences in the average prices in different zones are small. The total costs across different configurations are similar and the reduction of standard deviations in prices is also small based on this data set. A nodal pricing rule leads to the lowest total costs. We also analyze the quality of different pricing rules and their differences with respect to the quality of the price signals and the necessary uplift payments. While the study focuses on Germany, the analysis is relevant beyond and feeds into the broader discussion about pricing rules.",
        "references": [
            {
                "arxivId": "1605.05002",
                "title": "A Convex Primal Formulation for Convex Hull Pricing",
                "abstract": "In certain electricity markets, because of nonconvexities that arise from their operating characteristics, generators that follow the independent system operator's (ISO's) decisions may fail to recover their cost through sales of energy at locational marginal prices. The ISO makes discriminatory side payments to incentivize the compliance of generators. Convex hull pricing is a uniform pricing scheme that minimizes these side payments. The Lagrangian dual problem of the unit commitment problem has been solved in the dual space to determine convex hull prices. However, this approach is computationally expensive. We propose a polynomially solvable primal formulation for the Lagrangian dual problem. This formulation explicitly describes for each generating unit the convex hull of its feasible set and the convex envelope of its cost function. We cast our formulation as a second-order cone program when the cost functions are quadratic, and a linear program when the cost functions are piecewise linear. A 96-period 76-unit transmission-constrained example is solved in less than 15\u00a0s on a personal computer."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2403.18248",
        "category": "econ",
        "title": "Statistical Inference of Optimal Allocations I: Regularities and their Implications",
        "abstract": "In this paper, we develop a functional differentiability approach for solving statistical optimal allocation problems. We first derive Hadamard differentiability of the value function through a detailed analysis of the general properties of the sorting operator. Central to our framework are the concept of Hausdorff measure and the area and coarea integration formulas from geometric measure theory. Building on our Hadamard differentiability results, we demonstrate how the functional delta method can be used to directly derive the asymptotic properties of the value function process for binary constrained optimal allocation problems, as well as the two-step ROC curve estimator. Moreover, leveraging profound insights from geometric functional analysis on convex and local Lipschitz functionals, we obtain additional generic Fr\\'echet differentiability results for the value functions of optimal allocation problems. These compelling findings motivate us to study carefully the first order approximation of the optimal social welfare. In this paper, we then present a double / debiased estimator for the value functions. Importantly, the conditions outlined in the Hadamard differentiability section validate the margin assumption from the statistical classification literature employing plug-in methods that justifies a faster convergence rate.",
        "references": [
            {
                "arxivId": "1512.05635",
                "title": "The sorted effects method: discovering heterogeneous effects beyond their averages",
                "abstract": "The partial (ceteris paribus) e?ects of interest in nonlinear and interactive linear models are heterogeneous as they can vary dramatically with the underlying observed or unobserved covariates. Despite the apparent importance of heterogeneity, a common practice in modern empirical work is to largely ignore it by reporting average partial e?ects (or, at best, average e?ects for some groups, see e.g. Angrist and Pischke (2008)). While average e?ects provide very convenient scalar summaries of typical e?ects, by de?nition they fail to re?ect the entire variety of the heterogenous e?ects. In order to discover these e?ects much more fully, we propose to estimate and report sorted e?ects \u2013 a collection of estimated partial e?ects sorted in increasing order and indexed by percentiles. By construction the sorted e?ect curves completely represent and help visualize all of the heterogeneous e?ects in one plot. They are as convenient and easy to report in practice as the conventional average partial e?ects. We also provide a quanti?cation of uncertainty (standard errors and con?dence bands) for the estimated sorted e?ects. We apply the sorted e?ects method to demonstrate several striking patterns of gender-based discrimination in wages, and of race-based discrimination in mortgage lending. Using di?erential geometry and functional delta methods, we establish that the estimated sorted e?ects are consistent for the true sorted e?ects, and derive asymptotic normality and bootstrap approximation results, enabling construction of pointwise con?dence bands (point-wise with respect to percentile indices). We also derive functional central limit theorems and bootstrap approximation results, enabling construction of simultaneous con?dence bands (simultaneous with respect to percentile indices). The derived statistical results in turn rely on establishing Hadamard di?erentiability of the multivariate sorting operator, a result of independent mathematical interest."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2403.18694",
        "category": "econ",
        "title": "Designing Simple Mechanisms",
        "abstract": "Which mechanisms are simple to play? When is it easy for participants to see that a mechanism is incentive-compatible? I will start by explaining how and why economists came to ask these questions. Then I will discuss three recent answers, that capture different aspects of what makes a mechanism simple.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04276",
        "category": "econ",
        "title": "Recursive index for assessing value added of individual scientific publications",
        "abstract": "An aggregated recursive K-index is proposed as a new scientometric indicator of added value and scientific research output of individual publications. This index can be used instead of or in addition to the H-index (J.E. Hirsch. An index to quantify an individual's scientific research output, arXiv:physics/0508025). In particular, it is proposed to switch from a pure strategy for assessing the quality and effectiveness of R&D using the H-index (Hirsch index) to a mixed strategy (in the context of publication activity as a combination of cooperative and noncooperative games) using the K-index on subnational and H-index on international or differentiated levels. In the context of a hybrid strategy of the scientist's payoff functions. This transition is correct and in demand for a number of national scientific systems with limited financial, material, infrastructural and linguistic (in terms of the English language) potential. Scientific systems with highly developed indigenous (autochthonous) characteristics are also needed in some scientific areas.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04282",
        "category": "econ",
        "title": "Analyzing Economic Convergence Across the Americas: A Survival Analysis Approach to GDP per Capita Trajectories",
        "abstract": "By integrating survival analysis, machine learning algorithms, and economic interpretation, this research examines the temporal dynamics associated with attaining a 5 percent rise in purchasing power parity-adjusted GDP per capita over a period of 120 months (2013-2022). A comparative investigation reveals that DeepSurv is proficient at capturing non-linear interactions, although standard models exhibit comparable performance under certain circumstances. The weight matrix evaluates the economic ramifications of vulnerabilities, risks, and capacities. In order to meet the GDPpc objective, the findings emphasize the need of a balanced approach to risk-taking, strategic vulnerability reduction, and investment in governmental capacities and social cohesiveness. Policy guidelines promote individualized approaches that take into account the complex dynamics at play while making decisions.",
        "references": [
            {
                "arxivId": "2308.14343",
                "title": "Buy when? Survival machine learning model comparison for purchase timing",
                "abstract": "The value of raw data is unlocked by converting it into information and knowledge that drives decision-making. Machine Learning (ML) algorithms are capable of analysing large datasets and making accurate predictions. Market segmentation, client lifetime value, and marketing techniques have all made use of machine learning. This article examines marketing machine learning techniques such as Support Vector Machines, Genetic Algorithms, Deep Learning, and K-Means. ML is used to analyse consumer behaviour, propose items, and make other customer choices about whether or not to purchase a product or service, but it is seldom used to predict when a person will buy a product or a basket of products. In this paper, the survival models Kernel SVM, DeepSurv, Survival Random Forest, and MTLR are examined to predict tine-purchase individual decisions. Gender, Income, Location, PurchaseHistory, OnlineBehavior, Interests, PromotionsDiscounts and CustomerExperience all have an influence on purchasing time, according to the analysis. The study shows that the DeepSurv model predicted purchase completion the best. These insights assist marketers in increasing conversion rates."
            },
            {
                "arxivId": "2205.05322",
                "title": "Shared Frailty Methods for Complex Survival Data: A Review of Recent Advances",
                "abstract": "Dependent survival data arise in many contexts. One context is clustered survival data, where survival data are collected on clusters such as families or medical centers. Dependent survival data also arise when multiple survival times are recorded for each individual. Frailty models are one common approach to handle such data. In frailty models, the dependence is expressed in terms of a random effect, called the frailty. Frailty models have been used with both the Cox proportional hazards model and the accelerated failure time model. This article reviews recent developments in the area of frailty models in a variety of settings. In each setting we provide a detailed model description, assumptions, available estimation methods, and R packages. Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 10 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates."
            },
            {
                "arxivId": "2107.06039",
                "title": "AutoScore-Imbalance: An interpretable machine learning tool for development of clinical scores with rare events data",
                "abstract": null
            },
            {
                "arxivId": "2005.02509",
                "title": "Semiparametric analysis of clustered interval\u2010censored survival data using soft Bayesian additive regression trees (SBART)",
                "abstract": "Popular parametric and semiparametric hazards regression models for clustered survival data are inappropriate and inadequate when the unknown effects of different covariates and clustering are complex. This calls for a flexible modeling framework to yield efficient survival prediction. Moreover, for some survival studies involving time to occurrence of some asymptomatic events, survival times are typically interval censored between consecutive clinical inspections. In this article, we propose a robust semiparametric model for clustered interval\u2010censored survival data under a paradigm of Bayesian ensemble learning, called soft Bayesian additive regression trees or SBART (Linero and Yang, 2018), which combines multiple sparse (soft) decision trees to attain excellent predictive accuracy. We develop a novel semiparametric hazards regression model by modeling the hazard function as a product of a parametric baseline hazard function and a nonparametric component that uses SBART to incorporate clustering, unknown functional forms of the main effects, and interaction effects of various covariates. In addition to being applicable for left\u2010censored, right\u2010censored, and interval\u2010censored survival data, our methodology is implemented using a data augmentation scheme which allows for existing Bayesian backfitting algorithms to be used. We illustrate the practical implementation and advantages of our method via simulation studies and an analysis of a prostate cancer surgery study where dependence on the experience and skill level of the physicians leads to clustering of survival times. We conclude by discussing our method's applicability in studies involving high\u2010dimensional data with complex underlying associations."
            },
            {
                "arxivId": "1708.04649",
                "title": "Machine Learning for Survival Analysis: A Survey",
                "abstract": "Accurately predicting the time of occurrence of an event of interest is a critical problem in longitudinal data analysis. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04424",
        "category": "econ",
        "title": "Algorithmic Fairness and Social Welfare",
        "abstract": "Algorithms are increasingly used to guide high-stakes decisions about individuals. Consequently, substantial interest has developed around defining and measuring the ``fairness'' of these algorithms. These definitions of fair algorithms share two features: First, they prioritize the role of a pre-defined group identity (e.g., race or gender) by focusing on how the algorithm's impact differs systematically across groups. Second, they are statistical in nature; for example, comparing false positive rates, or assessing whether group identity is independent of the decision (where both are viewed as random variables). These notions are facially distinct from a social welfare approach to fairness, in particular one based on ``veil of ignorance'' thought experiments in which individuals choose how to structure society prior to the realization of their social identity. In this paper, we seek to understand and organize the relationship between these different approaches to fairness. Can the optimization criteria proposed in the algorithmic fairness literature also be motivated as the choices of someone from behind the veil of ignorance? If not, what properties distinguish either approach to fairness?",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04494",
        "category": "econ",
        "title": "Fast and simple inner-loop algorithms of static / dynamic BLP estimations",
        "abstract": "This study investigates computationally efficient inner-loop algorithms for estimating static / dynamic BLP models. It provides the following ideas to reduce the number of inner-loop iterations: (1). Add a term concerning the outside option share in the BLP contraction mapping; (2). Analytically represent mean product utilities as a function of value functions and solve for the value functions (for dynamic BLP); (3-1). Combine the spectral / SQUAREM algorithms; (3-2). Choice of the step sizes. These methods are independent and easy to implement. This study shows good performance of these ideas by numerical experiments.",
        "references": [
            {
                "arxivId": "1810.11163",
                "title": "SQUAREM: An R Package for Off-the-Shelf Acceleration of EM, MM and Other EM-Like Monotone Algorithms",
                "abstract": "We discuss R package SQUAREM for accelerating iterative algorithms which exhibit slow, monotone convergence. These include the well-known expectation-maximization algorithm, majorize-minimize (MM), and other EM-like algorithms such as expectation conditional maximization, and generalized EM algorithms. We demonstrate the simplicity, generality, and power of SQUAREM through a wide array of applications of EM/MM problems, including binary Poisson mixture, factor analysis, interval censoring, genetics admixture, and logistic regression maximum likelihood estimation (an MM problem). We show that SQUAREM is easy to apply, and can accelerate any fixed-point, smooth, contraction mapping with linear convergence rate. Squared iterative scheme (Squarem) algorithm provides significant speed-up of EM-like algorithms. The margin of the advantage for Squarem is especially huge for high-dimensional problems or when EM step is relatively time-consuming to evaluate. Squarem can be used off-the-shelf since there is no need for the user to tweak any control parameters to optimize performance. Given its remarkable ease of use, Squarem may be considered as a default accelerator for slowly converging EM-like algorithms. All the comparisons of CPU computing time in the paper are made on a quad-core 2.3 GHz Intel Core i7 Mac computer. R Package SQUAREM can be downloaded at this https URL."
            },
            {
                "arxivId": "2111.13744",
                "title": "Yogurts Choose Consumers? Estimation of Random Utility Models via Two-Sided Matching",
                "abstract": "The problem of demand inversion -- a crucial step in the estimation of random \nutility discrete-choice models -- is equivalent to the determination of stable outcomes in \ntwo-sided matching models. This equivalence applies to random utility models that are not \nnecessarily additive, smooth, nor even invertible. Based on this equivalence, algorithms \nfor the determination of stable matchings provide e ffective computational methods for \nestimating these models. For non-invertible models, the identifi ed set of utility vectors \nis a lattice, and the matching algorithms recover sharp upper and lower bounds on the \nutilities. For invertible models, our matching approach facilitates estimation of models that \nwere previously di\u000ecult to estimate, such as the pure characteristics model. An empirical \napplication to voting data from the 1999 European Parliament elections illustrates the \ngood performance of our matching-based demand inversion algorithms in practice."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04506",
        "category": "econ",
        "title": "Super Apps and the Digital Markets Act",
        "abstract": "The Digital Markets Act (DMA) aims to ensure contestability and fairness in digital markets, particularly focusing on regulating Big Tech companies. The paper explores the DMA's capacity to address both current and future challenges in digital market contestability and fairness, spotlighting the trend towards platform integration and the potential rise of\"super-apps\"akin to WeChat and KakaoTalk. Specifically, it investigates WhatsApp, owned by Meta, as a gatekeeper that might expand its service offerings, integrating additional functionalities like AI and metaverse technologies. The paper discusses whether the DMA's obligations, such as mandated interoperability and data portability, can mitigate the emergent risks to market fairness and contestability from such integrations. Despite recognizing that the DMA has the potential to address many issues arising from platform integration, it suggests the necessity for adaptability and a complementary relationship with traditional antitrust law to ensure sustained contestability and fairness in evolving digital markets.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04543",
        "category": "econ",
        "title": "Early Adoption of Generative AI by Global Business Leaders: Insights from an INSEAD Alumni Survey",
        "abstract": "How are new technologies like generative AI quickly adopted and used by executive and managerial leaders to create value in organizations? A survey of INSEAD's global alumni base revealed several intriguing insights into perceptions and engagements with generative AI across a broad spectrum of demographics, industries, and geographies. Notably, there's a prevailing optimism about the role of generative AI in enhancing productivity and innovation, as evidenced by the 90% of respondents being excited about its time-saving and efficiency benefits. Analysis revealed different attitudes about adoption and use across demographic variables. Younger respondents are significantly more excited about generative AI and more likely to be using it at work and in personal life than older participants. Those in Europe have a somewhat more distant view of generative AI than those in North America in Asia, in that they see the gains more likely to be captured by organizations than individuals, and are less likely to be using it in professional and personal contexts than those in North America and Asia. This may also be related to the fact that those in Europe are more likely to be working in Financial Services and less likely to be working in Information Technology industries than those in North America and Asia. Despite this, those in Europe are more likely to see AGI happening faster than those in North America, although this may reflect less interaction with generative AI in personal and professional contexts. These findings collectively underscore the complex and multifaceted perceptions of generative AI's role in society, pointing to both its promising potential and the challenges it presents.",
        "references": [
            {
                "arxivId": "2306.10052",
                "title": "Assigning AI: Seven Approaches for Students, with Prompts",
                "abstract": "This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the\"human in the loop,\"the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms"
            },
            {
                "arxivId": "2305.09573",
                "title": "Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs",
                "abstract": "Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate \u201cethics washing.\u201d Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies."
            },
            {
                "arxivId": "2303.08774",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04590",
        "category": "econ",
        "title": "Absolute Technical Efficiency Indices",
        "abstract": "Technical efficiency indices (TEIs) can be estimated using the traditional stochastic frontier analysis approach, which yields relative indices that do not allow self-interpretations. In this paper, we introduce a single-step estimation procedure for TEIs that eliminates the need to identify best practices and avoids imposing restrictive hypotheses on the error term. The resulting indices are absolute and allow for individual interpretation. In our model, we estimate a distance function using the inverse coefficient of resource utilization, rather than treating it as unobservable. We employ a Tobit model with a translog distance function as our econometric framework. Applying this model to a sample of 19 airline companies from 2012 to 2021, we find that: (1) Absolute technical efficiency varies considerably between companies with medium-haul European airlines being technically the most efficient, while Asian airlines are the least efficient; (2) Our estimated TEIs are consistent with the observed data with a decline in efficiency especially during the Covid-19 crisis and Brexit period; (3) All airlines contained in our sample would be able to increase their average technical efficiency by 0.209% if they reduced their average kerosene consumption by 1%; (4) Total factor productivity (TFP) growth slowed between 2013 and 2019 due to a decrease in Disembodied Technical Change (DTC) and a small effect from Scale Economies (SE). Toward the end of our study period, TFP growth seemed increasingly driven by the SE effect, with a sharp decline in 2020 followed by an equally sharp recovery in 2021 for most airlines.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04700",
        "category": "econ",
        "title": "Stratifying on Treatment Status",
        "abstract": "We investigate the estimation of treatment effects from a sample that is stratified on the binary treatment status. In the case of unconfounded assignment where the potential outcomes are independent of the treatment given covariates, we show that standard estimators of the average treatment effect are inconsistent. In the case of an endogenous treatment and a binary instrument, we show that the IV estimator is inconsistent for the local average treatment effect. In both cases, we propose simple alternative estimators that are consistent in stratified samples, assuming that the fraction treated in the population is known or can be estimated.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04707",
        "category": "econ",
        "title": "Gender Bias in Emerging New Research Topics: The Impact of COVID-19 on Women in Science (preprint)",
        "abstract": "We investigate the impact of new research opportunities on the long-standing under-representation of women in medical and academic leadership by assessing the impact of the emergence of COVID-19 as a new research topic in the life sciences on women's authorship. After collecting publication data from 2019 and 2020 on biomedical publications, where the position of first and last author is most important for future career development, we use the major Medical Subject Heading (MeSH) terms to identify the main research area of each publication and measure the relation of each paper to COVID-19. Using a Difference-in-Difference approach, we find that although the general female authorship trend is upwards, papers in areas related to COVID-19 are less likely to have a woman as first or last author compared to research areas not related to COVID-19. Conversely, new publication opportunities in the COVID-19 research field increase the proportion of women in middle, less-relevant, author positions. Stay-at-home mandates, journal importance, and access to new funds do not fully explain the drop in women's outcomes. The decline in female first authorship is related to the increase of teams in which both lead authors have no prior experience in the COVID-related research field. In addition, pre-existing publishing teams show reduced bias in female key authorship with respect to new teams specifically formed for COVID-related research. This suggests that opportunistic teams, transitioning into research areas with emerging interests, possess greater flexibility in choosing the primary and final authors, potentially reducing uncertainties associated with engaging in productions divergent from their past scientific experiences by excluding women scientists from key authorship positions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04709",
        "category": "econ",
        "title": "Two-Sided Flexibility in Platforms",
        "abstract": "Flexibility is a cornerstone of operations management, crucial to hedge stochasticity in product demands, service requirements, and resource allocation. In two-sided platforms, flexibility is also two-sided and can be viewed as the compatibility of agents on one side with agents on the other side. Platform actions often influence the flexibility on either the demand or the supply side. But how should flexibility be jointly allocated across different sides? Whereas the literature has traditionally focused on only one side at a time, our work initiates the study of two-sided flexibility in matching platforms. We propose a parsimonious matching model in random graphs and identify the flexibility allocation that optimizes the expected size of a maximum matching. Our findings reveal that flexibility allocation is a first-order issue: for a given flexibility budget, the resulting matching size can vary greatly depending on how the budget is allocated. Moreover, even in the simple and symmetric settings we study, the quest for the optimal allocation is complicated. In particular, easy and costly mistakes can be made if the flexibility decisions on the demand and supply side are optimized independently (e.g., by two different teams in the company), rather than jointly. To guide the search for optimal flexibility allocation, we uncover two effects, flexibility cannibalization, and flexibility abundance, that govern when the optimal design places the flexibility budget only on one side or equally on both sides. In doing so we identify the study of two-sided flexibility as a significant aspect of platform efficiency.",
        "references": [
            {
                "arxivId": "2104.14740",
                "title": "Driver Positioning and Incentive Budgeting with an Escrow Mechanism for Ridesharing Platforms",
                "abstract": "Drivers on the Lyft ride-share platform do not always know where the areas of supply shortage are in real time. This lack of information hurts both riders trying to find a ride and drivers trying to determine how to maximize their earnings opportunities. Lyft\u2019s Personal Power Zone (PPZ) product helps the company to maintain high levels of service on the platform by influencing the spatial distribution of drivers in real time via monetary incentives that encourage them to reposition their vehicles. The underlying system that powers the product has two main components: (1) a novel \u201cescrow mechanism\u201d that tracks available incentive budgets tied to locations within a city in real time, and (2) an algorithm that solves the stochastic driver-positioning problem to maximize short-run revenue from riders\u2019 fares. The optimization problem is a multiagent dynamic program that is too complicated to solve optimally for our large-scale application. Our approach is to decompose it into two subproblems. The first determines the set of drivers to incentivize and where to incentivize them to position themselves. The second determines how to fund each incentive using the escrow budget. By formulating it as two convex programs, we are able to use commercial solvers that find the optimal solution in a matter of seconds. Rolled out to all 320 cities in which Lyft operates in a little more than a year, the system now generates millions of bonuses that incentivize hundreds of thousands of active drivers to optimally position themselves in anticipation of ride requests every week. Together, the PPZ product and its underlying algorithms represent a paradigm shift in how Lyft drivers drive and generate earnings on the platform. Its direct business impact has been a 0.5% increase in incremental bookings, amounting to tens of millions of dollars per year. In addition, the product has brought about significant improvements to the driver and rider experience on the platform. These include statistically significant reductions in pick-up times and ride cancellations. Finally, internal surveys reveal that the vast majority of drivers prefer PPZs over the legacy system."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04822",
        "category": "econ",
        "title": "Some Characterizations of TTC in Multiple-Object Exchange Problems",
        "abstract": "This paper considers exchange of indivisible objects when agents are endowed with and desire bundles of objects. Agents are assumed to have lexicographic preferences over bundles. We show that Top Trading Cycles (TTC) is characterized by efficiency, the weak endowment lower bound, balancedness, and truncation-proofness. In the classic Shapley--Scarf Economy, TTC is characterized by efficiency, individual rationality, and truncation-proofness. These results strengthen the uniqueness results of Ma (1994) and, more recently, Altunta\\c{s} et al. (2023). In a model with variable endowments, TTC is susceptible to various forms of endowment manipulation. However, no rule is core-selecting and hiding-proof.",
        "references": [
            {
                "arxivId": "2106.14456",
                "title": "The Machiavellian frontier of top trading cycles",
                "abstract": "This paper studies the housing market problem introduced by Shapley and Scarf (1974). We probe the Machiavellian frontier of the well-known top trading cycles (TTC) rule by weakening strategy-proofness and providing new characterizations for this rule. Specifically, our contribution lies in three aspects. First, we weaken the concept of strategy-proofness and introduce a new incentive notion called truncation-invariance, where the truthful preference-reporting assignment cannot be altered by any agent through misreporting a truncation of the true preference at the assignment produced by the true preference unilaterally. Second, we characterize the TTC rule by the following three groups of axioms: individual rationality, pair-efficiency, truncation-invariance; individual rationality, Pareto efficiency, truncation-invariance; individual rationality, endowments-swapping-proofness, truncation-invariance.1 The new characterizations refine several previous results.2 Third, we show through examples that the characterization results of Takamiya (2001) and Miyagawa (2002) can no longer be obtained if strategy-proofness is replaced with truncation-invariance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04843",
        "category": "econ",
        "title": "Money Pumps and Bounded Rationality",
        "abstract": "The standard criterion of rationality in economics is the maximization of a utility function that is stable across multiple observations of an agent's choice behavior. In this paper, we discuss two notions of the money pump that characterize two corresponding notions of utility-maximization. We explain the senses in which the amount of money that can be pumped from a consumer is a useful measure of the consumer's departure from utility-maximization.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04847",
        "category": "econ",
        "title": "A many-to-one job market: more about the core and the competitive salaries",
        "abstract": "This paper studies many-to-one assignment markets, or matching markets with wages. Although it is well-known that the core of this model is non-empty, the structure of the core has not been fully investigated. To the known dissimilarities with the one-to-one assignment game, we add that the bargaining set does not coincide with the core and the kernel may not be included in the core. Besides, not all extreme core allocations can be obtained by means of a lexicographic maximization or a lexicographic minimization procedure, as it is the case in the one-to-one assignment game. The maximum and minimum competitive salaries are characterized in two ways: axiomatically and by means of easily verifiable properties of an associated directed graph. Regarding the remaining extreme core allocations of the many-to-one assignment game, we propose a lexicographic procedure that, for each order on the set of workers, sequentially maximizes or minimizes each worker's competitive salary. This procedure provides all extreme vectors of competitive salaries, that is all extreme core allocations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04974",
        "category": "econ",
        "title": "Neural Network Modeling for Forecasting Tourism Demand in Stopi\\'{c}a Cave: A Serbian Cave Tourism Study",
        "abstract": "For modeling the number of visits in Stopi\\'{c}a cave (Serbia) we consider the classical Auto-regressive Integrated Moving Average (ARIMA) model, Machine Learning (ML) method Support Vector Regression (SVR), and hybrid NeuralPropeth method which combines classical and ML concepts. The most accurate predictions were obtained with NeuralPropeth which includes the seasonal component and growing trend of time-series. In addition, non-linearity is modeled by shallow Neural Network (NN), and Google Trend is incorporated as an exogenous variable. Modeling tourist demand represents great importance for management structures and decision-makers due to its applicability in establishing sustainable tourism utilization strategies in environmentally vulnerable destinations such as caves. The data provided insights into the tourist demand in Stopi\\'{c}a cave and preliminary data for addressing the issues of carrying capacity within the most visited cave in Serbia.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04985",
        "category": "econ",
        "title": "Towards a generalized accessibility measure for transportation equity and efficiency",
        "abstract": "Locational measures of accessibility are widely used in urban and transportation planning to understand the impact of the transportation system on influencing people's access to places. However, there is a considerable lack of measurement standards and publicly available data. We propose a generalized measure of locational accessibility that has a comprehensible form for transportation planning analysis. This metric combines the cumulative opportunities approach with gravity-based measures and is capable of catering to multiple trip purposes, travel modes, cost thresholds, and scales of analysis. Using data from multiple publicly available datasets, this metric is computed by trip purpose and travel time threshold for all block groups in the United States, and the data is made publicly accessible. Further, case studies of three large metropolitan areas reveal substantial inefficiencies in transportation infrastructure, with the most inefficiency observed in sprawling and non-core urban areas, especially for bicycling. Subsequently, it is shown that targeted investment in facilities can contribute to a more equitable distribution of accessibility to essential shopping and service facilities. By assigning greater weights to socioeconomically disadvantaged neighborhoods, the proposed metric formally incorporates equity considerations into transportation planning, contributing to a more equitable distribution of accessibility to essential services and facilities.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.04989",
        "category": "econ",
        "title": "Towards a representative social cost of carbon",
        "abstract": "The majority of estimates of the social cost of carbon use preference parameters calibrated to data for North America and Europe. We here use representative data for attitudes to time and risk across the world. The social cost of carbon is substantially higher in the global north than in the south. The difference is more pronounced if we count people rather than countries.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.05021",
        "category": "econ",
        "title": "Context-dependent Causality (the Non-Monotonic Case)",
        "abstract": "We develop a novel identification strategy as well as a new estimator for context-dependent causal inference in non-parametric triangular models with non-separable disturbances. Departing from the common practice, our analysis does not rely on the strict monotonicity assumption. Our key contribution lies in leveraging on diffusion models to formulate the structural equations as a system evolving from noise accumulation to account for the influence of the latent context (confounder) on the outcome. Our identifiability strategy involves a system of Fredholm integral equations expressing the distributional relationship between a latent context variable and a vector of observables. These integral equations involve an unknown kernel and are governed by a set of structural form functions, inducing a non-monotonic inverse problem. We prove that if the kernel density can be represented as an infinite mixture of Gaussians, then there exists a unique solution for the unknown function. This is a significant result, as it shows that it is possible to solve a non-monotonic inverse problem even when the kernel is unknown. On the methodological front we leverage on a novel and enriched Contaminated Generative Adversarial (Neural) Networks (CONGAN) which we provide as a solution to the non-monotonic inverse problem.",
        "references": [
            {
                "arxivId": "2006.11239",
                "title": "Denoising Diffusion Probabilistic Models",
                "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"
            },
            {
                "arxivId": "1909.07862",
                "title": "Minimax confidence intervals for the Sliced Wasserstein distance",
                "abstract": "Motivated by the growing popularity of variants of the Wasserstein distance in statistics and machine learning, we study statistical inference for the Sliced Wasserstein distance--an easily computable variant of the Wasserstein distance. Specifically, we construct confidence intervals for the Sliced Wasserstein distance which have finite-sample validity under no assumptions or under mild moment assumptions. These intervals are adaptive in length to the regularity of the underlying distributions. We also bound the minimax risk of estimating the Sliced Wasserstein distance, and as a consequence establish that the lengths of our proposed confidence intervals are minimax optimal over appropriate distribution classes. To motivate the choice of these classes, we also study minimax rates of estimating a distribution under the Sliced Wasserstein distance. These theoretical findings are complemented with a simulation study demonstrating the deficiencies of the classical bootstrap, and the advantages of our proposed methods. We also show strong correspondences between our theoretical predictions and the adaptivity of our confidence interval lengths in simulations. We conclude by demonstrating the use of our confidence intervals in the setting of simulator-based likelihood-free inference. In this setting, contrasting popular approximate Bayesian computation methods, we develop uncertainty quantification methods with rigorous frequentist coverage guarantees."
            },
            {
                "arxivId": "1909.02210",
                "title": "Using Wasserstein Generative Adversarial Networks for the Design of Monte Carlo Simulations",
                "abstract": "When researchers develop new econometric methods it is common practice to compare the performance of the new methods to those of existing methods in Monte Carlo studies. The credibility of such Monte Carlo studies is often limited because of the freedom the researcher has in choosing the design. In recent years a new class of generative models emerged in the machine learning literature, termed Generative Adversarial Networks (GANs) that can be used to systematically generate artificial data that closely mimics real economic datasets, while limiting the degrees of freedom for the researcher and optionally satisfying privacy guarantees with respect to their training data. In addition if an applied researcher is concerned with the performance of a particular statistical method on a specific data set (beyond its theoretical properties in large samples), she may wish to assess the performance, e.g., the coverage rate of confidence intervals or the bias of the estimator, using simulated data which resembles her setting. Tol illustrate these methods we apply Wasserstein GANs (WGANs) to compare a number of different estimators for average treatment effects under unconfoundedness in three distinct settings (corresponding to three real data sets) and present a methodology for assessing the robustness of the results. In this example, we find that (i) there is not one estimator that outperforms the others in all three settings, so researchers should tailor their analytic approach to a given setting, and (ii) systematic simulation studies can be helpful for selecting among competing methods in this situation."
            },
            {
                "arxivId": "1805.08836",
                "title": "Nonparametric Density Estimation under Adversarial Losses",
                "abstract": "We study minimax convergence rates of nonparametric density estimation under a large class of loss functions called \"adversarial losses\", which, besides classical $\\mathcal{L}^p$ losses, includes maximum mean discrepancy (MMD), Wasserstein distance, and total variation distance. These losses are closely related to the losses encoded by discriminator networks in generative adversarial networks (GANs). In a general framework, we study how the choice of loss and the assumed smoothness of the underlying density together determine the minimax rate. We also discuss implications for training GANs based on deep ReLU networks, and more general connections to learning implicit generative models in a minimax statistical sense."
            },
            {
                "arxivId": "1803.01541",
                "title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect",
                "abstract": "Despite being impactful on a variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, called Wasserstein GAN (WGAN), hinges on the 1-Lipschitz continuity of the discriminator. In this paper, we propose a novel approach to enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning methods. As a result, it gives rise to not only better photo-realistic samples than the previous methods but also state-of-the-art semi-supervised learning results. In particular, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR-10 images and is the first that exceeds the accuracy of 90% on the CIFAR-10 dataset using only 4,000 labeled images, to the best of our knowledge."
            },
            {
                "arxivId": "1711.02184",
                "title": "Semiparametric estimation of structural functions in nonseparable triangular models",
                "abstract": "Triangular systems with nonadditively separable unobserved heterogeneity provide a theoretically appealing framework for the modeling of complex structural relationships. However, they are not commonly used in practice due to the need for exogenous variables with large support for identification, the curse of dimensionality in estimation, and the lack of inferential tools. This paper introduces two classes of semiparametric nonseparable triangular models that address these limitations. They are based on distribution and quantile regression modeling of the reduced form conditional distributions of the endogenous variables. We show that average, distribution, and quantile structural functions are identified in these systems through a control function approach that does not require a large support condition. We propose a computationally attractive three\u2010stage procedure to estimate the structural functions where the first two stages consist of quantile or distribution regressions. We provide asymptotic theory and uniform inference methods for each stage. In particular, we derive functional central limit theorems and bootstrap functional central limit theorems for the distribution regression estimators of the structural functions. These results establish the validity of the bootstrap for three\u2010stage estimators of structural functions, and lead to simple inference algorithms. We illustrate the implementation and applicability of all our methods with numerical simulations and an empirical application to demand analysis."
            },
            {
                "arxivId": "1705.09199",
                "title": "Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial Network training",
                "abstract": "Generative Adversarial Networks (GANs) have become a widely popular framework for generative modelling of high-dimensional datasets. However their training is well-known to be difficult. This work presents a rigorous statistical analysis of GANs providing straight-forward explanations for common training pathologies such as vanishing gradients. Furthermore, it proposes a new training objective, Kernel GANs, and demonstrates its practical effectiveness on large-scale real-world data sets. A key element in the analysis is the distinction between training with respect to the (unknown) data distribution, and its empirical counterpart. To overcome issues in GAN training, we pursue the idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating noise in the input distributions of the discriminator. As we show, this effectively leads to an empirical version of the JSD in which the true and the generator densities are replaced by kernel density estimates, which leads to Kernel GANs."
            },
            {
                "arxivId": "1704.00028",
                "title": "Improved Training of Wasserstein GANs",
                "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms."
            },
            {
                "arxivId": "1212.6757",
                "title": "TESTING REGRESSION MONOTONICITY IN ECONOMETRIC MODELS",
                "abstract": "Monotonicity is a key qualitative prediction of a wide array of economic models derived via robust comparative statics. It is therefore important to design effective and practical econometric methods for testing this prediction in empirical analysis. This article develops a general nonparametric framework for testing monotonicity of a regression function. Using this framework, a broad class of new tests is introduced, which gives an empirical researcher a lot of flexibility to incorporate ex ante information she might have. The article also develops new methods for simulating critical values, which are based on the combination of a bootstrap procedure and new selection algorithms. These methods yield tests that have correct asymptotic size and are asymptotically nonconservative. It is also shown how to obtain an adaptive and rate optimal test that has the best attainable rate of uniform consistency against models whose regression function has Lipschitz-continuous first-order derivatives and that automatically adapts to the unknown smoothness of the regression function. Simulations show that the power of the new tests in many cases significantly exceeds that of some prior tests, e.g., that of Ghosal, Sen, and Van der Vaart (2000)."
            },
            {
                "arxivId": "1207.5594",
                "title": "Nonparametric regression with nonparametrically generated covariates",
                "abstract": "We analyze the properties of non- and semiparametric estimation procedures involving nonparametric regression with generated covariates. Such estimators appear in numerous econometric applications, including nonparametric estimation of simultaneous equation models, sample selection models, treatment effect models, and censored regression models, but so far there seems to be no unified theory to establish their statistical properties. Our paper provides such results, allowing to establish asymptotic properties like rates of consistency or asymptotic normality for a wide range of semi- and nonparametric estimators. We also show how to account for the presence of nonparametrically generated regressors when computing standard errors."
            },
            {
                "arxivId": "0904.0951",
                "title": "Inference on Counterfactual Distributions",
                "abstract": "In this paper we develop procedures for performing inference in regression models about how potential policy interventions affect the entire marginal distribution of an outcome of interest. These policy interventions consist of either changes in the distribution of covariates related to the outcome holding the conditional distribution of the outcome given covariates fixed, or changes in the conditional distribution of the outcome given covariates holding the marginal distribution of the covariates fixed. Under either of these assumptions, we obtain uniformly consistent estimates and functional central limit theorems for the counterfactual and status quo marginal distributions of the outcome as well as other function-valued effects of the policy, including, for example, the effects of the policy on the marginal distribution function, quantile function, and other related functionals. We construct simultaneous confidence sets for these functions; these sets take into account the sampling variation in the estimation of the relationship between the outcome and covariates. Our procedures rely on, and our theory covers, all main regression approaches for modeling and estimating conditional distributions, focusing especially on classical, quantile, duration, and distribution regressions. Our procedures are general and accommodate both simple unitary changes in the values of a given covariate as well as changes in the distribution of the covariates or the conditional distribution of the outcome given covariates of general form. We apply the procedures to examine the effects of labor market institutions on the U.S. wage distribution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.05178",
        "category": "econ",
        "title": "Estimating granular house price distributions in the Australian market using Gaussian mixtures",
        "abstract": "A new methodology is proposed to approximate the time-dependent house price distribution at a fine regional scale using Gaussian mixtures. The means, variances and weights of the mixture components are related to time, location and dwelling type through a non linear function trained by a deep functional approximator. Price indices are derived as means, medians, quantiles or other functions of the estimated distributions. Price densities for larger regions, such as a city, are calculated via a weighted sum of the component density functions. The method is applied to a data set covering all of Australia at a fine spatial and temporal resolution. In addition to enabling a detailed exploration of the data, the proposed index yields lower prediction errors in the practical task of individual dwelling price projection from previous sales values within the three major Australian cities. The estimated quantiles are also found to be well calibrated empirically, capturing the complexity of house price distributions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.05209",
        "category": "econ",
        "title": "Maximally Forward-Looking Core Inflation",
        "abstract": "Timely monetary policy decision-making requires timely core inflation measures. We create a new core inflation series that is explicitly designed to succeed at that goal. Precisely, we introduce the Assemblage Regression, a generalized nonnegative ridge regression problem that optimizes the price index's subcomponent weights such that the aggregate is maximally predictive of future headline inflation. Ordering subcomponents according to their rank in each period switches the algorithm to be learning supervised trimmed inflation - or, put differently, the maximally forward-looking summary statistic of the realized price changes distribution. In an extensive out-of-sample forecasting experiment for the US and the euro area, we find substantial improvements for signaling medium-term inflation developments in both the pre- and post-Covid years. Those coming from the supervised trimmed version are particularly striking, and are attributable to a highly asymmetric trimming which contrasts with conventional indicators. We also find that this metric was indicating first upward pressures on inflation as early as mid-2020 and quickly captured the turning point in 2022. We also consider extensions, like assembling inflation from geographical regions, trimmed temporal aggregation, and building core measures specialized for either upside or downside inflation risks.",
        "references": [
            {
                "arxivId": "2311.16333",
                "title": "From Reactive to Proactive Volatility Modeling with Hemisphere Neural Networks",
                "abstract": "We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density forecasting through a novel neural network architecture with dedicated mean and variance hemispheres. Our architecture features several key ingredients making MLE work in this context. First, the hemispheres share a common core at the entrance of the network which accommodates for various forms of time variation in the error variance. Second, we introduce a volatility emphasis constraint that breaks mean/variance indeterminacy in this class of overparametrized nonlinear models. Third, we conduct a blocked out-of-bag reality check to curb overfitting in both conditional moments. Fourth, the algorithm utilizes standard deep learning software and thus handles large data sets - both computationally and statistically. Ergo, our Hemisphere Neural Network (HNN) provides proactive volatility forecasts based on leading indicators when it can, and reactive volatility based on the magnitude of previous prediction errors when it must. We evaluate point and density forecasts with an extensive out-of-sample experiment and benchmark against a suite of models ranging from classics to more modern machine learning-based offerings. In all cases, HNN fares well by consistently providing accurate mean/variance forecasts for all targets and horizons. Studying the resulting volatility paths reveals its versatility, while probabilistic forecasting evaluation metrics showcase its enviable reliability. Finally, we also demonstrate how this machinery can be merged with other structured deep learning models by revisiting Goulet Coulombe (2022)'s Neural Phillips Curve."
            },
            {
                "arxivId": "2308.11173",
                "title": "Forecasting inflation using disaggregates and machine learning (preprint)",
                "abstract": "This paper examines the effectiveness of several forecasting methods for predicting inflation, focusing on aggregating disaggregated forecasts - also known in the literature as the bottom-up approach. Taking the Brazilian case as an application, we consider different disaggregation levels for inflation and employ a range of traditional time series techniques as well as linear and nonlinear machine learning (ML) models to deal with a larger number of predictors. For many forecast horizons, the aggregation of disaggregated forecasts performs just as well survey-based expectations and models that generate forecasts using the aggregate directly. Overall, ML methods outperform traditional time series models in predictive accuracy, with outstanding performance in forecasting disaggregates. Our results reinforce the benefits of using models in a data-rich environment for inflation forecasting, including aggregating disaggregated forecasts from ML techniques, mainly during volatile periods. Starting from the COVID-19 pandemic, the random forest model based on both aggregate and disaggregated inflation achieves remarkable predictive performance at intermediate and longer horizons."
            },
            {
                "arxivId": "2306.05568",
                "title": "Maximally Machine-Learnable Portfolios",
                "abstract": "When it comes to stock returns, any form of predictability can bolster risk-adjusted profitability. We develop a collaborative machine learning algorithm that optimizes portfolio weights so that the resulting synthetic security is maximally predictable. Precisely, we introduce MACE, a multivariate extension of Alternating Conditional Expectations that achieves the aforementioned goal by wielding a Random Forest on one side of the equation, and a constrained Ridge Regression on the other. There are two key improvements with respect to Lo and MacKinlay's original maximally predictable portfolio approach. First, it accommodates for any (nonlinear) forecasting algorithm and predictor set. Second, it handles large portfolios. We conduct exercises at the daily and monthly frequency and report significant increases in predictability and profitability using very little conditioning information. Interestingly, predictability is found in bad as well as good times, and MACE successfully navigates the debacle of 2022."
            },
            {
                "arxivId": "2208.00139",
                "title": "Another look at forecast trimming for combinations: robustness, accuracy and diversity",
                "abstract": "Forecast combination is widely recognized as a preferred strategy over forecast selection due to its ability to mitigate the uncertainty associated with identifying a single\"best\"forecast. Nonetheless, sophisticated combinations are often empirically dominated by simple averaging, which is commonly attributed to the weight estimation error. The issue becomes more problematic when dealing with a forecast pool containing a large number of individual forecasts. In this paper, we propose a new forecast trimming algorithm to identify an optimal subset from the original forecast pool for forecast combination tasks. In contrast to existing approaches, our proposed algorithm simultaneously takes into account the robustness, accuracy and diversity issues of the forecast pool, rather than isolating each one of these issues. We also develop five forecast trimming algorithms as benchmarks, including one trimming-free algorithm and several trimming algorithms that isolate each one of the three key issues. Experimental results show that our algorithm achieves superior forecasting performance in general in terms of both point forecasts and prediction intervals. Nevertheless, we argue that diversity does not always have to be addressed in forecast trimming. Based on the results, we offer some practical guidelines on the selection of forecast trimming algorithms for a target series."
            },
            {
                "arxivId": "2205.04216",
                "title": "Forecast combinations: An over 50-year review",
                "abstract": null
            },
            {
                "arxivId": "2202.04146",
                "title": "A Neural Phillips Curve and a Deep Output Gap",
                "abstract": "Many problems plague the estimation of Phillips curves. Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include creating reasonable proxies for the notable absentees or extracting them via some form of assumptions-heavy filtering procedure. I propose an alternative route: a Hemisphere Neural Network (HNN) whose peculiar architecture yields a final layer where components can be interpreted as latent states within a Neural Phillips Curve. There are benefits. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, computations are fast. Third, forecasts are economically interpretable. Fourth, inflation volatility can also be predicted by merely adding a hemisphere to the model. Among other findings, the contribution of real activity to inflation appears severely underestimated in traditional econometric specifications. Also, HNN captures out-of-sample the 2021 upswing in inflation and attributes it first to an abrupt and sizable disanchoring of the expectations component, followed by a wildly positive gap starting from late 2020. HNN\u2019s gap unique path comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators \u2013 some of which are skyrocketing as of early 2022. *Departement des Sciences \u00c9conomiques, goulet_coulombe.philippe@uqam.ca. First and foremost, I am grateful to Hugo Couture for outstanding research assistance. For helpful discussions and/or comments on an earlier draft, I would like to thank Frank Diebold, Mikael Frenette, Alain Guay, Maximilian Goebel, Guillaume Poulin-Bellisle, Josefine Quast, Dalibor Stevanovic, David Wigglesworth, Boyuan Zhang, as well as participants at the UQAM weekly seminar, at at the CIRANO Montreal Macro Workshop, the SKEMA/CERGY Inflation Forecasting Workshop 2021, at the Bank of Canada Macro Brownbag Seminar, at the CFE 2021. ar X iv :2 20 2. 04 14 6v 1 [ ec on .E M ] 8 F eb 2 02 2"
            },
            {
                "arxivId": "2110.03411",
                "title": "Investigating Growth-at-Risk Using a Multicountry Non-parametric Quantile Factor Model*",
                "abstract": "We develop a Bayesian non-parametric quantile panel regression model. Within each quantile, the response function is a convex combination of a linear model and a non-linear function, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information at the pth quantile is captured through a conditionally heteroscedastic latent factor. The non-parametric feature of our model enhances flexibility, while the panel feature, by exploiting cross-country information, increases the number of observations in the tails. We develop Bayesian Markov chain Monte Carlo (MCMC) methods for estimation and forecasting with our quantile factor BART model (QF-BART), and apply them to study growth at risk dynamics in a panel of 11 advanced economies."
            },
            {
                "arxivId": "2103.03632",
                "title": "Modeling tail risks of inflation using unobserved component quantile regressions",
                "abstract": null
            },
            {
                "arxivId": "2011.07920",
                "title": "Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks",
                "abstract": null
            },
            {
                "arxivId": "2008.01714",
                "title": "Macroeconomic data transformations matter",
                "abstract": null
            },
            {
                "arxivId": "1808.04962",
                "title": "Recent Advances in Physical Reservoir Computing: A Review",
                "abstract": null
            },
            {
                "arxivId": "1406.4068",
                "title": "Functional Regression",
                "abstract": "Functional data analysis (FDA) involves the analysis of data whose ideal units of observation are functions defined on some continuous domain, and the observed data consist of a sample of functions taken from some population, sampled on a discrete grid. Ramsay and Silverman\u2019s 1997 textbook sparked the development of this field, which has accelerated in the past 10 years to become one of the fastest growing areas of statistics, fueled by the growing number of applications yielding this type of data. One unique characteristic of FDA is the need to combine information both across and within functions, which Ramsay and Silverman called replication and regularization, respectively. This article will focus on functional regression, the area of FDA that has received the most attention in applications and methodological development. First will be an introduction to basis functions, key building blocks for regularization in functional regression methods, followed by an overview of functional regression methods, split into three types: [1] functional predictor regression (scalar-on-function), [2] functional response regression (function-on-scalar) and [3] function-on-function regression. For each, the role of replication and regularization will be discussed and the methodological development described in a roughly chronological manner, at times deviating from the historical timeline to group together similar methods. The primary focus is on modeling and methodology, highlighting the modeling structures that have been developed and the various regularization approaches employed. At the end is a brief discussion describing potential areas of future development in this field."
            },
            {
                "arxivId": "0912.0902",
                "title": "Making and Evaluating Point Forecasts",
                "abstract": "Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.05349",
        "category": "econ",
        "title": "Common Trends and Long-Run Multipliers in Nonlinear Structural VARs",
        "abstract": "While it is widely recognised that linear (structural) VARs may omit important features of economic time series, the use of nonlinear SVARs has to date been almost entirely confined to the modelling of stationary time series, because of a lack of understanding as to how common stochastic trends may be accommodated within nonlinear VAR models. This has unfortunately circumscribed the range of series to which such models can be applied -- and/or required that these series be first transformed to stationarity, a potential source of misspecification -- and prevented the use of long-run identifying restrictions in these models. To address these problems, we develop a flexible class of additively time-separable nonlinear SVARs, which subsume models with threshold-type endogenous regime switching, both of the piecewise linear and smooth transition varieties. We extend the Granger-Johansen representation theorem to this class of models, obtaining conditions that specialise exactly to the usual ones when the model is linear. We further show that, as a corollary, these models are capable of supporting the same kinds of long-run identifying restrictions as are available in linear cointegrated SVARs.",
        "references": [
            {
                "arxivId": "2211.09604",
                "title": "Cointegration with Occasionally Binding Constraints",
                "abstract": "In the literature on nonlinear cointegration, a long-standing open problem relates to how a (nonlinear) vector autoregression, which provides a unified description of the short- and long-run dynamics of a collection of time series, can generate 'nonlinear cointegration' in the profound sense of those series sharing common nonlinear stochastic trends. We consider this problem in the setting of the censored and kinked structural VAR (CKSVAR), which provides a flexible yet tractable framework within which to model time series that are subject to threshold-type nonlinearities, such as those arising due to occasionally binding constraints, of which the zero lower bound (ZLB) on short-term nominal interest rates provides a leading example. We provide a complete characterisation of how common linear and {nonlinear stochastic trends may be generated in this model, via unit roots and appropriate generalisations of the usual rank conditions, providing the first extension to date of the Granger-Johansen representation theorem to a nonlinearly cointegrated setting, and thereby giving the first successful treatment of the open problem. The limiting common trend processes include regulated, censored and kinked Brownian motions, none of which have previously appeared in the literature on cointegrated VARs. Our results and running examples illustrate that the CKSVAR is capable of supporting a far richer variety of long-run behaviour than is a linear VAR, in ways that may be particularly useful for the identification of structural parameters."
            },
            {
                "arxivId": "2103.12779",
                "title": "Identification at the Zero Lower Bound",
                "abstract": "I show that the zero lower bound (ZLB) on interest rates can be used to identify the causal effects of monetary policy. Identification depends on the extent to which the ZLB limits the efficacy of monetary policy. I propose a simple way to test the efficacy of unconventional policies, modeled via a \u201cshadow rate.\u201d I apply this method to U.S. monetary policy using a three\u2010equation structural vector autoregressive model of inflation, unemployment, and the Federal Funds rate. I reject the null hypothesis that unconventional monetary policy has no effect at the ZLB, but find some evidence that it is not as effective as conventional monetary policy."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-09.json",
        "arxivId": "2404.05546",
        "category": "econ",
        "title": "Information Sale on Network",
        "abstract": "This paper studies a stylized model of a monopoly data seller when information-sharing network exists among data buyers. We show that, if the buyers' prior information is sufficiently noisy, the optimal selling strategy is characterized by a maximum independent set, which is the largest set of buyers who do not have information-sharing link at all. In addition, the precision of the seller's data decreases in the number of information-sharing links among buyers, but it is higher than the socially efficient level of precision.",
        "references": [
            {
                "arxivId": "1101.5617",
                "title": "Optimal Pricing in Networks with Externalities",
                "abstract": "We study the optimal pricing strategies of a monopolist selling a divisible good (service) to consumers who are embedded in a social network. A key feature of our model is that consumers experience a (positive) local network effect. In particular, each consumer's usage level depends directly on the usage of her neighbors in the social network structure. Thus, the monopolist's optimal pricing strategy may involve offering discounts to certain agents who have a central position in the underlying network. Our results can be summarized as follows. First, we consider a setting where the monopolist can offer individualized prices and derive a characterization of the optimal price for each consumer as a function of her network position. In particular, we show that it is optimal for the monopolist to charge each agent a price that consists of three components: (i) a nominal term that is independent of the network structure, (ii) a discount term proportional to the influence that this agent exerts over the rest of the social network (quantified by the agent's Bonacich centrality), and (iii) a markup term proportional to the influence that the network exerts on the agent. In the second part of the paper, we discuss the optimal strategy of a monopolist who can only choose a single uniform price for the good and derive an algorithm polynomial in the number of agents to compute such a price. Third, we assume that the monopolist can offer the good in two prices, full and discounted, and we study the problem of determining which set of consumers should be given the discount. We show that the problem is NP-hard; however, we provide an explicit characterization of the set of agents who should be offered the discounted price. Next, we describe an approximation algorithm for finding the optimal set of agents. We show that if the profit is nonnegative under any feasible price allocation, the algorithm guarantees at least 88% of the optimal profit. Finally, we highlight the value of network information by comparing the profits of a monopolist who does not take into account the network effects when choosing her pricing policy to those of a monopolist who uses this information optimally."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2106.14456",
        "category": "econ",
        "title": "The Machiavellian frontier of top trading cycles",
        "abstract": "This paper studies the housing market problem introduced by Shapley and Scarf (1974). We probe the Machiavellian frontier of the well-known top trading cycles (TTC) rule by weakening strategy-proofness and providing new characterizations for this rule. Specifically, our contribution lies in three aspects. First, we weaken the concept of strategy-proofness and introduce a new incentive notion called truncation-invariance, where the truthful preference-reporting assignment cannot be altered by any agent through misreporting a truncation of the true preference at the assignment produced by the true preference unilaterally. Second, we characterize the TTC rule by the following three groups of axioms: individual rationality, pair-efficiency, truncation-invariance; individual rationality, Pareto efficiency, truncation-invariance; individual rationality, endowments-swapping-proofness, truncation-invariance.1 The new characterizations refine several previous results.2 Third, we show through examples that the characterization results of Takamiya (2001) and Miyagawa (2002) can no longer be obtained if strategy-proofness is replaced with truncation-invariance.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2107.03253",
        "category": "econ",
        "title": "Dynamic ordered panel logit models",
        "abstract": "This paper studies a dynamic ordered logit model for panel data with fixed effects. The main contribution of the paper is to construct a set of valid moment conditions that are free of the fixed effects. The moment functions can be computed using four or more periods of data, and the paper presents sufficient conditions for the moment conditions to identify the common parameters of the model, namely the regression coefficients, the autoregressive parameters, and the threshold parameters. The availability of moment conditions suggests that these common parameters can be estimated using the generalized method of moments, and the paper documents the performance of this estimator using Monte Carlo simulations and an empirical illustration to self-reported health status using the British Household Panel Survey.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2212.14444",
        "category": "econ",
        "title": "Empirical Bayes When Estimation Precision Predicts Parameters",
        "abstract": "Empirical Bayes methods usually maintain a prior independence assumption: The unknown parameters of interest are independent from the known standard errors of the estimates. This assumption is often theoretically questionable and empirically rejected. This paper instead models the conditional distribution of the parameter given the standard errors as a flexibly parametrized family of distributions, leading to a family of methods that we call CLOSE. This paper establishes that (i) CLOSE is rate-optimal for squared error Bayes regret, (ii) squared error regret control is sufficient for an important class of economic decision problems, and (iii) CLOSE is worst-case robust when our assumption on the conditional distribution is misspecified. Empirically, using CLOSE leads to sizable gains for selecting high-mobility Census tracts. Census tracts selected by CLOSE are substantially more mobile on average than those selected by the standard shrinkage method.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2306.13383",
        "category": "econ",
        "title": "Fair integer programming under dichotomous and cardinal preferences",
        "abstract": "One cannot make truly fair decisions using integer linear programs unless one controls the selection probabilities of the (possibly many) optimal solutions. For this purpose, we propose a unified framework when binary decision variables represent agents with dichotomous preferences, who only care about whether they are selected in the final solution. We develop several general-purpose algorithms to fairly select optimal solutions, for example, by maximizing the Nash product or the minimum selection probability, or by using a random ordering of the agents as a selection criterion (Random Serial Dictatorship). We also discuss in detail how to extend the proposed methods when agents have cardinal preferences. As such, we embed the black-box procedure of solving an integer linear program into a framework that is explainable from start to finish. Lastly, we evaluate the proposed methods on two specific applications, namely kidney exchange (dichotomous preferences), and the scheduling problem of minimizing total tardiness on a single machine (cardinal preferences). We find that while the methods maximizing the Nash product or the minimum selection probability outperform the other methods on the evaluated welfare criteria, methods such as Random Serial Dictatorship perform reasonably well in computation times that are similar to those of finding a single optimal solution.",
        "references": [
            {
                "arxivId": "1403.0974",
                "title": "Parametrized algorithms for random serial dictatorship",
                "abstract": null
            },
            {
                "arxivId": "1304.3169",
                "title": "The Computational Complexity of Random Serial Dictatorship",
                "abstract": null
            },
            {
                "arxivId": "2106.01946",
                "title": "Convex optimization",
                "abstract": "This textbook is based on lectures given by the authors at MIPT (Moscow), HSE (Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all, the authors focused on the program of a two-semester course of lectures on convex optimization, which is given to students of MIPT. The first chapter of this book contains the materials of the first semester (\"Fundamentals of convex analysis and optimization\"), the second and third chapters contain the materials of the second semester (\"Numerical methods of convex optimization\"). The textbook has a number of features. First, in contrast to the classic manuals, this book does not provide proofs of all the theorems mentioned. This allowed, on one side, to describe more themes, but on the other side, made the presentation less self-sufficient. The second important point is that part of the material is advanced and is published in the Russian educational literature, apparently for the first time. Third, the accents that are given do not always coincide with the generally accepted accents in the textbooks that are now popular. First of all, we talk about a sufficiently advanced presentation of conic optimization, including robust optimization, as a vivid demonstration of the capabilities of modern convex analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2404.04335",
        "category": "econ",
        "title": "Estimating contagion mechanism in global equity market with time\u2010zone effect",
        "abstract": "This paper proposes a time-zone vector autoregression (VAR) model to investigate comovements in the global financial market. Analyzing daily data from 36 national equity markets, we explore the subprime and European debt crises using static analysis and the COVID-19 crisis through a rolling window method. Our study of comovements using VAR coefficients reveals a resonance effect in the global system. Findings on densities and assortativities suggest the existence of the transmission mechanism in all periods and abnormal structural changes during the crises. Strength analysis uncovers the information transmission mechanism across continents over normal and turmoil periods and emphasizes specific stock markets' unique roles. We examine dynamic continent strengths to demonstrate the contagion mechanism in the global equity market over an extended period. Incorporating the time-zone effect significantly enhances the VAR model's interpretability. Signed networks provide more information on global equity markets and better identifies critical contagion patterns than unsigned networks.",
        "references": [
            {
                "arxivId": "2403.19363",
                "title": "Dynamic correlation of market connectivity, risk spillover and abnormal volatility in stock price",
                "abstract": null
            },
            {
                "arxivId": "2012.12702",
                "title": "Systemic Risk in Financial Networks: A Survey",
                "abstract": "We provide an overview of the relationship between financial networks and systemic risk. We present a taxonomy of different types of systemic risk, differentiating between direct externalities between financial organizations (e.g., defaults, correlated portfolios, fire sales), and perceptions and feedback effects (e.g., bank runs, credit freezes). We also discuss optimal regulation and bailouts, measurements of systemic risk and financial centrality, choices by banks regarding their portfolios and partnerships, and the changing nature of financial networks."
            },
            {
                "arxivId": "1311.4798",
                "title": "The multiplex structure of interbank networks",
                "abstract": "The interbank market has a natural multiplex network representation. We employ a unique database of supervisory reports on Italian banks to the Banca d\u2019Italia that includes all bilateral exposures broken down by maturity and by the secured and unsecured nature of the contract. We find that layers have different topological properties and persistence over time. The presence of a link in a layer is not a good predictor of the presence of the same link in other layers. Maximum entropy models reveal different unexpected substructures, such as network motifs, in different layers. Using the total interbank network or focusing on a specific layer as representative of the other layers provides a poor representation of interlinkages in the interbank market and could lead to biased estimation of systemic risk."
            },
            {
                "arxivId": "1308.0925",
                "title": "Unveiling correlations between financial variables and topological metrics of trading networks: Evidence from a stock and its warrant",
                "abstract": null
            },
            {
                "arxivId": "1003.2459",
                "title": "Complex stock trading network among investors",
                "abstract": null
            },
            {
                "arxivId": "1001.3731",
                "title": "Experimental evidence for the interplay between individual wealth and transaction network",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0311416",
                "title": "The architecture of complex weighted networks.",
                "abstract": "Networked structures arise in a wide array of different contexts such as technological and transportation infrastructures, social phenomena, and biological systems. These highly interconnected systems have recently been the focus of a great deal of attention that has uncovered and characterized their topological complexity. Along with a complex topological structure, real networks display a large heterogeneity in the capacity and intensity of the connections. These features, however, have mainly not been considered in past studies where links are usually represented as binary states, i.e., either present or absent. Here, we study the scientific collaboration network and the world-wide air-transportation network, which are representative examples of social and large infrastructure systems, respectively. In both cases it is possible to assign to each edge of the graph a weight proportional to the intensity or capacity of the connections among the various elements of the network. We define appropriate metrics combining weighted and topological observables that enable us to characterize the complex statistical properties and heterogeneity of the actual strength of edges and vertices. This information allows us to investigate the correlations among weighted quantities and the underlying topological structure of the network. These results provide a better description of the hierarchies and organizational principles at the basis of the architecture of weighted networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2404.06019",
        "category": "econ",
        "title": "Robust Advertisement Pricing",
        "abstract": "We consider the robust pricing problem of an advertising platform that charges a producer for disclosing hard evidence of product quality to a consumer before trading. Multiple equilibria arise since consumer beliefs and producer's contingent advertisement purchases are interdependent. To tackle strategic uncertainty, the platform offers each producer's quality type a menu of disclosure-probability-and-price plans to maximize its revenue guaranteed across all equilibria. The optimal menus offer a continuum of plans with strictly increasing marginal prices for higher disclosure probabilities. Full disclosure is implemented in the unique equilibrium. All partial-disclosure plans, though off-path, preclude bad equilibrium play. This solution admits a tractable price function that suggests volume-based pricing can outperform click-based pricing when strategic uncertainty is accounted for. Moreover, the platform prioritizes attracting higher types into service and offers them higher rents despite symmetric information between the platform and the producer.",
        "references": [
            {
                "arxivId": "2010.08037",
                "title": "How to Sell Hard Information",
                "abstract": "\n The seller of an asset has the option to buy hard information about the value of the asset from an intermediary. The seller can then disclose this information before selling the asset in a competitive market. We study how the intermediary designs and sells hard information to robustly maximize the intermediary's revenue across all equilibria. Even though the intermediary could use an accurate test that reveals the asset\u2019s value, we show that robust revenue maximization leads to a noisy test with a continuum of possible scores. In addition, the intermediary always charges the seller for disclosing the test score to the market, but not necessarily for running the test. This enables the intermediary to robustly appropriate a significant share of the surplus resulting from the asset sale."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2404.06471",
        "category": "econ",
        "title": "Regression Discontinuity Design with Spillovers",
        "abstract": "Researchers who estimate treatment effects using a regression discontinuity design (RDD) typically assume that there are no spillovers between the treated and control units. This may be unrealistic. We characterize the estimand of RDD in a setting where spillovers occur between units that are close in their values of the running variable. Under the assumption that spillovers are linear-in-means, we show that the estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression. Specifically, RDD estimates direct treatment effect when radius is of larger order than the bandwidth, and total treatment effect when radius is of smaller order than the bandwidth. In the more realistic regime where radius is of similar order as the bandwidth, the RDD estimand is a mix of the above effects. To recover direct and spillover effects, we propose incorporating estimated spillover terms into local linear regression -- the local analog of peer effects regression. We also clarify the settings under which the donut-hole RD is able to eliminate the effects of spillovers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-10.json",
        "arxivId": "2404.06472",
        "category": "econ",
        "title": "High-skilled Human Workers in Non-Routine Jobs are Susceptible to AI Automation but Wage Benefits Differ between Occupations",
        "abstract": "Artificial Intelligence (AI) will change human work by taking over specific job tasks, but there is a debate which tasks are susceptible to automation, and whether AI will augment or replace workers and affect wages. By combining data on job tasks with a measure of AI susceptibility, we show that more highly skilled workers are more susceptible to AI automation, and that analytical non-routine tasks are at risk to be impacted by AI. Moreover, we observe that wage growth premiums for the lowest and the highest required skill level appear unrelated to AI susceptibility and that workers in occupations with many routine tasks saw higher wage growth if their work was more strongly susceptible to AI. Our findings imply that AI has the potential to affect human workers differently than canonical economic theories about the impact of technology on work these theories predict.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-11.json",
        "arxivId": "2106.01681",
        "category": "econ",
        "title": "The Origin of Corporate Control Power",
        "abstract": "How does the control power of corporate shareholder arise? On the fundamental principles of economics, we discover that the probability of top1 shareholder possessing optimal control power evolves in Fibonacci series pattern and emerges as the wave between 1/2 and 2/3 along with time in period of 12h (h is the time distance between the state and state of the evolution). This novel feature suggests the efficiency of the allocation of corporate shareholders' right and power. Data on the Chinese stock market support this prediction.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-11.json",
        "arxivId": "2106.05031",
        "category": "econ",
        "title": "Estimation of Optimal Dynamic Treatment Assignment Rules under Policy Constraints",
        "abstract": "This paper studies statistical decisions for dynamic treatment assignment problems. Many policies involve dynamics in their treatment assignments where treatments are sequentially assigned to individuals across multiple stages and the effect of treatment at each stage is usually heterogeneous with respect to the prior treatments, past outcomes, and observed covariates. We consider estimating an optimal dynamic treatment rule that guides the optimal treatment assignment for each individual at each stage based on the individual's history. This paper proposes an empirical welfare maximization approach in a dynamic framework. The approach estimates the optimal dynamic treatment rule using data from an experimental or quasi-experimental study. The paper proposes two estimation methods: one solves the treatment assignment problem at each stage through backward induction, and the other solves the whole dynamic treatment assignment problem simultaneously across all stages. We derive finite-sample upper bounds on worst-case average welfare regrets for the proposed methods and show $1/\\sqrt{n}$-minimax convergence rates. We also modify the simultaneous estimation method to incorporate intertemporal budget/capacity constraints.",
        "references": [
            {
                "arxivId": "2106.12886",
                "title": "Constrained Classification and Policy Learning",
                "abstract": "Modern machine learning approaches to classification, including AdaBoost, support vector machines, and deep neural networks, utilize surrogate loss techniques to circumvent the computational complexity of minimizing empirical classification risk. These techniques are also useful for causal policy learning problems, since estimation of individualized treatment rules can be cast as a weighted (cost-sensitive) classification problem. Consistency of the surrogate loss approaches studied in Zhang (2004) and Bartlett et al. (2006) crucially relies on the assumption of correct specification, meaning that the specified set of classifiers is rich enough to contain a first-best classifier. This assumption is, however, less credible when the set of classifiers is constrained by interpretability or fairness, leaving the applicability of surrogate loss based algorithms unknown in such second-best scenarios. This paper studies consistency of surrogate loss procedures under a constrained set of classifiers without assuming correct specification. We show that in the setting where the constraint restricts the classifier's prediction set only, hinge losses (i.e., $\\ell_1$-support vector machines) are the only surrogate losses that preserve consistency in second-best scenarios. If the constraint additionally restricts the functional form of the classifier, consistency of a surrogate loss approach is not guaranteed even with hinge loss. We therefore characterize conditions for the constrained set of classifiers that can guarantee consistency of hinge risk minimizing classifiers. Exploiting our theoretical results, we develop robust and computationally attractive hinge loss based procedures for a monotone classification problem."
            },
            {
                "arxivId": "1905.09751",
                "title": "Learning When-to-Treat Policies",
                "abstract": "Abstract Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an \u201cadvantage doubly robust\u201d estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1810.04778",
                "title": "Offline Multi-Action Policy Learning: Generalization and Optimization",
                "abstract": "As a result of digitization of the economy, more and more decision makers from a wide range of domains have gained the ability to target products, services, and information provision based on individual characteristics. Examples include selecting offers, prices, advertisements, or emails to send to consumers, choosing a bid to submit in a contextual first-price auctions, and determining which medication to prescribe to a patient. The key to enabling this is to learn a treatment policy from historical observational data in a sample-efficient way, hence uncovering the best personalized treatment choice recommendation. In \u201cOffline Policy Learning: Generalization and Optimization,\u201d Z. Zhou, S. Athey, and S. Wager provide a sample-optimal policy learning algorithm that is computationally efficient and that learns a tree-based treatment policy from observational data. In our quest toward fully automated personalization, the work provides a theoretically sound and practically implementable approach."
            },
            {
                "arxivId": "1805.09397",
                "title": "Identification in Nonparametric Models for Dynamic Treatment Effects",
                "abstract": "This paper develops a nonparametric model that represents how sequences of outcomes and treatment choices influence one another in a dynamic manner. In this setting, we are interested in identifying the average outcome for individuals in each period, had a particular treatment sequence been assigned. The identification of this quantity allows us to identify the average treatment effects (ATE's) and the ATE's on transitions, as well as the optimal treatment regimes, namely, the regimes that maximize the (weighted) sum of the average potential outcomes, possibly less the cost of the treatments. The main contribution of this paper is to relax the sequential randomization assumption widely used in the biostatistics literature by introducing a flexible choice-theoretic framework for a sequence of endogenous treatments. We show that the parameters of interest are identified under each period's two-way exclusion restriction, i.e., with instruments excluded from the outcome-determining process and other exogenous variables excluded from the treatment-selection process. We also consider partial identification in the case where the latter variables are not available. Lastly, we extend our results to a setting where treatments do not appear in every period."
            },
            {
                "arxivId": "1705.09952",
                "title": "Optimal sequential treatment allocation",
                "abstract": "In treatment allocation problems the individuals to be treated often arrive sequentially. We study a problem in which the policy maker is not only interested in the expected cumulative welfare but is also concerned about the uncertainty/risk of the treatment outcomes. At the outset, the total number of treatment assignments to be made may even be unknown. A sequential treatment policy which attains the minimax optimal regret is proposed. We also demonstrate that the expected number of suboptimal treatments only grows slowly in the number of treatments. Finally, we study a setting where outcomes are only observed with delay."
            },
            {
                "arxivId": "1609.03167",
                "title": "Model Selection for Treatment Choice: Penalized Welfare Maximization",
                "abstract": "This paper studies a new statistical decision rule for the treatment assignment problem. Consider a utilitarian policy maker who must use sample data to allocate one of two treatments to members of a population, based on their observable characteristics. In practice, it is often the case that policy makers do not have full discretion on how these covariates can be used, for legal, ethical or political reasons. We treat this constrained problem as a statistical decision problem, where we evaluate the performance of decision rules by their maximum regret. We focus on settings in which the policy maker may want to select amongst a collection of such constrained classes: examples we consider include choosing the number of covariates over which to perform best-subset selection, and model selection when approximating a complicated class via a sieve. We adapt and extend results from statistical learning to develop a decision rule which we call the Penalized Welfare Maximization (PWM) rule. We establish an oracle inequality for the regret of the PWM rule which shows that it is able to perform model selection over the collection of available classes. We then use this oracle inequality to derive relevant bounds on maximum regret for PWM. We illustrate the model-selection capabilities of our method with a small simulation exercise, and conclude by applying our rule to data from the Job Training Partnership Act (JTPA) study."
            },
            {
                "arxivId": "1606.01472",
                "title": "Interpretable Dynamic Treatment Regimes",
                "abstract": "ABSTRACT Precision medicine is currently a topic of great interest in clinical and intervention science.\u2009 A key component of precision medicine is that it is evidence-based, that is, data-driven, and consequently there has been tremendous interest in estimation of precision medicine strategies using observational or randomized study data. One way to formalize precision medicine is through a treatment regime, which is a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended treatment. An optimal treatment regime is defined as maximizing the mean of some cumulative clinical outcome if applied to a population of interest. It is well-known that even under simple generative models an optimal treatment regime can be a highly nonlinear function of patient information. Consequently, a focal point of recent methodological research has been the development of flexible models for estimating optimal treatment regimes. However, in many settings, estimation of an optimal treatment regime is an exploratory analysis intended to generate new hypotheses for subsequent research and not to directly dictate treatment to new patients. In such settings, an estimated treatment regime that is interpretable in a domain context may be of greater value than an unintelligible treatment regime built using \u201cblack-box\u201d estimation methods. We propose an estimator of an optimal treatment regime composed of a sequence of decision rules, each expressible as a list of \u201cif-then\u201d statements that can be presented as either a paragraph or as a simple flowchart that is immediately interpretable to domain experts. The discreteness of these lists precludes smooth, that is, gradient-based, methods of estimation and leads to nonstandard asymptotics. Nevertheless, we provide a computationally efficient estimation algorithm, prove consistency of the proposed estimator, and derive rates of convergence. We illustrate the proposed methods using a series of simulation examples and application to data from a sequential clinical trial on bipolar disorder. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "math/0702683",
                "title": "Risk bounds for statistical learning",
                "abstract": "We propose a general theorem providing upper bounds for the risk of an empirical risk minimizer (ERM).We essentially focus on the binary classification framework. We extend Tsybakov's analysis of the risk of an ERM under margin type conditions by using concentration inequalities for conveniently weighted empirical processes. This allows us to deal with ways of measuring the ``size'' of a class of classifiers other than entropy with bracketing as in Tsybakov's work. In particular, we derive new risk bounds for the ERM when the classification rules belong to some VC-class under margin conditions and discuss the optimality of these bounds in a minimax sense."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-11.json",
        "arxivId": "2204.08356",
        "category": "econ",
        "title": "Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes",
        "abstract": "This paper considers the problem of inference in cluster randomized experiments when cluster sizes are non-ignorable. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the cluster level. By non-ignorable cluster sizes, we refer to the possibility that the treatment effects may depend non-trivially on the cluster sizes. We frame our analysis in a super-population framework in which cluster sizes are random. In this way, our analysis departs from earlier analyses of cluster randomized experiments in which cluster sizes are treated as non-random. We distinguish between two different parameters of interest: the equally-weighted cluster-level average treatment effect, and the size-weighted cluster-level average treatment effect. For each parameter, we provide methods for inference in an asymptotic framework where the number of clusters tends to infinity and treatment is assigned using a covariate-adaptive stratified randomization procedure. We additionally permit the experimenter to sample only a subset of the units within each cluster rather than the entire cluster and demonstrate the implications of such sampling for some commonly used estimators. A small simulation study and empirical demonstration show the practical relevance of our theoretical results.",
        "references": [
            {
                "arxivId": "1902.01497",
                "title": "Asymptotic Theory for Clustered Samples",
                "abstract": "We provide a complete asymptotic distribution theory for clustered data with a large number of groups, generalizing the classic laws of large numbers, uniform laws, central limit theory, and clustered covariance matrix estimation. Our theory allows for clustered observations with heterogeneous and unbounded cluster sizes. Our conditions cleanly nest the classical results for i.n.i.d. observations, in the sense that our conditions specialize to the classical conditions under independent sampling. We use this theory to develop a full asymptotic distribution theory for estimation based on linear least-squares, 2SLS, nonlinear MLE, and nonlinear GMM."
            },
            {
                "arxivId": "1702.08615",
                "title": "Bridging Finite and Super Population Causal Inference",
                "abstract": "Abstract There are two general views in causal analysis of experimental data: the super population view that the units are an independent sample from some hypothetical infinite population, and the finite population view that the potential outcomes of the experimental units are fixed and the randomness comes solely from the treatment assignment. These two views differs conceptually and mathematically, resulting in different sampling variances of the usual difference-in-means estimator of the average causal effect. Practically, however, these two views result in identical variance estimators. By recalling a variance decomposition and exploiting a completeness-type argument, we establish a connection between these two views in completely randomized experiments. This alternative formulation could serve as a template for bridging finite and super population causal inference in other scenarios."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-11.json",
        "arxivId": "2306.08760",
        "category": "econ",
        "title": "Do Productivity Shocks Cause Inputs Misallocation?",
        "abstract": "This paper investigates how productivity dispersion relates to input misallocation using a model with staggered productivity shocks that create wedges between anticipated and realized productivity for any production input. With inputs allocated optimally ex ante but suboptimally ex post, dispersion in realized productivity contributes to ex post input misallocation. Analyzing European firm data from 2000-2017 reveals significant co-movement between productivity dispersion and capital/labor misallocation across industries. Productivity dispersion explains a substantial share of capital and labor misallocation (40% and 70%), and 10% of materials misallocation, confirming its key role in allocation frictions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-11.json",
        "arxivId": "2307.11732",
        "category": "econ",
        "title": "Advancing Ad Auction Realism: Practical Insights & Modeling Implications",
        "abstract": "Contemporary real-world online ad auctions differ from canonical models [Edelman et al., 2007; Varian, 2009] in at least four ways: (1) values and click-through rates can depend upon users' search queries, but advertisers can only partially\"tune\"their bids to specific queries; (2) advertisers do not know the number, identity, and precise value distribution of competing bidders; (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially known to bidders. These features make it virtually impossible to fully characterize equilibrium bidding behavior. This paper shows that, nevertheless, one can still gain useful insight into modern ad auctions by modeling advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. To demonstrate our approach, we first simulate\"soft-floor\"auctions [Zeithammer, 2019], a complex, real-world pricing rule for which no complete equilibrium characterization is known. We find that (i) when values and click-through rates are query-dependent, soft floors can improve revenues relative to standard auction formats even if bidder types are drawn from the same distribution; and (ii) with distributional asymmetries that reflect relevant real-world scenario, we find that soft floors yield lower revenues than suitably chosen reserve prices, even restricting attention to a single query. We then demonstrate how to infer advertiser value distributions from observed bids for a variety of pricing rules, and illustrate our approach with aggregate data from an e-commerce website.",
        "references": [
            {
                "arxivId": "2302.01523",
                "title": "Multi-channel Autobidding with Budget and ROI Constraints",
                "abstract": "In digital online advertising, advertisers procure ad impressions simultaneously on multiple platforms, or so-called channels, such as Google Ads, Meta Ads Manager, etc., each of which consists of numerous ad auctions. We study how an advertiser maximizes total conversion (e.g. ad clicks) while satisfying aggregate return-on-investment (ROI) and budget constraints across all channels. In practice, an advertiser does not have control over, and thus cannot globally optimize, which individual ad auctions she participates in for each channel, and instead authorizes a channel to procure impressions on her behalf: the advertiser can only utilize two levers on each channel, namely setting a per-channel budget and per-channel target ROI. In this work, we first analyze the effectiveness of each of these levers for solving the advertiser's global multi-channel problem. We show that when an advertiser only optimizes over per-channel ROIs, her total conversion can be arbitrarily worse than what she could have obtained in the global problem. Further, we show that the advertiser can achieve the global optimal conversion when she only optimizes over per-channel budgets. In light of this finding, under a bandit feedback setting that mimics real-world scenarios where advertisers have limited information on ad auctions in each channels and how channels procure ads, we present an efficient learning algorithm that produces per-channel budgets whose resulting conversion approximates that of the global optimal problem. Finally, we argue that all our results hold for both single-item and multi-item auctions from which channels procure impressions on advertisers' behalf."
            },
            {
                "arxivId": "2202.05947",
                "title": "Artificial Intelligence and Auction Design",
                "abstract": "Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about the lowest bid to win, as introduced by Google at the time of the switch to first-price auctions, increases competitiveness of auctions."
            },
            {
                "arxivId": "2107.06259",
                "title": "Robust Learning of Optimal Auctions",
                "abstract": "We study the problem of learning revenue-optimal multi-bidder auctions from samples when the samples of bidders' valuations can be adversarially corrupted or drawn from distributions that are adversarially perturbed. First, we prove tight upper bounds on the revenue we can obtain with a corrupted distribution under a population model, for both regular valuation distributions and distributions with monotone hazard rate (MHR). We then propose new algorithms that, given only an ``approximate distribution'' for the bidder's valuation, can learn a mechanism whose revenue is nearly optimal simultaneously for all ``true distributions'' that are $\\alpha$-close to the original distribution in Kolmogorov-Smirnov distance. The proposed algorithms operate beyond the setting of bounded distributions that have been studied in prior works, and are guaranteed to obtain a fraction $1-O(\\alpha)$ of the optimal revenue under the true distribution when the distributions are MHR. Moreover, they are guaranteed to yield at least a fraction $1-O(\\sqrt{\\alpha})$ of the optimal revenue when the distributions are regular. We prove that these upper bounds cannot be further improved, by providing matching lower bounds. Lastly, we derive sample complexity upper bounds for learning a near-optimal auction for both MHR and regular distributions."
            },
            {
                "arxivId": "2103.06177",
                "title": "Equilibria in Auctions with Ad Types",
                "abstract": "This paper studies equilibrium quality of semi-separable position auctions (known as the Ad Types setting [9]) with greedy or optimal allocation combined with generalized second-price (GSP) or Vickrey-Clarke-Groves (VCG) pricing. We make three contributions: first, we give upper and lower bounds on the Price of Anarchy (PoA) for auctions which use greedy allocation with GSP pricing, greedy allocation with VCG pricing, and optimal allocation with GSP pricing. Second, we give Bayes-Nash equilibrium characterizations for two-player, two-slot instances (for all auction formats) and show that there exists both a revenue hierarchy and revenue equivalence across some formats. Finally, we use no-regret learning algorithms and bidding data from a large online advertising platform to evaluate the performance of the mechanisms under semi-realistic conditions. We find that the VCG mechanism tends to obtain revenue and welfare comparable to or better than that of the other mechanisms. We also find that in practice, each of the mechanisms obtains significantly better welfare than our worst-case bounds might suggest."
            },
            {
                "arxivId": "2009.06136",
                "title": "Convergence Analysis of No-Regret Bidding Algorithms in Repeated Auctions",
                "abstract": "The connection between games and no-regret algorithms has been widely studied in the literature. A fundamental result is that when all players play no-regret strategies, this produces a sequence of actions whose time-average is a coarse-correlated equilibrium of the game. However, much less is known about equilibrium selection in the case that multiple equilibria exist. In this work, we study the convergence of no-regret bidding algorithms in auctions. Besides being of theoretical interest, bidding dynamics in auctions is an important question from a practical viewpoint as well. We study the repeated game between bidders in which a single item is sold at each time step and the bidder's value is drawn from an unknown distribution. We show that if the bidders use any mean-based learning rule then the bidders converge with high probability to the truthful pure Nash Equilibrium in a second price auction, in VCG auction in the multi-slot setting and to the Bayesian Nash equilibrium in a first price auction. We note mean-based algorithms cover a wide variety of known no-regret algorithms such as Exp3, UCB, \\epsilon-Greedy etc. Also, we analyze the convergence of the individual iterates produced by such learning algorithms, as opposed to the time-average of the sequence. Our experiments corroborate our theoretical findings and also find a similar convergence when we use other strategies such as Deep Q-Learning."
            },
            {
                "arxivId": "2006.05684",
                "title": "Auction learning as a two-player game",
                "abstract": "Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. While theoretical approaches to the problem have hit some limits, a recent research direction initiated by Duetting et al. (2019) consists in building neural network architectures to find optimal auctions. We propose two conceptual deviations from their approach which result in enhanced performance. First, we use recent results in theoretical auction design (Rubinstein and Weinberg, 2018) to introduce a time-independent Lagrangian. This not only circumvents the need for an expensive hyper-parameter search (as in prior work), but also provides a principled metric to compare the performance of two auctions (absent from prior work). Second, the optimization procedure in previous work uses an inner maximization loop to compute optimal misreports. We amortize this process through the introduction of an additional neural network. We demonstrate the effectiveness of our approach by learning competitive or strictly improved auctions compared to prior work. Both results together further imply a novel formulation of Auction Design as a two-player game with stationary utility functions."
            },
            {
                "arxivId": "2004.02764",
                "title": "Using Multi-Agent Reinforcement Learning in Auction Simulations",
                "abstract": "Game theory has been developed by scientists as a theory of strategic interaction among players who are supposed to be perfectly rational. These strategic interactions might have been presented in an auction, a business negotiation, a chess game, or even in a political conflict aroused between different agents. In this study, the strategic (rational) agents created by reinforcement learning algorithms are supposed to be bidder agents in various types of auction mechanisms such as British Auction, Sealed Bid Auction, and Vickrey Auction designs. Next, the equilibrium points determined by the agents are compared with the outcomes of the Nash equilibrium points for these environments. The bidding strategy of the agents is analyzed in terms of individual rationality, truthfulness (strategy-proof), and computational efficiency. The results show that using a multi-agent reinforcement learning strategy improves the outcomes of the auction simulations."
            },
            {
                "arxivId": "2003.09795",
                "title": "Optimal No-regret Learning in Repeated First-price Auctions",
                "abstract": "We study online learning in repeated first-price auctions with censored feedback, where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid in order to maximize her cumulative payoff. To achieve this goal, the bidder faces a challenging dilemma: if she wins the bid--the only way to achieve positive payoffs--then she is not able to observe the highest bid of the other bidders, which we assume is iid drawn from an unknown distribution. This dilemma, despite being reminiscent of the exploration-exploitation trade-off in contextual bandits, cannot directly be addressed by the existing UCB or Thompson sampling algorithms in that literature, mainly because contrary to the standard bandits setting, when a positive reward is obtained here, nothing about the environment can be learned. \nIn this paper, by exploiting the structural properties of first-price auctions, we develop the first learning algorithm that achieves $O(\\sqrt{T}\\log^2 T)$ regret bound when the bidder's private values are stochastically generated. We do so by providing an algorithm on a general class of problems, which we call monotone group contextual bandits, where the same regret bound is established under stochastically generated contexts. Further, by a novel lower bound argument, we characterize an $\\Omega(T^{2/3})$ lower bound for the case where the contexts are adversarially generated, thus highlighting the impact of the contexts generation mechanism on the fundamental learning limit. Despite this, we further exploit the structure of first-price auctions and develop a learning algorithm that operates sample-efficiently (and computationally efficiently) in the presence of adversarially generated private values. We establish an $O(\\sqrt{T}\\log^3 T)$ regret bound for this algorithm, hence providing a complete characterization of optimal learning guarantees for this problem."
            },
            {
                "arxivId": "2002.01129",
                "title": "Bayesian Meta-Prior Learning Using Empirical Bayes",
                "abstract": "Adding domain knowledge to a learning system is known to improve results. In multiparameter Bayesian frameworks, such knowledge is incorporated as a prior. On the other hand, the various model parameters can have different learning rates in real-world problems, especially with skewed data. Two often-faced challenges in operation management and management science applications are the absence of informative priors and the inability to control parameter learning rates. In this study, we propose a hierarchical empirical Bayes approach that addresses both challenges and that can generalize to any Bayesian framework. Our method learns empirical meta-priors from the data itself and uses them to decouple the learning rates of first-order and second-order features (or any other given feature grouping) in a generalized linear model. Because the first-order features are likely to have a more pronounced effect on the outcome, focusing on learning first-order weights first is likely to improve performance and convergence time. Our empirical Bayes method clamps features in each group together and uses the deployed model\u2019s observed data to empirically compute a hierarchical prior in hindsight. We report theoretical results for the unbiasedness, strong consistency, and optimal frequentist cumulative regret properties of our meta-prior variance estimator. We apply our method to a standard supervised learning optimization problem as well as an online combinatorial optimization problem in a contextual bandit setting implemented in an Amazon production system. During both simulations and live experiments, our method shows marked improvements, especially in cases of small traffic. Our findings are promising because optimizing over sparse data is often a challenge. This paper was accepted by Hamid Nazerzadeh, special issue on data-driven prescriptive analytics."
            },
            {
                "arxivId": "2004.00100",
                "title": "Optimal Bidding Strategy without Exploration in Real-time Bidding",
                "abstract": "Maximizing utility with a budget constraint is the primary goal for advertisers in real-time bidding (RTB) systems. The policy maximizing the utility is referred to as the optimal bidding strategy. Earlier works on optimal bidding strategy apply model-based batch reinforcement learning methods which can not generalize to unknown budget and time constraint. Further, the advertiser observes a censored market price which makes direct evaluation infeasible on batch test datasets. Previous works ignore the losing auctions to alleviate the difficulty with censored states; thus significantly modifying the test distribution. We address the challenge of lacking a clear evaluation procedure as well as the error propagated through batch reinforcement learning methods in RTB systems. We exploit two conditional independence structures in the sequential bidding process that allow us to propose a novel practical framework using the maximum entropy principle to imitate the behavior of the true distribution observed in real-time traffic. Moreover, the framework allows us to train a model that can generalize to the unseen budget conditions than limit only to those observed in history. We compare our methods on two real-world RTB datasets with several baselines and demonstrate significantly improved performance under various budget settings."
            },
            {
                "arxivId": "1904.02235",
                "title": "Robust Multi-agent Counterfactual Prediction",
                "abstract": "We consider the problem of using logged data to make predictions about what would happen if we changed the `rules of the game' in a multi-agent system. This task is difficult because in many cases we observe actions individuals take but not their private information or their full reward functions. In addition, agents are strategic, so when the rules change, they will also change their actions. Existing methods (e.g. structural estimation, inverse reinforcement learning) make counterfactual predictions by constructing a model of the game, adding the assumption that agents' behavior comes from optimizing given some goals, and then inverting observed actions to learn agent's underlying utility function (a.k.a. type). Once the agent types are known, making counterfactual predictions amounts to solving for the equilibrium of the counterfactual environment. This approach imposes heavy assumptions such as rationality of the agents being observed, correctness of the analyst's model of the environment/parametric form of the agents' utility functions, and various other conditions to make point identification possible. We propose a method for analyzing the sensitivity of counterfactual conclusions to violations of these assumptions. We refer to this method as robust multi-agent counterfactual prediction (RMAC). We apply our technique to investigating the robustness of counterfactual claims for classic environments in market design: auctions, school choice, and social choice. Importantly, we show RMAC can be used in regimes where point identification is impossible (e.g. those which have multiple equilibria or non-injective maps from type distributions to outcomes)."
            },
            {
                "arxivId": "1809.09582",
                "title": "Contextual Bandits with Cross-learning",
                "abstract": "In the classic contextual bandits problem, in each round t, a learner observes some context c, chooses some action i to perform, and receives some reward [Formula: see text]. We consider the variant of this problem in which in addition to receiving the reward [Formula: see text], the learner also learns the values of [Formula: see text] for some other contexts [Formula: see text] in set [Formula: see text], that is, the rewards that would be achieved by performing that action under different contexts [Formula: see text]. This variant arises in several strategic settings, such as learning how to bid in nontruthful repeated auctions, which has gained a lot of attention lately as many platforms have switched to running first price auctions. We call this problem the contextual bandits problem with cross-learning. The best algorithms for the classic contextual bandits problem achieve [Formula: see text] regret against all stationary policies, in which C is the number of contexts, K the number of actions, and T the number of rounds. We design and analyze new algorithms for the contextual bandits problem with cross-learning and show that their regret has better dependence on the number of contexts. Under complete cross-learning in which the rewards for all contexts are learned when choosing an action, that is, set [Formula: see text] contains all contexts, we show that our algorithms achieve regret [Formula: see text], removing the dependence on C. For any other cases, that is, under partial cross-learning in which [Formula: see text] for some context\u2013action pair of (i, c), the regret bounds depend on how the sets [Formula: see text] impact the degree to which cross-learning between contexts is possible. We simulate our algorithms on real auction data from an ad exchange running first price auctions and show that they outperform traditional contextual bandit algorithms."
            },
            {
                "arxivId": "1802.08365",
                "title": "Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising",
                "abstract": "Real-time bidding (RTB) is an important mechanism in online display advertising, where a proper bid for each page view plays an essential role for good marketing results. Budget constrained bidding is a typical scenario in RTB where the advertisers hope to maximize the total value of the winning impressions under a pre-set budget constraint. However, the optimal bidding strategy is hard to be derived due to the complexity and volatility of the auction environment. To address these challenges, in this paper, we formulate budget constrained bidding as a Markov Decision Process and propose a model-free reinforcement learning framework to resolve the optimization problem. Our analysis shows that the immediate reward from environment is misleading under a critical resource constraint. Therefore, we innovate a reward function design methodology for the reinforcement learning problems with constraints. Based on the new reward design, we employ a deep neural network to learn the appropriate reward so that the optimal policy can be learned effectively. Different from the prior model-based work, which suffers from the scalability problem, our framework is easy to be deployed in large-scale industrial applications. The experimental evaluations demonstrate the effectiveness of our framework on large-scale real datasets."
            },
            {
                "arxivId": "1711.01333",
                "title": "Learning to Bid Without Knowing your Value",
                "abstract": "We address online learning in complex auction settings, such as sponsored search auctions, where the value of the bidder is unknown to her, evolving in an arbitrary manner and observed only if the bidder wins an allocation. We leverage the structure of the utility of the bidder and the partial feedback that bidders typically receive in auctions, in order to provide algorithms with regret rates against the best fixed bid in hindsight, that are exponentially faster in convergence in terms of dependence on the action space, than what would have been derived by applying a generic bandit algorithm and almost equivalent to what would have been achieved in the full information setting. Our results are enabled by analyzing a new online learning setting with outcome-based feedback, which generalizes learning with feedback graphs. We provide an online learning algorithm for this setting, of independent interest, with regret that grows only logarithmically with the number of actions and linearly only in the number of potential outcomes (the latter being very small in most auction settings). Last but not least, we show that our algorithm outperforms the bandit approach experimentally and that this performance is robust to dropping some of our theoretical assumptions or introducing noise in the feedback that the bidder receives."
            },
            {
                "arxivId": "1511.05720",
                "title": "Online learning in repeated auctions",
                "abstract": "Motivated by online advertising auctions, we consider repeated Vickrey auctions where goods of unknown value are sold sequentially and bidders only learn (potentially noisy) information about a good's value once it is purchased. We adopt an online learning approach with bandit feedback to model this problem and derive bidding strategies for two models: stochastic and adversarial. In the stochastic model, the observed values of the goods are random variables centered around the true value of the good. In this case, logarithmic regret is achievable when competing against well behaved adversaries. In the adversarial model, the goods need not be identical and we simply compare our performance against that of the best fixed bid in hindsight. We show that sublinear regret is also achievable in this case and prove matching minimax lower bounds. To our knowledge, this is the first complete set of strategies for bidders participating in auctions of this type."
            },
            {
                "arxivId": "1505.00720",
                "title": "Econometrics for Learning Agents",
                "abstract": "The main goal of this paper is to develop a theory of inference of player valuations from observed data in the generalized second price auction without relying on the Nash equilibrium assumption. Existing work in Economics on inferring agent values from data relies on the assumption that all participant strategies are best responses of the observed play of other players, i.e. they constitute a Nash equilibrium. In this paper, we show how to perform inference relying on a weaker assumption instead: assuming that players are using some form of no-regret learning. Learning outcomes emerged in recent years as an attractive alternative to Nash equilibrium in analyzing game outcomes, modeling players who haven't reached a stable equilibrium, but rather use algorithmic learning, aiming to learn the best way to play from previous observations. In this paper we show how to infer values of players who use algorithmic learning strategies. Such inference is an important first step before we move to testing any learning theoretic behavioral model on auction data. We apply our techniques to a dataset from Microsoft's sponsored search ad auction system."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-11.json",
        "arxivId": "2402.09928",
        "category": "econ",
        "title": "When Can We Use Two-Way Fixed-Effects (TWFE): A Comparison of TWFE and Novel Dynamic Difference-in-Differences Estimators",
        "abstract": "The conventional Two-Way Fixed-Effects (TWFE) estimator has come under scrutiny lately. Recent literature has revealed potential shortcomings of TWFE when the treatment effects are heterogeneous. Scholars have developed new advanced dynamic Difference-in-Differences (DiD) estimators to tackle these potential shortcomings. However, confusion remains in applied research as to when the conventional TWFE is biased and what issues the novel estimators can and cannot address. In this study, we first provide an intuitive explanation of the problems of TWFE and elucidate the key features of the novel alternative DiD estimators. We then systematically demonstrate the conditions under which the conventional TWFE is inconsistent. We employ Monte Carlo simulations to assess the performance of dynamic DiD estimators under violations of key assumptions, which likely happens in applied cases. While the new dynamic DiD estimators offer notable advantages in capturing heterogeneous treatment effects, we show that the conventional TWFE performs generally well if the model specifies an event-time function. All estimators are equally sensitive to violations of the parallel trends assumption, anticipation effects or violations of time-varying exogeneity. Despite their advantages, the new dynamic DiD estimators tackle a very specific problem and they do not serve as a universal remedy for violations of the most critical assumptions. We finally derive, based on our simulations, recommendations for how and when to use TWFE and the new DiD estimators in applied research.",
        "references": [
            {
                "arxivId": "2312.05985",
                "title": "Fused Extended Two-Way Fixed Effects for Difference-in-Differences with Staggered Adoptions",
                "abstract": "To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We demonstrate FETWFE in simulation studies and an empirical application."
            },
            {
                "arxivId": "2309.15983",
                "title": "What To Do (and Not to Do) with Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study",
                "abstract": "Two-way fixed effects (TWFE) models are ubiquitous in causal panel analysis in political science. However, recent methodological discussions challenge their validity in the presence of heterogeneous treatment effects (HTE) and violations of the parallel trends assumption (PTA). This burgeoning literature has introduced multiple estimators and diagnostics, leading to confusion among empirical researchers on two fronts: the reliability of existing results based on TWFE models and the current best practices. To address these concerns, we examined, replicated, and reanalyzed 37 articles from three leading political science journals that employed observational panel data with binary treatments. Using six newly introduced HTE-robust estimators, we find that although precision may be affected, the core conclusions derived from TWFE estimates largely remain unchanged. PTA violations and insufficient statistical power, however, continue to be significant obstacles to credible inferences. Based on these findings, we offer recommendations for improving practice in empirical research."
            },
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            },
            {
                "arxivId": "2108.12419",
                "title": "Revisiting event study designs: robust and efficient estimation",
                "abstract": "We develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Our method applies with time-varying controls, in triple-difference designs, and with certain non-binary treatments. We show the practical relevance of our results in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter - about half as large as benchmark estimates used to calibrate macroeconomic models - and predominantly occurs in the first month after the rebate."
            },
            {
                "arxivId": "1804.05785",
                "title": "Estimating Dynamic Treatment Effects in Event Studies With Heterogeneous Treatment Effects",
                "abstract": "Event studies are frequently used to estimate average treatment effects on the treated (ATT). In estimating the ATT, researchers commonly use fixed effects models that implicitly assume constant treatment effects across cohorts. We show that this is not an innocuous assumption. In fixed effect models where the sole regressor is treatment status, the OLS coefficient is a non-convex average of the heterogeneous cohort-specific ATTs. When regressors containing lags and leads of treatment are added, the OLS coefficient corresponding to a given lead or lag picks up spurious terms consisting of treatment effects from other periods. Therefore, estimates from these commonly used models are not causally interpretable. We propose alternative estimators that identify certain convex averages of the cohort-specific ATTs, hence allowing for causal interpretation even under heterogeneous treatment effects. To illustrate the empirical content of our results, we show that the fixed effects estimators and our proposed estimators differ substantially in an application to the economic consequences of hospitalization."
            },
            {
                "arxivId": "1803.09015",
                "title": "Difference-in-Differences with Multiple Time Periods",
                "abstract": "Difference-in-Differences (DID) is one of the most important and popular designs for evaluating causal effects of policy changes. In its standard format, there are two time periods and two groups: in the first period no one is treated, and in the second period a \"treatment group\" becomes treated, whereas a \"control group\" remains untreated. However, many em- pirical applications of the DID design have more than two periods and variation in treatment timing. In this article, we consider identification and estimation of treatment effect parameters using DID with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the \"parallel trends assumption\" holds potentially only after conditioning on observed covariates. We propose a simple two-step estimation strategy, establish the asymptotic prop- erties of the proposed estimators, and prove the validity of a computationally convenient bootstrap procedure. Furthermore we propose a semiparametric data-driven testing procedure to assess the credibility of the DID design in our context. Finally, we analyze the effect of the minimum wage on teen employment from 2001-2007. By using our proposed methods we confront the challenges related to variation in the timing of the state-level minimum wage policy changes. Open-source software is available for implementing the proposed methods."
            },
            {
                "arxivId": "1803.08807",
                "title": "Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects",
                "abstract": "Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)"
            },
            {
                "arxivId": "1710.10251",
                "title": "Matrix Completion Methods for Causal Panel Data Models",
                "abstract": "Abstract In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \u201cmissing\u201d elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2103.06020",
        "category": "econ",
        "title": "A UNIVERSAL BASIC INCOME FOR BRAZIL: FISCAL AND DISTRIBUTIONAL EFFECTS OF ALTERNATIVE SCHEMES",
        "abstract": "ABSTRACT The Covid-19 pandemic outbreak has led to an increasing interest in universal basic income (UBI) proposals, as it exposed the inadequacy of traditional welfare systems to provide basic financial security to a large share of the population. In this paper, we use a static tax-benefit microsimulation model to analyse the fiscal and distributional effects of the hypothetical implementation in Brazil of alternative UBI schemes that partially replace the existing tax-transfer system. The results indicate that introducing a UBI/Flat Tax system in the country could be both extremely effective in reducing poverty and inequality and economically viable.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2205.04990",
        "category": "econ",
        "title": "Stable outcomes and information in games: An empirical framework",
        "abstract": null,
        "references": [
            {
                "arxivId": "1905.06491",
                "title": "Inference in a Class of Optimization Problems: Confidence Regions and Finite Sample Bounds on Errors in Coverage Probabilities",
                "abstract": "Abstract This article describes three methods for carrying out nonasymptotic inference on partially identified parameters that are solutions to a class of optimization problems. Applications in which the optimization problems arise include estimation under shape restrictions, estimation of models of discrete games, and estimation based on grouped data. The partially identified parameters are characterized by restrictions that involve the unknown population means of observed random variables in addition to structural parameters. Inference consists of finding confidence intervals for functions of the structural parameters. Our theory provides finite-sample lower bounds on the coverage probabilities of the confidence intervals under three sets of assumptions of increasing strength. With the moderate sample sizes found in most economics applications, the bounds become tighter as the assumptions strengthen. We discuss estimation of population parameters that the bounds depend on and contrast our methods with alternative methods for obtaining confidence intervals for partially identified parameters. The results of Monte Carlo experiments and empirical examples illustrate the usefulness of our method."
            },
            {
                "arxivId": "1710.03830",
                "title": "Inference on Auctions with Weak Assumptions on Information",
                "abstract": "Given a sample of bids from independent auctions, this paper examines the question of inference on auction fundamentals (e.g. valuation distributions, welfare measures) under weak assumptions on information structure. The question is important as it allows us to learn about the valuation distribution in a robust way, i.e., without assuming that a particular information structure holds across observations. We leverage the recent contributions of \\cite{Bergemann2013} in the robust mechanism design literature that exploit the link between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in incomplete information games to construct an econometrics framework for learning about auction fundamentals using observed data on bids. We showcase our construction of identified sets in private value and common value auctions. Our approach for constructing these sets inherits the computational simplicity of solving for correlated equilibria: checking whether a particular valuation distribution belongs to the identified set is as simple as determining whether a {\\it linear} program is feasible. A similar linear program can be used to construct the identified set on various welfare measures and counterfactual objects. For inference and to summarize statistical uncertainty, we propose novel finite sample methods using tail inequalities that are used to construct confidence regions on sets. We also highlight methods based on Bayesian bootstrap and subsampling. A set of Monte Carlo experiments show adequate finite sample properties of our inference procedures. We illustrate our methods using data from OCS auctions."
            },
            {
                "arxivId": "1508.01982",
                "title": "JuMP: A Modeling Language for Mathematical Optimization",
                "abstract": "JuMP is an open-source modeling language that allows users to express a wide range of optimization problems (linear, mixed-integer, quadratic, conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic syntax. JuMP takes advantage of advanced features of the Julia programming language to offer unique functionality while achieving performance on par with commercial modeling tools for standard tasks. In this work we will provide benchmarks, present the novel aspects of the implementation, and discuss how JuMP can be extended to new problem classes and composed with state-of-the-art tools for visualization and interactivity."
            },
            {
                "arxivId": "2102.12249",
                "title": "Set Identification in Models with Multiple Equilibria",
                "abstract": "We propose a computationally feasible way of deriving the identified features of models with multiple equilibria in pure or mixed strategies. It is shown that in the case of Shapley regular normal form games, the identified set is characterized by the inclusion of the true data distribution within the core of a Choquet capacity, which is interpreted as the generalized likelihood of the model. In turn, this inclusion is characterized by a finite set of inequalities and efficient and easily implementable combinatorial methods are described to check them. In all normal form games, the identified set is characterized in terms of the value of a submodular or convex optimization program. Efficient algorithms are then given and compared to check inclusion of a parameter in this identified set. The latter are illustrated with family bargaining games and oligopoly entry games. Copyright 2011, Oxford University Press."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2307.03693",
        "category": "econ",
        "title": "Are There Dragon Kings in the Stock Market?",
        "abstract": "In this study, we undertake a systematic study of historic market volatility spanning roughly five preceding decades. We focus specifically on the time series of the realized volatility (RV) of the S&P500 index and its distribution function. As expected, the largest values of RV coincide with the largest economic upheavals of the period: Savings and Loan Crisis, Tech Bubble, Financial Crisis and Covid Pandemic. We address the question of whether these values belong to one of the three categories: Black Swans (BS), that is, they lie on scale-free, power-law tails of the distribution; Dragon Kings (DK), defined as statistically significant upward deviations from BS; or Negative Dragons Kings (nDK), defined as statistically significant downward deviations from BS. In analyzing the tails of the distribution with RV>40, we observe the appearance of \u201cpotential\u201d DK, which eventually terminate in an abrupt plunge to nDK. This phenomenon becomes more pronounced with the increase in the number of days over which the average RV is calculated\u2014here from daily, n=1, to \u201cmonthly\u201d, n=21. We fit the entire distribution with a modified Generalized Beta (mGB) distribution function, which terminates at a finite value of the variable but exhibits a long power-law stretch prior to that, as well as a Generalized Beta Prime (GB2) distribution function, which has a power-law tail. We also fit the tails directly with a straight line on a log-log scale. In order to ascertain BS, DK or nDK behavior, all fits include their confidence intervals and p-values are evaluated for the data points to check whether they can come from the respective distributions.",
        "references": [
            {
                "arxivId": "1205.1002",
                "title": "Dragon-kings: Mechanisms, statistical methods and empirical evidence",
                "abstract": "This introductory article presents the special Discussion and Debate volume \u201cFrom black swans to dragon-kings, is there life beyond power laws?\u201d We summarize and put in perspective the contributions into three main themes: (i) mechanisms for dragon-kings, (ii) detection of dragon-kings and statistical tests and (iii) empirical evidence in a large variety of natural and social systems. Overall, we are pleased to witness significant advances both in the introduction and clarification of underlying mechanisms and in the development of novel efficient tests that demonstrate clear evidence for the presence of dragon-kings in many systems. However, this positive view should be balanced by the fact that this remains a very delicate and difficult field, if only due to the scarcity of data as well as the extraordinary important implications with respect to hazard assessment, risk control and predictability."
            },
            {
                "arxivId": "1104.5156",
                "title": "Robust statistical tests of Dragon-Kings beyond power law distributions",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0010050",
                "title": "Large Stock Market Price Drawdowns are Outliers",
                "abstract": "Drawdowns (loss from the last local maximum to the next local minimum) are essential aspects of risk assessment in investment management. They offer a more natural measure of real market risks than the variance or other cumulants of daily (or some other fixed time scale) distributions of returns. Here, we extend considerably our previous analysis by analyzing the major financial indices, the major currencies, gold, the twenty largest U.S. companies in terms of capitalisation as well as nine others chosen randomly. We find for the major financial markets that approximately 98% of the distributions of drawdowns is well-represented by an exponential (or a minor modification of it with a slightly fatter tail), while the largest to the few ten largest drawdowns are occurring with a significantly larger rate than predicted by the exponential: the largest drops thus constitute genuine outliers. This is confirmed by extensive testing on surrogate data, which unambiguously show that large stock market drops (and crashes) cannot be accounted for by the distribution of returns characterising the smaller market moves. They thus belong to a different class of their own and call for a specific amplification mechanism. A similar scenario is found for the majority of the company stocks analysed. Drawups (gain from the last local minimum to the next local maximum) exhibit a similar behavior in only about half the markets that we examined. In the spirit of Bacon in Novum Organum about 400 years ago, \"Errors of Nature, Sports and Monsters correct the understanding in regard to ordinary things, and reveal general forms. For whoever knows the ways of Nature will more easily notice her deviations; and, on the other hand, whoever knows her deviations will more accurately describe her ways,\" we propose that outliers reveal fundamental properties of the stock market."
            },
            {
                "arxivId": "cond-mat/9712005",
                "title": "Stock market crashes are outliers",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2309.11058",
        "category": "econ",
        "title": "require: Package dependencies for reproducible research",
        "abstract": "The ability to conduct reproducible research in Stata is often limited by the lack of version control for community-contributed packages. This article introduces the require command, a tool designed to ensure Stata package dependencies are compatible across users and computer systems. Given a list of Stata packages, require verifies that each package is installed, checks for a minimum or exact version or package release date, and optionally installs the package if prompted by the researcher.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2311.09972",
        "category": "econ",
        "title": "Inference in Auctions with Many Bidders Using Transaction Prices",
        "abstract": "This paper considers inference in first-price and second-price sealed-bid auctions in empirical settings where we observe auctions with a large number of bidders. Relevant applications include online auctions, treasury auctions, spectrum auctions, art auctions, and IPO auctions, among others. Given the abundance of bidders in each auction, we propose an asymptotic framework in which the number of bidders diverges while the number of auctions remains fixed. This framework allows us to perform asymptotically exact inference on key model features using only transaction price data. Specifically, we examine inference on the expected utility of the auction winner, the expected revenue of the seller, and the tail properties of the valuation distribution. Simulations confirm the accuracy of our inference methods in finite samples. Finally, we also apply them to Hong Kong car license auction data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2312.14325",
        "category": "econ",
        "title": "Exploring Distributions of House Prices and House Price Indices",
        "abstract": "We use house prices (HP) and house price indices (HPI) as a proxy to income distribution. Specifically, we analyze distribution of sale prices in the 1970\u20132010 window of over 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati metro area of about 2.2 million people. We also analyze distributions of HPI, published by Federal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that cover a period of over 40 years starting in 1980\u2019s. If HP can be viewed as a first derivative of income, HPI can be viewed as its second derivative. We use generalized beta (GB) family of functions to fit distributions of HP and HPI since GB naturally arises from the models of economic exchange described by stochastic differential equations. Our main finding is that HP and multi-year HPI exhibit a negative Dragon King (nDK) behavior, wherein power-law distribution tail gives way to an abrupt decay to a finite upper limit value, which is similar to our recent findings for realized volatility of S&P500 index in the US stock market. This type of tail behavior is best fitted by a modified GB (mGB) distribution. Tails of single-year HPI appear to show more consistency with power-law behavior, which is better described by a GB Prime (GB2) distribution. We supplement full distribution fits by mGB and GB2 with direct linear fits (LF) of the tails. Our numerical procedure relies on evaluation of confidence intervals (CI) of the fits, as well as of p-values that give the likelihood that data come from the fitted distributions.",
        "references": [
            {
                "arxivId": "2307.03693",
                "title": "Are There Dragon Kings in the Stock Market?",
                "abstract": "In this study, we undertake a systematic study of historic market volatility spanning roughly five preceding decades. We focus specifically on the time series of the realized volatility (RV) of the S&P500 index and its distribution function. As expected, the largest values of RV coincide with the largest economic upheavals of the period: Savings and Loan Crisis, Tech Bubble, Financial Crisis and Covid Pandemic. We address the question of whether these values belong to one of the three categories: Black Swans (BS), that is, they lie on scale-free, power-law tails of the distribution; Dragon Kings (DK), defined as statistically significant upward deviations from BS; or Negative Dragons Kings (nDK), defined as statistically significant downward deviations from BS. In analyzing the tails of the distribution with RV>40, we observe the appearance of \u201cpotential\u201d DK, which eventually terminate in an abrupt plunge to nDK. This phenomenon becomes more pronounced with the increase in the number of days over which the average RV is calculated\u2014here from daily, n=1, to \u201cmonthly\u201d, n=21. We fit the entire distribution with a modified Generalized Beta (mGB) distribution function, which terminates at a finite value of the variable but exhibits a long power-law stretch prior to that, as well as a Generalized Beta Prime (GB2) distribution function, which has a power-law tail. We also fit the tails directly with a straight line on a log-log scale. In order to ascertain BS, DK or nDK behavior, all fits include their confidence intervals and p-values are evaluated for the data points to check whether they can come from the respective distributions."
            },
            {
                "arxivId": "1906.04822",
                "title": "From a stochastic model of economic exchange to measures of inequality",
                "abstract": null
            },
            {
                "arxivId": "1208.2696",
                "title": "Distribution Of Wealth In A Network Model Of The Economy",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0002374",
                "title": "Wealth condensation in a simple model of economy",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2403.04131",
        "category": "econ",
        "title": "Extract Mechanisms from Heterogeneous Effects: Identification Strategy for Mediation Analysis",
        "abstract": "Understanding causal mechanisms is essential for explaining and generalizing empirical phenomena. Causal mediation analysis offers statistical techniques to quantify mediation effects. However, existing methods typically require strong identification assumptions or sophisticated research designs. We develop a new identification strategy that simplifies these assumptions, enabling the simultaneous estimation of causal and mediation effects. The strategy is based on a novel decomposition of total treatment effects, which transforms the challenging mediation problem into a simple linear regression problem. The new method establishes a new link between causal mediation and causal moderation. We discuss several research designs and estimators to increase the usability of our identification strategy for a variety of empirical studies. We demonstrate the application of our method by estimating the causal mediation effect in experiments concerning common pool resource governance and voting information. Additionally, we have created statistical software to facilitate the implementation of our method.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.04077",
        "category": "econ",
        "title": "The forgotten pillar of sustainability: development of the S-assessment tool to evaluate Organizational Social Sustainability",
        "abstract": "Pursuing sustainable development has become a global imperative, underscored adopting of the 2030 Agenda for Sustainable Development and its 17 Sustainable Development Goals (SDG). At the heart of this agenda lies the recognition of social sustainability as a pivotal component, emphasizing the need for inclusive societies where every individual can thrive. Despite its significance, social sustainability remains a\"forgotten pillar,\"often overshadowed by environmental concerns. In response, this paper presents the development and validation of the S-Assessment Tool for Social Sustainability, a comprehensive questionnaire designed to evaluate organizations' performance across critical dimensions such as health and wellness, gender equality, decent work, and economic growth, reducing inequalities, and responsible production and consumption. The questionnaire was constructed on the critical dimensions identified through a systematic and narrative hybrid approach to the analysis of peer-reviewed literature. The framework has been structured around the values of the SDGs. It aims to empower organizations to better understand and address their social impact, fostering positive change and contributing to the collective effort towards a more equitable and sustainable future. Through collaborative partnerships and rigorous methodology, this research underscores the importance of integrating social sustainability into organizational practices and decision-making processes, ultimately advancing the broader agenda of sustainable development.",
        "references": [
            {
                "arxivId": "2006.10194",
                "title": "Gender Inequality in Research Productivity During the COVID-19 Pandemic",
                "abstract": "Problem definition: We study the disproportionate impact of the lockdown as a result of the COVID-19 outbreak on female and male academic research productivity in social science. Academic/practical relevance: The lockdown has caused substantial disruptions to academic activities, requiring people to work from home. How this disruption affects productivity and the related gender equity is an important operations and societal question. Methodology: We collect data from the largest open-access preprint repository for social science on 41,858 research preprints in 18 disciplines produced by 76,832 authors across 25 countries over a span of two years. We use a difference-in-differences approach leveraging the exogenous pandemic shock. Results: Our results indicate that, in the 10 weeks after the lockdown in the United States, although total research productivity increased by 35%, female academics\u2019 productivity dropped by 13.2% relative to that of male academics. We also show that this intensified productivity gap is more pronounced for assistant professors and for academics in top-ranked universities and is found in six other countries. Managerial implications: Our work points out the fairness issue in productivity caused by the lockdown, a finding that universities will find helpful when evaluating faculty productivity. It also helps organizations realize the potential unintended consequences that can arise from telecommuting."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.04979",
        "category": "econ",
        "title": "CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference",
        "abstract": "Social science research often hinges on the relationship between categorical variables and outcomes. We introduce CAVIAR, a novel method for embedding categorical variables that assume values in a high-dimensional ambient space but are sampled from an underlying manifold. Our theoretical and numerical analyses outline challenges posed by such categorical variables in causal inference. Specifically, dynamically varying and sparse levels can lead to violations of the Donsker conditions and a failure of the estimation functionals to converge to a tight Gaussian process. Traditional approaches, including the exclusion of rare categorical levels and principled variable selection models like LASSO, fall short. CAVIAR embeds the data into a lower-dimensional global coordinate system. The mapping can be derived from both structured and unstructured data, and ensures stable and robust estimates through dimensionality reduction. In a dataset of direct-to-consumer apparel sales, we illustrate how high-dimensional categorical variables, such as zip codes, can be succinctly represented, facilitating inference and analysis.",
        "references": [
            {
                "arxivId": "1907.01860",
                "title": "Encoding High-Cardinality String Categorical Variables",
                "abstract": "Statistical models usually require vector representations of categorical variables, using for instance one-hot encoding. This strategy breaks down when the number of categories grows, as it creates high-dimensional feature vectors. Additionally, for string entries, one-hot encoding does not capture morphological information in their representation. Here, we seek low-dimensional encoding of high-cardinality string categorical variables. Ideally, these should be: scalable to many categories; interpretable to end users; and facilitate statistical analysis. We introduce two encoding approaches for string categories: a Gamma-Poisson matrix factorizationon substring counts, and a min-hash encoder, for fast approximation of string similarities. We show that min-hash turns set inclusions into inequality relations that are easier to learn. Both approaches are scalable and streamable. Experiments on real and simulated data show that these methods improve supervised learning with high-cardinality categorical variables. We recommend the following: if scalability is central, the min-hash encoder is the best option as it does not require any data fit; if interpretability is important, the Gamma-Poisson factorization is the best alternative, as it can be interpreted as one-hot encoding on inferred categories with informative feature names. Both models enable autoML on string entries as they remove the need for feature engineering or data cleaning."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07331",
        "category": "econ",
        "title": "Financial climate risk: a review of recent advances and key challenges",
        "abstract": "The document provides an overview of financial climate risks. It delves into how climate change impacts the global financial system, distinguishing between physical risks (such as extreme weather events) and transition risks (stemming from policy changes and economic transitions towards low carbon technologies). The paper underlines the complexity of accurately defining financial climate risk, citing the integration of climate science with financial risk analysis as a significant challenge. The paper highlights the pivotal role of microfinance institutions (MFIs) in addressing financial climate risk, especially for populations vulnerable to climate change. The document emphasizes the importance of updating risk management practices within MFIs to explicitly include climate risk assessments and suggests leveraging technology to improve these practices.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07574",
        "category": "econ",
        "title": "International environmental treaties: An honest or a misguided effort",
        "abstract": "Climate change and environmental concerns represent a global crisis accompanied by significant economic challenges. Regular international conferences held to address these issues, such as in the UK (2021) and Egypt (2022), spark debate about the effectiveness and practicality of international commitments. This study examines international treaties from a different perspective, emphasizing the need to understand the power dynamics and stakeholder interests that delay logical actions to mitigate anthropogenic contributions to climate change and their impacts. Environmental and social concerns tend to increase within nations as their economies develop, where they fight to keep acceptable standards of living while reducing emissions volume. So, nations play disproportionate roles in global decision-making based on the size of their economies. Addressing climate change requires a paradigm shift to emphasize acknowledging and adhering to global commitments through civil pressure, rather than relying on traditional yet biased systems of international political diplomacy. Here, climate-friendly actions are evaluated and ideas to promote such activities are proposed. We introduce a\"transition regime\"as a solution to this metastasis challenge which gradually infects all nations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07630",
        "category": "econ",
        "title": "Short, Disclose, and Distort",
        "abstract": "We investigate the voluntary disclosure decision of activist speculators under uncertainty about information endowment (Dye 1985). In our model, a speculator first uncovers initial evidence about the target firm and then seeks additional information to help interpret the initial evidence. The speculator takes a position in the firm's stock, then voluntarily discloses some or all of their findings, and finally closes their position after the disclosure. We present three main findings. First, the speculator will always disclose the initial evidence, even though the market is uncertain about whether the speculator possesses such evidence. Second, the speculator's disclosure strategy of the additional information increases stock price volatility: they disclose extreme news and withhold moderate news. Third, this distortion in disclosure enables the speculator to engage in market manipulation, whereby they take a short (long) position despite having good (bad) news. Keywords: activist speculators, short and distort, voluntary disclosure, complex information, market manipulation JEL Classifications: D82, D83, G14, M41",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07651",
        "category": "econ",
        "title": "Impacto Distributivo Potencial de Reformas na Tributacao Indireta no Brasil: Simulacoes Baseadas na PEC 45/2019",
        "abstract": "This paper analyzes the redistributive impacts of indirect taxation reforms in Brazil inspired by PEC 45/2019, particularly in the version that led to EC 132/2023. Comparisons are made between the current system and the simulated reforms, considering the distribution of the tax burden among families in different income classes, as well as the impact on poverty and inequality indicators. The simulations are conducted based on the combination of effective tax rates that apply to goods and services consumed by households, using nationally representative microdata from family budgets.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07678",
        "category": "econ",
        "title": "On the role of ethics and sustainability in business innovation",
        "abstract": "For organizations to survive and flourish in the long term, innovation and novelty must be continually introduced, which is particularly true in today's rapidly changing world. This raises a variety of ethical and sustainability considerations that seldom receive the attention they deserve. Existing innovation adoption frameworks often focus on technological, organizational, environmental, and social factors impacting adoption. In this chapter, we explore the ethical and sustainability angles, particularly as they relate to emerging technologies, artificial intelligence (AI) being a prominent example. We consider how to facilitate the development and cultivation of innovation cultures in organizations, including budding startups as well as established enterprises, through approaches such as systems thinking.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07684",
        "category": "econ",
        "title": "Merger Analysis with Latent Price",
        "abstract": "Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer/producer surplus, and compensating marginal cost reductions associated with a merger. I also describe assumptions on demand that facilitate the identification of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Staples/Office Depot merger (2016).",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07750",
        "category": "econ",
        "title": "The Broken Rung: Gender and the Leadership Gap",
        "abstract": "Addressing female underrepresentation in leadership positions has become a key policy objective. However, little is known about the extent to which leadership appeals differently to women. Collecting new data from a large firm, I document that women are substantially less likely to apply for early-career promotions. Realized application patterns and large-scale surveys reveal the role of an understudied feature of promotions -- having to assume responsibility over a team -- which is less appealing to women. This gender difference is not accounted for by standard explanations, such as success likelihood or confidence, but is rather a product of common design features of leadership positions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-12.json",
        "arxivId": "2404.07935",
        "category": "econ",
        "title": "Compositional Growth Models",
        "abstract": "We review models of compositional growth, which were introduced to explain the growth statistics of various quantities ranging from firm sizes to GDP. In these models, entities are decomposed into units that grow independently. Thus, the growth rate of the entity is the addition of the growth rates of the composing units, with possibly heterogeneous weights. We review such models and show that they can be understood through a unifying theoretical framework, explaining the resulting growth rate distributions using mixtures of Gaussians.",
        "references": [
            {
                "arxivId": "2404.15226",
                "title": "Revisiting Granular Models of Firm Growth",
                "abstract": "We revisit\"granular models of firm growth\"that have been proposed in the literature to explain the anomalously slow decrease of growth volatility with firms size and how this phenomenon shapes the distribution of their growth rates. In these models, firms' sales are viewed as collections of independent\"sub-units\", and these non-trivial statistical properties occur as a direct result of the fat-tailed distribution of the number or sizes of these sub-units. We present and discuss new theoretical results on the relation between firm size and growth rate statistics. Our results can be understood by noting that granular models imply the existence of three types of firms: well-diversified firms, with a size evenly distributed among several sub-units; firms with many sub-units but with their total size concentrated on only a handful of them, and lastly firms which are poorly diversified simply because they are made up of a small number of sub-units. We establish new empirical facts about growth rates and their relation with size. As predicted by the model, the distribution of growth rate volatilities is to a good approximation {independent of firm size}, once rescaled by the average size-conditioned volatility. However, the tail of this distribution is much too thin to be consistent with a granular mechanism. Moreover, the moments of growth volatility scale with size in a way that is at odds with theoretical predictions. We also find that the distribution of growth rates rescaled by firm-specific volatility, which is predicted to be Gaussian by all the models we consider, remains very fat-tailed in the data, even for large firms. This paper, in ruling out the granularity scenario, suggests that the overarching mechanisms underlying the growth of firms are not satisfactorily understood, and argues that they deserve further theoretical investigations."
            },
            {
                "arxivId": "2302.09906",
                "title": "Revealing production networks from firm growth dynamics",
                "abstract": "We study the correlation structure of firm growth rates. We show that most firms are correlated because of their exposure to a common factor but that firms linked through the supply chain exhibit a stronger correlation on average than firms that are not. Removing this common factor significantly reduces the average correlation between two firms with no relationship in the supply chain while maintaining a significant correlation between two firms that are linked. We then investigate if this observation can be used to reconstruct the topology of a supply chain network using Gaussian Markov Models."
            },
            {
                "arxivId": "1004.5397",
                "title": "The cause of universality in growth fluctuations",
                "abstract": "Phenomena as diverse as breeding bird populations, the size of U.S. firms, money invested in mutual funds, the GDP of individual countries and the scientific output of universities all show unusual but remarkably similar growth fluctuations. The fluctuations display characteristic features, including double exponential scaling in the body of the distribution and power law scaling of the standard deviation as a function of size. To explain this we propose a remarkably simple additive replication model: At each step each individual is replaced by a new number of individuals drawn from the same replication distribution. If the replication distribution is sufficiently heavy tailed then the growth fluctuations are Levy distributed. We analyze the data from bird populations, firms, and mutual funds and show that our predictions match the data well, in several respects: Our theory results in a much better collapse of the individual distributions onto a single curve and also correctly predicts the scaling of the standard deviation with size. To illustrate how this can emerge from a collective microscopic dynamics we propose a model based on stochastic influence dynamics over a scale-free contact network and show that it produces results similar to those observed. We also extend the model to deal with correlations between individual elements. Our main conclusion is that the universality of growth fluctuations is driven by the additivity of growth processes and the action of the generalized central limit theorem."
            },
            {
                "arxivId": "0904.1404",
                "title": "The size variance relationship of business firm growth rates",
                "abstract": "The relationship between the size and the variance of firm growth rates is known to follow an approximate power-law behavior \u03c3(S) \u2248 S\u2212\u03b2(S) where S is the firm size and \u03b2(S) \u2248 0.2 is an exponent that weakly depends on S. Here, we show how a model of proportional growth, which treats firms as classes composed of various numbers of units of variable size, can explain this size-variance dependence. In general, the model predicts that \u03b2(S) must exhibit a crossover from \u03b2(0) = 0 to \u03b2(\u221e) = 1/2. For a realistic set of parameters, \u03b2(S) is approximately constant and can vary from 0.14 to 0.2 depending on the average number of units in the firm. We test the model with a unique industry-specific database in which firm sales are given in terms of the sum of the sales of all their products. We find that the model is consistent with the empirically observed size-variance relationship."
            },
            {
                "arxivId": "cond-mat/0210479",
                "title": "Statistical Models for Company Growth",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "1906.04822",
        "category": "econ",
        "title": "From a stochastic model of economic exchange to measures of inequality",
        "abstract": null,
        "references": [
            {
                "arxivId": "1505.01274",
                "title": "Kinetic models of immediate exchange",
                "abstract": null
            },
            {
                "arxivId": "1305.4173",
                "title": "A model for stock returns and volatility",
                "abstract": null
            },
            {
                "arxivId": "1208.2696",
                "title": "Distribution Of Wealth In A Network Model Of The Economy",
                "abstract": null
            },
            {
                "arxivId": "0912.5448",
                "title": "Universal Behavior of Extreme Price Movements in Stock Markets",
                "abstract": "Many studies assume stock prices follow a random process known as geometric Brownian motion. Although approximately correct, this model fails to explain the frequent occurrence of extreme price movements, such as stock market crashes. Using a large collection of data from three different stock markets, we present evidence that a modification to the random model\u2014adding a slow, but significant, fluctuation to the standard deviation of the process\u2014accurately explains the probability of different-sized price changes, including the relative high frequency of extreme movements. Furthermore, we show that this process is similar across stocks so that their price fluctuations can be characterized by a single curve. Because the behavior of price fluctuations is rooted in the characteristics of volatility, we expect our results to bring increased interest to stochastic volatility models, and especially to those that can produce the properties of volatility reported here."
            },
            {
                "arxivId": "0709.1543",
                "title": "Kinetic exchange models for income and wealth distributions",
                "abstract": null
            },
            {
                "arxivId": "physics/0611245",
                "title": "Basic kinetic wealth-exchange models: common features and open problems",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0203046",
                "title": "Probability distribution of returns in the Heston model with stochastic volatility",
                "abstract": "Abstract We study the Heston model, where the stock price dynamics is governed by a geometrical (multiplicative) Brownian motion with stochastic variance. We solve the corresponding Fokker\u2010Planck equation exactly and, after integrating out the variance, find an analytic formula for the time\u2010dependent probability distribution of stock price changes (returns). The formula is in excellent agreement with the Dow\u2010Jones index for time lags from 1 to 250 trading days. For large returns, the distribution is exponential in log\u2010returns with a time\u2010dependent exponent, whereas for small returns it is Gaussian. For time lags longer than the relaxation time of variance, the probability distribution can be expressed in a scaling form using a Bessel function. The Dow\u2010Jones data for 1982\u20132001 follow the scaling function for seven orders of magnitude."
            },
            {
                "arxivId": "cond-mat/0109493",
                "title": "Volatility driven market in a generalized Lotka\u2013Voltera formalism",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0004256",
                "title": "Statistical mechanics of money: how saving propensity affects its distribution",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0002374",
                "title": "Wealth condensation in a simple model of economy",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "2307.15540",
        "category": "econ",
        "title": "Quantifying the Influence of Climate on Human Mind and Culture: Evidence from Visual Art",
        "abstract": "While connections between climate and the human mind and culture are widely acknowledged, they are not thoroughly quantified. Analyzing 100,000 paintings and data on 2,000 artists from the 13th to 21st centuries, the study reveals that the lightness of the paintings exhibited an interesting U-shaped pattern mirroring global temperature trends. There is a significant association between the two, even after controlling for various factors. Event study analysis using the artist-level data further reveals that high-temperature shocks resulted in brighter paintings in later periods for artists who experienced them compared to the control group. The effect is particularly pronounced in art genres that rely on artists' imaginations, indicating a notable influence on artists' minds. These findings underscore the enduring impact of climate on the human mind and culture throughout history and highlight art as a valuable tool for understanding people's minds and cultures.",
        "references": [
            {
                "arxivId": "1804.05785",
                "title": "Estimating Dynamic Treatment Effects in Event Studies With Heterogeneous Treatment Effects",
                "abstract": "Event studies are frequently used to estimate average treatment effects on the treated (ATT). In estimating the ATT, researchers commonly use fixed effects models that implicitly assume constant treatment effects across cohorts. We show that this is not an innocuous assumption. In fixed effect models where the sole regressor is treatment status, the OLS coefficient is a non-convex average of the heterogeneous cohort-specific ATTs. When regressors containing lags and leads of treatment are added, the OLS coefficient corresponding to a given lead or lag picks up spurious terms consisting of treatment effects from other periods. Therefore, estimates from these commonly used models are not causally interpretable. We propose alternative estimators that identify certain convex averages of the cohort-specific ATTs, hence allowing for causal interpretation even under heterogeneous treatment effects. To illustrate the empirical content of our results, we show that the fixed effects estimators and our proposed estimators differ substantially in an application to the economic consequences of hospitalization."
            },
            {
                "arxivId": "1711.08412",
                "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes",
                "abstract": "Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science."
            },
            {
                "arxivId": "1709.00071",
                "title": "Weather impacts expressed sentiment",
                "abstract": "We conduct the largest ever investigation into the relationship between meteorological conditions and the sentiment of human expressions. To do this, we employ over three and a half billion social media posts from tens of millions of individuals from both Facebook and Twitter between 2009 and 2016. We find that cold temperatures, hot temperatures, precipitation, narrower daily temperature ranges, humidity, and cloud cover are all associated with worsened expressions of sentiment, even when excluding weather-related posts. We compare the magnitude of our estimates with the effect sizes associated with notable historical events occurring within our data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "2404.07222",
        "category": "econ",
        "title": "Liquidity Jump, Liquidity Diffusion, and Treatment on Wash Trading of Crypto Assets",
        "abstract": "We propose that the liquidity of an asset includes two components: liquidity jump and liquidity diffusion. We show that liquidity diffusion has a higher correlation with crypto wash trading than liquidity jump and demonstrate that treatment on wash trading significantly reduces the level of liquidity diffusion, but only marginally reduces that of liquidity jump. We confirm that the autoregressive models are highly effective in modeling the liquidity-adjusted return with and without treatment on wash trading. We argue that treatment on wash trading is unnecessary in modeling established crypto assets that trade in unregulated but mainstream exchanges.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "2404.08288",
        "category": "econ",
        "title": "Istanbul Flower Auction: The Need for Speed",
        "abstract": "We examine the unique format of the Istanbul Flower Auction and compare it to traditional Dutch and English auctions, emphasizing the need to auction large volumes rapidly. In a model with time costs, we study how this auction format, which cleverly combines Dutch and English auction mechanisms, manages time costs by dynamically adapting to initial bidding behaviors. Our numerical analysis considers specific time cost functions and reveals the high performance of the Istanbul Flower Auction in comparison to standard auction formats, in terms of both auctioneer and bidder utilities. This work highlights the critical role of auction design in improving social welfare, particularly in scenarios demanding the quick sale of numerous lots.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "2404.08474",
        "category": "econ",
        "title": "The Squared Kemeny Rule for Averaging Rankings",
        "abstract": "For the problem of aggregating several rankings into one ranking, Kemeny (1959) proposed two methods: the median rule which selects the ranking with the smallest total swap distance to the input rankings, and the mean rule which minimizes the squared swap distances to the input rankings. The median rule has been extensively studied since and is now known simply as Kemeny's rule. It exhibits majoritarian properties, so for example if more than half of the input rankings are the same, then the output of the rule is the same ranking. We observe that this behavior is undesirable in many rank aggregation settings. For example, when we rank objects by different criteria (quality, price, etc.) and want to aggregate them with specified weights for the criteria, then a criterion with weight 51% should have 51% influence on the output instead of 100%. We show that the Squared Kemeny rule (i.e., the mean rule) behaves this way, by establishing a bound on the distance of the output ranking to any input rankings, as a function of their weights. Furthermore, we give an axiomatic characterization of the Squared Kemeny rule, which mirrors the existing characterization of the Kemeny rule but replaces the majoritarian Condorcet axiom by a proportionality axiom. Finally, we discuss the computation of the rule and show its behavior in a simulation study.",
        "references": [
            {
                "arxivId": "2305.09780",
                "title": "Diversity, Agreement, and Polarization in Elections",
                "abstract": "We consider the notions of agreement, diversity, and polarization in ordinal elections (that is, in elections where voters rank the candidates). While (computational) social choice offers good measures of agreement between the voters, such measures for the other two notions are lacking. We attempt to rectify this issue by designing appropriate measures, providing means of their (approximate) computation, and arguing that they, indeed, capture diversity and polarization well. In particular, we present \"maps of preference orders\" that highlight relations between the votes in a given election and which help in making arguments about their nature."
            },
            {
                "arxivId": "2205.09092",
                "title": "Preference Restrictions in Computational Social Choice: A Survey",
                "abstract": "Social choice becomes easier on restricted preference domains such as single-peaked, single-crossing, and Euclidean preferences. Many impossibility theorems disappear, the structure makes it easier to reason about preferences, and computational problems can be solved more efficiently. In this survey, we give a thorough overview of many classic and modern restricted preference domains and explore their properties and applications. We do this from the viewpoint of computational social choice, letting computational problems drive our interest, but we include a comprehensive discussion of the economics and social choice literatures as well. Particular focus areas of our survey include algorithms for recognizing whether preferences belong to a particular preference domain, and algorithms for winner determination of voting rules that are hard to compute if preferences are unrestricted."
            },
            {
                "arxivId": "2204.03589",
                "title": "Collecting, Classifying, Analyzing, and Using Real-World Elections",
                "abstract": "We present a collection of $7582$ real-world elections divided into $25$ datasets from various sources ranging from sports competitions over music charts to survey- and indicator-based rankings. We provide evidence that the collected elections complement already publicly available data from the PrefLib database, which is currently the biggest and most prominent source containing $701$ real-world elections from $36$ datasets. Using the map of elections framework, we divide the datasets into three categories and conduct an analysis of the nature of our elections. To evaluate the practical applicability of previous theoretical research on (parameterized) algorithms and to gain further insights into the collected elections, we analyze different structural properties of our elections including the level of agreement between voters and election's distances from restricted domains such as single-peakedness. Lastly, we use our diverse set of collected elections to shed some further light on several traditional questions from social choice, for instance, on the number of occurrences of the Condorcet paradox and on the consensus among different voting rules."
            },
            {
                "arxivId": "2008.13276",
                "title": "Proportional Participatory Budgeting with Additive Utilities",
                "abstract": "We study voting rules for participatory budgeting, where a group of voters collectively decides which projects should be funded using a common budget. We allow the projects to have arbitrary costs, and the voters to have arbitrary additive valuations over the projects. We formulate an axiom (Extended Justi\ufb01ed Representation, EJR) that guarantees proportional representation to groups of voters with common interests. We propose a simple and attractive voting rule called the Method of Equal Shares that satis\ufb01es this axiom for arbitrary costs and approval utilities, and that satis\ufb01es the axiom up to one project for arbitrary additive valuations. This method can be computed in polynomial time. In contrast, we show that the standard method for achieving proportionality in committee elections, Proportional Approval Voting (PAV), cannot be extended to work with arbitrary costs. Finally, we introduce a strengthened axiom (Full Justi\ufb01ed Representation, FJR) and show that it is also satis\ufb01able, though by a computationally more expensive and less natural voting rule."
            },
            {
                "arxivId": "2007.01795",
                "title": "Multi-Winner Voting with Approval Preferences",
                "abstract": null
            },
            {
                "arxivId": "1709.02850",
                "title": "Mixed Integer Programming with Convex/Concave Constraints: Fixed-Parameter Tractability and Applications to Multicovering and Voting",
                "abstract": null
            },
            {
                "arxivId": "1704.06304",
                "title": "k-Majority Digraphs and the Hardness of Voting with a Constant Number of Voters",
                "abstract": null
            },
            {
                "arxivId": "1704.02453",
                "title": "Consistent Approval-Based Multi-Winner Rules",
                "abstract": "This paper is an axiomatic study of consistent approval-based multi-winner rules, i.e., voting rules that select a fixed-size group of candidates based on approval ballots. We introduce the class of counting rules, provide an axiomatic characterization of this class and, in particular, show that counting rules are consistent. Building upon this result, we axiomatically characterize three important consistent multi-winner rules: Proportional Approval Voting, Multi-Winner Approval Voting and the Approval Chamberlin--Courant rule. Our results demonstrate the variety of multi-winner rules and illustrate three different, orthogonal principles that multi-winner voting rules may represent: individual excellence, diversity, and proportionality."
            },
            {
                "arxivId": "1604.01529",
                "title": "Axiomatic Characterization of Committee Scoring Rules",
                "abstract": null
            },
            {
                "arxivId": "1407.8269",
                "title": "Justified representation in approval-based committee voting",
                "abstract": null
            },
            {
                "arxivId": "1010.2563",
                "title": "Mixing of diffusing particles.",
                "abstract": "We study how the order of N independent random walks in one dimension evolves with time. Our focus is statistical properties of the inversion number m, defined as the number of pairs that are out of sort with respect to the initial configuration. In the steady state, the distribution of the inversion number is Gaussian with the average \u2243 N\u00b2/4 and the standard deviation \u03c3 \u2243 N\u00b3/\u00b2/6. The survival probability, Sm(t), which measures the likelihood that the inversion number remains below m until time t, decays algebraically in the long-time limit, Sm \u223c t(-\u03b2m). Interestingly, there is a spectrum of N(N-1)/2 distinct exponents \u03b2m(N). We also find that the kinetics of first passage in a circular cone provides a good approximation for these exponents. When N is large, the first-passage exponents are a universal function of a single scaling variable, \u03b2m(N) \u2192 \u03b2(z), with z = (m-)/\u03c3. In the cone approximation, the scaling function is a root of a transcendental equation involving the parabolic cylinder equation, D2\u03b2(-z)=0, and surprisingly, numerical simulations show this prediction to be exact."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "2404.08492",
        "category": "econ",
        "title": "Strategic Interactions between Large Language Models-based Agents in Beauty Contests",
        "abstract": "The growing adoption of large language models (LLMs) presents substantial potential for deeper understanding of human behaviours within game theory frameworks through simulations. Leveraging on the diverse pool of LLM types and addressing the gap in research on competitive games, this paper examines the strategic interactions among multiple types of LLM-based agents in a classical game of beauty contest. Drawing parallels to experiments involving human subjects, LLM-based agents are assessed similarly in terms of strategic levels. They demonstrate varying depth of reasoning that falls within a range of level-0 and 1, and show convergence in actions in repeated settings. Furthermore, I also explore how variations in group composition of agent types influence strategic behaviours, where I found higher proportion of fixed-strategy opponents enhances convergence for LLM-based agents, and having a mixed environment with agents of differing relative strategic levels accelerates convergence for all agents. There could also be higher average payoffs for the more intelligent agents, albeit at the expense of the less intelligent agents. These results not only provide insights into outcomes for simulated agents under specified scenarios, it also offer valuable implications for understanding strategic interactions between algorithms.",
        "references": [
            {
                "arxivId": "2401.01735",
                "title": "Economics Arena for Large Language Models",
                "abstract": "Large language models (LLMs) have been extensively used as the backbones for general-purpose agents, and some economics literature suggest that LLMs are capable of playing various types of economics games. Following these works, to overcome the limitation of evaluating LLMs using static benchmarks, we propose to explore competitive games as an evaluation for LLMs to incorporate multi-players and dynamicise the environment. By varying the game history revealed to LLMs-based players, we find that most of LLMs are rational in that they play strategies that can increase their payoffs, but not as rational as indicated by Nash Equilibria (NEs). Moreover, when game history are available, certain types of LLMs, such as GPT-4, can converge faster to the NE strategies, which suggests higher rationality level in comparison to other models. In the meantime, certain types of LLMs can win more often when game history are available, and we argue that the winning rate reflects the reasoning ability with respect to the strategies of other players. Throughout all our experiments, we observe that the ability to strictly follow the game rules described by natural languages also vary among the LLMs we tested. In this work, we provide an economics arena for the LLMs research community as a dynamic simulation to test the above-mentioned abilities of LLMs, i.e. rationality, strategic reasoning ability, and instruction-following capability."
            },
            {
                "arxivId": "2311.05232",
                "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-15.json",
        "arxivId": "2404.08620",
        "category": "econ",
        "title": "Natural disasters, personal attributes, and social entrepreneurship: an attention-based view",
        "abstract": null,
        "references": [
            {
                "arxivId": "2207.12492",
                "title": "Natural disasters, entrepreneurship activity, and the moderating role of country governance",
                "abstract": null
            },
            {
                "arxivId": "2104.12008",
                "title": "Weathering the Storm: How Foreign Aid and Institutions Affect Entrepreneurship Activity Following Natural Disasters",
                "abstract": "This study examines how foreign aid and institutions affect entrepreneurship activity following natural disasters. We use insights from the entrepreneurship, development, and institutions literature to develop a model of entrepreneurship activity in the aftermath of natural disasters. First, we hypothesize the effect of natural disasters on entrepreneurship activity depends on the amount of foreign aid received. Second, we hypothesize that natural disasters and foreign aid either encourages or discourages entrepreneurship activity depending on two important institutional conditions: the quality of government and economic freedom. The findings from our panel of 85 countries from 2006 to 2016 indicate that natural disasters are negatively associated with entrepreneurship activity, but both foreign aid and economic freedom attenuate this effect. In addition, we observe that foreign aid is positively associated with entrepreneurship activity but only in countries with high quality government. Hence, we conclude that the effect of natural disasters on entrepreneurship depends crucially on the quality of government, economic freedom, and foreign aid. Our findings provide new insights into how natural disasters and foreign aid affect entrepreneurship and highlight the important role of the institutional context."
            },
            {
                "arxivId": "1903.02934",
                "title": "Entrepreneurship, Institutions, and Economic Growth: Does the Level of Development Matter?",
                "abstract": "This study questions the assumption that entrepreneurship unequivocally leads to economic growth. Using insights from institutional theory and development economics, we reevaluate entrepreneurship\u2019s contribution towards economic growth. Our study uses Global Entrepreneurship Monitor (GEM) data for a panel of 83 countries from 2002 to 2014 and highlights several important findings. First, our evidence suggests that entrepreneurship encourages economic growth but not in developing countries. Second, we find that a country\u2019s institutional environment\u2014measured by GEM\u2019s Entrepreneurial Framework Conditions (EFCs)\u2014contributes to economic growth in more developed countries but not in developing countries. Lastly, we find that opportunity-motivated entrepreneurship encourages economic growth in developed countries, while necessity-motivated entrepreneurship discourages economic growth in developing countries. These findings have important policy implications. Namely, our evidence contradicts policy proposals that suggest entrepreneurship and the adoption of pro-market institutions that support it will encourage economic growth in developing countries. Our evidence suggests these policy proposals are unlikely to generate the desired economic growth."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2206.08503",
        "category": "econ",
        "title": "Semiparametric Single-Index Estimation for Average Treatment Effects",
        "abstract": "We propose a semiparametric method to estimate the average treatment effect under the assumption of unconfoundedness given observational data. Our estimation method alleviates misspecification issues of the propensity score function by estimating the single-index link function involved through Hermite polynomials. Our approach is computationally tractable and allows for moderately large dimension covariates. We provide the large sample properties of the estimator and show its validity. Also, the average treatment effect estimator achieves the parametric rate and asymptotic normality. Our extensive Monte Carlo study shows that the proposed estimator is valid in finite samples. We also provide an empirical analysis on the effect of maternal smoking on babies' birth weight and the effect of job training program on future earnings.",
        "references": [
            {
                "arxivId": "2110.06136",
                "title": "A Response to Philippe Lemoine's Critique on our Paper\"Causal Impact of Masks, Policies, Behavior on Early Covid-19 Pandemic in the U.S.\"",
                "abstract": "Recently, Phillippe Lemoine posted a critique of our paper\"Causal Impact of Masks, Policies, Behavior on Early Covid-19 Pandemic in the U.S.\"[arXiv:2005.14168] at his post titled\"Lockdowns, econometrics and the art of putting lipstick on a pig.\"Although Lemoine's critique appears ideologically driven and overly emotional, some of his points are worth addressing. In particular, the sensitivity of our estimation results for (i) including\"masks in public spaces\"and (ii) updating the data seems important critiques and, therefore, we decided to analyze the updated data ourselves. This note summarizes our findings from re-examining the updated data and responds to Phillippe Lemoine's critique on these two important points. We also briefly discuss other points Lemoine raised in his post. After analyzing the updated data, we find evidence that reinforces the conclusions reached in the original study."
            },
            {
                "arxivId": "1810.01370",
                "title": "Covariate Distribution Balance via Propensity Scores",
                "abstract": "This paper proposes new estimators for the propensity score that aim to maximize the covariate distribution balance among different treatment groups. Heuristically, our proposed procedure attempts to estimate a propensity score model by making the underlying covariate distribution of different treatment groups as close to each other as possible. Our estimators are data-driven, do not rely on tuning parameters such as bandwidths, admit an asymptotic linear representation, and can be used to estimate different treatment effect parameters under different identifying assumptions, including unconfoundedness and local treatment effects. We derive the asymptotic properties of inverse probability weighted estimators for the average, distributional, and quantile treatment effects based on the proposed propensity score estimator and illustrate their finite sample performance via Monte Carlo simulations and two empirical applications."
            },
            {
                "arxivId": "1601.06003",
                "title": "Estimation for single-index and partially linear single-index integrated models",
                "abstract": "Estimation mainly for two classes of popular models, single-index and partially linear single-index models, is studied in this paper. Such models feature nonstationarity. Orthogonal series expansion is used to approximate the unknown integrable link functions in the models and a profile approach is used to derive the estimators. The findings include the dual rate of convergence of the estimators for the single-index models and a trio of convergence rates for the partially linear single-index models. A new central limit theorem is established for a plug-in estimator of the unknown link function. Meanwhile, a considerable extension to a class of partially nonlinear single-index models is discussed in Section 4. Monte Carlo simulation verifies these theoretical results. An empirical study furnishes an application of the proposed estimation procedures in practice."
            },
            {
                "arxivId": "1404.1785",
                "title": "Balancing Covariates via Propensity Score Weighting",
                "abstract": "ABSTRACT Covariate balance is crucial for unconfounded descriptive or causal comparisons. However, lack of balance is common in observational studies. This article considers weighting strategies for balancing covariates. We define a general class of weights\u2014the balancing weights\u2014that balance the weighted distributions of the covariates between treatment groups. These weights incorporate the propensity score to weight each group to an analyst-selected target population. This class unifies existing weighting methods, including commonly used weights such as inverse-probability weights as special cases. General large-sample results on nonparametric estimation based on these weights are derived. We further propose a new weighting scheme, the overlap weights, in which each unit\u2019s weight is proportional to the probability of that unit being assigned to the opposite group. The overlap weights are bounded, and minimize the asymptotic variance of the weighted average treatment effect among the class of balancing weights. The overlap weights also possess a desirable small-sample exact balance property, based on which we propose a new method that achieves exact balance for means of any selected set of covariates. Two applications illustrate these methods and compare them with other approaches."
            },
            {
                "arxivId": "1309.4686",
                "title": "Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations",
                "abstract": null
            },
            {
                "arxivId": "1308.5732",
                "title": "High dimensional generalized empirical likelihood for moment restrictions with dependent data",
                "abstract": null
            },
            {
                "arxivId": "1304.0593",
                "title": "EFFICIENT ESTIMATION IN SUFFICIENT DIMENSION REDUCTION.",
                "abstract": "We develop an efficient estimation procedure for identifying and estimating the central subspace. Using a new way of parameterization, we convert the problem of identifying the central subspace to the problem of estimating a finite dimensional parameter in a semiparametric model. This conversion allows us to derive an efficient estimator which reaches the optimal semiparametric efficiency bound. The resulting efficient estimator can exhaustively estimate the central subspace without imposing any distributional assumptions. Our proposed efficient estimation also provides a possibility for making inference of parameters that uniquely identify the central subspace. We conduct simulation studies and a real data analysis to demonstrate the finite sample performance in comparison with several existing methods."
            },
            {
                "arxivId": "1212.0442",
                "title": "Some new asymptotic theory for least squares series: Pointwise and uniform results",
                "abstract": null
            },
            {
                "arxivId": "0804.2958",
                "title": "Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data",
                "abstract": "When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2209.09810",
        "category": "econ",
        "title": "The Boosted HP Filter Is More General Than You Might Think",
        "abstract": "The global financial crisis and Covid recession have renewed discussion concerning trend-cycle discovery in macroeconomic data, and boosting has recently upgraded the popular HP filter to a modern machine learning device suited to data-rich and rapid computational environments. This paper extends boosting's trend determination capability to higher order integrated processes and time series with roots that are local to unity. The theory is established by understanding the asymptotic effect of boosting on a simple exponential function. Given a universe of time series in FRED databases that exhibit various dynamic patterns, boosting timely captures downturns at crises and recoveries that follow.",
        "references": [
            {
                "arxivId": "2005.14057",
                "title": "Machine Learning Time Series Regressions With an Application to Nowcasting",
                "abstract": "Abstract This article introduces structured machine learning regressions for high-dimensional time series data potentially sampled at different frequencies. The sparse-group LASSO estimator can take advantage of such time series data structures and outperforms the unstructured LASSO. We establish oracle inequalities for the sparse-group LASSO estimator within a framework that allows for the mixing processes and recognizes that the financial and the macroeconomic data may have heavier than exponential tails. An empirical application to nowcasting US GDP growth indicates that the estimator performs favorably compared to other alternatives and that text data can be a useful addition to more traditional numerical data. Our methodology is implemented in the R package midasml, available from CRAN."
            },
            {
                "arxivId": "1810.10987",
                "title": "Nuclear norm regularized estimation of panel regression models",
                "abstract": "In this paper we investigate panel regression models with interactive fixed effects. We propose two new estimation methods that are based on minimizing convex objective functions. The fi rst method minimizes the sum of squared residuals with a nuclear (trace) norm regularization. The second method minimizes the nuclear norm of the residuals. We establish the consistency of the two resulting estimators. Those estimators have a very important computational advantage compared to the existing least squares (LS) estimator, in that they are de fined as minimizers of a convex objective function. In addition, the nuclear norm penalization helps to resolve a potential identifi cation problem for interactive fixed effect models, in particular when the regressors are low-rank and the number of the factors is unknown. We also show how to construct estimators that are asymptotically equivalent to the least squares (LS) estimator in Bai (2009) and Moon and Weidner (2017) by using our nuclear norm regularized or minimized estimators as initial values for a nite number of LS minimizing iteration steps. This iteration avoids any non-convex minimization, while the original LS estimation problem is generally non-convex, and can have multiple local minima."
            },
            {
                "arxivId": "1809.09953",
                "title": "Deep Neural Networks for Estimation and Inference: Application to Causal Effects and Other Semiparametric Estimands",
                "abstract": "We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second\u2010step inference after first\u2010step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now\u2010common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed\u2010width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression\u2010type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing."
            },
            {
                "arxivId": "1710.10251",
                "title": "Matrix Completion Methods for Causal Panel Data Models",
                "abstract": "Abstract In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \u201cmissing\u201d elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2212.07384",
        "category": "econ",
        "title": "Valuing Pharmaceutical Drug Innovations",
        "abstract": "We propose a methodology to estimate the market value of pharmaceutical drugs. Our approach combines an event study with a model of discounted cash flows and uses stock market responses to drug development announcements to infer the values. We estimate that, on average, a successful drug is valued at \\$1.62 billion, and its value at the discovery stage is \\$64.3 million, with substantial heterogeneity across major diseases. Leveraging these estimates, we also determine the average drug development costs at various stages. Furthermore, we explore applying our estimates to design policies that support drug development through drug buyouts and cost-sharing agreements.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2212.14475",
        "category": "econ",
        "title": "Innovation through intra and inter-regional interaction in economic geography",
        "abstract": "We develop a two-region economic geography model with vertical innovations that improve the quality of manufactured varieties produced in each region. The chance of innovation depends on the \\emph{related variety}, i.e. the importance of interaction between researchers within the same region rather than across different regions. As economic integration increases from a low level, a higher related variety is associated with more agglomerated spatial configurations. However, if the interaction with foreign scientists is relatively more important for innovation, economic activities may (completely) re-disperse after an initial phase of agglomeration due to the increase in the relative importance of a higher chance of innovation in the less industrialized region. This non-monotonic relationship between economic integration and spatial imbalances may exhibit very diverse qualitative properties, not yet described in the literature.",
        "references": [
            {
                "arxivId": "2001.05095",
                "title": "Production externalities and dispersion process in a multi-region economy",
                "abstract": "We consider an economic geography model with two inter-regional proximity structures: one governing goods trade and the other governing production externalities across regions. We investigate how the introduction of the latter affects the timing of endogenous agglomeration and the spatial distribution of workers across regions. As transportation costs decline, the economy undergoes a progressive dispersion process. Monocentric agglomeration emerges when inter-regional trade and/or production externalities incur high transportation costs, while uniform dispersion occurs when these costs become negligibly small (i.e., when distance dies). In a multi-regional geography, the network structure of production externalities can determine the geographical distribution of workers as economic integration increases. If production externalities are governed solely by geographical distance, a mono-centric spatial distribution emerges in the form of suburbanization. However, if geographically distant pairs of regions are connected through tight production linkages, multi-centric spatial distribution can be sustainable."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2302.03135",
        "category": "econ",
        "title": "Monotone Function Intervals: Theory and Applications",
        "abstract": "A monotone function interval is the set of monotone functions that lie pointwise between two fixed monotone functions. We characterize the set of extreme points of monotone function intervals and apply this to a number of economic settings. First, we leverage the main result to characterize the set of distributions of posterior quantiles that can be induced by a signal, with applications to political economy, Bayesian persuasion, and the psychology of judgment. Second, we combine our characterization with properties of convex optimization problems to unify and generalize seminal results in the literature on security design under adverse selection and moral hazard.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2309.03419",
        "category": "econ",
        "title": "Motives for Delegating Financial Decisions",
        "abstract": "Why do some investors delegate financial decisions to supposed experts? We report a laboratory experiment designed to disentangle four possible motives. Almost 600 investors drawn from the Prolific subject pool choose whether or not to delegate a real-stakes choice among lotteries to a previous investor (an ``expert'') after seeing information on the performance of several available experts. We find that a surprisingly large fraction of investors delegate even trivial choice tasks, suggesting a major role for the blame shifting motive. A larger fraction of investors delegate our more complex tasks, suggesting that decision costs play a role for some investors. Some investors who delegate choose a low quality expert with high earnings, suggesting a role for chasing past performance. We find no evidence for a fourth possible motive, that delegation makes risk more acceptable.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2309.09176",
        "category": "econ",
        "title": "Odd period cycles and ergodic properties in price dynamics for an exchange economy",
        "abstract": "In the first part of this paper (Sections 1-4), we study a standard exchange economy model with Cobb-Douglas type consumers and give a necessary and sufficient condition for the existence of an odd period cycle in the Walras-Samuelson (tatonnement) price adjustment process. We also give a sufficient condition for a price to be eventually attracted to a chaotic region. In the second part (Sections 5 and 6), we investigate ergodic properties of the price dynamics showing that the existence of chaos is not necessarily bad. (The future is still predictable on average.) Moreover, supported by a celebrated work of Avila et al. (Invent. Math., 2003), we conduct a sensitivity analysis to investigate a relationship between the ergodic sum (of prices) and the speed of price adjustment. We believe that our methods in this paper can be used to analyse many other chaotic economic models.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2312.06107",
        "category": "econ",
        "title": "Simple Proofs of the Variational and Multiple Priors Representations",
        "abstract": "This note gives simpler proofs of the variational and multiple priors representations in Maccheroni et al. (2006) and Gilboa and Schmeidler (1989).",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2312.15563",
        "category": "econ",
        "title": "Dynamics of Global Emission Permit Prices and Regional Social Cost of Carbon under Noncooperation",
        "abstract": "We build a dynamic multi-region model of climate and economy with emission permit trading among 12 aggregated regions in the world. We solve for the dynamic Nash equilibrium under noncooperation, wherein each region adheres to the emission cap constraints following commitments that were first outlined in the 2015 Paris Agreement and updated in subsequent years. Our model shows that the emission permit price reaches $811 per ton of carbon by 2050. We demonstrate that a regional carbon tax is complementary to the global cap-and-trade system, and the optimal regional carbon tax is equal to the difference between the regional marginal abatement cost and the permit price.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2401.12084",
        "category": "econ",
        "title": "Temporal Aggregation for the Synthetic Control Method",
        "abstract": "The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit with panel data. Two challenges arise with higher frequency data (e.g., monthly versus yearly): (1) achieving excellent pre-treatment fit is typically more challenging; and (2) overfitting to noise is more likely. Aggregating data over time can mitigate these problems but can also destroy important signal. In this paper, we bound the bias for SCM with disaggregated and aggregated outcomes and give conditions under which aggregating tightens the bounds. We then propose finding weights that balance both disaggregated and aggregated series.",
        "references": [
            {
                "arxivId": "2311.16260",
                "title": "Using multiple outcomes to improve the synthetic control method",
                "abstract": "When there are multiple outcome series of interest, Synthetic Control analyses typically proceed by estimating separate weights for each outcome. In this paper, we instead propose estimating a common set of weights across outcomes, by balancing either a vector of all outcomes or an index or average of them. Under a low-rank factor model, we show that these approaches lead to lower bias bounds than separate weights, and that averaging leads to further gains when the number of outcomes grows. We illustrate this via simulation and in a re-analysis of the impact of the Flint water crisis on educational outcomes."
            },
            {
                "arxivId": "1811.04170",
                "title": "The Augmented Synthetic Control Method",
                "abstract": "Abstract The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The \u201csynthetic control\u201d is a weighted average of control units that balances the treated unit\u2019s pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package."
            },
            {
                "arxivId": "1711.06940",
                "title": "Robust Synthetic Control",
                "abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. To begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. We provide the first finite sample analysis for a broader class of models, the Latent Variable Model, in contrast to Factor Models previously considered in the literature. Further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same setting, we prove that the mean-squared-error (MSE) in our prediction estimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the noise variance. Using a data aggregation method, we show that the MSE can be made as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to a consistent estimator. We also introduce a Bayesian framework to quantify the model uncertainty through posterior probabilities. Our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method."
            },
            {
                "arxivId": "1911.08521",
                "title": "Synthetic controls with imperfect pretreatment fit",
                "abstract": "We analyze the properties of the Synthetic Control (SC) and related estimators when the pre\u2010treatment fit is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre\u2010treatment periods goes to infinity. Still, we show that a demeaned version of the SC method can improve in terms of bias and variance relative to the difference\u2010in\u2010difference estimator. We also derive a specification test for the demeaned SC estimator in this setting with imperfect pre\u2010treatment fit. Given our theoretical results, we provide practical guidance for applied researchers on how to justify the use of such estimators in empirical applications."
            },
            {
                "arxivId": "1610.07748",
                "title": "Balancing, Regression, Difference-in-Differences and Synthetic Control Methods: A Synthesis",
                "abstract": "In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2401.17929",
        "category": "econ",
        "title": "Technological Shocks and Algorithmic Decision Aids in Credence Goods Markets",
        "abstract": "In credence goods markets such as health care or repair services, consumers rely on experts with superior information to adequately diagnose and treat them. Experts, however, are constrained in their diagnostic abilities, which hurts market efficiency and consumer welfare. Technological breakthroughs that substitute or complement expert judgments have the potential to alleviate consumer mistreatment. This article studies how competitive experts adopt novel diagnostic technologies when skills are heterogeneously distributed and obfuscated to consumers. We differentiate between novel technologies that increase expert abilities, and algorithmic decision aids that complement expert judgments, but do not affect an expert's personal diagnostic precision. We show that high-ability experts may be incentivized to forego the decision aid in order to escape a pooling equilibrium by differentiating themselves from low-ability experts. Results from an online experiment support our hypothesis, showing that high-ability experts are significantly less likely than low-ability experts to invest into an algorithmic decision aid. Furthermore, we document pervasive under-investments, and no effect on expert honesty.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2402.11652",
        "category": "econ",
        "title": "Doubly Robust Inference in Causal Latent Factor Models",
        "abstract": "This article introduces a new estimator of average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.",
        "references": [
            {
                "arxivId": "2211.14297",
                "title": "Doubly robust nearest neighbors in factor models",
                "abstract": "We introduce and analyze an improved variant of nearest neighbors (NN) for estimation with missing data in latent factor models. We consider a matrix completion problem with missing data, where the $(i, t)$-th entry, when observed, is given by its mean $f(u_i, v_t)$ plus mean-zero noise for an unknown function $f$ and latent factors $u_i$ and $v_t$. Prior NN strategies, like unit-unit NN, for estimating the mean $f(u_i, v_t)$ relies on existence of other rows $j$ with $u_j \\approx u_i$. Similarly, time-time NN strategy relies on existence of columns $t'$ with $v_{t'} \\approx v_t$. These strategies provide poor performance respectively when similar rows or similar columns are not available. Our estimate is doubly robust to this deficit in two ways: (1) As long as there exist either good row or good column neighbors, our estimate provides a consistent estimate. (2) Furthermore, if both good row and good column neighbors exist, it provides a (near-)quadratic improvement in the non-asymptotic error and admits a significantly narrower asymptotic confidence interval when compared to both unit-unit or time-time NN."
            },
            {
                "arxivId": "2202.06891",
                "title": "Counterfactual inference for sequential experiments",
                "abstract": "We consider after-study statistical inference for sequentially designed experiments wherein multiple units are assigned treatments for multiple time points using treatment policies that adapt over time. Our goal is to provide inference guarantees for the counterfactual mean at the smallest possible scale -- mean outcome under different treatments for each unit and each time -- with minimal assumptions on the adaptive treatment policy. Without any structural assumptions on the counterfactual means, this challenging task is infeasible due to more unknowns than observed data points. To make progress, we introduce a latent factor model over the counterfactual means that serves as a non-parametric generalization of the non-linear mixed effects model and the bilinear latent factor model considered in prior works. For estimation, we use a non-parametric method, namely a variant of nearest neighbors, and establish a non-asymptotic high probability error bound for the counterfactual mean for each unit and each time. Under regularity conditions, this bound leads to asymptotically valid confidence intervals for the counterfactual mean as the number of units and time points grows to $\\infty$ together at suitable rates. We illustrate our theory via several simulations and a case study involving data from a mobile health clinical trial HeartSteps."
            },
            {
                "arxivId": "2109.15154",
                "title": "Causal Matrix Completion",
                "abstract": "Matrix completion is the study of recovering an underlying matrix from a sparse subset of noisy observations. Traditionally, it is assumed that the entries of the matrix are\"missing completely at random\"(MCAR), i.e., each entry is revealed at random, independent of everything else, with uniform probability. This is likely unrealistic due to the presence of\"latent confounders\", i.e., unobserved factors that determine both the entries of the underlying matrix and the missingness pattern in the observed matrix. For example, in the context of movie recommender systems -- a canonical application for matrix completion -- a user who vehemently dislikes horror films is unlikely to ever watch horror films. In general, these confounders yield\"missing not at random\"(MNAR) data, which can severely impact any inference procedure that does not correct for this bias. We develop a formal causal model for matrix completion through the language of potential outcomes, and provide novel identification arguments for a variety of causal estimands of interest. We design a procedure, which we call\"synthetic nearest neighbors\"(SNN), to estimate these causal estimands. We prove finite-sample consistency and asymptotic normality of our estimator. Our analysis also leads to new theoretical results for the matrix completion literature. In particular, we establish entry-wise, i.e., max-norm, finite-sample consistency and asymptotic normality results for matrix completion with MNAR data. As a special case, this also provides entry-wise bounds for matrix completion with MCAR data. Across simulated and real data, we demonstrate the efficacy of our proposed estimator."
            },
            {
                "arxivId": "2106.02290",
                "title": "Matrix Completion With Data-Dependent Missingness Probabilities",
                "abstract": "The problem of completing a large matrix with lots of missing entries has received widespread attention in the last couple of decades. Two popular approaches to the matrix completion problem are based on singular value thresholding and nuclear norm minimization. Most of the past works on this subject assume that there is a single number <inline-formula> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula> such that each entry of the matrix is available independently with probability <inline-formula> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula> and missing otherwise. This assumption may not be realistic for many applications. In this work, we replace it with the assumption that the probability that an entry is available is an unknown function <inline-formula> <tex-math notation=\"LaTeX\">$f$ </tex-math></inline-formula> of the entry itself. For example, if the entry is the rating given to a movie by a viewer, then it seems plausible that high value entries have greater probability of being available than low value entries. We propose two new estimators, based on singular value thresholding and nuclear norm minimization, to recover the matrix under this assumption. The estimators involve no tuning parameters, and are shown to be consistent under a low rank assumption. We also provide a consistent estimator of the unknown function <inline-formula> <tex-math notation=\"LaTeX\">$f$ </tex-math></inline-formula>."
            },
            {
                "arxivId": "1910.12774",
                "title": "Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption",
                "abstract": "Matrix completion is often applied to data with entries missing not at random (MNAR). For example, consider a recommendation system where users tend to only reveal ratings for items they like. In this case, a matrix completion method that relies on entries being revealed at uniformly sampled row and column indices can yield overly optimistic predictions of unseen user ratings. Recently, various papers have shown that we can reduce this bias in MNAR matrix completion if we know the probabilities of different matrix entries being missing. These probabilities are typically modeled using logistic regression or naive Bayes, which make strong assumptions and lack guarantees on the accuracy of the estimated probabilities. In this paper, we suggest a simple approach to estimating these probabilities that avoids these shortcomings. Our approach follows from the observation that missingness patterns in real data often exhibit low nuclear norm structure. We can then estimate the missingness probabilities by feeding the (always fully-observed) binary matrix specifying which entries are revealed to an existing nuclear-norm-constrained matrix completion algorithm by Davenport et al. [2014]. Thus, we tackle MNAR matrix completion by solving a different matrix completion problem first that recovers missingness probabilities. We establish finite-sample error bounds for how accurate these probability estimates are and how well these estimates debias standard matrix completion losses for the original matrix to be completed. Our experiments show that the proposed debiasing strategy can improve a variety of existing matrix completion algorithms, and achieves downstream matrix completion accuracy at least as good as logistic regression and naive Bayes debiasing baselines that require additional auxiliary information."
            },
            {
                "arxivId": "1910.06677",
                "title": "Matrix Completion, Counterfactuals, and Factor Analysis of Missing Data",
                "abstract": "Abstract This article proposes an imputation procedure that uses the factors estimated from a tall block along with the re-rotated loadings estimated from a wide block to impute missing values in a panel of data. Assuming that a strong factor structure holds for the full panel of data and its sub-blocks, it is shown that the common component can be consistently estimated at four different rates of convergence without requiring regularization or iteration. An asymptotic analysis of the estimation error is obtained. An application of our analysis is estimation of counterfactuals when potential outcomes have a factor structure. We study the estimation of average and individual treatment effects on the treated and establish a normal distribution theory that can be useful for hypothesis testing."
            },
            {
                "arxivId": "1907.11705",
                "title": "Low-Rank Matrix Completion: A Contemporary Survey",
                "abstract": "As a paradigm to recover unknown entries of a matrix from partial observations, low-rank matrix completion (LRMC) has generated a great deal of interest. Over the years, there have been lots of works on this topic, but it might not be easy to grasp the essential knowledge from these studies. This is mainly because many of these works are highly theoretical or a proposal of new LRMC technique. In this paper, we give a contemporary survey on LRMC. In order to provide a better view, insight, and understanding of potentials and limitations of the LRMC, we present early scattered results in a structured and accessible way. Specifically, we classify the state-of-the-art LRMC techniques into two main categories and then explain each category in detail. We next discuss the issues to be considered when one considers using the LRMC techniques. These include intrinsic properties required for the matrix recovery and how to exploit a special structure in the LRMC design. We also discuss the convolutional neural network (CNN)-based LRMC algorithms exploiting the graph structure of a low-rank matrix. Furthermore, we present the recovery performance and the computational complexity of state-of-the-art LRMC techniques. Our hope is that this paper will serve as a useful guide for practitioners and non-experts to catch the gist of the LRMC."
            },
            {
                "arxivId": "1902.10920",
                "title": "On Robustness of Principal Component Regression",
                "abstract": "Abstract Principal component regression (PCR) is a simple, but powerful and ubiquitously utilized method. Its effectiveness is well established when the covariates exhibit low-rank structure. However, its ability to handle settings with noisy, missing, and mixed-valued, that is, discrete and continuous, covariates is not understood and remains an important open challenge. As the main contribution of this work, we establish the robustness of PCR, without any change, in this respect and provide meaningful finite-sample analysis. To do so, we establish that PCR is equivalent to performing linear regression after preprocessing the covariate matrix via hard singular value thresholding (HSVT). As a result, in the context of counterfactual analysis using observational data, we show PCR is equivalent to the recently proposed robust variant of the synthetic control method, known as robust synthetic control (RSC). As an immediate consequence, we obtain finite-sample analysis of the RSC estimator that was previously absent. As an important contribution to the synthetic controls literature, we establish that an (approximate) linear synthetic control exists in the setting of a generalized factor model, or latent variable model; traditionally in the literature, the existence of a synthetic control needs to be assumed to exist as an axiom. We further discuss a surprising implication of the robustness property of PCR with respect to noise, that is, PCR can learn a good predictive model even if the covariates are tactfully transformed to preserve differential privacy. Finally, this work advances the state-of-the-art analysis for HSVT by establishing stronger guarantees with respect to the -norm rather than the Frobenius norm as is commonly done in the matrix estimation literature, which may be of interest in its own right."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1705.04867",
                "title": "Nearest Neighbors for Matrix Estimation Interpreted as Blind Regression for Latent Variable Model",
                "abstract": "We consider the setup of nonparametric <italic>blind regression</italic> for estimating the entries of a large <inline-formula> <tex-math notation=\"LaTeX\">${m} \\times {n}$ </tex-math></inline-formula> matrix, when provided with a small, random fraction of noisy measurements. We assume that all rows <inline-formula> <tex-math notation=\"LaTeX\">${u} \\in [{m}]$ </tex-math></inline-formula> and columns <inline-formula> <tex-math notation=\"LaTeX\">${i} \\in [{n}]$ </tex-math></inline-formula> of the matrix are associated to latent features <inline-formula> <tex-math notation=\"LaTeX\">${x}_{\\text {row}}({{u}})$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">${x}_{\\text {col}}({{i}})$ </tex-math></inline-formula> respectively, and the <inline-formula> <tex-math notation=\"LaTeX\">$({\\it{ u, i}})$ </tex-math></inline-formula>-th entry of the matrix, <inline-formula> <tex-math notation=\"LaTeX\">${A}({\\it{ u, i}})$ </tex-math></inline-formula> is equal to <inline-formula> <tex-math notation=\"LaTeX\">${f}({x}_{\\text {row}}({{u}}), {x}_{\\text {col}}({{i}}))$ </tex-math></inline-formula> for a latent function <inline-formula> <tex-math notation=\"LaTeX\">$f$ </tex-math></inline-formula>. Given noisy observations of a small, random subset of the matrix entries, our goal is to estimate the unobserved entries of the matrix as well as to \u201cde-noise\u201d the observed entries. As the main result of this work, we introduce a nearest-neighbor-based estimation algorithm, and establish its consistency when the underlying latent function <inline-formula> <tex-math notation=\"LaTeX\">$f$ </tex-math></inline-formula> is Lipschitz, the underlying latent space is a bounded diameter Polish space, and the random fraction of observed entries in the matrix is at least <inline-formula> <tex-math notation=\"LaTeX\">$\\max \\big ({m}^{-1 + \\delta }, {n}^{-1/2 + \\delta } \\big)$ </tex-math></inline-formula>, for any <inline-formula> <tex-math notation=\"LaTeX\">$\\delta > 0$ </tex-math></inline-formula>. As an important byproduct, our analysis sheds light into the performance of the classical collaborative filtering algorithm for matrix completion, which has been widely utilized in practice. Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides a principled improvement over basic collaborative filtering and is competitive with matrix factorization methods. Our algorithm has a natural extension to the setting of tensor completion via flattening the tensor to matrix. When applied to the setting of image in-painting, which is a 3-order tensor, we find that our approach is competitive with respect to state-of-art tensor completion algorithms across benchmark images."
            },
            {
                "arxivId": "1212.1247",
                "title": "Matrix estimation by Universal Singular Value Thresholding",
                "abstract": "Consider the problem of estimating the entries of a large matrix, when the observed entries are noisy versions of a small random fraction of the original entries. This problem has received widespread attention in recent times, especially after the pioneering works of Emmanuel Cand\\`{e}s and collaborators. This paper introduces a simple estimation procedure, called Universal Singular Value Thresholding (USVT), that works for any matrix that has \"a little bit of structure.\" Surprisingly, this simple estimator achieves the minimax error rate up to a constant factor. The method is applied to solve problems related to low rank matrix estimation, blockmodels, distance matrix completion, latent space models, positive definite matrix completion, graphon estimation and generalized Bradley--Terry models for pairwise comparison."
            },
            {
                "arxivId": "1004.4389",
                "title": "User-Friendly Tail Bounds for Sums of Random Matrices",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2402.14005",
        "category": "econ",
        "title": "Information Elicitation in Agency Games",
        "abstract": "Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making. These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute. In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm. To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively. We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?* There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail. We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs. Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation. Still, giving the agent the ability to garble can lead to higher total welfare. Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare.",
        "references": [
            {
                "arxivId": "1508.03080",
                "title": "The Strange Case of Privacy in Equilibrium Models",
                "abstract": "We study how privacy technologies affect user and advertiser behavior in a simple economic model of targeted advertising. In our model, a consumer first decides whether or not to buy a good, and then an advertiser chooses an advertisement to show the consumer. The consumer's value for the good is correlated with her type, which determines which ad the advertiser would prefer to show to her---and hence, the advertiser would like to use information about the consumer's purchase decision to target the ad that he shows. In our model, the advertiser is given only a differentially private signal about the consumer's behavior---which can range from no signal at all to a perfect signal, as we vary the differential privacy parameter. This allows us to study equilibrium behavior as a function of the level of privacy provided to the consumer. We show that this behavior can be highly counter-intuitive, and that the effect of adding privacy in equilibrium can be completely different from what we would expect if we ignored equilibrium incentives. Specifically, we show that increasing the level of privacy can actually increase the amount of information about the consumer's type contained in the signal the advertiser receives, lead to decreased utility for the consumer, and increased profit for the advertiser, and that generally these quantities can be non-monotonic and even discontinuous in the privacy level of the signal."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2402.14764",
        "category": "econ",
        "title": "A Combinatorial Central Limit Theorem for Stratified Randomization",
        "abstract": "This paper establishes a combinatorial central limit theorem for stratified randomization, which holds under a Lindeberg-type condition. The theorem allows for an arbitrary number or sizes of strata, with the sole requirement being that each stratum contains at least two units. This flexibility accommodates both a growing number of large and small strata simultaneously, while imposing minimal conditions. We then apply this result to derive the asymptotic distributions of two test statistics proposed for instrumental variables settings in the presence of potentially many strata of unrestricted sizes.",
        "references": [
            {
                "arxivId": "2111.08157",
                "title": "Optimal Stratification of Survey Experiments",
                "abstract": "This paper studies a two-stage model of experimentation, where the researcher first samples representative units from an eligible pool, then assigns each sampled unit to treatment or control. To implement balanced sampling and assignment, we introduce a new family of finely stratified designs that generalize matched pairs randomization to propensities p(x) not equal to 1/2. We show that two-stage stratification nonparametrically dampens the variance of treatment effect estimation. We formulate and solve the optimal stratification problem with heterogeneous costs and fixed budget, providing simple heuristics for the optimal design. In settings with pilot data, we show that implementing a consistent estimate of this design is also efficient, minimizing asymptotic variance subject to the budget constraint. We also provide new asymptotically exact inference methods, allowing experimenters to fully exploit the efficiency gains from both stratified sampling and assignment. An application to nine papers recently published in top economics journals demonstrates the value of our methods."
            },
            {
                "arxivId": "1706.06469",
                "title": "On mitigating the analytical limitations of finely stratified experiments",
                "abstract": "Although attractive from a theoretical perspective, finely stratified experiments such as paired designs suffer from certain analytical limitations that are not present in block\u2010randomized experiments with multiple treated and control individuals in each block. In short, when using a weighted difference in means to estimate the sample average treatment effect, the traditional variance estimator in a paired experiment is conservative unless the pairwise average treatment effects are constant across pairs; however, in more coarsely stratified experiments, the corresponding variance estimator is unbiased if treatment effects are constant within blocks, even if they vary across blocks. Using insights from classical least squares theory, we present an improved variance estimator that is appropriate in finely stratified experiments. The variance estimator remains conservative in expectation but is asymptotically no more conservative than the classical estimator and can be considerably less conservative. The magnitude of the improvement depends on the extent to which effect heterogeneity can be explained by observed covariates. Aided by this estimator, a new test for the null hypothesis of a constant treatment effect is proposed. These findings extend to some, but not all, superpopulation models, depending on whether the covariates are viewed as fixed across samples."
            },
            {
                "arxivId": "1610.04821",
                "title": "General Forms of Finite Population Central Limit Theorems with Applications to Causal Inference",
                "abstract": "ABSTRACT Frequentists\u2019 inference often delivers point estimators associated with confidence intervals or sets for parameters of interest. Constructing the confidence intervals or sets requires understanding the sampling distributions of the point estimators, which, in many but not all cases, are related to asymptotic Normal distributions ensured by central limit theorems. Although previous literature has established various forms of central limit theorems for statistical inference in super population models, we still need general and convenient forms of central limit theorems for some randomization-based causal analyses of experimental data, where the parameters of interests are functions of a finite population and randomness comes solely from the treatment assignment. We use central limit theorems for sample surveys and rank statistics to establish general forms of the finite population central limit theorems that are particularly useful for proving asymptotic distributions of randomization tests under the sharp null hypothesis of zero individual causal effects, and for obtaining the asymptotic repeated sampling distributions of the causal effect estimators. The new central limit theorems hold for general experimental designs with multiple treatment levels, multiple treatment factors and vector outcomes, and are immediately applicable for studying the asymptotic properties of many methods in causal inference, including instrumental variable, regression adjustment, rerandomization, cluster-randomized experiments, and so on. Previously, the asymptotic properties of these problems are often based on heuristic arguments, which in fact rely on general forms of finite population central limit theorems that have not been established before. Our new theorems fill this gap by providing more solid theoretical foundation for asymptotic randomization-based causal inference. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2402.14952",
        "category": "econ",
        "title": "Implementations of Cooperative Games Under Non-Cooperative Solution Concepts",
        "abstract": "Cooperative games can be distinguished as non-cooperative games in which players can freely sign binding agreements to form coalitions. These coalitions inherit a joint strategy set and seek to maximize collective payoffs. When the payoffs to each coalition under some non-cooperative solution concept coincide with their value in the cooperative game, the cooperative game is said to be implementable and the non-cooperative game its implementation. This paper proves that all strictly superadditive partition function form games are implementable under Nash equilibrium and rationalizability; that all weakly superadditive characteristic function form games are implementable under Nash equilibrium; and that all weakly superadditive partition function form games are implementable under trembling hand perfect equilibrium. Discussion then proceeds on the appropriate choice of non-cooperative solution concept for the implementation.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2403.10907",
        "category": "econ",
        "title": "Macroeconomic Spillovers of Weather Shocks across U.S. States",
        "abstract": "We estimate the short-run effects of severe weather shocks on local economic activity and cross-border spillovers operating through economic linkages between U.S. states. We measure weather shocks using emergency declarations triggered by natural disasters and estimate their impacts with a monthly Global Vector Autoregressive (GVAR) model for U.S. states. Impulse responses highlight country-wide effects of weather shocks hitting individual regions. Taking into account economic interconnections between states allows capturing much stronger spillovers than those associated with mere spatial adjacency. Also, geographical heterogeneity is critical for assessing country-wide effects of weather shocks, and network effects amplify the local impacts of shocks.",
        "references": [
            {
                "arxivId": "2012.14693",
                "title": "The Impact of Climate on Economic and Financial Cycles: A Markov-switching Panel Approach",
                "abstract": "This paper examines the impact of climate shocks on 13 European economies analysing jointly business and financial cycles, in different phases and disentangling the effects for different sector channels. A Bayesian Panel Markov-switching framework is proposed to jointly estimate the impact of extreme weather events on the economies as well as the interaction between business and financial cycles. Results from the empirical analysis suggest that extreme weather events impact asymmetrically across the different phases of the economy and heterogeneously across the EU countries. Moreover, we highlight how the manufacturing output, a component of the industrial production index, constitutes the main channel through which climate shocks impact the EU economies."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2403.13773",
        "category": "econ",
        "title": "The Limits of Identification in Discrete Choice",
        "abstract": "We study identification and linear independence in random utility models. We characterize the dimension of the random utility model as the cyclomatic complexity of a specific graphical representation of stochastic choice data. We show that, as the number of alternatives grows, any linearly independent set of preferences is a vanishingly small subset of the set of all preferences. We introduce a new condition on sets of preferences which is sufficient for linear independence. We demonstrate by example that the condition is not necessary, but is strictly weaker than other existing sufficient conditions.",
        "references": [
            {
                "arxivId": "2302.03913",
                "title": "Axiomatization of Random Utility Model with Unobservable Alternatives",
                "abstract": "The random utility model is one of the most fundamental models in discrete choice analysis in economics. Although Falmagne (1978) obtained an axiomatization of the random utility model, his characterization requires strong observability of choices, i.e., the frequencies of choices must be observed from all subsets of the set of alternatives. Little is known, however, about the axiomatization when the frequencies on some choice sets are not observable. In fact, the problem of obtaining a tight characterization appears to be out of reach in most cases in view of a related NP-hard problem. We consider the following incomplete dataset. Let X be a finite set of alternatives. Let X* \u2286 X bea set of unobservable alternatives. Let D \u2286 2X be the set of choice sets. We assume that the choice frequency \u03c1(D, x) is unobservable (i.e., not defined) if and only if x \u2208 X* or D \u2209 D. Let M* \u2261 {(D,x)|x \u2208 D \u2208 2X and [x \u2208 X* or D \u2209 D]} be the set of all pairs (D,x) such that \u03c1(D, x) is not observable. To state our theorem, for any \u03c1 and (D, x) \u2208 M \u2261 {(D, x) \u2208 D \u00d7 X | x \u2208 D}, define a Block-Marschak polynomial by K(\u03c1, D, x) = \u03a3E:E\u2287D(\u22121)|E\\D|\u03c1(E,x)."
            },
            {
                "arxivId": "2208.08492",
                "title": "Marginal stochastic choice",
                "abstract": "Models of stochastic choice typically use conditional choice probabilities given menus as the primitive for analysis, but in the field these are often hard to observe. Moreover, studying preferences over menus is not possible with this data. We assume that an analyst can observe marginal frequencies of choice and availability, but not conditional choice frequencies, and study the testable implications of some prominent models of stochastic choice for this dataset. We also analyze whether parameters of these models can be identified. Finally, we characterize the marginal distributions that can arise under two-stage models in the spirit of Gul and Pesendorfer [2001] and of kreps [1979] where agents select the menu before choosing an alternative."
            },
            {
                "arxivId": "2205.01882",
                "title": "Approximating Choice Data by Discrete Choice Models",
                "abstract": "We obtain a necessary and sufficient condition under which random-coefficient discrete choice models, such as mixed-logit models, are rich enough to approximate any nonparametric random utility models arbitrarily well across choice sets. The condition turns out to be the affine-independence of the set of characteristic vectors. When the condition fails, resulting in some random utility models that cannot be closely approximated, we identify preferences and substitution patterns that are challenging to approximate accurately. We also propose algorithms to quantify the magnitude of approximation errors."
            },
            {
                "arxivId": "2103.05084",
                "title": "Correlated Choice",
                "abstract": "We study random joint choice rules, allowing for interdependence of choice across agents. These capture random choice by multiple agents, or a single agent across goods or time periods. Our interest is in separable choice rules, where each agent can be thought of as acting independently of the other. A random joint choice rule satisfies marginality if for every individual choice set, we can determine the individual's choice probabilities over alternatives independently of the other individual's choice set. We offer two characterizations of random joint choice rules satisfying marginality in terms of separable choice rules. While marginality is a necessary condition for separability, we show that it fails to be sufficient. We provide an additional condition on the marginal choice rules which, along with marginality, is sufficient for separability."
            },
            {
                "arxivId": "2102.05570",
                "title": "Identification in the random utility model",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.07396",
        "category": "econ",
        "title": "ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past",
        "abstract": "This study investigates whether OpenAI's ChatGPT-3.5 and ChatGPT-4 can accurately forecast future events using two distinct prompting strategies. To evaluate the accuracy of the predictions, we take advantage of the fact that the training data at the time of experiment stopped at September 2021, and ask about events that happened in 2022 using ChatGPT-3.5 and ChatGPT-4. We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters that share events that have happened to them, but after ChatGPT's training data had been collected. Concentrating on events in 2022, we prompted ChatGPT to engage in storytelling, particularly within economic contexts. After analyzing 100 prompts, we discovered that future narrative prompts significantly enhanced ChatGPT-4's forecasting accuracy. This was especially evident in its predictions of major Academy Award winners as well as economic trends, the latter inferred from scenarios where the model impersonated public figures like the Federal Reserve Chair, Jerome Powell. These findings indicate that narrative prompts leverage the models' capacity for hallucinatory narrative construction, facilitating more effective data synthesis and extrapolation than straightforward predictions. Our research reveals new aspects of LLMs' predictive capabilities and suggests potential future applications in analytical contexts.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08712",
        "category": "econ",
        "title": "Machine learning and economic forecasting: the role of international trade networks",
        "abstract": "This study examines the effects of de-globalization trends on international trade networks and their role in improving forecasts for economic growth. Using section-level trade data from nearly 200 countries from 2010 to 2022, we identify significant shifts in the network topology driven by rising trade policy uncertainty. Our analysis highlights key global players through centrality rankings, with the United States, China, and Germany maintaining consistent dominance. Using a horse race of supervised regressors, we find that network topology descriptors evaluated from section-specific trade networks substantially enhance the quality of a country's GDP growth forecast. We also find that non-linear models, such as Random Forest, XGBoost, and LightGBM, outperform traditional linear models used in the economics literature. Using SHAP values to interpret these non-linear model's predictions, we find that about half of most important features originate from the network descriptors, underscoring their vital role in refining forecasts. Moreover, this study emphasizes the significance of recent economic performance, population growth, and the primary sector's influence in shaping economic growth predictions, offering novel insights into the intricacies of economic growth forecasting.",
        "references": [
            {
                "arxivId": "2012.12802",
                "title": "Machine Learning Advances for Time Series Forecasting",
                "abstract": "In this paper we survey the most recent advances in supervised machine learning and high-dimensional models for time series forecasting. We consider both linear and nonlinear alternatives. Among the linear methods we pay special attention to penalized regressions and ensemble of models. The nonlinear methods considered in the paper include shallow and deep neural networks, in their feed-forward and recurrent versions, and tree-based methods, such as random forests and boosted trees. We also consider ensemble and hybrid models by combining ingredients from different alternatives. Tests for superior predictive ability are briefly reviewed. Finally, we discuss application of machine learning in economics and finance and provide an illustration with high-frequency financial data."
            },
            {
                "arxivId": "1405.6974",
                "title": "Futility Analysis in the Cross-Validation of Machine Learning Models",
                "abstract": "Many machine learning models have important structural tuning parameters that cannot be directly estimated from the data. The common tactic for setting these parameters is to use resampling methods, such as cross--validation or the bootstrap, to evaluate a candidate set of values and choose the best based on some pre--defined criterion. Unfortunately, this process can be time consuming. However, the model tuning process can be streamlined by adaptively resampling candidate values so that settings that are clearly sub-optimal can be discarded. The notion of futility analysis is introduced in this context. An example is shown that illustrates how adaptive resampling can be used to reduce training time. Simulation studies are used to understand how the potential speed--up is affected by parallel processing techniques."
            },
            {
                "arxivId": "cond-mat/0408566",
                "title": "Characterization and modeling of weighted networks",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08757",
        "category": "econ",
        "title": "Strategic Informed Trading and the Value of Private Information",
        "abstract": "We consider a market of risky financial assets where the participants are an informed trader, a mass of uniformed traders and noisy liquidity providers. We prove the existence of a market-clearing equilibrium when the insider internalizes her power to impact prices. In the price-impact equilibrium the insider strategically reveals a noisier (compared to when the insider takes prices as given) signal, and prices are less reactive to the publicly available information. In contrast to the related literature, we show that in the price-impact equilibrium, the insider's ex-ante welfare monotonically increases in the signal precision. This clarifies when a trader with market power is motivated to both obtain and refine her private information. Furthermore, even though the uniformed traders act as price-takers, the effect of price impact is ex-ante welfare improving for them. By contrast, internalization of price impact may reduce insider ex-ante welfare. This happens provided the insider is sufficiently risk averse and the uninformed traders are sufficiently risk tolerant.",
        "references": [
            {
                "arxivId": "2111.12373",
                "title": "Solving Cubic Matrix Equations Arising in Conservative Dynamics",
                "abstract": null
            },
            {
                "arxivId": "1802.09954",
                "title": "Price impact under heterogeneous beliefs and restricted participation",
                "abstract": "We consider a financial market in which traders potentially face restrictions in trading some of the available securities. Traders are heterogeneous with respect to their beliefs and risk profiles, and the market is assumed thin: traders strategically trade against their price impacts. We prove existence and uniqueness of a corresponding equilibrium, and provide an efficient algorithm to numerically obtain the equilibrium prices and allocations given market's inputs. We find that restrictions may increase the market's welfare if traders have different views regarding the covariance matrix of securities returns. The latter heterogeneity regarding covariance matrix disagreement is essential in modelling; for instance, when traders agree on the covariance matrix, restricting participation in some securities for some traders leaves equilibrium prices unaltered in the unrestricted securities, a certainly undesirable model effect."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08762",
        "category": "econ",
        "title": "Competition for Budget-Constrained Buyers: Exploring All-Pay Auctions",
        "abstract": "This note pursues two primary objectives. First, we analyze the outcomes of an all-pay auction within a store where buyers with and without financial constraints arrive at varying rates, and where buyer types are private information. Second, we investigate the selection of an auction format (comprising first-price, second-price, and all-pay formats) in a competitive search setting, where sellers try to attract customers. Our results indicate that if the budget constraint is not too restrictive, the all-pay rule emerges as the preferred selling format in the unique symmetric equilibrium. This is thanks to its ability to prompt buyers to submit lower bids, thereby generally avoiding budget constraints, while allowing the seller to collect all bids.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08816",
        "category": "econ",
        "title": "Evaluating the Quality of Answers in Political Q&A Sessions with Large Language Models",
        "abstract": "This paper presents a new approach to evaluating the quality of answers in political question-and-answer sessions. We propose to measure an answer's quality based on the degree to which it allows us to infer the initial question accurately. This conception of answer quality inherently reflects their relevance to initial questions. Drawing parallels with semantic search, we argue that this measurement approach can be operationalized by fine-tuning a large language model on the observed corpus of questions and answers without additional labeled data. We showcase our measurement approach within the context of the Question Period in the Canadian House of Commons. Our approach yields valuable insights into the correlates of the quality of answers in the Question Period. We find that answer quality varies significantly based on the party affiliation of the members of Parliament asking the questions and uncover a meaningful correlation between answer quality and the topics of the questions.",
        "references": [
            {
                "arxivId": "2102.02111",
                "title": "Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis",
                "abstract": "Transformer-based models for transfer learning have the potential to achieve high prediction accuracies on text-based supervised learning tasks with relatively few training data instances. These models are thus likely to benefit social scientists that seek to have as accurate as possible text-based measures but only have limited resources for annotating training data. To enable social scientists to leverage these potential benefits for their research, this paper explains how these methods work, why they might be advantageous, and what their limitations are. Additionally, three Transformer-based models for transfer learning, BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), and the Longformer (Beltagy et al. 2020), are compared to conventional machine learning algorithms on three applications. Across all evaluated tasks, textual styles, and training data set sizes, the conventional models are consistently outperformed by transfer learning with Transformers, thereby demonstrating the benefits these models can bring to textbased social science research."
            },
            {
                "arxivId": "2004.09297",
                "title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
                "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\\footnote{\\url{this https URL}}."
            },
            {
                "arxivId": "1908.10084",
                "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
                "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
            },
            {
                "arxivId": "1907.11692",
                "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08839",
        "category": "econ",
        "title": "Multiply-Robust Causal Change Attribution",
        "abstract": "Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application.",
        "references": [
            {
                "arxivId": "2307.05284",
                "title": "On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets",
                "abstract": "Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of how distributions differ."
            },
            {
                "arxivId": "2302.00736",
                "title": "Approximating the Shapley Value without Marginal Contributions",
                "abstract": "The Shapley value, which is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, has recently been used intensively in explainable artificial intelligence. Its meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley value, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contribution. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparing ourselves with state-of-the-art methods."
            },
            {
                "arxivId": "2210.10275",
                "title": "Towards Explaining Distribution Shifts",
                "abstract": "A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work focuses on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of optimal transport, where the candidate mappings are restricted to a set of interpretable mappings. We then inspect multiple quintessential use-cases of distribution shift in real-world tabular, text, and image datasets to showcase how our explanatory mappings provide a better balance between detail and interpretability than baseline explanations by both visual inspection and our PercentExplained metric."
            },
            {
                "arxivId": "2208.08399",
                "title": "The Counterfactual-Shapley Value: Attributing Change in System Metrics",
                "abstract": "Given an unexpected change in the output metric of a large-scale system, it is important to answer why the change occurred: which inputs caused the change in metric? A key component of such an attribution question is estimating the counterfactual: the (hypothetical) change in the system metric due to a specified change in a single input. However, due to inherent stochasticity and complex interactions between parts of the system, it is difficult to model an output metric directly. We utilize the computational structure of a system to break up the modelling task into sub-parts, such that each sub-part corresponds to a more stable mechanism that can be modelled accurately over time. Using the system's structure also helps to view the metric as a computation over a structural causal model (SCM), thus providing a principled way to estimate counterfactuals. Specifically, we propose a method to estimate counterfactuals using time-series predictive models and construct an attribution score, CF-Shapley, that is consistent with desirable axioms for attributing an observed change in the output metric. Unlike past work on causal shapley values, our proposed method can attribute a single observed change in output (rather than a population-level effect) and thus provides more accurate attribution scores when evaluated on simulated datasets. As a real-world application, we analyze a query-ad matching system with the goal of attributing observed change in a metric for ad matching density. Attribution scores explain how query volume and ad demand from different query categories affect the ad matching density, leading to actionable insights and uncovering the role of external events (e.g.,\"Cheetah Day\") in driving the matching density."
            },
            {
                "arxivId": "2207.07605",
                "title": "Algorithms to estimate Shapley value feature attributions",
                "abstract": null
            },
            {
                "arxivId": "2203.13887",
                "title": "Automatic Debiased Machine Learning for Dynamic Treatment Effects",
                "abstract": "We extend the idea of automated debiased machine learning to the dynamic treatment regime and more generally to nested functionals. We show that the multiply robust formula for the dynamic treatment regime with discrete treatments can be re-stated in terms of a recursive Riesz representer characterization of nested mean regressions. We then apply a recursive Riesz representer estimation learning algorithm that estimates de-biasing corrections without the need to characterize how the correction terms look like, such as for instance, products of inverse probability weighting terms, as is done in prior work on doubly robust estimation in the dynamic regime. Our approach defines a sequence of loss minimization problems, whose minimizers are the mulitpliers of the de-biasing correction, hence circumventing the need for solving auxiliary propensity models and directly optimizing for the mean squared error of the target de-biasing correction. We provide further applications of our approach to estimation of dynamic discrete choice models and estimation of long-term effects with surrogates."
            },
            {
                "arxivId": "2112.13398",
                "title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning",
                "abstract": "We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects and avearage derivatives) the bound is shown to depend on easily interpretable quantities that measure the explanatory power of the omitted variables. Therefore, simple plausibility judgments on the maximum explanatory power of omitted variables (in explaining treatment and outcome variation) are sufficient to place overall bounds on the size of the bias. Furthermore, we use debiased machine learning to provide flexible and efficient statistical inference on learnable components of the bounds. Finally, empirical examples demonstrate the usefulness of the approach."
            },
            {
                "arxivId": "2104.14737",
                "title": "Automatic Debiased Machine Learning via Riesz Regression",
                "abstract": "A variety of interesting parameters may depend on high dimensional regressions. Machine learning can be used to estimate such parameters. However estimators based on machine learners can be severely biased by regularization and/or model selection. Debiased machine learning uses Neyman orthogonal estimating equations to reduce such biases. Debiased machine learning generally requires estimation of unknown Riesz representers. A primary innovation of this paper is to provide Riesz regression estimators of Riesz representers that depend on the parameter of interest, rather than explicit formulae, and that can employ any machine learner, including neural nets and random forests. End-to-end algorithms emerge where the researcher chooses the parameter of interest and the machine learner and the debiasing follows automatically. Another innovation here is debiased machine learners of parameters depending on generalized regressions, including high-dimensional generalized linear models. An empirical example of automatic debiased machine learning using neural nets is given. We find in Monte Carlo examples that automatic debiasing sometimes performs better than debiasing via inverse propensity scores and never worse. Finite sample mean square error bounds for Riesz regression estimators and asymptotic theory are also given."
            },
            {
                "arxivId": "2007.00714",
                "title": "Quantifying intrinsic causal contributions via structure preserving interventions",
                "abstract": "We propose a notion of causal influence that describes the `intrinsic' part of the contribution of a node on a target node in a DAG. By recursively writing each node as a function of the upstream noise terms, we separate the intrinsic information added by each node from the one obtained from its ancestors. To interpret the intrinsic information as a {\\it causal} contribution, we consider `structure-preserving interventions' that randomize each node in a way that mimics the usual dependence on the parents and does not perturb the observed joint distribution. To get a measure that is invariant with respect to relabelling nodes we use Shapley based symmetrization and show that it reduces in the linear case to simple ANOVA after resolving the target node into noise variables. We describe our contribution analysis for variance and entropy, but contributions for other target metrics can be defined analogously. The code is available in the package gcm of the open source library DoWhy."
            },
            {
                "arxivId": "1912.02724",
                "title": "Causal structure based root cause analysis of outliers",
                "abstract": "We describe a formal approach to identify 'root causes' of outliers observed in $n$ variables $X_1,\\dots,X_n$ in a scenario where the causal relation between the variables is a known directed acyclic graph (DAG). To this end, we first introduce a systematic way to define outlier scores. Further, we introduce the concept of 'conditional outlier score' which measures whether a value of some variable is unexpected *given the value of its parents* in the DAG, if one were to assume that the causal structure and the corresponding conditional distributions are also valid for the anomaly. Finally, we quantify to what extent the high outlier score of some target variable can be attributed to outliers of its ancestors. This quantification is defined via Shapley values from cooperative game theory."
            },
            {
                "arxivId": "1901.01230",
                "title": "Permutation Weighting",
                "abstract": "This work introduces permutation weighting: a weighting estimator for observational causal inference under general treatment regimes which preserves arbitrary measures of covariate balance. We show that estimating weights which obey balance constraints is equivalent to a simple binary classification problem between the observed data and a permuted dataset (no matter the cardinality of treatment). Arbitrary probabilistic classifiers may be used in this method; the hypothesis space of the classifier corresponds to the nature of the balance constraints imposed through the resulting weights.We show equivalence between existing covariate balancing weight estimators and permutation weighting and demonstrate estimation with improved efficiency through this regime. We provide theoretical results on the consistency of estimation of causal effects, and the necessity of balance infinite samples. Empirical evaluations indicate that the proposed method outperforms existing state of the art weighting methods for causal effect estimation, even when the data generating process corresponds to the assumptions imposed by prior work."
            },
            {
                "arxivId": "1811.11603",
                "title": "Distribution regression with sample selection, with an application to wage decompositions in the UK",
                "abstract": "We develop a distribution regression model under endogenous sample selection. This model is a semiparametric generalization of the Heckman selection model that accommodates much richer patterns of heterogeneity in the selection process and effect of the covariates. The model applies to continuous, discrete and mixed outcomes. We study the identification of the model, and develop a computationally attractive two-step method to estimate the model parameters, where the first step is a probit regression for the selection equation and the second step consists of multiple distribution regressions with selection corrections for the outcome equation. We construct estimators of functionals of interest such as actual and counterfactual distributions of latent and observed outcomes via plug-in rule. We derive functional central limit theorems for all the estimators and show the validity of multiplier bootstrap to carry out functional inference. We apply the methods to wage decompositions in the UK using new data. Here we decompose the difference between the male and female wage distributions into four effects: composition, wage structure, selection structure and selection sorting. After controlling for endogenous employment selection, we still find substantial gender wage gap -- ranging from 21% to 40% throughout the (latent) offered wage distribution that is not explained by observable labor market characteristics. We also uncover positive sorting for single men and negative sorting for married women that accounts for a substantive fraction of the gender wage gap at the top of the distribution. These findings can be interpreted as evidence of assortative matching in the marriage market and glass-ceiling in the labor market."
            },
            {
                "arxivId": "1801.09138",
                "title": "Cross-fitting and fast remainder rates for semiparametric estimation",
                "abstract": "There are many interesting and widely used estimators of a functional with ?nite semi-parametric variance bound that depend on nonparametric estimators of nuisance func-tions. We use cross-?tting to construct such estimators with fast remainder rates. We give cross-?t doubly robust estimators that use separate subsamples to estimate di?erent nuisance functions. We show that a cross-?t doubly robust spline regression estimator of the expected conditional covariance is semiparametric e?cient under minimal conditions. Corresponding estimators of other average linear functionals of a conditional expectation are shown to have the fastest known remainder rates under certain smoothness conditions. The cross-?t plug-in estimator shares some of these properties but has a remainder term that is larger than the cross-?t doubly robust estimator. As speci?c examples we consider the expected conditional covariance, mean with randomly missing data, and a weighted average derivative."
            },
            {
                "arxivId": "1512.05635",
                "title": "The sorted effects method: discovering heterogeneous effects beyond their averages",
                "abstract": "The partial (ceteris paribus) e?ects of interest in nonlinear and interactive linear models are heterogeneous as they can vary dramatically with the underlying observed or unobserved covariates. Despite the apparent importance of heterogeneity, a common practice in modern empirical work is to largely ignore it by reporting average partial e?ects (or, at best, average e?ects for some groups, see e.g. Angrist and Pischke (2008)). While average e?ects provide very convenient scalar summaries of typical e?ects, by de?nition they fail to re?ect the entire variety of the heterogenous e?ects. In order to discover these e?ects much more fully, we propose to estimate and report sorted e?ects \u2013 a collection of estimated partial e?ects sorted in increasing order and indexed by percentiles. By construction the sorted e?ect curves completely represent and help visualize all of the heterogeneous e?ects in one plot. They are as convenient and easy to report in practice as the conventional average partial e?ects. We also provide a quanti?cation of uncertainty (standard errors and con?dence bands) for the estimated sorted e?ects. We apply the sorted e?ects method to demonstrate several striking patterns of gender-based discrimination in wages, and of race-based discrimination in mortgage lending. Using di?erential geometry and functional delta methods, we establish that the estimated sorted e?ects are consistent for the true sorted e?ects, and derive asymptotic normality and bootstrap approximation results, enabling construction of pointwise con?dence bands (point-wise with respect to percentile indices). We also derive functional central limit theorems and bootstrap approximation results, enabling construction of simultaneous con?dence bands (simultaneous with respect to percentile indices). The derived statistical results in turn rely on establishing Hadamard di?erentiability of the multivariate sorting operator, a result of independent mathematical interest."
            },
            {
                "arxivId": "1311.2645",
                "title": "Program evaluation and causal inference with high-dimensional data",
                "abstract": "In this paper, we provide efficient estimators and honest con fidence bands for a variety of treatment eff ects including local average (LATE) and local quantile treatment eff ects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment e ffects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces ecient estimators and honest bands for (functional) average treatment eff ects (ATE) and quantile treatment eff ects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the eff ect of 401(k) eligibility and participation on accumulated assets. The results on program evaluation are obtained as a consequence of more general results on honest inference in a general moment condition framework, which arises from structural equation models in econometrics. Here too the crucial ingredient is the use of orthogonal moment conditions, which can be constructed from the initial moment conditions. We provide results on honest inference for (function-valued) parameters within this general framework where any high-quality, modern machine learning methods can be used to learn the nonparametric/high-dimensional components of the model. These include a number of supporting auxilliary results that are of major independent interest: namely, we (1) prove uniform validity of a multiplier bootstrap, (2) o er a uniformly valid functional delta method, and (3) provide results for sparsity-based estimation of regression functions for function-valued outcomes."
            },
            {
                "arxivId": "1210.4654",
                "title": "Semiparametric Theory for Causal Mediation Analysis: efficiency bounds, multiple robustness, and sensitivity analysis.",
                "abstract": "Whilst estimation of the marginal (total) causal effect of a point exposure on an outcome is arguably the most common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon evaluating the total effect of the exposure, investigators routinely wish to make inferences about the direct or indirect pathways of the effect of the exposure not through or through a mediator variable that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies, that produce double robust and highly efficient estimates of the marginal total causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable."
            },
            {
                "arxivId": "1202.3775",
                "title": "Kernel-based Conditional Independence Test and Application in Causal Discovery",
                "abstract": "Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08908",
        "category": "econ",
        "title": "Reference Model Based Learning in Expectation Formation: Experimental Evidence",
        "abstract": "How do people form expectations about future prices in financial markets? One of the dominant learning rules that explains the forecasting behavior is the Adaptive Expectation Rule (ADA), which suggests that people adjust their predictions by adapting to the most recent prediction error at a constant weight. However, this rule also implies that they will continually learn and adapt until the prediction error is zero, which contradicts recent experimental evidence showing that people usually stop learning long before reaching zero prediction error. A more recent learning rule, Reference Model Based Learning (RMBL), extends and generalizes ADA, hypothesizing that: i) People apply ADA but dynamically adjust the adaptive coefficient with regards to the auto-correlation of the prediction error in the most recent two periods; ii) Meanwhile, they also utilize a satisficing rule so that people would only adjust their adaptive coefficient when the prediction error is higher than their anticipation. This paper utilizes a rich set of experimental data with observations of 41,490 predictions from 801 subjects from the Learning-to-Forecast Experiments (LtFEs), i.e., the experiment that has been used to study expectation formation. Our results concludes that RMBL fits better than ADA in all the experiments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08984",
        "category": "econ",
        "title": "Long run consequence of p-hacking",
        "abstract": "We study the theoretical consequence of p-hacking on the accumulation of knowledge under the framework of mis-specified Bayesian learning. A sequence of researchers, in turn, choose projects that generate noisy information in a field. In choosing projects, researchers need to carefully balance as projects generates big information are less likely to succeed. In doing the project, a researcher p-hacks at intensity $\\varepsilon$ so that the success probability of a chosen project increases (unduly) by a constant $\\varepsilon$. In interpreting previous results, researcher behaves as if there is no p-hacking because the intensity $\\varepsilon$ is unknown and presumably small. We show that over-incentivizing information provision leads to the failure of learning as long as $\\varepsilon\\neq 0$. If the incentives of information provision is properly provided, learning is correct almost surely as long as $\\varepsilon$ is small.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.08991",
        "category": "econ",
        "title": "Business models for the simulation hypothesis",
        "abstract": "The simulation hypothesis suggests that we live in a computer simulation. That notion has attracted significant scholarly and popular interest. This article explores the simulation hypothesis from a business perspective. Due to the lack of a name for a universe consistent with the simulation hypothesis, we propose the term simuverse. We argue that if we live in a simulation, there must be a business justification. Therefore, we ask: If we live in a simuverse, what is its business model? We identify and explore business model scenarios, such as simuverse as a project, service, or platform. We also explore business model pathways and risk management issues. The article contributes to the simulation hypothesis literature and is the first to provide a business model perspective on the simulation hypothesis. The article discusses theoretical and practical implications and identifies opportunities for future research related to sustainability, digital transformation, and Artificial Intelligence (AI).",
        "references": [
            {
                "arxivId": "2206.15331",
                "title": "GitHub Copilot AI pair programmer: Asset or Liability?",
                "abstract": "Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans' solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired."
            },
            {
                "arxivId": "2008.12254",
                "title": "A Bayesian Approach to the Simulation Argument",
                "abstract": "The Simulation Argument posed by Bostrom (2003) suggests that we may be living inside a sophisticated computer simulation. If post-human civilizations eventually have both the capability and desire to generate such Bostrom-like simulations, then the number of simulated realities would greatly exceed the one base reality, ostensibly indicating a high probability that we do not live in said base reality. In this work, it is argued that since the hypothesis that such simulations are technically possible remains unproven, then statistical calculations need to consider not just the number of state spaces, but the intrinsic model uncertainty. This is achievable through a Bayesian treatment of the problem, which is presented here. Using Bayesian model averaging, it is shown that the probability that we are sims is in fact less than 50%, tending towards that value in the limit of an infinite number of simulations. This result is broadly indifferent as to whether one conditions upon the fact that humanity has not yet birthed such simulations, or ignore it. As argued elsewhere, it is found that if humanity does start producing such simulations, then this would radically shift the odds and make it very probable that we are in fact sims."
            },
            {
                "arxivId": "2007.05995",
                "title": "The Complexity and Information Content of Simulated Universes",
                "abstract": null
            },
            {
                "arxivId": "2001.10439",
                "title": "How Many Simulations Do We Exist In? A Practical Mathematical Solution to the Simulation Argument",
                "abstract": "The Simulation Argument has gained significant traction in the public arena. It has offered a hypothesis based on probabilistic analysis of its assumptions that we are likely to exist within a computer simulation. This has been derived from factors including the prediction of computing power, human existence, extinction and population dynamics, and suggests a very large value for the number of possible simulations within which we may exist. On evaluating this argument through the application of tangible real-world evidence and projections, it is possible to calculate real numerical solutions for the Simulation Argument. This reveals a much smaller number of possible simulations within which we may exist, and offers a novel practicable approach in which to appraise the variety and multitude of conjectures and theories associated with the Simulation Hypothesis."
            },
            {
                "arxivId": "1905.05792",
                "title": "Simulation Typology and Termination Risks",
                "abstract": "The goal of the article is to explore what is the most probable type of simulation in which humanity lives (if any) and how this affects simulation termination risks. We firstly explore the question of what kind of simulation in which humanity is most likely located based on pure theoretical reasoning. We suggest a new patch to the classical simulation argument, showing that we are likely simulated not by our own descendants, but by alien civilizations. Based on this, we provide classification of different possible simulations and we find that simpler, less expensive and one-person-centered simulations, resurrectional simulations, or simulations of the first artificial general intelligence's (AGI's) origin (singularity simulations) should dominate. Also, simulations which simulate the 21st century and global catastrophic risks are probable. We then explore whether the simulation could collapse or be terminated. Most simulations must be terminated after they model the singularity or after they model a global catastrophe before the singularity. Undeniably observed glitches, but not philosophical speculations could result in simulation termination. The simulation could collapse if it is overwhelmed by glitches. The Doomsday Argument in simulations implies termination soon. We conclude that all types of the most probable simulations except resurrectional simulations are prone to termination risks in a relatively short time frame of hundreds of years or less from now."
            },
            {
                "arxivId": "1803.03453",
                "title": "The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities",
                "abstract": "Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems."
            },
            {
                "arxivId": "1703.00058",
                "title": "On testing the simulation theory",
                "abstract": "Can the theory that reality is a simulation be tested? We investigate this question based on the assumption that if the system performing the simulation is finite (i.e. has limited resources), then to achieve low computational complexity, such a system would, as in a video game, render content (reality) only at the moment that information becomes available for observation by a player and not at the moment of detection by a machine (that would be part of the simulation and whose detection would also be part of the internal computation performed by the Virtual Reality server before rendering content to the player). Guided by this principle we describe conceptual wave/particle duality experiments aimed at testing the simulation theory."
            },
            {
                "arxivId": "1210.1847",
                "title": "Constraints on the universe as a numerical simulation",
                "abstract": null
            },
            {
                "arxivId": "0704.0646",
                "title": "The Mathematical Universe",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09117",
        "category": "econ",
        "title": "Identifying Causal Effects under Kink Setting: Theory and Evidence",
        "abstract": "This paper develops a generalized framework for identifying causal impacts in a reduced-form manner under kinked settings when agents can manipulate their choices around the threshold. The causal estimation using a bunching framework was initially developed by Diamond and Persson (2017) under notched settings. Many empirical applications of bunching designs involve kinked settings. We propose a model-free causal estimator in kinked settings with sharp bunching and then extend to the scenarios with diffuse bunching, misreporting, optimization frictions, and heterogeneity. The estimation method is mostly non-parametric and accounts for the interior response under kinked settings. Applying the proposed approach, we estimate how medical subsidies affect outpatient behaviors in China.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09297",
        "category": "econ",
        "title": "Belief Bias Identification",
        "abstract": "This paper proposes a unified theoretical model to identify and test a comprehensive set of probabilistic updating biases within a single framework. The model achieves separate identification by focusing on the updating of belief distributions, rather than classic point-belief measurements. Testing the model in a laboratory experiment reveals significant heterogeneity at the individual level: All tested biases are present, and each participant exhibits at least one identifiable bias. Notably, motivated-belief biases (optimism and pessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy) are identified as key drivers of biased inference. Moreover, at the population level, base rate neglect emerges as a persistent influence. This study contributes to the belief-updating literature by providing a methodological toolkit for researchers examining links between different conflicting biases, or exploring connections between updating biases and other behavioural phenomena.",
        "references": [
            {
                "arxivId": "2109.09871",
                "title": "Overinference from Weak Signals and Underinference from Strong Signals",
                "abstract": "We study how overreaction and underreaction to signals depend on their informativeness. While a large literature has studied belief updating in response to highly informative signals, people in important real-world settings are often faced with a steady stream of weak signals. We use a tightly controlled experiment and new empirical evidence from betting and financial markets to demonstrate that updating behavior differs meaningfully by signal strength: across domains, our consistent and robust finding is overreaction to weak signals and underreaction to strong signals. Both sets of results align well with a simple theory of cognitive imprecision about signal informativeness. Our framework and findings can help harmonize apparently contradictory results from the experimental and empirical literatures."
            },
            {
                "arxivId": "1607.00022",
                "title": "Modeling confirmation bias and polarization",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09523",
        "category": "econ",
        "title": "More, better or different? Trade-offs between group size and competence development in jury theorems",
        "abstract": "In many circumstances there is a trade off between the number of voters and the time they can be given before having to make a decision since both aspects are costly. An example is the hiring of a committee with a fixed salary budget: more people but a shorter time for each to develop their competence about the issue at hand or less people with a longer time for competence development? In this paper we investigate the interaction between the number of voters, the development of their competence over time and the final probability for an optimal majority decision. Among other things we consider how different learning profiles, or rates of relevant competence increase, for the members of a committee affects the optimal committee size. To the best of our knowledge, our model is the first that includes the potentially positive effects of having a heterogeneous group of voters on majority decisions in a satisfactory way. We also discuss how some earlier attempts fail to capture the effect of heterogeneity correctly.",
        "references": [
            {
                "arxivId": "1311.3223",
                "title": "Further results on consensus formation in the Deffuant model",
                "abstract": "The so-called Deffuant model describes a pattern for social interaction, in which two neighboring individuals randomly meet and share their opinions on a certain topic, if their discrepancy is not beyond a given threshold $\\theta$. The major focus of the analyses, both theoretical and based on simulations, lies on whether these single interactions lead to a global consensus in the long run or not. First, we generalize a result of Lanchier for the Deffuant model on $\\mathbb{Z}$, determining the critical value for $\\theta$ at which a phase transition of the long term behavior takes place, to other distributions of the initial opinions than i.i.d. uniform on $[0,1]$. Then we shed light on the situations where the underlying line graph $\\mathbb{Z}$ is replaced by higher-dimensional lattices $\\mathbb{Z}^d,\\ d\\geq2$, or the infinite cluster of supercritical i.i.d. bond percolation on these lattices."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09528",
        "category": "econ",
        "title": "Overfitting Reduction in Convex Regression",
        "abstract": "Convex regression is a method for estimating an unknown function $f_0$ from a data set of $n$ noisy observations when $f_0$ is known to be convex. This method has played an important role in operations research, economics, machine learning, and many other areas. It has been empirically observed that the convex regression estimator produces inconsistent estimates of $f_0$ and extremely large subgradients near the boundary of the domain of $f_0$ as $n$ increases. In this paper, we provide theoretical evidence of this overfitting behaviour. We also prove that the penalised convex regression estimator, one of the variants of the convex regression estimator, exhibits overfitting behaviour. To eliminate this behaviour, we propose two new estimators by placing a bound on the subgradients of the estimated function. We further show that our proposed estimators do not exhibit the overfitting behaviour by proving that (a) they converge to $f_0$ and (b) their subgradients converge to the gradient of $f_0$, both uniformly over the domain of $f_0$ with probability one as $n \\rightarrow \\infty$. We apply the proposed methods to compute the cost frontier function for Finnish electricity distribution firms and confirm their superior performance in predictive power over some existing methods.",
        "references": [
            {
                "arxivId": "2209.12538",
                "title": "Convex support vector regression",
                "abstract": null
            },
            {
                "arxivId": "2107.03119",
                "title": "Variable selection in convex quantile regression: L1-norm or L0-norm regularization?",
                "abstract": null
            },
            {
                "arxivId": "2003.04433",
                "title": "Least squares estimation of a quasiconvex regression function",
                "abstract": "\n We develop a new approach for the estimation of a multivariate function based on the economic axioms of quasiconvexity (and monotonicity). On the computational side, we prove the existence of the quasiconvex constrained least squares estimator (LSE) and provide a characterisation of the function space to compute the LSE via a mixed-integer quadratic programme. On the theoretical side, we provide finite sample risk bounds for the LSE via a sharp oracle inequality. Our results allow for errors to depend on the covariates and to have only two finite moments. We illustrate the superior performance of the LSE against some competing estimators via simulation. Finally, we use the LSE to estimate the production function for the Japanese plywood industry and the cost function for hospitals across the US."
            },
            {
                "arxivId": "1509.08165",
                "title": "A Computational Framework for Multivariate Convex Regression and Its Variants",
                "abstract": "ABSTRACT We study the nonparametric least squares estimator (LSE) of a multivariate convex regression function. The LSE, given as the solution to a quadratic program with O(n2) linear constraints (n being the sample size), is difficult to compute for large problems. Exploiting problem specific structure, we propose a scalable algorithmic framework based on the augmented Lagrangian method to compute the LSE. We develop a novel approach to obtain smooth convex approximations to the fitted (piecewise affine) convex LSE and provide formal bounds on the quality of approximation. When the number of samples is not too large compared to the dimension of the predictor, we propose a regularization scheme\u2014Lipschitz convex regression\u2014where we constrain the norm of the subgradients, and study the rates of convergence of the obtained LSE. Our algorithmic framework is simple and flexible and can be easily adapted to handle variants: estimation of a nondecreasing/nonincreasing convex/concave (with or without a Lipschitz bound) function. We perform numerical studies illustrating the scalability of the proposed algorithm\u2014on some instances our proposal leads to more than a 10,000-fold improvement in runtime when compared to off-the-shelf interior point solvers for problems with n = 500."
            },
            {
                "arxivId": "1105.1924",
                "title": "Multivariate convex regression with adaptive partitioning",
                "abstract": "We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is a computationally efficient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options."
            },
            {
                "arxivId": "1003.4765",
                "title": "Nonparametric Least Squares Estimation of a Multivariate Convex Regression Function",
                "abstract": "This paper deals with the consistency of the least squares estimator of a convex regression function when the predictor is multidimensional. We characterize and discuss the computation of such an estimator via the solution of certain quadratic and linear programs. Mild sufficient conditions for the consistency of this estimator and its subdifferentials in fixed and stochastic design regression settings are provided. We also consider a regression function which is known to be convex and componentwise nonincreasing and discuss the characterization, computation and consistency of its least squares estimator."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09553",
        "category": "econ",
        "title": "Has Anti-corruption Efforts lowered Enterprises Innovation Efficiency? -An Empirical Analysis from China",
        "abstract": "This study adopts the fixed effects panel model and provincial panel data on anticorruption and the innovation efficiency of high-level technology and new technology enterprises in China from 2005 to 2014, to estimate the effects of the anticorruption movement on the innovation efficiency of enterprises at different corruption levels. The empirical results show that anticorruption is positively correlated with the innovation efficiency of enterprises; however, the correlation is differentiated according to different corruption levels and business natures. At a high level of corruption, anticorruption has positive impacts on enterprises' innovation; at a low level of corruption, it negatively affects innovation efficiency. However, anticorruption has negative effects on the innovation efficiency of state-owned enterprises at both high and low corruption levels; for nonstate-owned enterprises, its effects are positive at a high corruption level and negative at a low corruption level. The effects remain the same across different regions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09629",
        "category": "econ",
        "title": "Quantifying fair income distribution in Thailand",
        "abstract": "Given a vast concern about high income inequality in Thailand as opposed to empirical findings around the world showing people\u2019s preference for fair income inequality over unfair income equality, it is therefore important to examine whether inequality in income distribution in Thailand over the past three decades is fair, and what fair inequality in income distribution in Thailand should be. To quantitatively measure fair income distribution, this study employs the fairness benchmarks that are derived from the distributions of athletes\u2019 salaries in professional sports which satisfy the concepts of distributive justice and procedural justice, the no-envy principle of fair allocation, and the general consensus or the international norm criterion of a meaningful benchmark. By using the data on quintile income shares and the income Gini index of Thailand from the National Social and Economic Development Council, this study finds that, throughout the period from 1988 to 2021, the Thai income earners in the bottom 20%, the second 20%, and the top 20% receive income shares more than the fair shares whereas those in the third 20% and the fourth 20% receive income shares less than the fair shares. Provided that there are infinite combinations of quintile income shares that can have the same value of income Gini index but only one of them is regarded as fair, this study demonstrates the use of fairness benchmarks as a practical guideline for designing policies with an aim to achieve fair income distribution in Thailand. Moreover, a comparative analysis is conducted by employing the method for estimating optimal (fair) income distribution representing feasible income equality in order to provide an alternative recommendation on what optimal (fair) income distribution characterizing feasible income equality in Thailand should be.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-16.json",
        "arxivId": "2404.09796",
        "category": "econ",
        "title": "A note on heterogeneity, trade integration and spatial inequality",
        "abstract": "We study the impact of economic integration on spatial development in a model where all consumers are inter-regionally mobile and have heterogeneous preferences regarding their residential location choices. This heterogeneity is the unique dispersion force in the model. We show that, under reasonable values for the elasticity of substitution among varieties of consumption goods, a higher trade integration always promotes more symmetric patterns, irrespective of the functional form of the dispersion force. We also show that an increase in the degree of heterogeneity in preferences for location leads to less spatial inequality.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2109.14204",
        "category": "econ",
        "title": "Strategic formation of collaborative networks",
        "abstract": "We examine behavior in an experimental collaboration game that incorporates endogenous network formation. The environment is modeled as a generalization of the voluntary contributions mechanism. By varying the information structure in a controlled laboratory experiment, we examine the underlying mechanisms of reciprocity that generate emergent patterns in linking and contribution decisions. Providing players more detailed information about the sharing behavior of others drastically increases efficiency, and positively affects a number of other key outcomes. To understand the driving causes of these changes in behavior we develop and estimate a structural model for actions and small network panels and identify how social preferences affect behavior. We find that the treatment reduces altruism but stimulates reciprocity, helping players coordinate to reach mutually beneficial outcomes. In a set of counterfactual simulations, we show that increasing trust in the community would encourage higher average contributions at the cost of mildly increased free-riding. Increasing overall reciprocity greatly increases collaborative behavior when there is limited information but can backfire in the treatment, suggesting that negative reciprocity and punishment can reduce efficiency. The largest returns would come from an intervention that drives players away from negative and toward positive reciprocity.",
        "references": [
            {
                "arxivId": "2102.01587",
                "title": "Games on Endogenous Networks",
                "abstract": "We study network games in which players choose both the partners with whom they associate and an action level (e.g., effort) that creates spillovers for those partners. We introduce a framework and two solution concepts, extending standard approaches for analyzing each choice in isolation: Nash equilibrium in actions and pairwise stability in links. Our main results show that, under suitable order conditions on incentives, stable networks take simple forms. The first condition concerns whether links create positive or negative payoff spillovers. The second concerns whether actions are strategic complements to links, or strategic substitutes. Together, these conditions yield a taxonomy of the relationship between network structure and economic primitives organized around two network architectures: ordered overlapping cliques and nested split graphs. We apply our model to understand the consequences of competition for status, to microfound matching models that assume clique formation, and to interpret empirical findings that highlight unintended consequences of group design."
            },
            {
                "arxivId": "2006.16516",
                "title": "Mixed Logit Models and Network Formation",
                "abstract": "\n The study of network formation is pervasive in economics, sociology, and many other fields. In this article, we model network formation as a \u2018choice\u2019 that is made by nodes of a network to connect to other nodes. We study these \u2018choices\u2019 using discrete-choice models, in which agents choose between two or more discrete alternatives. We employ the \u2018repeated-choice\u2019 (RC) model to study network formation. We argue that the RC model overcomes important limitations of the multinomial logit (MNL) model, which gives one framework for studying network formation, and that it is well-suited to study network formation. We also illustrate how to use the RC model to accurately study network formation using both synthetic and real-world networks. Using edge-independent synthetic networks, we also compare the performance of the MNL model and the RC model. We find that the RC model estimates the data-generation process of our synthetic networks more accurately than the MNL model. Using a patent citation network, which forms sequentially, we present a case study of a qualitatively interesting scenario\u2014the fact that new patents are more likely to cite older, more cited, and similar patents\u2014for which employing the RC model yields interesting insights."
            },
            {
                "arxivId": "1911.06872",
                "title": "Innovation and Strategic Network Formation",
                "abstract": "\n We study a model of innovation with a large number of firms that create new technologies by combining several discrete ideas. These ideas are created via private investment and spread between firms. Firms face a choice between secrecy, which protects existing intellectual property, and openness, which facilitates learning from others. Their decisions determine interaction rates between firms, and these interaction rates enter our model as link probabilities in a learning network. Higher interaction rates impose both positive and negative externalities, as there is more learning but also more competition. We show that the equilibrium learning network is at a critical threshold between sparse and dense networks. At equilibrium, the positive externality from interaction dominates: the innovation rate and welfare would be dramatically higher if the network were denser. So there are large returns to increasing interaction rates above the critical threshold. Nevertheless, several natural types of interventions fail to move the equilibrium away from criticality. One effective policy solution is to introduce informational intermediaries, such as public innovators who do not have incentives to be secretive. These intermediaries can facilitate a high-innovation equilibrium by transmitting ideas from one private firm to another."
            },
            {
                "arxivId": "1901.00373",
                "title": "Nash Equilibria on (Un)Stable Networks",
                "abstract": "In response to a change, individuals may choose to follow the responses of their friends or, alternatively, to change their friends. To model these decisions, consider a game where players choose their behaviors and friendships. In equilibrium, players internalize the need for consensus in forming friendships and choose their optimal strategies on subsets of \n k players\u2014a form of bounded rationality. The \n k\u2010player consensual dynamic delivers a probabilistic ranking of a game's equilibria, and via a varying \n k, facilitates estimation of such games.\n \n Applying the model to adolescents' smoking suggests that: (a) the response of the friendship network to changes in tobacco price amplifies the intended effect of price changes on smoking, (b) racial desegregation of high schools decreases the overall smoking prevalence, (c) peer effect complementarities are substantially stronger between smokers compared to between nonsmokers."
            },
            {
                "arxivId": "1811.05008",
                "title": "Choosing to Grow a Graph: Modeling Network Formation as Discrete Choice",
                "abstract": "We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a \u201cchoice\u201d made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age."
            },
            {
                "arxivId": "1709.10402",
                "title": "Distributions of Centrality on Networks",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2203.09000",
        "category": "econ",
        "title": "Lorenz map, inequality ordering and curves based on multidimensional rearrangements",
        "abstract": "We propose a multivariate extension of the Lorenz curve based on multivariate rearrangements of optimal transport theory. We define a vector Lorenz map as the integral of the vector quantile map associated with a multivariate resource allocation. Each component of the Lorenz map is the cumulative share of each resource, as in the traditional univariate case. The pointwise ordering of such Lorenz maps defines a new multivariate majorization order, which is equivalent to preference by any social planner with inequality averse multivariate rank dependent social evaluation functional. We define a family of multi-attribute Gini index and complete ordering based on the Lorenz map. We propose the level sets of an Inverse Lorenz Function as a practical tool to visualize and compare inequality in two dimensions, and apply it to income-wealth inequality in the United States between 1989 and 2022.",
        "references": [
            {
                "arxivId": "2211.10822",
                "title": "Center-Outward Multiple-Output Lorenz Curves and Gini Indices a measure transportation approach",
                "abstract": "Based on measure transportation ideas and the related concepts of center-outward quantile functions, we propose multiple-output center-outward generalizations of the traditional univariate concepts of Lorenz and concentration functions, and the related Gini and Kakwani coefficients. These new concepts have a natural interpretation, either in terms of contributions of central (\"middle-class\") regions to the expectation of some variable of interest, or in terms of the physical notions of work and energy, which sheds new light on the nature of economic and social inequalities. Importantly, the proposed concepts pave the way to statistically sound definitions, based on multiple variables, of quantiles and quantile regions, and the concept of\"middle class,\"of high relevance in various socio-economic contexts."
            },
            {
                "arxivId": "2101.04748",
                "title": "A multivariate extension of the Lorenz curve based on copulas and a related multivariate Gini coefficient",
                "abstract": null
            },
            {
                "arxivId": "1908.11533",
                "title": "A Newton algorithm for semi-discrete optimal transport with storage fees and quantitative convergence of cells",
                "abstract": "We introduce and prove convergence of a damped Newton algorithm to approximate solutions of the semi-discrete optimal transport problem with storage fees, corresponding to a problem with hard capacity constraints. This is a variant of the optimal transport problem arising in queue penalization problems, and has applications to data clustering. Our result is novel as it does not require any connectedness assumptions on the support of the source measure, in contrast with previous results. Furthermore we find some stability results of the associated Laguerre cells. All of our results come with quantitative rates"
            },
            {
                "arxivId": "1412.8434",
                "title": "Monge-Kantorovich Depth, Quantiles, Ranks and Signs",
                "abstract": "We propose new concepts of statistical depth, multivariate quantiles,ranks and signs, based on canonical transportation maps between a distributionof interest on IRd and a reference distribution on the d-dimensionalunit ball. The new depth concept, called Monge-Kantorovich depth, specializesto halfspace depth in the case of elliptical distributions, but, for more generaldistributions, differs from the latter in the ability for its contours to account fornon convex features of the distribution of interest. We propose empirical counterpartsto the population versions of those Monge-Kantorovich depth contours,quantiles, ranks and signs, and show their consistency by establishing a uniformconvergence property for transport maps, which is of independent interest."
            },
            {
                "arxivId": "1409.1279",
                "title": "A Numerical Algorithm for L2 Semi-Discrete Optimal Transport in 3D",
                "abstract": "This paper introduces a numerical algorithm to compute the L2 optimal transport map between two measures \u00b5 and \u03bd, where \u00b5 derives from a density \u03c1 defined as a piecewise linear function (supported by a tetrahedral mesh), and where \u03bd is a sum of Dirac masses. I first give an elementary presentation of some known results on optimal transport and then observe a relation with another problem (optimal sampling). This relation gives simple arguments to study the objective functions that characterize both problems. I then propose a practical algorithm to compute the optimal transport map between a piecewise linear density and a sum of Dirac masses in 3D. In this semi-discrete setting, Aurenhammer et.al [8th Symposium on Computational Geometry conf. proc., ACM (1992)] showed that the optimal transport map is determined by the weights of a power diagram. The optimal weights are computed by minimizing a convex objective function with a quasi-Newton method. To evaluate the value and gradient of this objective function, I propose an efficient and robust algorithm, that computes at each iteration the intersection between a power diagram and the tetrahedral mesh that defines the measure \u00b5. The numerical algorithm is experimented and evaluated on several datasets, with up to hundred thousands tetrahedra and one million Dirac masses. Resume. Cet article decrit un algorithme numerique pour calculer l'application de transport optimal L2 entre deux mesures \u00b5 et \u03bd, o` u \u00b5 derive d'une densite \u03c1 lineaire par morceaux (supportee par un maillage tetraedrique), et o` u \u03bd est une somme de masses de Dirac. Je donne tout d'abord une presentation elementaire de quelques resultats connus sur le transport optimal , et observe ensuite une relation avec un autreprobi eme (l'\u00b4 echantillonage optimal). Cette relation fournit des arguments simples pour etudier les fonctions objectifs caracterisant les deuxprobi emes. Je propose ensuite un algorithme pratique pour calculer le transport optimal entre une densite lineaire par morceaux et une somme de masses de Dirac en 3D. Dans ce cas semi-discret, Auren-hammer et.al [8th Symposium on Computational Geometry conf. proc., ACM (1992)] ont montre que l'application de transport optimal est determinee par les poids d'un diagramme de puissance. Les poids optimaux sont calcules en minimisant une fonction objectif convexe a l'aide d'une methode quasi-Newton. Pour evaluer cette fonction objectif et son gradient, je propose un algorithme efficace et robuste, qui calcule a chaque iteration l'intersection entre un diagramme de puissance et le maillage tetrahedrique qui definit la mesure \u00b5. L'algorithme numerique est experimente et evalue sur plusieurs jeux de donnees , comportant jusqu'` a plusieurs centaines de milliers detetra edres et un million de masses de Dirac."
            },
            {
                "arxivId": "2102.06075",
                "title": "Local Utility and Multivariate Risk Aversion",
                "abstract": "We revisit Machina's local utility as a tool to analyze attitudes to multivariate risks. Using martingale embedding techniques, we show that for non-expected utility maximizers choosing between multivariate prospects, aversion to multivariate mean preserving increases in risk is equivalent to the concavity of the local utility functions, thereby generalizing Machina's result in [18]. To analyze comparative risk attitudes within the multivariate extension of rank dependent expected utility of [10], we extend Quiggin's monotone mean and utility preserving increases in risk and show that the useful characterization given in [17] still holds in the multivariate case."
            },
            {
                "arxivId": "2102.04175",
                "title": "COMONOTONIC MEASURES OF MULTIVARIATE RISKS",
                "abstract": "We propose a multivariate extension of a well\u2010known characterization by S. Kusuoka of regular and coherent risk measures as maximal correlation functionals. This involves an extension of the notion of comonotonicity to random vectors through generalized quantile functions. Moreover, we propose to replace the current law invariance, subadditivity, and comonotonicity axioms by an equivalent property we call\u2002strong coherence\u2002and that we argue has more natural economic interpretation. Finally, we reformulate the computation of regular and coherent risk measures as an optimal transportation problem, for which we provide an algorithm and implementation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2208.04813",
        "category": "econ",
        "title": "The explosive value of the networks",
        "abstract": null,
        "references": [
            {
                "arxivId": "1607.01032",
                "title": "Echo Chambers: Emotional Contagion and Group Polarization on Facebook",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2401.13057",
        "category": "econ",
        "title": "Inference under partial identification with minimax test statistics",
        "abstract": "We provide a means of computing and estimating the asymptotic distributions of statistics based on an outer minimization of an inner maximization. Such test statistics, which arise frequently in moment models, are of special interest in providing hypothesis tests under partial identification. Under general conditions, we provide an asymptotic characterization of such test statistics using the minimax theorem, and a means of computing critical values using the bootstrap. Making some light regularity assumptions, our results augment several asymptotic approximations that have been provided for partially identified hypothesis tests, and extend them by mitigating their dependence on local linear approximations of the parameter space. These asymptotic results are generally simple to state and straightforward to compute (esp.\\ adversarially).",
        "references": [
            {
                "arxivId": "2007.06169",
                "title": "An Adversarial Approach to Structural Estimation",
                "abstract": "We propose a new simulation\u2010based estimation method, adversarial estimation, for structural models. The estimator is formulated as the solution to a minimax problem between a generator (which generates simulated observations using the structural model) and a discriminator (which classifies whether an observation is simulated). The discriminator maximizes the accuracy of its classification while the generator minimizes it. We show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. We advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence."
            },
            {
                "arxivId": "1910.07689",
                "title": "A Projection Framework for Testing Shape Restrictions That Form Convex Cones",
                "abstract": "This paper develops a uniformly valid and asymptotically nonconservative test based on projection for a class of shape restrictions. The key insight we exploit is that these restrictions form convex cones, a simple and yet elegant structure that has been barely harnessed in the literature. Based on a monotonicity property afforded by such a geometric structure, we construct a bootstrap procedure that, unlike many studies in nonstandard settings, dispenses with estimation of local parameter spaces, and the critical values are obtained in a way as simple as computing the test statistic. Moreover, by appealing to strong approximations, our framework accommodates nonparametric regression models as well as distributional/density\u2010related and structural settings. Since the test entails a tuning parameter (due to the nonstandard nature of the problem), we propose a data\u2010driven choice and prove its validity. Monte Carlo simulations confirm that our test works well."
            },
            {
                "arxivId": "1509.06311",
                "title": "Constrained Conditional Moment Restriction Models",
                "abstract": "Shape restrictions have played a central role in economics as both testable implications of theory and sufficient conditions for obtaining informative counterfactual predictions. In this paper, we provide a general procedure for inference under shape restrictions in identified and partially identified models defined by conditional moment restrictions. Our test statistics and proposed inference methods are based on the minimum of the generalized method of moments (GMM) objective function with and without shape restrictions. Uniformly valid critical values are obtained through a bootstrap procedure that approximates a subset of the true local parameter space. In an empirical analysis of the effect of childbearing on female labor supply, we show that employing shape restrictions in linear instrumental variables (IV) models can lead to shorter confidence regions for both local and average treatment effects. Other applications we discuss include inference for the variability of quantile IV treatment effects and for bounds on average equivalent variation in a demand model with general heterogeneity."
            },
            {
                "arxivId": "0907.3503",
                "title": "Intersection bounds: estimation and inference",
                "abstract": "We develop a practical and novel method for inference on intersection bounds, namely bounds defined by either the infimum or supremum of a parametric or nonparametric function, or equivalently, the value of a linear programming problem with a potentially infinite constraint set. Our approach is especially convenient in models comprised of a continuum of inequalities that are separable in parameters, and also applies to models with inequalities that are non-separable in parameters. Since analog estimators for intersection bounds can be severely biased in finite samples, routinely underestimating the length of the identified set, we also offer a (downward/upward) median unbiased estimator of these (upper/lower) bounds as a natural by-product of our inferential procedure. Furthermore, our method appears to be the first and currently only method for inference in nonparametric models with a continuum of inequalities. We develop asymptotic theory for our method based on the strong approximation of a sequence of studentized empirical processes by a sequence of Gaussian or other pivotal processes. We provide conditions for the use of nonparametric kernel and series estimators, including a novel result that establishes strong approximation for general series estimators, which may be of independent interest. We illustrate the usefulness of our method with Monte Carlo experiments and an empirical example."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2401.16412",
        "category": "econ",
        "title": "Learning to Manipulate under Limited Information",
        "abstract": "By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained over 70,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with full information. For the two probability models for elections that we use, the overall least manipulable of the 8 methods we study are Condorcet methods, namely Minimax and Split Cycle.",
        "references": [
            {
                "arxivId": "2108.02768",
                "title": "Learning to Elect",
                "abstract": "Voting systems have a wide range of applications including recommender systems, web search, product design and elections. Limited by the lack of general-purpose analytical tools, it is difficult to hand-engineer desirable voting rules for each use case. For this reason, it is appealing to automatically discover voting rules geared towards each scenario. In this paper, we show that set-input neural network architectures such as Set Transformers, fully-connected graph networks and DeepSets are both theoretically and empirically well-suited for learning voting rules. In particular, we show that these network models can not only mimic a number of existing voting rules to compelling accuracy -- both position-based (such as Plurality and Borda) and comparison-based (such as Kemeny, Copeland and Maximin) -- but also discover near-optimal voting rules that maximize different social welfare functions. Furthermore, the learned voting rules generalize well to different voter utility distributions and election sizes unseen during training."
            },
            {
                "arxivId": "2010.14495",
                "title": "Are wider nets better given the same number of parameters?",
                "abstract": "Empirical studies demonstrate that the performance of neural networks improves with increasing number of parameters. In most of these studies, the number of parameters is increased by increasing the network width. This begs the question: Is the observed improvement due to the larger number of parameters, or is it due to the larger width itself? We compare different ways of increasing model width while keeping the number of parameters constant. We show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the number of weights is secondary, as long as trainability is ensured. As a step towards understanding this effect, we analyze these models in the framework of Gaussian Process kernels. We find that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance."
            },
            {
                "arxivId": "1909.10492",
                "title": "Modeling People's Voting Behavior with Poll Information",
                "abstract": "Despite the prevalence of voting systems in the real world there is no consensus among researchers of how people vote strategically, even in simple voting settings. This paper addresses this gap by comparing different approaches that have been used to model strategic voting, including expected utility maximization, heuristic decision-making, and bounded rationality models. The models are applied to data collected from hundreds of people in controlled voting experiments, where people vote after observing non-binding poll information. We introduce a new voting model, the Attainability-Utility (AU) heuristic, which weighs the popularity of a candidate according to the poll, with the utility of the candidate to the voter. We argue that the AU model is cognitively plausible, and show that it is able to predict people's voting behavior significantly better than other models from the literature. It was almost at par with (and sometimes better than) a machine learning algorithm that uses substantially more information. Our results provide new insights into the strategic considerations of voters, that undermine the prevalent assumptions of much theoretical work in social choice."
            },
            {
                "arxivId": "1901.09791",
                "title": "Practical Algorithms for Multi-Stage Voting Rules with Parallel Universes Tiebreaking",
                "abstract": "STV and ranked pairs (RP) are two well-studied voting rules for group decision-making. They proceed in multiple rounds, and are affected by how ties are broken in each round. However, the literature is surprisingly vague about how ties should be broken. We propose the first algorithms for computing the set of alternatives that are winners under some tiebreaking mechanism under STV and RP, which is also known as parallel-universes tiebreaking (PUT). Unfortunately, PUT-winners are NP-complete to compute under STV and RP, and standard search algorithms from AI do not apply. We propose multiple DFS-based algorithms along with pruning strategies, heuristics, sampling and machine learning to prioritize search direction to significantly improve the performance. We also propose novel ILP formulations for PUT-winners under STV and RP, respectively. Experiments on synthetic and realworld data show that our algorithms are overall faster than ILP."
            },
            {
                "arxivId": "1712.00409",
                "title": "Deep Learning Scaling is Predictable, Empirically",
                "abstract": "Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. \nThis paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling."
            },
            {
                "arxivId": "1611.03530",
                "title": "Understanding deep learning requires rethinking generalization",
                "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \nWe interpret our experimental findings by comparison with traditional models."
            },
            {
                "arxivId": "1404.4688",
                "title": "A local-dominance theory of voting equilibria",
                "abstract": "We suggest a new model for strategic voting based on local dominance, where voters consider a set of possible outcomes without assigning probabilities to them. We prove that voting equilibria under the Plurality rule exist for a broad class of local dominance relations. Furthermore, we show that local dominance-based dynamics quickly converge to an equilibrium if voters start from the truthful state, and we provide weaker convergence guarantees in more general settings. Using extensive simulations of strategic voting on generated and real profiles, we show that emerging equilibria replicate widely known patterns of human voting behavior such as Duverger's law, and that they generally improve the quality of the winner compared to non-strategic voting."
            },
            {
                "arxivId": "1210.4895",
                "title": "Bayesian Vote Manipulation: Optimal Strategies and Impact on Welfare",
                "abstract": "Most analyses of manipulation of voting schemes have adopted two assumptions that greatly diminish their practical import. First, it is usually assumed that the manipulators have full knowledge of the votes of the nonmanipulating agents. Second, analysis tends to focus on the probability of manipulation rather than its impact on the social choice objective (e.g., social welfare). We relax both of these assumptions by analyzing optimal Bayesian manipulation strategies when the manipulators have only partial probabilistic information about nonmanipulator votes, and assessing the expected loss in social welfare (in the broad sense of the term). We present a general optimization framework for the derivation of optimal manipulation strategies given arbitrary voting rules and distributions over preferences. We theoretically and empirically analyze the optimal manipulability of some popular voting rules using distributions and real data sets that go well beyond the common, but unrealistic, impartial culture assumption. We also shed light on the stark difference between the loss in social welfare and the probability of manipulation by showing that even when manipulation is likely, impact to social welfare is slight (and often negligible)."
            },
            {
                "arxivId": "1106.5448",
                "title": "Dominating Manipulations in Voting with Partial Information",
                "abstract": "\n \n We consider manipulation problems when the manipulator only has partial information about the votes of the non-manipulators. Such partial information is described by an {\\em information set}, which is the set of profiles of the non-manipulators that are indistinguishable to the manipulator. Given such an information set, a {\\em dominating manipulation} is a non-truthful vote that the manipulator can cast which makes the winner at least as preferable (and sometimes more preferable) as the winner when the manipulator votes truthfully. When the manipulator has full information, computing whether or not there exists a dominating manipulation is in P for many common voting rules (by known results). We show that when the manipulator has no information, there is no dominating manipulation for many common voting rules. When the manipulator's information is represented by partial orders and only a small portion of the preferences are unknown, computing a dominating manipulation is NP-hard for many common voting rules. Our results thus throw light on whether we can prevent strategic behavior by limiting information about the votes of other voters.\n \n"
            },
            {
                "arxivId": "1106.5312",
                "title": "Manipulation of Nanson's and Baldwin's Rules",
                "abstract": "\n \n Nanson's and Baldwin's voting rules selecta winner by successively eliminatingcandidates with low Borda scores. We showthat these rules have a number of desirablecomputational properties. In particular,with unweighted votes, it isNP-hard to manipulate either rule with one manipulator, whilstwith weighted votes, it isNP-hard to manipulate either rule with a small number ofcandidates and a coalition of manipulators.As only a couple of other voting rulesare known to be NP-hard to manipulatewith a single manipulator, Nanson'sand Baldwin's rules appearto be particularly resistant to manipulation from a theoretical perspective.We also propose a number of approximation methodsfor manipulating these two rules.Experiments demonstrate that both rules areoften difficult to manipulate in practice.These results suggest that elimination stylevoting rules deserve further study.\n \n"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2402.12838",
        "category": "econ",
        "title": "Extending the Scope of Inference About Predictive Ability to Machine Learning Methods",
        "abstract": "Though out-of-sample forecast evaluation is systematically employed with modern machine learning methods and there exists a well-established classic inference theory for predictive ability, see, e.g., West (1996, Asymptotic Inference About Predictive Ability, Econometrica, 64, 1067-1084), such theory is not directly applicable to modern machine learners such as the Lasso in the high dimensional setting. We investigate under which conditions such extensions are possible. Two key properties for standard out-of-sample asymptotic inference to be valid with machine learning are (i) a zero-mean condition for the score of the prediction loss function; and (ii) a fast rate of convergence for the machine learner. Monte Carlo simulations confirm our theoretical findings. We recommend a small out-of-sample vs in-sample size ratio for accurate finite sample inferences with machine learning. We illustrate the wide applicability of our results with a new out-of-sample test for the Martingale Difference Hypothesis (MDH). We obtain the asymptotic null distribution of our test and use it to evaluate the MDH of some major exchange rates at daily and higher frequencies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2403.07236",
        "category": "econ",
        "title": "Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model",
        "abstract": "It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary while imposing very few restrictions on the underlying data generating process. I construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available. This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified or seek out data aggregated at the finest level possible.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2404.09309",
        "category": "econ",
        "title": "Julia as a universal platform for statistical software development",
        "abstract": "Like Python and Java, which are integrated into Stata, Julia is a free programming language that runs on all major operating systems. The julia package links Stata to Julia as well. Users can transfer data between Stata and Julia at high speed, issue Julia commands from Stata to analyze and plot, and pass results back to Stata. Julia's econometric software ecosystem is not as mature as Stata's or R's, or even Python's. But Julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms. The boottest program for wild bootstrap-based inference (Roodman et al. 2019) can call a Julia back end for a 33-50% speed-up, even as the R package fwildclusterboot (Fischer and Roodman 2021) uses the same back end for inference after instrumental variables estimation. reghdfejl mimics reghdfe (Correia 2016) in fitting linear models with high-dimensional fixed effects but calls an independently developed Julia package for tenfold acceleration on hard problems. reghdfejl also supports nonlinear models--preliminarily, as the Julia package for that purpose matures.",
        "references": [
            {
                "arxivId": "1903.01690",
                "title": "Fast Poisson estimation with high-dimensional fixed effects",
                "abstract": "In this article, we present ppmlhdfe, a new command for estimation of (pseudo-)Poisson regression models with multiple high-dimensional fixed effects (HDFE). Estimation is implemented using a modified version of the iteratively reweighted least-squares algorithm that allows for fast estimation in the presence of HDFE. Because the code is built around the reghdfe package (Correia, 2014, Statistical Software Components S457874, Department of Economics, Boston College), it has similar syntax, supports many of the same functionalities, and benefits from reghdfe\u2018s fast convergence properties for computing high-dimensional leastsquares problems. Performance is further enhanced by some new techniques we introduce for accelerating HDFE iteratively reweighted least-squares estimation specifically. ppmlhdfe also implements a novel and more robust approach to check for the existence of (pseudo)maximum likelihood estimates."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2404.10006",
        "category": "econ",
        "title": "Harmony in the Australian Domain Space",
        "abstract": "In this paper we use for the first time a systematic approach in the study of harmonic centrality at a Web domain level, and gather a number of significant new findings about the Australian web. In particular, we explore the relationship between economic diversity at the firm level and the structure of the Web within the Australian domain space, using harmonic centrality as the main structural feature. The distribution of harmonic centrality values is analyzed over time, and we find that the distributions exhibit a consistent pattern across the different years. The observed distribution is well captured by a partition of the domain space into six clusters; the temporal movement of domain names across these six positions yields insights into the Australian Domain Space and exhibits correlations with other non-structural characteristics. From a more global perspective, we find a significant correlation between the median harmonic centrality of all domains in each OECD country and one measure of global trust, the WJP Rule of Law Index. Further investigation demonstrates that 35 countries in OECD share similar harmonic centrality distributions. The observed homogeneity in distribution presents a compelling avenue for exploration, potentially unveiling critical corporate, regional, or national insights.",
        "references": [
            {
                "arxivId": "2207.06218",
                "title": "Monotonicity in Undirected Networks",
                "abstract": "\n Is it always beneficial to create a new relationship (have a new follower/friend) in a social network? This question can be formally stated as a property of the centrality measure that defines the importance of the actors of the network. Score monotonicity means that adding an arc increases the centrality score of the target of the arc; rank monotonicity means that adding an arc improves the importance of the target of the arc relatively to the remaining nodes. It is known that most centralities are both score and rank monotone on directed, strongly connected graphs. In this paper, we study the problem of score and rank monotonicity for classical centrality measures in the case of undirected networks: in this case, we require that score, or relative importance, improves at both endpoints of the new edge. We show that, surprisingly, the situation in the undirected case is very different, and in particular that closeness, harmonic centrality, betweenness, eigenvector centrality, Seeley\u2019s index, Katz\u2019s index, and PageRank are not rank monotone; betweenness and PageRank are not even score monotone. In other words, while it is always a good thing to get a new follower, it is not always beneficial to get a new friend."
            },
            {
                "arxivId": "2204.04381",
                "title": "Harmonic Centralization of Some Graph Families",
                "abstract": "Centrality describes the importance of nodes in a graph and is modeled by various measures. Its global analogue, called centralization, is a general formula for calculating a graph-level centrality score based on the node-level centrality measure. The latter enables us to compare graphs based on the extent to which the connections of a given network are concentrated on a single vertex or group of vertices. One of the measures of centrality in social network analysis is harmonic centrality. It sums the inverse of the geodesic distances of each node to other nodes where it is 0 if there is no path from one node to another, with the sum normalized by dividing it by $m-1$, where $m$ is the number of nodes of the graph. In this paper, we present some results regarding the harmonic centralization of some important families of graphs with the hope that formulas generated herein will be of use when one determines the harmonic centralization of more complex graphs."
            },
            {
                "arxivId": "2003.07049",
                "title": "Evolution of diversity and dominance of companies in online activity",
                "abstract": "Ever since the web began, the number of websites has been growing exponentially. These websites cover an ever-increasing range of online services that fill a variety of social and economic functions across a growing range of industries. Yet the networked nature of the web, combined with the economics of preferential attachment, increasing returns and global trade, suggest that over the long run a small number of competitive giants are likely to dominate each functional market segment, such as search, retail and social media. Here we perform a large scale longitudinal study to quantify the distribution of attention given in the online environment to competing organisations. In two large online social media datasets, containing more than 10 billion posts and spanning more than a decade, we tally the volume of external links posted towards the organisations\u2019 main domain name as a proxy for the online attention they receive. We also use the Common Crawl dataset\u2014which contains the linkage patterns between more than a billion different websites\u2014to study the patterns of link concentration over the past three years across the entire web. Lastly, we showcase the linking between economic, financial and market data by exploring the relationships between online attention on social media and the growth in enterprise value in the electric carmaker Tesla. Our analysis shows that despite the fact that we observe consistent growth in all the macro indicators\u2014the total amount of online attention, in the number of organisations with an online presence, and in the functions they perform\u2014we also observe that a smaller number of organisations account for an ever-increasing proportion of total user attention, usually with one large player dominating each function. These results highlight how evolution of the online economy involves innovation, diversity, and then competitive dominance."
            },
            {
                "arxivId": "1603.02754",
                "title": "XGBoost: A Scalable Tree Boosting System",
                "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
            },
            {
                "arxivId": "1308.2140",
                "title": "Axioms for Centrality",
                "abstract": "Abstract Given a social network, which of its nodes are more central? This question has been asked many times in sociology, psychology, and computer science, and a whole plethora of centrality measures (a.k.a. centrality indices, or rankings) were proposed to account for the importance of the nodes of a network. In this study, we try to provide a mathematically sound survey of the most important classic centrality measures known from the literature and propose an axiomatic approach to establish whether they are actually doing what they have been designed to do. Our axioms suggest some simple, basic properties that a centrality measure should exhibit. Surprisingly, only a new simple measure based on distances, harmonic centrality, turns out to satisfy all axioms; essentially, harmonic centrality is a correction to Bavelas\u2019s classic closeness centrality [Bavelas 50] designed to take unreachable nodes into account in a natural way. As a sanity check, we examine in turn each measure under the lens of information retrieval, leveraging state-of-the-art knowledge in the discipline to measure the effectiveness of the various indices in locating webpages that are relevant to a query. Although there are some examples of such comparisons in the literature, here, for the first time, we also take into consideration centrality measures based on distances, such as closeness, in an information-retrieval setting. The results closely match the data we gathered using our axiomatic approach. Our results suggest that centrality measures based on distances, which in recent years have been neglected in information retrieval in favor of spectral centrality measures, do provide high-quality signals; moreover, harmonic centrality pops up as an excellent general-purpose centrality index for arbitrary directed graphs."
            },
            {
                "arxivId": "cond-mat/0106096",
                "title": "Statistical mechanics of complex networks",
                "abstract": "The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them. \nTraditionally complex networks have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network. \nThe scale-free topology of real networks has very important consequences on their functioning. For example, we have discovered that scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other. \nThe non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2404.10090",
        "category": "econ",
        "title": "Intergenerational Insurance",
        "abstract": "How should successive generations insure each other when the young can default on previously promised transfers to the old? This paper studies intergenerational insurance that maximizes the expected discounted utility of all generations subject to participation constraints for each generation. If complete insurance is unattainable, the optimal intergenerational insurance is history-dependent even when the environment is stationary. The risk from a generational shock is spread into the future, with periodic resetting. Interpreting intergenerational insurance in terms of debt, the fiscal reaction function is nonlinear and the risk premium on debt is lower than the risk premium with complete insurance.",
        "references": [
            {
                "arxivId": "1409.2286",
                "title": "Stochastic stability of monotone economies in regenerative environments",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-17.json",
        "arxivId": "2404.10111",
        "category": "econ",
        "title": "From Predictive Algorithms to Automatic Generation of Anomalies",
        "abstract": "Machine learning algorithms can find predictive signals that researchers fail to notice; yet they are notoriously hard-to-interpret. How can we extract theoretical insights from these black boxes? History provides a clue. Facing a similar problem -- how to extract theoretical insights from their intuitions -- researchers often turned to ``anomalies:'' constructed examples that highlight flaws in an existing theory and spur the development of new ones. Canonical examples include the Allais paradox and the Kahneman-Tversky choice experiments for expected utility theory. We suggest anomalies can extract theoretical insights from black box predictive algorithms. We develop procedures to automatically generate anomalies for an existing theory when given a predictive algorithm. We cast anomaly generation as an adversarial game between a theory and a falsifier, the solutions to which are anomalies: instances where the black box algorithm predicts - were we to collect data - we would likely observe violations of the theory. As an illustration, we generate anomalies for expected utility theory using a large, publicly available dataset on real lottery choices. Based on an estimated neural network that predicts lottery choices, our procedures recover known anomalies and discover new ones for expected utility theory. In incentivized experiments, subjects violate expected utility theory on these algorithmically generated anomalies; moreover, the violation rates are similar to observed rates for the Allais paradox and Common ratio effect.",
        "references": [
            {
                "arxivId": "2202.04796",
                "title": "The Transfer Performance of Economic Models",
                "abstract": "Economists often estimate models using data from a particular domain, e.g. estimating risk preferences in a particular subject pool or for a specific class of lotteries. Whether a model's predictions extrapolate well across domains depends on whether the estimated model has captured generalizable structure. We provide a tractable formulation for this\"out-of-domain\"prediction problem and define the transfer error of a model based on how well it performs on data from a new domain. We derive finite-sample forecast intervals that are guaranteed to cover realized transfer errors with a user-selected probability when domains are iid, and use these intervals to compare the transferability of economic models and black box algorithms for predicting certainty equivalents. We find that in this application, the black box algorithms we consider outperform standard economic models when estimated and tested on data from the same domain, but the economic models generalize across domains better than the black-box algorithms do."
            },
            {
                "arxivId": "2111.13786",
                "title": "Learning from learning machines: a new generation of AI technology to meet the needs of science",
                "abstract": "We outline emerging opportunities and challenges to enhance the utility of AI for scientific discovery. The distinct goals of AI for industry versus the goals of AI for science create tension between identifying patterns in data versus discovering patterns in the world from data. If we address the fundamental challenges associated with\"bridging the gap\"between domain-driven scientific models and data-driven AI learning machines, then we expect that these AI models can transform hypothesis generation, scientific discovery, and the scientific process itself."
            },
            {
                "arxivId": "2103.15348",
                "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis",
                "abstract": null
            },
            {
                "arxivId": "2007.09213",
                "title": "How Flexible is that Functional Form? Quantifying the Restrictiveness of Theories",
                "abstract": "\n We propose a restrictiveness measure for economic models based on how well they fit pre-defined synthetic data. This measure, together with a measure for how well the model fits real data, outlines a Pareto frontier, where models that rule out more regularities, yet capture the regularities that are present in real data, are preferred. To illustrate our approach, we evaluate the restrictiveness of models in two laboratory settings\u2014certainty equivalents and initial play\u2014and one field setting\u2014takeup of microfinance in Indian villages. The restrictiveness measure reveals insights about each, including that some economic models with only a few parameters are very flexible."
            },
            {
                "arxivId": "2006.08141",
                "title": "Nonconvex Min-Max Optimization: Applications, Challenges, and Recent Theoretical Advances",
                "abstract": "The min-max optimization problem, also known as the saddle point problem, is a classical optimization problem that is also studied in the context of zero-sum games. Given a class of objective functions, the goal is to find a value for the argument that leads to a small objective value even for the worst-case function in the given class. Min-max optimization problems have recently become very popular in a wide range of signal and data processing applications, such as fair beamforming, training generative adversarial networks (GANs), and robust machine learning (ML), to just name a few."
            },
            {
                "arxivId": "2003.11755",
                "title": "A Survey of Deep Learning for Scientific Discovery",
                "abstract": "Over the past few years, we have seen fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. At the same time, the amount of data collected in a wide array of scientific domains is dramatically increasing in both size and complexity. Taken together, this suggests many exciting opportunities for deep learning applications in scientific settings. But a significant challenge to this is simply knowing where to start. The sheer breadth and diversity of different deep learning techniques makes it difficult to determine what scientific problems might be most amenable to these methods, or which specific combination of methods might offer the most promising first approach. In this survey, we focus on addressing this central issue, providing an overview of many widely used deep learning models, spanning visual, sequential and graph structured data, associated tasks and different training methods, along with techniques to use deep learning with less data and better interpret these complex models --- two central considerations for many scientific use cases. We also include overviews of the full design process, implementation tips, and links to a plethora of tutorials, research summaries and open-sourced deep learning pipelines and pretrained models, developed by the community. We hope that this survey will help accelerate the use of deep learning across different scientific domains."
            },
            {
                "arxivId": "1910.07022",
                "title": "Measuring the Completeness of Economic Models",
                "abstract": "Economic models are evaluated by testing the correctness of their predictions. We suggest an additional measure, \u201ccompleteness\u201d: the fraction of the predictable variation in the data that the model captures. We calculate the completeness of prominent models in three problems from experimental economics: assigning certainty equivalents to lotteries, predicting initial play in games, and predicting human generation of random sequences. The completeness measure reveals new insights about these models, including how much room there is for improving their predictions."
            },
            {
                "arxivId": "1802.02988",
                "title": "Stochastic subgradient method converges at the rate O(k-1/4) on weakly convex functions",
                "abstract": "We prove that the projected stochastic subgradient method, applied to a weakly convex problem, drives the gradient of the Moreau envelope to zero at the rate $O(k^{-1/4})$."
            },
            {
                "arxivId": "1801.00553",
                "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey",
                "abstract": "Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction."
            },
            {
                "arxivId": "1706.06083",
                "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
                "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL."
            },
            {
                "arxivId": "1606.04819",
                "title": "Nonparametric Analysis of Random Utility Models",
                "abstract": "This paper develops and implements a nonparametric test of Random Utility Models. The motivating application is to test the null hypothesis that a sample of cross-sectional demand distributions was generated by a population of rational consumers. We test a necessary and sufficient condition for this that does not rely on any restriction on unobserved heterogeneity or the number of goods. We also propose and implement a control function approach to account for endogenous expenditure. An econometric result of independent interest is a test for linear inequality constraints when these are represented as the vertices of a polyhedron rather than its faces. An empirical application to the U.K. Household Expenditure Survey illustrates computational feasibility of the method in demand problems with 5 goods."
            },
            {
                "arxivId": "1510.04342",
                "title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests",
                "abstract": "ABSTRACT Many scientific and engineering challenges\u2014ranging from personalized medicine to customized marketing recommendations\u2014require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman\u2019s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates."
            },
            {
                "arxivId": "1306.0918",
                "title": "Predicting human behavior in unrepeated, simultaneous-move games",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "1908.01109",
        "category": "econ",
        "title": "The Use of Binary Choice Forests to Model and Estimate Discrete Choices",
        "abstract": "We show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. This is confirmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. The framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets.",
        "references": [
            {
                "arxivId": "1705.10883",
                "title": "Optimization of Tree Ensembles",
                "abstract": "From Tree Ensemble Models to Decisions"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2112.04297",
        "category": "econ",
        "title": "Mathematical Model of International Trade and Global Economy",
        "abstract": "This work was partially supported by the Program of Fundamental Research of the Department of Physics and Astronomy of the National Academy of Sciences of Ukraine\"Mathematical models of non equilibrium processes in open systems\"N 0120U100857.",
        "references": [
            {
                "arxivId": "1601.04949",
                "title": "General Equilibrium and Recession Phenomenon",
                "abstract": "The theorems we proved describe the structure of economic equilibrium in the exchange economy model. We have studied the structure of property vectors under given structure of demand vectors at which given price vector is equilibrium one. On this ground, we describe the general structure of the equilibrium state and give characteristic of equilibrium state describing economic recession. The theory developed is applied to explain the state of the economy in some European countries."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2204.01884",
        "category": "econ",
        "title": "Policy Learning with Competing Agents",
        "abstract": "Decision makers often aim to learn a treatment assignment policy under a capacity constraint on the number of agents that they can treat. When agents can respond strategically to such policies, competition arises, complicating estimation of the optimal policy. In this paper, we study capacity-constrained treatment assignment in the presence of such interference. We consider a dynamic model where the decision maker allocates treatments at each time step and heterogeneous agents myopically best respond to the previous treatment assignment policy. When the number of agents is large but finite, we show that the threshold for receiving treatment under a given policy converges to the policy's mean-field equilibrium threshold. Based on this result, we develop a consistent estimator for the policy gradient. In a semi-synthetic experiment with data from the National Education Longitudinal Study of 1988, we demonstrate that this estimator can be used for learning capacity-constrained policies in the presence of strategic behavior.",
        "references": [
            {
                "arxivId": "2203.00124",
                "title": "On classification of strategic agents who can both game and improve",
                "abstract": "In this work, we consider classification of agents who can both game and improve. For example, people wishing to get a loan may be able to take some actions that increase their perceived credit-worthiness and others that also increase their true credit-worthiness. A decision-maker would like to define a classification rule with few false-positives (does not give out many bad loans) while yielding many true positives (giving out many good loans), which includes encouraging agents to improve to become true positives if possible. We consider two models for this problem, a general discrete model and a linear model, and prove algorithmic, learning, and hardness results for each. For the general discrete model, we give an efficient algorithm for the problem of maximizing the number of true positives subject to no false positives, and show how to extend this to a partial-information learning setting. We also show hardness for the problem of maximizing the number of true positives subject to a nonzero bound on the number of false positives, and that this hardness holds even for a finite-point version of our linear model. We also show that maximizing the number of true positives subject to no false positive is NP-hard in our full linear model. We additionally provide an algorithm that determines whether there exists a linear classifier that classifies all agents accurately and causes all improvable agents to become qualified, and give additional results for low-dimensional data."
            },
            {
                "arxivId": "2202.04357",
                "title": "Generalized Strategic Classification and the Case of Aligned Incentives",
                "abstract": "Strategic classification studies learning in settings where self-interested users can strategically modify their features to obtain favorable predictive outcomes. A key working assumption, however, is that\"favorable\"always means\"positive\"; this may be appropriate in some applications (e.g., loan approval), but reduces to a fairly narrow view of what user interests can be. In this work we argue for a broader perspective on what accounts for strategic user behavior, and propose and study a flexible model of generalized strategic classification. Our generalized model subsumes most current models but includes other novel settings; among these, we identify and target one intriguing sub-class of problems in which the interests of users and the system are aligned. This setting reveals a surprising fact: that standard max-margin losses are ill-suited for strategic inputs. Returning to our fully generalized model, we propose a novel max-margin framework for strategic learning that is practical and effective, and which we analyze theoretically. We conclude with a set of experiments that empirically demonstrate the utility of our approach."
            },
            {
                "arxivId": "2109.11647",
                "title": "Treatment Effects in Market Equilibrium",
                "abstract": "When randomized trials are run in a marketplace equilibriated by prices, interference arises. To analyze this, we build a stochastic model of treatment effects in equilibrium. We characterize the average direct (ADE) and indirect treatment effect (AIE) asymptotically. A standard RCT can consistently estimate the ADE, but confidence intervals and AIE estimation require price elasticity estimates, which we provide using a novel experimental design. We define heterogeneous treatment effects and derive an optimal targeting rule that meets an equilibrium stability condition. We illustrate our results using a freelance labor market simulation and data from a cash transfer experiment."
            },
            {
                "arxivId": "2106.12705",
                "title": "Alternative Microfoundations for Strategic Classification",
                "abstract": "When reasoning about strategic behavior in a machine learning context it is tempting to combine standard microfoundations of rational agents with the statistical decision theory underlying classification. In this work, we argue that a direct combination of these standard ingredients leads to brittle solution concepts of limited descriptive and prescriptive value. First, we show that rational agents with perfect information produce discontinuities in the aggregate response to a decision rule that we often do not observe empirically. Second, when any positive fraction of agents is not perfectly strategic, desirable stable points -- where the classifier is optimal for the data it entails -- cease to exist. Third, optimal decision rules under standard microfoundations maximize a measure of negative externality known as social burden within a broad class of possible assumptions about agent behavior. Recognizing these limitations we explore alternatives to standard microfoundations for binary classification. We start by describing a set of desiderata that help navigate the space of possible assumptions about how agents respond to a decision rule. In particular, we analyze a natural constraint on feature manipulations, and discuss properties that are sufficient to guarantee the robust existence of stable points. Building on these insights, we then propose the noisy response model. Inspired by smoothed analysis and empirical observations, noisy response incorporates imperfection in the agent responses, which we show mitigates the limitations of standard microfoundations. Our model retains analytical tractability, leads to more robust insights about stable points, and imposes a lower social burden at optimality."
            },
            {
                "arxivId": "2005.08377",
                "title": "The Role of Randomness and Noise in Strategic Classification",
                "abstract": "We investigate the problem of designing optimal classifiers in the strategic classification setting, where the classification is part of a game in which players can modify their features to attain a favorable classification outcome (while incurring some cost). Previously, the problem has been considered from a learning-theoretic perspective and from the algorithmic fairness perspective. Our main contributions include 1. Showing that if the objective is to maximize the efficiency of the classification process (defined as the accuracy of the outcome minus the sunk cost of the qualified players manipulating their features to gain a better outcome), then using randomized classifiers (that is, ones where the probability of a given feature vector to be accepted by the classifier is strictly between 0 and 1) is necessary. 2. Showing that in many natural cases, the imposed optimal solution (in terms of efficiency) has the structure where players never change their feature vectors (the randomized classifier is structured in a way, such that the gain in the probability of being classified as a 1 does not justify the expense of changing one's features). 3. Observing that the randomized classification is not a stable best-response from the classifier's viewpoint, and that the classifier doesn't benefit from randomized classifiers without creating instability in the system. 4. Showing that in some cases, a noisier signal leads to better equilibria outcomes -- improving both accuracy and fairness when more than one subpopulation with different feature adjustment costs are involved. This is interesting from a policy perspective, since it is hard to force institutions to stick to a particular randomized classification strategy (especially in a context of a market with multiple classifiers), but it is possible to alter the information environment to make the feature signals inherently noisier."
            },
            {
                "arxivId": "2004.03865",
                "title": "Manipulation-Proof Machine Learning",
                "abstract": "An increasing number of decisions are guided by machine learning algorithms. In many settings, from consumer credit to criminal justice, those decisions are made by applying an estimator to data on an individual's observed behavior. But when consequential decisions are encoded in rules, individuals may strategically alter their behavior to achieve desired outcomes. This paper develops a new class of estimator that is stable under manipulation, even when the decision rule is fully transparent. We explicitly model the costs of manipulating different behaviors, and identify decision rules that are stable in equilibrium. Through a large field experiment in Kenya, we show that decision rules estimated with our strategy-robust method outperform those based on standard supervised learning approaches."
            },
            {
                "arxivId": "1908.10330",
                "title": "Improving Information from Manipulable Data",
                "abstract": "\n Data-based decision making must account for the manipulation of data by agents who are aware of how decisions are being made and want to affect their allocations. We study a framework in which, due to such manipulation, data become less informative when decisions depend more strongly on data. We formalize why and how a decision maker should commit to underutilizing data. Doing so attenuates information loss and thereby improves allocation accuracy."
            },
            {
                "arxivId": "1903.02124",
                "title": "Experimenting in Equilibrium",
                "abstract": "Classical approaches to experimental design assume that intervening on one unit does not affect other units. There are many important settings, however, where this noninterference assumption does not hold, as when running experiments on supply-side incentives on a ride-sharing platform or subsidies in an energy marketplace. In this paper, we introduce a new approach to experimental design in large-scale stochastic systems with considerable cross-unit interference, under an assumption that the interference is structured enough that it can be captured via mean-field modeling. Our approach enables us to accurately estimate the effect of small changes to system parameters by combining unobtrusive randomization with lightweight modeling, all while remaining in equilibrium. We can then use these estimates to optimize the system by gradient descent. Concretely, we focus on the problem of a platform that seeks to optimize supply-side payments [Formula: see text] in a centralized marketplace where different suppliers interact via their effects on the overall supply-demand equilibrium, and we show that our approach enables the platform to optimize [Formula: see text] in large systems using vanishingly small perturbations. This paper was accepted by Hamid Nazerzadeh, big data analytics."
            },
            {
                "arxivId": "1807.05307",
                "title": "How Do Classifiers Induce Agents to Invest Effort Strategically?",
                "abstract": "Algorithms are often used to produce decision-making rules that classify or evaluate individuals. When these individuals have incentives to be classified a certain way, they may behave strategically to influence their outcomes. We develop a model for how strategic agents can invest effort in order to change the outcomes they receive, and we give a tight characterization of when such agents can be incentivized to invest specified forms of effort into improving their outcomes as opposed to \"gaming\" the classifier. We show that whenever any \"reasonable\" mechanism can do so, a simple linear mechanism suffices."
            },
            {
                "arxivId": "1711.06399",
                "title": "AVERAGE TREATMENT EFFECTS IN THE PRESENCE OF UNKNOWN INTERFERENCE.",
                "abstract": "We investigate large-sample properties of treatment effect estimators under unknown interference in randomized experiments. The inferential target is a generalization of the average treatment effect estimand that marginalizes over potential spillover effects. We show that estimators commonly used to estimate treatment effects under no interference are consistent for the generalized estimand for several common experimental designs under limited but otherwise arbitrary and unknown interference. The rates of convergence depend on the rate at which the amount of interference grows and the degree to which it aligns with dependencies in treatment assignment. Importantly for practitioners, the results imply that if one erroneously assumes that units do not interfere in a setting with limited, or even moderate, interference, standard estimators are nevertheless likely to be close to an average treatment effect if the sample is sufficiently large. Conventional confidence statements may, however, not be accurate."
            },
            {
                "arxivId": "1710.07887",
                "title": "Strategic Classification from Revealed Preferences",
                "abstract": "We study an online linear classification problem in which the data is generated by strategic agents who manipulate their features in an effort to change the classification outcome. In rounds, the learner deploys a classifier, then an adversarially chosen agent arrives and possibly manipulates her features to optimally respond to the learner's choice of classifier. The learner has no knowledge of the agents' utility functions or \"real\" features, which may vary widely across agents. Instead, the learner is only able to observe their \"revealed preferences\", i.e., the manipulated feature vectors they provide. For a broad family of agent cost functions, we give a computationally efficient learning algorithm that is able to obtain diminishing \"Stackelberg regret\" --- a form of policy regret that guarantees that the learner is realizing loss nearly as small as that of the best classifier in hindsight, even allowing for the fact that agents would have best-responded differently to the optimal classifier."
            },
            {
                "arxivId": "1506.06980",
                "title": "Strategic Classification",
                "abstract": "Machine learning relies on the assumption that unseen test instances of a classification problem follow the same distribution as observed training data. However, this principle can break down when machine learning is used to make important decisions about the welfare (employment, education, health) of strategic individuals. Knowing information about the classifier, such individuals may manipulate their attributes in order to obtain a better classification outcome. As a result of this behavior -- often referred to as gaming -- the performance of the classifier may deteriorate sharply. Indeed, gaming is a well-known obstacle for using machine learning methods in practice; in financial policy-making, the problem is widely known as Goodhart's law. In this paper, we formalize the problem, and pursue algorithms for learning classifiers that are robust to gaming. We model classification as a sequential game between a player named \"Jury\" and a player named \"Contestant.\" Jury designs a classifier, and Contestant receives an input to the classifier drawn from a distribution. Before being classified, Contestant may change his input based on Jury's classifier. However, Contestant incurs a cost for these changes according to a cost function. Jury's goal is to achieve high classification accuracy with respect to Contestant's original input and some underlying target classification function, assuming Contestant plays best response. Contestant's goal is to achieve a favorable classification outcome while taking into account the cost of achieving it. For a natural class of \"separable\" cost functions, and certain generalizations, we obtain computationally efficient learning algorithms which are near optimal, achieving a classification error that is arbitrarily close to the theoretical minimum. Surprisingly, our algorithms are efficient even on concept classes that are computationally hard to learn. For general cost functions, designing an approximately optimal strategy-proof classifier, for inverse-polynomial approximation, is NP-hard."
            },
            {
                "arxivId": "1506.02084",
                "title": "Exact p-Values for Network Interference",
                "abstract": "ABSTRACT We study the calculation of exact p-values for a large class of nonsharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit\u2019s treatment status on another according to the distance between units, for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (e.g., based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, e.g., only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2208.03489",
        "category": "econ",
        "title": "Forecasting Algorithms for Causal Inference with Panel Data",
        "abstract": "Conducting causal inference with panel data is a core challenge in social science research. We adapt a deep neural architecture for time series forecasting (the N-BEATS algorithm) to more accurately impute the counterfactual evolution of a treated unit had treatment not occurred. Across a range of settings, the resulting estimator (``SyNBEATS'') significantly outperforms commonly employed methods (synthetic controls, two-way fixed effects), and attains comparable or more accurate performance compared to recently proposed methods (synthetic difference-in-differences, matrix completion). An implementation of this estimator is available for public use. Our results highlight how advances in the forecasting literature can be harnessed to improve causal inference in panel data settings.",
        "references": [
            {
                "arxivId": "2104.05522",
                "title": "Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx",
                "abstract": null
            },
            {
                "arxivId": "1912.10610",
                "title": "Randomization Tests in Observational Studies With Staggered Adoption of Treatment",
                "abstract": "ABSTRACT This article considers the problem of inference in observational studies with time-varying adoption of treatment. In addition to an unconfoundedness assumption that the potential outcomes are independent of the times at which units adopt treatment conditional on the units\u2019 observed characteristics, our analysis assumes that the time at which each unit adopts treatment follows a Cox proportional hazards model. This assumption permits the time at which each unit adopts treatment to depend on the observed characteristics of the unit, but imposes the restriction that the probability of multiple units adopting treatment at the same time is zero. In this context, we study randomization tests of a null hypothesis that specifies that there is no treatment effect for all units and all time periods in a distributional sense. We first show that an infeasible test that treats the parameters of the Cox model as known has rejection probability under the null hypothesis no greater than the nominal level in finite samples. Since these parameters are unknown in practice, this result motivates a feasible test that replaces these parameters with consistent estimators. While the resulting test does not need to have the same finite-sample validity as the infeasible test, we show that it has limiting rejection probability under the null hypothesis no greater than the nominal level. In a simulation study, we examine the practical relevance of our theoretical results, including robustness to misspecification of the model for the time at which each unit adopts treatment. Finally, we provide an empirical application of our methodology using the synthetic control-based test statistic and tobacco legislation data found in Abadie, Diamond and Hainmueller. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1905.10437",
                "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting",
                "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy."
            },
            {
                "arxivId": "1904.01490",
                "title": "Synthetic learner: model-free inference on treatments over time",
                "abstract": null
            },
            {
                "arxivId": "1812.10820",
                "title": "A $t$-test for synthetic controls",
                "abstract": "We propose a practical and robust method for making inferences on average treatment effects estimated by synthetic controls. We develop a $K$-fold cross-fitting procedure for bias-correction. To avoid the difficult estimation of the long-run variance, inference is based on a self-normalized $t$-statistic, which has an asymptotically pivotal $t$-distribution. Our $t$-test is easy to implement, provably robust against misspecification, valid with non-stationary data, and demonstrates an excellent small sample performance. Compared to difference-in-differences, our method often yields more than 50% shorter confidence intervals and is robust to violations of parallel trends assumptions. An R-package for implementing our methods is available."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1811.03378",
                "title": "Activation Functions: Comparison of trends in Practice and Research for Deep Learning",
                "abstract": "Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date."
            },
            {
                "arxivId": "1712.09089",
                "title": "An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls",
                "abstract": "Abstract We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available."
            },
            {
                "arxivId": "1712.03553",
                "title": "RNN\u2010based counterfactual prediction, with an application to homestead policy and public schooling",
                "abstract": "This paper proposes a method for estimating the effect of a policy intervention on an outcome over time. We train recurrent neural networks (RNNs) on the history of control unit outcomes to learn a useful representation for predicting future outcomes. The learned representation of control units is then applied to the treated units for predicting counterfactual outcomes. RNNs are specifically structured to exploit temporal dependencies in panel data and are able to learn negative and non\u2010linear interactions between control unit outcomes. We apply the method to the problem of estimating the long\u2010run impact of US homestead policy on public school spending."
            },
            {
                "arxivId": "1710.10251",
                "title": "Matrix Completion Methods for Causal Panel Data Models",
                "abstract": "Abstract In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the \u201cmissing\u201d elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data."
            },
            {
                "arxivId": "1610.07748",
                "title": "Balancing, Regression, Difference-in-Differences and Synthetic Control Methods: A Synthesis",
                "abstract": "In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units."
            },
            {
                "arxivId": "0910.0651",
                "title": "A Simpler Approach to Matrix Completion",
                "abstract": "This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Candes and Recht (2009), Candes and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory."
            },
            {
                "arxivId": "0903.1476",
                "title": "The Power of Convex Relaxation: Near-Optimal Matrix Completion",
                "abstract": "This paper is concerned with the problem of recovering an unknown matrix from a small fraction of its entries. This is known as the matrix completion problem, and comes up in a great number of applications, including the famous Netflix Prize and other similar questions in collaborative filtering. In general, accurate recovery of a matrix from a small number of entries is impossible, but the knowledge that the unknown matrix has low rank radically changes this premise, making the search for solutions meaningful. This paper presents optimality results quantifying the minimum number of entries needed to recover a matrix of rank r exactly by any method whatsoever (the information theoretic limit). More importantly, the paper shows that, under certain incoherence assumptions on the singular vectors of the matrix, recovery is possible by solving a convenient convex program as soon as the number of entries is on the order of the information theoretic limit (up to logarithmic factors). This convex program simply finds, among all matrices consistent with the observed entries, that with minimum nuclear norm. As an example, we show that on the order of nr log(n) samples are needed to recover a random n x n matrix of rank r by any method, and to be sure, nuclear norm minimization succeeds as soon as the number of entries is of the form nr polylog(n)."
            },
            {
                "arxivId": "math/0612783",
                "title": "Causal Inference Through Potential Outcomes and Principal Stratification: Application to Studies with \u201cCensoring\u201d Due to Death",
                "abstract": "Causal inference is best understood using potential out- comes. This use is particularly important in more complex settings, that is, observational studies or randomized experiments with compli- cations such as noncompliance. The topic of this lecture, the issue of estimating the causal effect of a treatment on a primary outcome that is \"censored\" by death, is another such complication. For example, sup- pose that we wish to estimate the effect of a new drug on Quality of Life (QOL) in a randomized experiment, where some of the patients die before the time designated for their QOL to be assessed. Another example with the same structure occurs with the evaluation of an ed- ucational program designed to increase final test scores, which are not defined for those who drop out of school before taking the test. A fur- ther application is to studies of the effect of job-training programs on wages, where wages are only defined for those who are employed. The analysis of examples like these is greatly clarified using potential out- comes to define causal effects, followed by principal stratification on the intermediated outcomes (e.g., survival)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2209.03218",
        "category": "econ",
        "title": "Local Projection Inference in High Dimensions",
        "abstract": "In this paper, we estimate impulse responses by local projections in high-dimensional settings. We use the desparsified (de-biased) lasso to estimate the high-dimensional local projections, while leaving the impulse response parameter of interest unpenalized. We establish the uniform asymptotic normality of the proposed estimator under general conditions. Finally, we demonstrate small sample performance through a simulation study and consider two canonical applications in macroeconomic research on monetary policy and government spending.",
        "references": [
            {
                "arxivId": "2104.00655",
                "title": "Local Projections vs. VARs: Lessons From Thousands of DGPs",
                "abstract": "We conduct a simulation study of Local Projection (LP) and Vector Autoregression (VAR) estimators of structural impulse responses across thousands of data generating processes, designed to mimic the properties of the universe of U.S. macroeconomic data. Our analysis considers various identification schemes and several variants of LP and VAR estimators, employing bias correction, shrinkage, or model averaging. A clear bias-variance trade-off emerges: LP estimators have lower bias than VAR estimators, but they also have substantially higher variance at intermediate and long horizons. Bias-corrected LP is the preferred method if and only if the researcher overwhelmingly prioritizes bias. For researchers who also care about precision, VAR methods are the most attractive -- Bayesian VARs at short and long horizons, and least-squares VARs at intermediate and long horizons."
            },
            {
                "arxivId": "2007.15535",
                "title": "Structural inference in sparse high-dimensional vector autoregressions",
                "abstract": null
            },
            {
                "arxivId": "2007.13888",
                "title": "Local Projection Inference Is Simpler and More Robust Than You Think",
                "abstract": "Applied macroeconomists often compute confidence intervals for impulse responses using local projections, that is, direct linear regressions of future outcomes on current covariates. This paper proves that local projection inference robustly handles two issues that commonly arise in applications: highly persistent data and the estimation of impulse responses at long horizons. We consider local projections that control for lags of the variables in the regression. We show that lag\u2010augmented local projections with normal critical values are asymptotically valid uniformly over (i) both stationary and non\u2010stationary data, and also over (ii) a wide range of response horizons. Moreover, lag augmentation obviates the need to correct standard errors for serial correlation in the regression residuals. Hence, local projection inference is arguably both simpler than previously thought and more robust than standard autoregressive inference, whose validity is known to depend sensitively on the persistence of the data and on the length of the horizon."
            },
            {
                "arxivId": "2007.10952",
                "title": "Lasso inference for high-dimensional time series",
                "abstract": null
            },
            {
                "arxivId": "1912.09002",
                "title": "Regularized estimation of high\u2010dimensional vector autoregressions with weakly dependent innovations",
                "abstract": "There has been considerable advance in understanding the properties of sparse regularization procedures in high\u2010dimensional models. In time series context, it is mostly restricted to Gaussian autoregressions or mixing sequences. We study oracle properties of LASSO estimation of weakly sparse vector\u2010autoregressive models with heavy tailed, weakly dependent innovations. In contrast to current literature, our innovation process satisfy an L1 mixingale type condition on the centered conditional covariance matrices. This condition covers L1\u2010NED sequences and strong ( \u03b1 \u2010) mixing sequences as particular examples."
            },
            {
                "arxivId": "1708.05499",
                "title": "Inference for high-dimensional instrumental variables regression",
                "abstract": null
            },
            {
                "arxivId": "1311.4175",
                "title": "Regularized estimation in sparse high-dimensional time series models",
                "abstract": "Many scientific and economic problems involve the analysis of high-dimensional time series datasets. However, theoretical studies in high-dimensional statistics to date rely primarily on the assumption of independent and identically distributed (i.i.d.) samples. In this work, we focus on stable Gaussian processes and investigate the theoretical properties of $\\ell _1$-regularized estimates in two important statistical problems in the context of high-dimensional time series: (a) stochastic regression with serially correlated errors and (b) transition matrix estimation in vector autoregressive (VAR) models. We derive nonasymptotic upper bounds on the estimation errors of the regularized estimates and establish that consistent estimation under high-dimensional scaling is possible via $\\ell_1$-regularization for a large class of stable processes under sparsity constraints. A key technical contribution of the work is to introduce a measure of stability for stationary processes using their spectral properties that provides insight into the effect of dependence on the accuracy of the regularized estimates. With this proposed stability measure, we establish some useful deviation bounds for dependent data, which can be used to study several important regularized estimates in a time series setting."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2305.14503",
        "category": "econ",
        "title": "On the Instability of Fractional Reserve Banking",
        "abstract": "This paper develops a dynamic monetary model to study the (in)stability of the fractional reserve banking system. The model shows that the fractional reserve banking system can endanger stability in that equilibrium is more prone to exhibit endogenous cyclic, chaotic, and stochastic dynamics under lower reserve requirements, although it can increase consumption in the steady-state. Introducing endogenous unsecured credit to the baseline model does not change the main results. The calibrated exercise suggests that this channel could be another source of economic fluctuations. This paper also provides empirical evidence that is consistent with the prediction of the model.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.09467",
        "category": "econ",
        "title": "The Role of Carbon Pricing in Food Inflation: Evidence from Canadian Provinces",
        "abstract": "Carbon pricing, including carbon tax and cap-and-trade, is usually seen as an effective policy tool for mitigating emissions. Although such policies are often perceived as worsening affordability issues, earlier studies find insignificant or deflationary effects of carbon pricing. We verify this result for the food sector by using provincial-level data on food CPI from Canada. By using a staggered difference-in-difference (DiD) approach, we show that the deflationary effects of carbon pricing on food do exist. Additionally, such effects are weak at first and grow stronger after two years of implementation. However, the overall magnitudes are too small to make carbon pricing blamable for the current high inflation. Our subsequent analysis suggests a reduction in consumption is likely to be the cause of deflation. Contrarily, carbon pricing has little impact on farm production costs owing to the special treatment farmers receive within carbon pricing systems. This paper decomposes the long-term influence Canadian carbon pricing has on food affordability and its possible mechanisms.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.10900",
        "category": "econ",
        "title": "Allocation Mechanisms in Decentralized Exchange Markets with Frictions",
        "abstract": "The classical theory of efficient allocations of an aggregate endowment in a pure-exchange economy has hitherto primarily focused on the Pareto-efficiency of allocations, under the implicit assumption that transfers between agents are frictionless, and hence costless to the economy. In this paper, we argue that certain transfers cause frictions that result in costs to the economy. We show that these frictional costs are tantamount to a form of subadditivity of the cost of transferring endowments between agents. We suggest an axiomatic study of allocation mechanisms, that is, the mechanisms that transform feasible allocations into other feasible allocations, in the presence of such transfer costs. Among other results, we provide an axiomatic characterization of those allocation mechanisms that admit representations as robust (worst-case) linear allocation mechanisms, as well as those mechanisms that admit representations as worst-case conditional expectations. We call the latter Robust Conditional Mean Allocation mechanisms, and we relate our results to the literature on (decentralized) risk sharing within a pool of agents.",
        "references": [
            {
                "arxivId": "0912.0509",
                "title": "Pareto efficiency for the concave order and multivariate comonotonicity",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.10931",
        "category": "econ",
        "title": "The Relationship between Consumer Theories with and without Utility Maximization",
        "abstract": "To study the assumption that the utility maximization hypothesis implicitly adds to consumer theory, we consider a mathematical representation of pre-marginal revolution consumer theory based on subjective exchange ratios. We introduce two axioms on subjective exchange ratio, and show that both axioms hold if and only if consumer behavior is consistent with the utility maximization hypothesis. Moreover, we express the process for a consumer to find the transaction stopping point in terms of differential equations, and prove that the conditions for its stability are equal to the two axioms introduced in the above argument. Therefore, the consumer can find his/her transaction stopping point if and only if his/her behavior is consistent with the utility maximization hypothesis. In addition to these results, we discuss equivalence conditions for axioms to evaluate their mathematical strength, and methods for expressing the theory of subjective exchange ratios in terms of binary relations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11057",
        "category": "econ",
        "title": "Partial Identification of Heteroskedastic Structural VARs: Theory and Bayesian Inference",
        "abstract": "We consider structural vector autoregressions identified through stochastic volatility. Our focus is on whether a particular structural shock is identified by heteroskedasticity without the need to impose any sign or exclusion restrictions. Three contributions emerge from our exercise: (i) a set of conditions under which the matrix containing structural parameters is partially or globally unique; (ii) a statistical procedure to assess the validity of the conditions mentioned above; and (iii) a shrinkage prior distribution for conditional variances centred on a hypothesis of homoskedasticity. Such a prior ensures that the evidence for identifying a structural shock comes only from the data and is not favoured by the prior. We illustrate our new methods using a U.S. fiscal structural model.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11063",
        "category": "econ",
        "title": "Attitudinal Loyalty Manifestation in Banking CSR: Cross-Buying Behavior and Customer Advocacy",
        "abstract": "This study in the banking industry examines the influence of attitudinal loyalty on customer advocacy and cross buying behavior, alongside the moderating roles of Quality of Life and Corporate Social Responsibility support in the CSR fit and loyalty relationship. Employing Structural Equation Modeling, it reveals that higher attitudinal loyalty significantly boosts customer advocacy and propensity for cross buying. The findings highlight the importance of nurturing customer loyalty through valuable and relevant offerings, as CSR fit alone does not define the loyalty of the banking customer. Banks are advised to target customers with a high Quality of Life and engage with those who support CSR initiatives aligning with the banks objectives, to enhance loyalty and deepen customer relationships.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11092",
        "category": "econ",
        "title": "Estimation for conditional moment models based on martingale difference divergence",
        "abstract": "We provide a new estimation method for conditional moment models via the martingale difference divergence (MDD).Our MDD-based estimation method is formed in the framework of a continuum of unconditional moment restrictions. Unlike the existing estimation methods in this framework, the MDD-based estimation method adopts a non-integrable weighting function, which could grab more information from unconditional moment restrictions than the integrable weighting function to enhance the estimation efficiency. Due to the nature of shift-invariance in MDD, our MDD-based estimation method can not identify the intercept parameters. To overcome this identification issue, we further provide a two-step estimation procedure for the model with intercept parameters. Under regularity conditions, we establish the asymptotics of the proposed estimators, which are not only easy-to-implement with analytic asymptotic variances, but also applicable to time series data with an unspecified form of conditional heteroskedasticity. Finally, we illustrate the usefulness of the proposed estimators by simulations and two real examples.",
        "references": [
            {
                "arxivId": "2103.09621",
                "title": "Feasible IV regression without excluded instruments",
                "abstract": "\n The relevance condition of Integrated Conditional Moment (ICM) estimators is significantly weaker than the conventional IV\u2019s in at least two respects: (1) consistent estimation without excluded instruments is possible, provided endogenous covariates are non-linearly mean-dependent on exogenous covariates, and (2) endogenous covariates may be uncorrelated with but mean-dependent on instruments. These remarkable properties notwithstanding, multiplicative-kernel ICM estimators suffer diminished identification strength, large bias, and severe size distortions even for a moderately sized instrument vector. This paper proposes a computationally fast linear ICM estimator that better preserves identification strength in the presence of multiple instruments and a test of the ICM relevance condition. Monte Carlo simulations demonstrate a considerably better size control in the presence of multiple instruments and a favourably competitive performance in general. An empirical example illustrates the practical usefulness of the estimator, where estimates remain plausible when no excluded instrument is used."
            },
            {
                "arxivId": "2012.09422",
                "title": "The Variational Method of Moments",
                "abstract": "\n The conditional moment problem is a powerful formulation for describing structural causal parameters in terms of observables, a prominent example being instrumental variable regression. We introduce a very general class of estimators called the variational method of moments (VMM), motivated by a variational minimax reformulation of optimally weighted generalized method of moments for finite sets of moments. VMM controls infinitely for many moments characterized by flexible function classes such as neural nets and kernel methods, while provably maintaining statistical efficiency unlike existing related minimax estimators. We also develop inference algorithms and demonstrate the empirical strengths of VMM estimation and inference in experiments."
            },
            {
                "arxivId": "1606.05481",
                "title": "Applications of distance correlation to time series",
                "abstract": "The use of empirical characteristic functions for inference problems, including estimation in some special parametric settings and testing for goodness of fit, has a long history dating back to the 70s (see for example, Feuerverger and Mureika (1977), Csorgo (1981a,1981b,1981c), Feuerverger (1993)). More recently, there has been renewed interest in using empirical characteristic functions in other inference settings. The distance covariance and correlation, developed by Szekely and Rizzo (2009) for measuring dependence and testing independence between two random vectors, are perhaps the best known illustrations of this. We apply these ideas to stationary univariate and multivariate time series to measure lagged auto- and cross-dependence in a time series. Assuming strong mixing, we establish the relevant asymptotic theory for the sample auto- and cross-distance correlation functions. We also apply the auto-distance correlation function (ADCF) to the residuals of an autoregressive processes as a test of goodness of fit. Under the null that an autoregressive model is true, the limit distribution of the empirical ADCF can differ markedly from the corresponding one based on an iid sequence. We illustrate the use of the empirical auto- and cross-distance correlation functions for testing dependence and cross-dependence of time series in a variety of different contexts."
            },
            {
                "arxivId": "0803.4101",
                "title": "Measuring and testing dependence by correlation of distances",
                "abstract": "Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11198",
        "category": "econ",
        "title": "Forecasting with panel data: Estimation uncertainty versus parameter heterogeneity",
        "abstract": "We provide a comprehensive examination of the predictive accuracy of panel forecasting methods based on individual, pooling, fixed effects, and Bayesian estimation, and propose optimal weights for forecast combination schemes. We consider linear panel data models, allowing for weakly exogenous regressors and correlated heterogeneity. We quantify the gains from exploiting panel data and demonstrate how forecasting performance depends on the degree of parameter heterogeneity, whether such heterogeneity is correlated with the regressors, the goodness of fit of the model, and the cross-sectional ($N$) and time ($T$) dimensions. Monte Carlo simulations and empirical applications to house prices and CPI inflation show that forecast combination and Bayesian forecasting methods perform best overall and rarely produce the least accurate forecasts for individual series.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11235",
        "category": "econ",
        "title": "Bayesian Markov-Switching Vector Autoregressive Process",
        "abstract": "This study introduces marginal density functions of the general Bayesian Markov-Switching Vector Autoregressive (MS-VAR) process. In the case of the Bayesian MS-VAR process, we provide closed--form density functions and Monte-Carlo simulation algorithms, including the importance sampling method. The Monte--Carlo simulation method departs from the previous simulation methods because it removes the duplication in a regime vector.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11324",
        "category": "econ",
        "title": "Weighted-Average Least Squares for Negative Binomial Regression",
        "abstract": "Model averaging methods have become an increasingly popular tool for improving predictions and dealing with model uncertainty, especially in Bayesian settings. Recently, frequentist model averaging methods such as information theoretic and least squares model averaging have emerged. This work focuses on the issue of covariate uncertainty where managing the computational resources is key: The model space grows exponentially with the number of covariates such that averaged models must often be approximated. Weighted-average least squares (WALS), first introduced for (generalized) linear models in the econometric literature, combines Bayesian and frequentist aspects and additionally employs a semiorthogonal transformation of the regressors to reduce the computational burden. This paper extends WALS for generalized linear models to the negative binomial (NB) regression model for overdispersed count data. A simulation experiment and an empirical application using data on doctor visits were conducted to compare the predictive power of WALS for NB regression to traditional estimators. The results show that WALS for NB improves on the maximum likelihood estimator in sparse situations and is competitive with lasso while being computationally more efficient.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11334",
        "category": "econ",
        "title": "The dynamics of diversity on corporate boards",
        "abstract": "Diversity in leadership positions and corporate boards is an important aspect of equality. It is important because it is the key to better decision-making and innovation, and above all, it paves the way for future generations to participate and shape our society. Many studies emphasize the importance of the visibility of role models and the effect that connectivity has on the success of minorities in leadership. However, the connectivity of firms, the dynamics of the adoption of minorities, and the long-term effects have not been well understood. Here, we present a model that shows how these effects work together in a dynamic model that is calibrated with empirical data of firm and board networks. We show that homophily -- the appointment of minorities is influenced by the presence of minorities in a board and its neighboring entities -- is an important effect shaping the trajectory towards equality. We further show how perception biases and feedback related to the centrality of minority members influence the dynamic. We find that reaching equality can be sped up or slowed down depending on the distribution of minorities in central firms. These insights bear significant implications for policy-making geared towards fostering equality and diversity within corporate boards.",
        "references": [
            {
                "arxivId": "1201.4564",
                "title": "Homophily and Long-Run Integration in Social Networks",
                "abstract": "We model network formation when heterogeneous nodes enter sequentially and form connections through both random meetings and network-based search, but with type-dependent biases. We show that there is \u201clong-run integration\u201d, whereby the composition of types in sufficiently old nodes\u02bc neighborhoods approaches the global type-distribution, provided that the network-based search is unbiased. However, younger nodes\u02bc connections still reflect the biased meetings process. We derive the type-based degree distributions and group-level homophily patterns when there are two types and location-based biases. Finally, we illustrate aspects of the model with an empirical application to data on citations in physics journals."
            },
            {
                "arxivId": "cond-mat/0011029",
                "title": "Competition and multiscaling in evolving networks",
                "abstract": "The rate at which nodes in a network increase their connectivity depends on their fitness to compete for links. For example, in social networks some individuals acquire more social links than others, or on the www some webpages attract considerably more links than others. We find that this competition for links translates into multiscaling, i.e. a fitness-dependent dynamic exponent, allowing fitter nodes to overcome the more connected but less fit ones. Uncovering this fitter-gets-richer phenomenon can help us understand in quantitative terms the evolution of many competitive systems in nature and society."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-18.json",
        "arxivId": "2404.11407",
        "category": "econ",
        "title": "Generalizing Instant Runoff Voting to Allow Indifferences",
        "abstract": "Instant Runoff Voting (IRV) is used in elections for many political offices around the world. It allows voters to specify their preferences among candidates as a ranking. We identify a generalization of the rule, called Approval-IRV, that allows voters more freedom by allowing them to give equal preference to several candidates. Such weak orders are a more expressive input format than linear orders, and they help reduce the cognitive effort of voting. Just like standard IRV, Approval-IRV proceeds in rounds by successively eliminating candidates. It interprets each vote as an approval vote for its most-preferred candidates among those that have not been eliminated. At each step, it eliminates the candidate who is approved by the fewest voters. Among the large class of scoring elimination rules, we prove that Approval-IRV is the unique way of extending IRV to weak orders that preserves its characteristic axiomatic properties, in particular independence of clones and respecting a majority's top choices. We also show that Approval-IRV is the unique extension of IRV among rules in this class that satisfies a natural monotonicity property defined for weak orders. Prior work has proposed a different generalization of IRV, which we call Split-IRV, where instead of approving, each vote is interpreted as splitting 1 point equally among its top choices (for example, 0.25 points each if a vote has 4 top choices), and then eliminating the candidate with the lowest score. Split-IRV fails independence of clones, may not respect majority wishes, and fails our monotonicity condition. The multi-winner version of IRV is known as Single Transferable Vote (STV). We prove that Approval-STV continues to satisfy the strong proportional representation properties of STV, underlining that the approval way is the right way of extending the IRV/STV idea to weak orders.",
        "references": [
            {
                "arxivId": "2303.05985",
                "title": "Ranked Choice Bedlam in a 2022 Oakland School Director Election",
                "abstract": "The November 2022 ranked choice election for District 4 School Director in Oakland, CA, was very interesting from the perspective of social choice theory. The election did not contain a Condorcet winner and exhibited downward and upward monotonicity paradoxes, for example. Furthermore, an error in the settings of the ranked choice tabulation software led to the wrong candidate being declared the winner. This article explores the strange features of this election and places it in the broader context of ranked choice elections in the United States."
            },
            {
                "arxivId": "2302.01989",
                "title": "Robust and Verifiable Proportionality Axioms for Multiwinner Voting",
                "abstract": "When selecting a subset of candidates (a so-called committee) based on the preferences of voters, proportional representation is often a major desideratum. When going beyond simplistic models such as party-list or district-based elections, it is surprisingly challenging to capture proportionality formally. As a consequence, the literature has produced numerous competing criteria of when a selected committee qualifies as proportional. Two of the most prominent notions are proportionality for solid coalitions (PSC) [Dummett, 1984] and extended justified representation (EJR) [Aziz et al., 2017]. Both definitions guarantee proportional representation to groups of voters with very similar preferences; such groups are referred to as solid coalitions by Dummett and as cohesive groups by Aziz et al. However, they lose their bite when groups are only almost solid or cohesive."
            },
            {
                "arxivId": "2301.12075",
                "title": "An Examination of Ranked-Choice Voting in the United States, 2004\u20132022",
                "abstract": "From the perspective of social choice theory, ranked-choice voting (RCV) is known to have many flaws. RCV can fail to elect a Condorcet winner and is susceptible to monotonicity paradoxes and the spoiler effect, for example. We use a database of 182 American ranked-choice elections for political office from the years 2004-2022 to investigate empirically how frequently RCV's deficiencies manifest in practice. Our general finding is that RCV's weaknesses are rarely observed in real-world elections, with the exception that ballot exhaustion frequently causes majoritarian failures."
            },
            {
                "arxivId": "2205.00492",
                "title": "Understanding Distance Measures Among Elections",
                "abstract": "Motivated by putting empirical work based on (synthetic) election data on a more solid mathematical basis, we analyze six distances among elections, including, e.g., the challenging-to-compute but very precise swap distance and the distance used to form the so-called map of elections. Among the six, the latter seems to strike the best balance between its computational complexity and expressiveness."
            },
            {
                "arxivId": "2102.05801",
                "title": "The vote Package: Single Transferable Vote and Other Electoral Systems in R",
                "abstract": "We describe the vote package in R, which implements the plurality (or first-past-the-post), two-round runoff, score, approval and single transferable vote (STV) electoral systems, as well as methods for selecting the Condorcet winner and loser. We emphasize the STV system, which we have found to work well in practice for multi-winner elections with small electorates, such as committee and council elections, and the selection of multiple job candidates. For single-winner elections, the STV is also called instant runoff voting (IRV), ranked choice voting (RCV), or the alternative vote (AV) system. The package also implements the STV system with equal preferences, for the first time in a software package, to our knowledge. It also implements a new variant of STV, in which a minimum number of candidates from a specified group are required to be elected. We illustrate the package with several real examples."
            },
            {
                "arxivId": "2008.13276",
                "title": "Proportional Participatory Budgeting with Additive Utilities",
                "abstract": "We study voting rules for participatory budgeting, where a group of voters collectively decides which projects should be funded using a common budget. We allow the projects to have arbitrary costs, and the voters to have arbitrary additive valuations over the projects. We formulate an axiom (Extended Justi\ufb01ed Representation, EJR) that guarantees proportional representation to groups of voters with common interests. We propose a simple and attractive voting rule called the Method of Equal Shares that satis\ufb01es this axiom for arbitrary costs and approval utilities, and that satis\ufb01es the axiom up to one project for arbitrary additive valuations. This method can be computed in polynomial time. In contrast, we show that the standard method for achieving proportionality in committee elections, Proportional Approval Voting (PAV), cannot be extended to work with arbitrary costs. Finally, we introduce a strengthened axiom (Full Justi\ufb01ed Representation, FJR) and show that it is also satis\ufb01able, though by a computationally more expensive and less natural voting rule."
            },
            {
                "arxivId": "2004.02350",
                "title": "Split Cycle: a new Condorcet-consistent voting method independent of clones and immune to spoilers",
                "abstract": null
            },
            {
                "arxivId": "1911.11747",
                "title": "Proportionality and the Limits of Welfarism",
                "abstract": "We study two influential voting rules proposed in the 1890s by Phragmen and Thiele, which elect a committee of k candidates which proportionally represents the voters. Voters provide their preferences by approving an arbitrary number of candidates. Previous work has proposed proportionality axioms satisfied by Thiele's rule (now known as Proportional Approval Voting, PAV) but not by Phragmen's rule. By proposing two new proportionality axioms (laminar proportionality and priceability) satisfied by Phragmen but not Thiele, we show that the two rules achieve two distinct forms of proportional representation. Phragmen's rule ensures that all voters have a similar amount of influence on the committee, and Thiele's rule ensures a fair utility distribution. Thiele's rule is a welfarist voting rule (one that maximizes a function of voter utilities). We show that no welfarist rule can satisfy our new axioms, and we prove that no such rule can satisfy the core. Conversely, some welfarist fairness properties cannot be guaranteed by Phragmen-type rules. This formalizes the difference between the two types of proportionality. We then introduce an attractive committee rule which satisfies a property intermediate between the core and extended justified representation (EJR). It satisfies laminar proportionality, priceability, and is computable in polynomial time. We show that our new rule provides a logarithmic approximation to the core. On the other hand, PAV provides a factor-2 approximation to the core, and this factor is optimal for rules that are fair in the sense of the Pigou--Dalton principle. The full version of the paper is available at http://arxiv.org/pdf/1911.11747.pdf."
            },
            {
                "arxivId": "1901.09217",
                "title": "What Do Multiwinner Voting Rules Do? An Experiment Over the Two-Dimensional Euclidean Domain",
                "abstract": "\n \n We visualize aggregate outputs of popular multiwinner voting rules \u2014 SNTV, STV, Bloc, k-Borda, Monroe, Chamberlin\u2013Courant, and PAV \u2014 for elections generated according to the two-dimensional Euclidean model. We consider three applications of multiwinner voting, namely, parliamentary elections, portfolio/movie selection, and shortlisting, and use our results to understand which of our rules seem to be best suited for each application. In particular, we show that STV (one of the few nontrivial rules used in real high-stake elections) exhibits excellent performance, whereas the Bloc rule (also often used in practice) performs poorly.\n \n"
            },
            {
                "arxivId": "1611.08826",
                "title": "Phragm\u00e9n's and Thiele's election methods",
                "abstract": "The election methods introduced in 1894--1895 by Phragmen and Thiele, and their somewhat later versions for ordered (ranked) ballots, are discussed in detail. The paper includes definitions and examples and discussion of whether the methods satisfy some properties, including monotonicity, consistency and various proportionality criteria. The relation with STV is also discussed. The paper also contains historical information on the methods."
            },
            {
                "arxivId": "1008.4331",
                "title": "Geometric construction of voting methods that protect voters' first choices",
                "abstract": "We consider the possibility of designing an election method that eliminates the incentives for a voter to rank any other candidate equal to or ahead of his or her sincere favorite. We refer to these methods as satisfying the ``Strong Favorite Betrayal Criterion\" (SFBC). Methods satisfying our strategic criteria can be classified into four categories, according to their geometrical properties. We prove that two categories of methods are highly restricted and closely related to positional methods (point systems) that give equal points to a voter's first and second choices. The third category is tightly restricted, but if criteria are relaxed slightly a variety of interesting methods can be identified. Finally, we show that methods in the fourth category are largely irrelevant to public elections. Interestingly, most of these methods for satisfying the SFBC do so only ``weakly,\" in that these methods make no meaningful distinction between the first and second place on the ballot. However, when we relax our conditions and allow (but do not require) equal rankings for first place, a wider range of voting methods are possible, and these methods do indeed make meaningful distinctions between first and second place."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-19.json",
        "arxivId": "2404.11839",
        "category": "econ",
        "title": "(Empirical) Bayes Approaches to Parallel Trends",
        "abstract": "We consider Bayes and Empirical Bayes (EB) approaches for dealing with violations of parallel trends. In the Bayes approach, the researcher specifies a prior over both the pre-treatment violations of parallel trends $\\delta_{pre}$ and the post-treatment violations $\\delta_{post}$. The researcher then updates their posterior about the post-treatment bias $\\delta_{post}$ given an estimate of the pre-trends $\\delta_{pre}$. This allows them to form posterior means and credible sets for the treatment effect of interest, $\\tau_{post}$. In the EB approach, the prior on the violations of parallel trends is learned from the pre-treatment observations. We illustrate these approaches in two empirical applications.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-19.json",
        "arxivId": "2404.11883",
        "category": "econ",
        "title": "Testing the simplicity of strategy-proof mechanisms",
        "abstract": "This paper experimentally evaluates four mechanisms intended to achieve the Uniform outcome in rationing problems (Sprumont, 1991). Our benchmark is the dominant-strategy, direct-revelation mechanism of the Uniform rule. A strategically equivalent mechanism that provides non-binding feedback during the reporting period greatly improves performance. A sequential revelation mechanism produces modest improvements despite not possessing dominant strategies. A novel, obviously strategy-proof mechanism, devised by Arribillaga et al. (2023), does not improve performance. We characterize each alternative to the direct mechanism, finding general lessons about the advantages of real-time feedback and sequentiality of play as well as the potential shortcomings of an obviously strategy-proof mechanism.",
        "references": [
            {
                "arxivId": "1812.00849",
                "title": "Strategically Simple Mechanisms",
                "abstract": "We define and investigate a property of mechanisms that we call \u201cstrategic simplicity,\u201d and that is meant to capture the idea that, in strategically simple mechanisms, strategic choices require limited strategic sophistication. We define a mechanism to be strategically simple if choices can be based on first\u2010order beliefs about the other agents' preferences and first\u2010order certainty about the other agents' rationality alone, and there is no need for agents to form higher\u2010order beliefs, because such beliefs are irrelevant to the optimal strategies. All dominant strategy mechanisms are strategically simple. But many more mechanisms are strategically simple. In particular, strategically simple mechanisms may be more flexible than dominant strategy mechanisms in the bilateral trade problem and the voting problem."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-19.json",
        "arxivId": "2404.12193",
        "category": "econ",
        "title": "Portrait comparison of binary and weighted Skill Relatedness Networks",
        "abstract": "In this paper we compare Skill-Relatedness Networks (SRNs) for selected countries, that is to say statistically significant inter-industrial interactions representing latent skills exchanges derived from observed labor flows, a kind of industry spaces. Using data from Argentina (ARG), Germany (DEU) and Sweden (SWE), we compare their SRNs utilizing an information-theoretic method that permits to compare networks of\"non-aligned\"nodes, which is the case of interest. For each SRN we extract its portrait, a fingerprint of structural measures of the distributions of their shortest paths, and calculate their pairwise divergences. This allows us also to contrast differences in structural (binary) connectivity with differences in the information provided by the (weighted) skill relatedness indicator (SR). We find that, in the case of ARG, structural connectivity is very different from their counterpart in DEU and SWE, but through the glass of SR the distances analyzed are all substantially smaller and more alike. These results qualify the role of the SR indicator as revealing some hidden dimension different from connectivity alone, providing empirical support to the suggestion that industry spaces may differ across countries.",
        "references": [
            {
                "arxivId": "2010.06568",
                "title": "A bi-directional approach to comparing the modular structure of networks",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2110.15517",
        "category": "econ",
        "title": "CP Factor Model for Dynamic Tensors",
        "abstract": "Observations in various applications are frequently represented as a time series of multidimensional arrays, called tensor time series, preserving the inherent multidimensional structure. In this paper, we present a factor model approach, in a form similar to tensor CP decomposition, to the analysis of high-dimensional dynamic tensor time series. As the loading vectors are uniquely defined but not necessarily orthogonal, it is significantly different from the existing tensor factor models based on Tucker-type tensor decomposition. The model structure allows for a set of uncorrelated one-dimensional latent dynamic factor processes, making it much more convenient to study the underlying dynamics of the time series. A new high order projection estimator is proposed for such a factor model, utilizing the special structure and the idea of the higher order orthogonal iteration procedures commonly used in Tucker-type tensor factor model and general tensor CP decomposition procedures. Theoretical investigation provides statistical error bounds for the proposed methods, which shows the significant advantage of utilizing the special model structure. Simulation study is conducted to further demonstrate the finite sample properties of the estimators. Real data application is used to illustrate the model and its interpretations.",
        "references": [
            {
                "arxivId": "2011.07131",
                "title": "Rank Determination in Tensor Factor Model",
                "abstract": "Factor model is an appealing and effective analytic tool for high-dimensional time series, with a wide range of applications in economics, finance and statistics. One of the fundamental issues in using factor model for time series in practice is the determination of the number of factors to use. This paper develops two criteria for such a task for tensor factor models where the signal part of an observed time series in tensor form assumes a Tucker decomposition with the core tensor as the factor tensor. The task is to determine the dimensions of the core tensor. One of the proposed criteria is similar to information based criteria of model selection, and the other is an extension of the approaches based on the ratios of consecutive eigenvalues often used in factor analysis for panel time series. The new criteria are designed to locate the gap between the true smallest non-zero eigenvalue and the zero eigenvalues of a functional of the population version of the auto-cross-covariances of the tensor time series using their sample versions. As sample size and tensor dimension increase, such a gap increases under regularity conditions, resulting in consistency of the rank estimator. The criteria are built upon the existing non-iterative and iterative estimation procedures of tensor factor model, yielding different performances. We provide sufficient conditions and convergence rate for the consistency of the criteria as the sample size $T$ and the dimensions of the observed tensor time series go to infinity. The results include the vector factor models as special cases, with an additional convergence rates. The results also include the cases when there exist factors with different signal strength. In addition, the convergence rates of the eigenvalue estimators are established. Simulation studies provide promising finite sample performance for the two criteria."
            },
            {
                "arxivId": "2001.01890",
                "title": "Statistical Inference for High-Dimensional Matrix-Variate Factor Models",
                "abstract": "ABSTRACT This article considers the estimation and inference of the low-rank components in high-dimensional matrix-variate factor models, where each dimension of the matrix-variates (p \u00d7 q) is comparable to or greater than the number of observations (T). We propose an estimation method called \u03b1-PCA that preserves the matrix structure and aggregates mean and contemporary covariance through a hyper-parameter \u03b1. We develop an inferential theory, establishing consistency, the rate of convergence, and the limiting distributions, under general conditions that allow for correlations across time, rows, or columns of the noise. We show both theoretical and empirical methods of choosing the best \u03b1, depending on the use-case criteria. Simulation results demonstrate the adequacy of the asymptotic results in approximating the finite sample properties. The \u03b1-PCA compares favorably with the existing ones. Finally, we illustrate its applications with a real numeric dataset and two real image datasets. In all applications, the proposed estimation procedure outperforms previous methods in the power of variance explanation using out-of-sample 10-fold cross-validation. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1905.07530",
                "title": "Factor Models for High-Dimensional Tensor Time Series",
                "abstract": "Abstract Large tensor (multi-dimensional array) data routinely appear nowadays in a wide range of applications, due to modern data collection capabilities. Often such observations are taken over time, forming tensor time series. In this article we present a factor model approach to the analysis of high-dimensional dynamic tensor time series and multi-category dynamic transport networks. This article presents two estimation procedures along with their theoretical properties and simulation results. We present two applications to illustrate the model and its interpretations."
            },
            {
                "arxivId": "1811.05076",
                "title": "Learning from Binary Multiway Data: Probabilistic Tensor Decomposition and its Statistical Optimality",
                "abstract": "We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We propose a multilinear Bernoulli model, develop a rank-constrained likelihood-based estimation method, and obtain the theoretical accuracy guarantees. In contrast to continuous-valued problems, the binary tensor problem exhibits an interesting phase transition phenomenon according to the signal-to-noise ratio. The error bound for the parameter tensor estimation is established, and we show that the obtained rate is minimax optimal under the considered model. Furthermore, we develop an alternating optimization algorithm with convergence guarantees. The efficacy of our approach is demonstrated through both simulations and analyses of multiple data sets on the tasks of tensor completion and clustering."
            },
            {
                "arxivId": "1801.09326",
                "title": "Sparse and Low-Rank Tensor Estimation via Cubic Sketchings",
                "abstract": "In this paper, we propose a general framework for sparse and low-rank tensor estimation from cubic sketchings. A two-stage non-convex implementation is developed based on sparse tensor decomposition and thresholded gradient descent, which ensures exact recovery in the noiseless case and stable recovery in the noisy case with high probability. The non-asymptotic analysis sheds light on an interplay between optimization error and statistical error. The proposed procedure is shown to be rate-optimal under certain conditions. As a technical by-product, novel high-order concentration inequalities are derived for studying high-moment sub-Gaussian tensors. An interesting tensor formulation illustrates the potential application to high-order interaction pursuit in high-dimensional linear regression."
            },
            {
                "arxivId": "1711.01598",
                "title": "Multilayer tensor factorization with applications to recommender systems",
                "abstract": "Recommender systems have been widely adopted by electronic commerce and entertainment industries for individualized prediction and recommendation, which benefit consumers and improve business intelligence. In this article, we propose an innovative method, namely the recommendation engine of multilayers (REM), for tensor recommender systems. The proposed method utilizes the structure of a tensor response to integrate information from multiple modes, and creates an additional layer of nested latent factors to accommodate between-subjects dependency. One major advantage is that the proposed method is able to address the \"cold-start\" issue in the absence of information from new customers, new products or new contexts. Specifically, it provides more effective recommendations through sub-group information. To achieve scalable computation, we develop a new algorithm for the proposed method, which incorporates a maximum block improvement strategy into the cyclic blockwise-coordinate-descent algorithm. In theory, we investigate both algorithmic properties for global and local convergence, along with the asymptotic consistency of estimated parameters. Finally, the proposed method is applied in simulations and IRI marketing data with 116 million observations of product sales. Numerical studies demonstrate that the proposed method outperforms existing competitors in the literature."
            },
            {
                "arxivId": "1710.06075",
                "title": "Constrained Factor Models for High-Dimensional Matrix-Variate Time Series",
                "abstract": "Abstract High-dimensional matrix-variate time series data are becoming widely available in many scientific fields, such as economics, biology, and meteorology. To achieve significant dimension reduction while preserving the intrinsic matrix structure and temporal dynamics in such data, Wang, Liu, and Chen proposed a matrix factor model, that is, shown to be able to provide effective analysis. In this article, we establish a general framework for incorporating domain and prior knowledge in the matrix factor model through linear constraints. The proposed framework is shown to be useful in achieving parsimonious parameterization, facilitating interpretation of the latent matrix factor, and identifying specific factors of interest. Fully utilizing the prior-knowledge-induced constraints results in more efficient and accurate modeling, inference, dimension reduction as well as a clear and better interpretation of the results. Constrained, multi-term, and partially constrained factor models for matrix-variate time series are developed, with efficient estimation procedures and their asymptotic properties. We show that the convergence rates of the constrained factor loading matrices are much faster than those of the conventional matrix factor analysis under many situations. Simulation studies are carried out to demonstrate finite-sample performance of the proposed method and its associated asymptotic properties. We illustrate the proposed model with three applications, where the constrained matrix-factor models outperform their unconstrained counterparts in the power of variance explanation under the out-of-sample 10-fold cross-validation setting. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1502.01425",
                "title": "Provable sparse tensor decomposition",
                "abstract": "We propose a novel sparse tensor decomposition method, namely the tensor truncated power method, that incorporates variable selection in the estimation of decomposition components. The sparsity is achieved via an efficient truncation step embedded in the tensor power iteration. Our method applies to a broad family of high dimensional latent variable models, including high dimensional Gaussian mixtures and mixtures of sparse regressions. A thorough theoretical investigation is further conducted. In particular, we show that the final decomposition estimator is guaranteed to achieve a local statistical rate, and we further strengthen it to the global statistical rate by introducing a proper initialization procedure. In high dimensional regimes, the statistical rate obtained significantly improves those shown in the existing non\u2010sparse decomposition methods. The empirical advantages of tensor truncated power are confirmed in extensive simulation results and two real applications of click\u2010through rate prediction and high dimensional gene clustering."
            },
            {
                "arxivId": "1412.0048",
                "title": "MULTILINEAR TENSOR REGRESSION FOR LONGITUDINAL RELATIONAL DATA.",
                "abstract": "A fundamental aspect of relational data, such as from a social network, is the possibility of dependence among the relations. In particular, the relations between members of one pair of nodes may have an effect on the relations between members of another pair. This article develops a type of regression model to estimate such effects in the context of longitudinal and multivariate relational data, or other data that can be represented in the form of a tensor. The model is based on a general multilinear tensor regression model, a special case of which is a tensor autoregression model in which the tensor of relations at one time point are parsimoniously regressed on relations from previous time points. This is done via a separable, or Kronecker-structured, regression parameter along with a separable covariance model. In the context of an analysis of longitudinal multivariate relational data, it is shown how the multilinear tensor regression model can represent patterns that often appear in relational and network data, such as reciprocity and transitivity."
            },
            {
                "arxivId": "1411.1076",
                "title": "A statistical model for tensor PCA",
                "abstract": "We consider the Principal Component Analysis problem for large tensors of arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio $\\beta$ becomes larger than $C\\sqrt{k\\log k}$ (and in particular $\\beta$ can remain bounded as the problem dimensions increase). \nOn the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. \nWe discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate."
            },
            {
                "arxivId": "1406.3836",
                "title": "Projected Principal Component Analysis in Factor Models",
                "abstract": "This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employees principal component analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates. When it applies to high-dimensional factor analysis, the projection removes noise components. We show that the unobserved latent factors can be more accurately estimated than the conventional PCA if the projection is genuine, or more precisely, when the factor loading matrices are related to the projected linear space. When the dimensionality is large, the factors can be estimated accurately even when the sample size is finite. We propose a flexible semi-parametric factor model, which decomposes the factor loading matrix into the component that can be explained by subject-specific covariates and the orthogonal residual component. The covariates' effects on the factor loadings are further modeled by the additive model via sieve approximations. By using the newly proposed Projected-PCA, the rates of convergence of the smooth factor loading matrices are obtained, which are much faster than those of the conventional factor analysis. The convergence is achieved even when the sample size is finite and is particularly appealing in the high-dimension-low-sample-size situation. This leads us to developing nonparametric tests on whether observed covariates have explaining powers on the loadings and whether they fully explain the loadings. The proposed method is illustrated by both simulated data and the returns of the components of the S&P 500 index."
            },
            {
                "arxivId": "1402.5180",
                "title": "Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-1 Updates",
                "abstract": "Author(s): Anandkumar, Animashree; Ge, Rong; Janzamin, Majid | Abstract: In this paper, we provide local and global convergence guarantees for recovering CP (Candecomp/Parafac) tensor decomposition. The main step of the proposed algorithm is a simple alternating rank-$1$ update which is the alternating version of the tensor power iteration adapted for asymmetric tensors. Local convergence guarantees are established for third order tensors of rank $k$ in $d$ dimensions, when $k=o \\bigl( d^{1.5} \\bigr)$ and the tensor components are incoherent. Thus, we can recover overcomplete tensor decomposition. We also strengthen the results to global convergence guarantees under stricter rank condition $k \\le \\beta d$ (for arbitrary constant $\\beta g 1$) through a simple initialization procedure where the algorithm is initialized by top singular vectors of random tensor slices. Furthermore, the approximate local convergence guarantees for $p$-th order tensors are also provided under rank condition $k=o \\bigl( d^{p/2} \\bigr)$. The guarantees also include tight perturbation analysis given noisy tensor."
            },
            {
                "arxivId": "1302.2684",
                "title": "A tensor approach to learning mixed membership community models",
                "abstract": "Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of 3-star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g. singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model."
            },
            {
                "arxivId": "1211.3813",
                "title": "SEPARABLE FACTOR ANALYSIS WITH APPLICATIONS TO MORTALITY DATA.",
                "abstract": "Human mortality data sets can be expressed as multiway data arrays, the dimensions of which correspond to categories by which mortality rates are reported, such as age, sex, country and year. Regression models for such data typically assume an independent error distribution or an error model that allows for dependence along at most one or two dimensions of the data array. However, failing to account for other dependencies can lead to inefficient estimates of regression parameters, inaccurate standard errors and poor predictions. An alternative to assuming independent errors is to allow for dependence along each dimension of the array using a separable covariance model. However, the number of parameters in this model increases rapidly with the dimensions of the array and, for many arrays, maximum likelihood estimates of the covariance parameters do not exist. In this paper, we propose a submodel of the separable covariance model that estimates the covariance matrix for each dimension as having factor analytic structure. This model can be viewed as an extension of factor analysis to array-valued data, as it uses a factor model to estimate the covariance along each dimension of the array. We discuss properties of this model as they relate to ordinary factor analysis, describe maximum likelihood and Bayesian estimation methods, and provide a likelihood ratio testing procedure for selecting the factor model ranks. We apply this methodology to the analysis of data from the Human Mortality Database, and show in a cross-validation experiment how it outperforms simpler methods. Additionally, we use this model to impute mortality rates for countries that have no mortality data for several years. Unlike other approaches, our methodology is able to estimate similarities between the mortality rates of countries, time periods and sexes, and use this information to assist with the imputations."
            },
            {
                "arxivId": "1210.7559",
                "title": "Tensor decompositions for learning latent variable models",
                "abstract": "This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models--including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation--which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models."
            },
            {
                "arxivId": "1206.0613",
                "title": "Factor modeling for high-dimensional time series: inference for the number of factors",
                "abstract": "This paper deals with the factor modeling for high-dimensional time series based on a dimension-reduction viewpoint. Under stationary settings, the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a nonnegative definite matrix, and is therefore applicable when the dimension of time series is on the order of a few thousands. Asymptotic properties of the proposed method are investigated under two settings: (i) the sample size goes to infinity while the dimension of time series is fixed; and (ii) both the sample size and the dimension of time series go to infinity together. In particular, our estimators for zero-eigenvalues enjoy faster convergence (or slower divergence) rates, hence making the estimation for the number of factors easier. In particular, when the sample size and the dimension of time series go to infinity together, the estimators for the eigenvalues are no longer consistent. However, our estimator for the number of the factors, which is based on the ratios of the estimated eigenvalues, still works fine. Furthermore, this estimation shows the so-called \u201cblessing of dimensionality\u201d property in the sense that the performance of the estimation may improve when the dimension of time series increases. A two-step procedure is investigated when the factors are of different degrees of strength. Numerical illustration with both simulated and real data is also reported."
            },
            {
                "arxivId": "1201.0175",
                "title": "Large covariance estimation by thresholding principal orthogonal complements",
                "abstract": "The paper deals with the estimation of a high dimensional covariance with a conditional sparsity structure and fast diverging eigenvalues. By assuming a sparse error covariance matrix in an approximate factor model, we allow for the presence of some cross\u2010sectional correlation even after taking out common but unobservable factors. We introduce the principal orthogonal complement thresholding method \u2018POET\u2019 to explore such an approximate factor structure with sparsity. The POET\u2010estimator includes the sample covariance matrix, the factor\u2010based covariance matrix, the thresholding estimator and the adaptive thresholding estimator as specific examples. We provide mathematical insights when the factor analysis is approximately the same as the principal component analysis for high dimensional data. The rates of convergence of the sparse residual covariance matrix and the conditional sparse covariance matrix are studied under various norms. It is shown that the effect of estimating the unknown factors vanishes as the dimensionality increases. The uniform rates of convergence for the unobserved factors and their factor loadings are derived. The asymptotic results are also verified by extensive simulation studies. Finally, a real data application on portfolio allocation is presented."
            },
            {
                "arxivId": "1105.4292",
                "title": "High Dimensional Covariance Matrix Estimation in Approximate Factor Models",
                "abstract": "The variance covariance matrix plays a central role in the inferential theories of high dimensional factor models in finance and economics. Popular regularization methods of directly exploiting sparsity are not directly applicable to many financial problems. Classical methods of estimating the covariance matrices are based on the strict factor models, assuming independent idiosyncratic components. This assumption, however, is restrictive in practical applications. By assuming sparse error covariance matrix, we allow the presence of the cross-sectional correlation even after taking out common factors, and it enables us to combine the merits of both methods. We estimate the sparse covariance using the adaptive thresholding technique as in Cai and Liu (2011), taking into account the fact that direct observations of the idiosyncratic components are unavailable. The impact of high dimensionality on the covariance matrix estimation based on the factor structure is then studied."
            },
            {
                "arxivId": "1008.2169",
                "title": "Separable covariance arrays via the Tucker product, with applications to multivariate relational data",
                "abstract": "Modern datasets are often in the form of matrices or arrays, potentially having correlations along each set of data indices. For example, data involving repeated measurements of several variables over time may exhibit temporal correlation as well as correlation among the variables. A possible model for matrix-valued data is the class of matrix normal distributions, which is parametrized by two covariance matrices, one for each index set of the data. In this article we describe an extension of the matrix normal model to accommodate multidimensional data arrays, or tensors. We generate a class of array normal distributions by applying a group of multilinear transformations to an array of independent standard normal random variables. The covariance structures of the resulting class take the form of outer products of dimension-specific covariance matrices. We derive some properties of these covariance structures and the corresponding array normal distributions, discuss maximum likelihood and Bayesian estimation of covariance parameters and illustrate the model in an analysis of multivariate longitudinal network data. Some key words: Gaussian, matrix normal, multiway data, network, tensor, Tucker decomposition."
            },
            {
                "arxivId": "0911.1393",
                "title": "Most Tensor Problems Are NP-Hard",
                "abstract": "We prove that multilinear (tensor) analogues of many efficiently computable problems in numerical linear algebra are NP-hard. Our list includes: determining the feasibility of a system of bilinear equations, deciding whether a 3-tensor possesses a given eigenvalue, singular value, or spectral norm; approximating an eigenvalue, eigenvector, singular vector, or the spectral norm; and determining the rank or best rank-1 approximation of a 3-tensor. Furthermore, we show that restricting these problems to symmetric tensors does not alleviate their NP-hardness. We also explain how deciding nonnegative definiteness of a symmetric 4-tensor is NP-hard and how computing the combinatorial hyperdeterminant is NP-, #P-, and VNP-hard."
            },
            {
                "arxivId": "0902.0582",
                "title": "A Bernstein type inequality and moderate deviations for weakly dependent sequences",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2202.04573",
        "category": "econ",
        "title": "On the Uniqueness and Stability of the Equilibrium Price in Quasi-Linear Economies",
        "abstract": "In this paper, we show that if every consumer in an economy has a quasi-linear utility function, then the normalized equilibrium price is unique, and is locally stable with respect to the t\\^atonnement process. Our study can be seen as that extends the results in partial equilibrium theory to economies with more than two dimensional consumption space. Moreover, we discuss the surplus analysis in such economies.",
        "references": [
            {
                "arxivId": "2211.14272",
                "title": "A Rigorous Proof of the Index Theorem for Economists",
                "abstract": "This paper provides a rigorous and gap-free proof of the index theorem used in the theory of regular economy. In the index theorem that is the subject of this paper, the assumptions for the excess demand function are only several usual assumptions and continuous differentiability around any equilibrium price, and thus it has a form that is applicable to many economies. However, the textbooks on this theme contain only abbreviated proofs and there is no known monograph that contains a rigorous proof of this theorem. Hence, the purpose of this paper is to make this theorem available to more economists by constructing a readable proof."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2308.00179",
        "category": "econ",
        "title": "Position Uncertainty in a Sequential Public Goods Game: An Experiment",
        "abstract": "Gallice and Monz\\'on (2019) present a natural environment that sustains full co-operation in one-shot social dilemmas among a finite number of self-interested agents. They demonstrate that in a sequential public goods game, where agents lack knowledge of their position in the sequence but can observe some predecessors' actions, full contribution emerges in equilibrium due to agents' incentive to induce potential successors to follow suit. In this study, we aim to test the theoretical predictions of this model through an economic experiment. We conducted three treatments, varying the amount of information about past actions that a subject can observe, as well as their positional awareness. Through rigorous structural econometric analysis, we found that approximately 25% of the subjects behaved in line with the theoretical predictions. However, we also observed the presence of alternative behavioural types among the remaining subjects. The majority were classified as conditional co-operators, showing a willingness to cooperate based on others' actions. Some subjects exhibited altruistic tendencies, while only a small minority engaged in free-riding behaviour.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2310.05971",
        "category": "econ",
        "title": "Theoretical Economics as Successive Approximations of Statistical Moments",
        "abstract": "This paper studies the links between the descriptions of macroeconomic variables and statistical moments of market trade, price, and return. The randomness of market trade values and volumes during the averaging interval {\\Delta} results in the random properties of price and return. We describe how averages and volatilities of price and return depend on the averages, volatilities, and correlations of market trade values and volumes. The averages, volatilities, and correlations of market trade, price, and return can behave randomly during the long interval {\\Delta}2>>{\\Delta}. To describe their statistical properties during the long interval {\\Delta}2, we introduce the secondary averaging procedure of trade, price, and return. We explain why, in the coming years, predictions of market-based probabilities of price and return will be limited by Gaussian distributions. We discuss the roots of the internal weakness of the commonly used hedging tool, Value-at-Risk, that cannot be solved and remains the source of additional risks and losses. One should consider theoretical economics as a set of successive approximations, each of which describes the next array of the n-th statistical moments of market trades, price, return, and macroeconomic variables, which are repeatedly averaged during the sequence of increasing time intervals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2311.05883",
        "category": "econ",
        "title": "Time-Varying Identification of Monetary Policy Shocks",
        "abstract": "We propose a new Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. Based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in US monetary policy shock identification. In the sample-dominating first regime, systematic monetary policy follows a Taylor rule extended by the term spread, effectively curbing inflation. In the second regime, occurring after 2000 and gaining more persistence after the global financial and COVID crises, it is characterized by a money-augmented Taylor rule. This regime's unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. Absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.",
        "references": [
            {
                "arxivId": "2111.07225",
                "title": "Large Order-Invariant Bayesian VARs with Stochastic Volatility",
                "abstract": "Many popular specifications for Vector Autoregressions (VARs) with multivariate stochastic volatility are not invariant to the way the variables are ordered due to the use of a Cholesky decomposition for the error covariance matrix. We show that the order invariance problem in existing approaches is likely to become more serious in large VARs. We propose the use of a specification which avoids the use of this Cholesky decomposition. We show that the presence of multivariate stochastic volatility allows for identification of the proposed model and prove that it is invariant to ordering. We develop a Markov Chain Monte Carlo algorithm which allows for Bayesian estimation and prediction. In exercises involving artificial and real macroeconomic data, we demonstrate that the choice of variable ordering can have non-negligible effects on empirical results. In a macroeconomic forecasting exercise involving VARs with 20 variables we find that our order-invariant approach leads to the best forecasts and that some choices of variable ordering can lead to poor forecasts using a conventional, non-order invariant, approach."
            },
            {
                "arxivId": "1812.07259",
                "title": "Comparing Spike and Slab Priors for Bayesian Variable Selection",
                "abstract": "An important task in building regression models is to decide which regressors should be included in the final model. In a Bayesian approach, variable selection can be performed using mixture priors with a spike and a slab component for the effects subject to selection. As the spike is concentrated at zero, variable selection is based on the probability of assigning the corresponding regression effect to the slab component. These posterior inclusion probabilities can be determined by MCMC sampling. In this paper we compare the MCMC implementations for several spike and slab priors with regard to posterior inclusion probabilities and their sampling efficiency for simulated data. Further, we investigate posterior inclusion probabilities analytically for different slabs in two simple settings. Application of variable selection with spike and slab priors is illustrated on a data set of psychiatric patients where the goal is to identify covariates affecting metabolism."
            },
            {
                "arxivId": "0907.4010",
                "title": "Simulation of truncated normal variables",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2404.02612",
        "category": "econ",
        "title": "Exploring Sustainable Clothing Consumption in Middle-Income Countries: A case study of Romanian consumers",
        "abstract": "The overconsumption of consumers under today's increasingly scarce natural resources has overwhelmed the textile industry in middle-income countries, such as Romania. It is becoming more and more essential to encourage sustainable clothing consumption behaviors, such as purchasing recyclable clothes. Notwithstanding there is a limited number of studies trying to understand the intrinsic factors that motivate consumers' purchase intention toward sustainable clothes in middle-income countries. Moreover, the effect of consumers' environmental knowledge on determining their purchase intention of sustainable clothes remains understudied. Consequently, the purpose of this paper is to make a significant contribution to the sustainable consumption literature by providing a consolidated framework that explores the behavioral factors inclining Romanian consumers' purchase intention towards sustainable clothes. The foundation of this study combines consumers' social value orientation and the theory of planned behavior. the partial least square path modelling procedure was used to analyze the data of 1,018 Romanian respondents. The findings of this study show that altruistic value orientation, subjective norms, and sustainable attitudes have a positive effect on Romanian consumers' purchase intention of sustainable clothing. Thus, these insights provide essential practical implications of advocating for the consumption of sustainable clothes along with useful guidelines for practitioners in the textile industry among middle-income countries, especially in Romania, to reduce overconsumption.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2404.12462",
        "category": "econ",
        "title": "Axiomatic modeling of fixed proportion technologies",
        "abstract": "Understanding input substitution and output transformation possibilities is critical for efficient resource allocation and firm strategy. There are important examples of fixed proportion technologies where certain inputs are non-substitutable and/or certain outputs are non-transformable. However, there is widespread confusion about the appropriate modeling of fixed proportion technologies in data envelopment analysis. We point out and rectify several misconceptions in the existing literature, and show how fixed proportion technologies can be correctly incorporated into the axiomatic framework. A Monte Carlo study is performed to demonstrate the proposed solution.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2404.12477",
        "category": "econ",
        "title": "Sustainable regional economic development and land use: a case of Russia",
        "abstract": "This paper analyzes sustainable regional economic development and land use employing a case study of Russia. The economics of land management in Russia which is shaped by both historical legacies and contemporary policies represents an interesting conundrum. Following the dissolution of the Soviet Union, Russia embarked on a thorny and complex path towards the economic reforms and transformation characterized, among all, by the privatization and decentralization of land ownership. This transition was aimed at improving agricultural productivity and fostering sustainable regional economic development but also led to new challenges such as uneven distribution of land resources, unclear property rights, and underinvestment in rural infrastructure. However, managing all of that effectively poses significant challenges and opportunities. With the help of the comprehensive bibliographic network analysis, this study sheds some light on the current state of sustainable regional economic development and land use management in Russia. Its results and outcomes might be helpful for the researchers and stakeholders alike in devising effective strategies aimed at maximizing resources for sustainable land use, particularly within their respective regional economies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2404.12581",
        "category": "econ",
        "title": "Two-step Estimation of Network Formation Models with Unobserved Heterogeneities and Strategic Interactions",
        "abstract": "In this paper, I characterize the network formation process as a static game of incomplete information, where the latent payoff of forming a link between two individuals depends on the structure of the network, as well as private information on agents' attributes. I allow agents' private unobserved attributes to be correlated with observed attributes through individual fixed effects. Using data from a single large network, I propose a two-step estimator for the model primitives. In the first step, I estimate agents' equilibrium beliefs of other people's choice probabilities. In the second step, I plug in the first-step estimator to the conditional choice probability expression and estimate the model parameters and the unobserved individual fixed effects together using Joint MLE. Assuming that the observed attributes are discrete, I showed that the first step estimator is uniformly consistent with rate $N^{-1/4}$, where $N$ is the total number of linking proposals. I also show that the second-step estimator converges asymptotically to a normal distribution at the same rate.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2404.12974",
        "category": "econ",
        "title": "Quantifying seasonal hydrogen storage demands under cost and market uptake uncertainties in energy system transformation pathways",
        "abstract": "Climate neutrality paradigms put electricity systems at the core of a clean energy supply. At the same time, indirect electrification, with a potential uptake of hydrogen or derived fuel economy, plays a crucial role in decarbonising the energy supply and industrial processes. Besides energy markets coordinating the transition, climate and energy policy targets require fundamental changes and expansions in the energy transmission, import, distribution, and storage infrastructures. While existing studies identify relevant demands for hydrogen, critical decisions involve imports versus domestic fuel production and investments in new or repurposing existing pipeline and storage infrastructure. Linking the pan-European energy system planning model SCOPE SD with the multiperiod European gas market model IMAGINE, the case study analysis and its transformation pathway results indicate extensive network development of hydrogen infrastructure, including expansion beyond refurbished methane infrastructure. However, the ranges of future hydrogen storage costs and market uptake restrictions expose and quantify the uncertainty of its role in Europes transformation. The study finds that rapidly planning the construction of hydrogen storage and pipeline infrastructure is crucial to achieving the required capacity by 2050.",
        "references": [
            {
                "arxivId": "2310.01250",
                "title": "Impact of large-scale hydrogen electrification and retrofitting of natural gas infrastructure on the European power system",
                "abstract": null
            },
            {
                "arxivId": "2305.02232",
                "title": "Ramping up the hydrogen sector: An energy system modeling framework",
                "abstract": null
            },
            {
                "arxivId": "2207.05816",
                "title": "The potential role of a hydrogen network in Europe",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-22.json",
        "arxivId": "2404.12988",
        "category": "econ",
        "title": "Understanding Intra-Household Educational Inequalities: Gender, Birth Order, and Ability Dynamics in Benin's Households",
        "abstract": "This paper investigates the nuanced interplay of gender, birth order, and innate ability in shaping educational disparities among children within households, employing both a reduced-form analysis and a structural model of household resource allocation. By decomposing overall disparities, I identify the contributions of gender, birth order, and innate ability differences. In the context of Benin, for households with non-educated heads and mixed-gender children, total inequality comprises 50% gender disparity, 20% birth order effect, and 30% ability gap. Conversely, in households with only sons or only daughters and non-educated heads, total inequality is predominantly driven by ability disparities 70% for daughters and 83% for sons, with lesser contributions from birth order effects. Furthermore, my analysis reveals that in households with non-educated heads, firstborn daughters surpass their younger brothers in educational attainment on the intensive margin if their innate ability exceeds their brothers' by at least 13%, a figure reduced to 8% in households with college-educated parents. Additionally, the study unveils parental preferences favoring sons' education over daughters' among non-educated parents, with a perceived 22% higher average benefit. Targeted policies aimed at reducing composite education costs prove effective in mitigating gender and birth order gaps, albeit with a modest 5% reduction in total inequality. Overall, this research underscores the complex dynamics influencing intra-household educational inequalities and suggests policy avenues to address them.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-23.json",
        "arxivId": "2206.01840",
        "category": "econ",
        "title": "The Effect of External Debt on Greenhouse Gas Emissions",
        "abstract": "We estimate the causal effect of external debt on greenhouse gas emissions in a panel of 78 emerging market and developing economies over the 1990-2015 period. Unlike previous literature, we use external instruments to address the potential endogeneity in the relationship between external debt and greenhouse gas emissions. Specifically, we use international liquidity shocks as instrumental variables for external debt. We find that dealing with the potential endogeneity problem brings about a positive and statistically significant effect of external debt on greenhouse gas emissions: a 1 percentage point (pp.) rise in external debt causes, on average, a 0.5% increase in greenhouse gas emissions. One possible mechanism of action could be that, as external debt increases, governments are less able to enforce environmental regulations because their main priority is to increase the tax base to pay increasing debt services or because they are captured by the private sector who owns that debt and prevented from tightening such regulations.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-23.json",
        "arxivId": "2404.13178",
        "category": "econ",
        "title": "The benefits and costs of agglomeration: insights from economics and complexity",
        "abstract": "There are many benefits and costs that come from people and firms clustering together in space. Agglomeration economies, in particular, are the manifestation of centripetal forces that make larger cities disproportionately more wealthy than smaller cities, pulling together individuals and firms in close physical proximity. Measuring agglomeration economies, however, is not easy, and the identification of its causes is still debated. Such association of productivity with size can arise from interactions that are facilitated by cities (\"positive externalities\"), but also from more productive individuals moving in and sorting into large cities (\"self-sorting\"). Under certain circumstances, even pure randomness can generate increasing returns to scale. In this chapter, we discuss some of the empirical observations, models, measurement challenges, and open question associated with the phenomenon of agglomeration economies. Furthermore, we discuss the implications of urban complexity theory, and in particular urban scaling, for the literature in agglomeration economies.",
        "references": [
            {
                "arxivId": "2006.14140",
                "title": "Spatial interactions in urban scaling laws",
                "abstract": "Analyses of urban scaling laws assume that observations in different cities are independent of the existence of nearby cities. Here we introduce generative models and data-analysis methods that overcome this limitation by modelling explicitly the effect of interactions between individuals at different locations. Parameters that describe the scaling law and the spatial interactions are inferred from data simultaneously, allowing for rigorous (Bayesian) model comparison and overcoming the problem of defining the boundaries of urban regions. Results in five different datasets show that including spatial interactions typically leads to better models and a change in the exponent of the scaling law."
            },
            {
                "arxivId": "1910.07166",
                "title": "Evidence for localization and urbanization economies in urban scaling",
                "abstract": "We study the scaling of (i) numbers of workers and aggregate incomes by occupational categories against city size, and (ii) total incomes against numbers of workers in different occupations, across the functional metropolitan areas of Australia and the USA. The number of workers and aggregate incomes in specific high-income knowledge economy-related occupations and industries show increasing returns to scale by city size, showing that localization economies within particular industries account for superlinear effects. However, when total urban area incomes and/or gross domestic products are regressed using a generalized Cobb\u2013Douglas function against the number of workers in different occupations as labour inputs, constant returns to scale in productivity against city size are observed. This implies that the urbanization economies at the whole city level show linear scaling or constant returns to scale. Furthermore, industrial and occupational organizations, not population size, largely explain the observed productivity variable. The results show that some very specific industries and occupations contribute to the observed overall superlinearity. The findings suggest that it is not just size but also that it is the diversity of specific intra-city organization of economic and social activity and physical infrastructure that should be used to understand urban scaling behaviours."
            },
            {
                "arxivId": "1802.00972",
                "title": "Unveiling relationships between crime and property in England and Wales via density scale-adjusted metrics and network tools",
                "abstract": "Scale-adjusted metrics (SAMs) are a significant achievement of the urban scaling hypothesis. SAMs remove the inherent biases of per capita measures computed in the absence of isometric allometries. However, this approach is limited to urban areas, while a large portion of the world\u2019s population still lives outside cities and rural areas dominate land use worldwide. Here, we extend the concept of SAMs to population density scale-adjusted metrics (DSAMs) to reveal relationships among different types of crime and property metrics. Our approach allows all human environments to be considered, avoids problems in the definition of urban areas, and accounts for the heterogeneity of population distributions within urban regions. By combining DSAMs, cross-correlation, and complex network analysis, we find that crime and property types have intricate and hierarchically organized relationships leading to some striking conclusions. Drugs and burglary had uncorrelated DSAMs and, to the extent property transaction values are indicators of affluence, twelve out of fourteen crime metrics showed no evidence of specifically targeting affluence. Burglary and robbery were the most connected in our network analysis and the modular structures suggest an alternative to \u201czero-tolerance\u201d policies by unveiling the crime and/or property types most likely to affect each other."
            },
            {
                "arxivId": "1604.07876",
                "title": "Explaining the prevalence, scaling and variance of urban phenomena",
                "abstract": null
            },
            {
                "arxivId": "1604.02872",
                "title": "Is this scaling nonlinear?",
                "abstract": "One of the most celebrated findings in complex systems in the last decade is that different indexes y (e.g. patents) scale nonlinearly with the population x of the cities in which they appear, i.e. y\u223cx\u03b2,\u03b2\u22601. More recently, the generality of this finding has been questioned in studies that used new databases and different definitions of city boundaries. In this paper, we investigate the existence of nonlinear scaling, using a probabilistic framework in which fluctuations are accounted for explicitly. In particular, we show that this allows not only to (i) estimate \u03b2 and confidence intervals, but also to (ii) quantify the evidence in favour of \u03b2\u22601 and (iii) test the hypothesis that the observations are compatible with the nonlinear scaling. We employ this framework to compare five different models to 15 different datasets and we find that the answers to points (i)\u2013(iii) crucially depend on the fluctuations contained in the data, on how they are modelled, and on the fact that the city sizes are heavy-tailed distributed."
            },
            {
                "arxivId": "1301.5919",
                "title": "The hypothesis of urban scaling: formalization, implications and challenges",
                "abstract": "There is strong expectation that cities, across time, culture and level of development, share much in common in terms of their form and function. Recently, attempts to formalize mathematically these expectations have led to the hypothesis of urban scaling, namely that certain properties of all cities change, on average, with their size in predictable scale-invariant ways. The emergence of these scaling relations depends on a few general properties of cities as social networks, co-located in space and time, that conceivably apply to a wide range of human settlements. Here, we discuss the present evidence for the hypothesis of urban scaling, some of the methodological issues dealing with proxy measurements and units of analysis and place these ndings in the context of other theories of cities and urban systems. We show that a large body of evidence about the scaling properties of cities indicates, in analogy to other complex systems, that they cannot be treated as extensive systems and discuss the consequences of these results for an emerging statistical theory of cities."
            },
            {
                "arxivId": "1105.5170",
                "title": "Modeling Users' Activity on Twitter Networks: Validation of Dunbar's Number",
                "abstract": "Microblogging and mobile devices appear to augment human social capabilities, which raises the question whether they remove cognitive or biological constraints on human communication. In this paper we analyze a dataset of Twitter conversations collected across six months involving 1.7 million individuals and test the theoretical cognitive limit on the number of stable social relationships known as Dunbar's number. We find that the data are in agreement with Dunbar's result; users can entertain a maximum of 100\u2013200 stable relationships. Thus, the \u2018economy of attention\u2019 is limited in the online world by cognitive and biological constraints as predicted by Dunbar's theory. We propose a simple model for users' behavior that includes finite priority queuing and time resources that reproduces the observed social behavior."
            },
            {
                "arxivId": "1102.4101",
                "title": "Scaling and Hierarchy in Urban Economies",
                "abstract": "In several recent publications, Bettencourt, West and collaborators claim that properties of cities such as gross economic production, personal income, numbers of patents filed, number of crimes committed, etc., show super-linear power-scaling with total population, while measures of resource use show sub-linear power-law scaling. Re-analysis of the gross economic production and personal income for cities in the United States, however, shows that the data cannot distinguish between power laws and other functional forms, including logarithmic growth, and that size predicts relatively little of the variation between cities. The striking appearance of scaling in previous work is largely artifact of using extensive quantities (city-wide totals) rather than intensive ones (per-capita rates). The remaining dependence of productivity on city size is explained by concentration of specialist service industries, with high value-added per worker, in larger cities, in accordance with the long-standing economic notion of the \"hierarchy of central places\"."
            },
            {
                "arxivId": "0909.3890",
                "title": "The building blocks of economic complexity",
                "abstract": "For Adam Smith, wealth was related to the division of labor. As people and firms specialize in different activities, economic efficiency increases, suggesting that development is associated with an increase in the number of individual activities and with the complexity that emerges from the interactions between them. Here we develop a view of economic growth and development that gives a central role to the complexity of a country's economy by interpreting trade data as a bipartite network in which countries are connected to the products they export, and show that it is possible to quantify the complexity of a country's economy by characterizing the structure of this network. Furthermore, we show that the measures of complexity we derive are correlated with a country's level of income, and that deviations from this relationship are predictive of future growth. This suggests that countries tend to converge to the level of income dictated by the complexity of their productive structures, indicating that development efforts should focus on generating the conditions that would allow complexity to emerge to generate sustained growth and prosperity."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-23.json",
        "arxivId": "2404.13189",
        "category": "econ",
        "title": "Use of two Public Distributed Ledgers to track the money of an economy",
        "abstract": "A tool to improve the effectiveness and the efficiency of public spending is proposed here. In the 19th century banknotes had a serial number. However, in modern days the use of digital transactions that do not use physical currency has opened the possibility to digitally track almost each cent of the economy. In this article a serial number or tracking number for each cent, pence or any other monetary unit of the economy is proposed. Then, almost all cents can be tracked by recording the transactions in a public distributed ledger, rather than recording the amount of the transaction, the information recorded in the block of the transaction is the actual serial number or tracking number for each cent that changes ownership. In order to keep the privacy of the transaction, only generic identification of private companies and individuals are recorded along with generic information about the concept of transaction, the region and the date/time. A secondary public distributed ledger whose blocks are identified by a hash reference that is recorded in the bank statement available to the payer and the payee allows for checking the accuracy of the first public distributed ledger by comparing the transactions made in one day, one region and one type of concept. However, the transactions made or received by the government are recorded with a much higher level of detail in the first ledger and a higher level of disclosure in the second ledger. The result is a tool that is able to accurately track public spending, to keep privacy of individuals and companies and to make statistical analysis and experiments or real tests in the economy of a country. This tool has the potential to assist public policymakers in demonstrating the societal benefits resulting from their policies, thereby enabling more informed decision-making for future policy endeavours.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-23.json",
        "arxivId": "2404.13198",
        "category": "econ",
        "title": "An economically-consistent discrete choice model with flexible utility specification based on artificial neural networks",
        "abstract": "Random utility maximisation (RUM) models are one of the cornerstones of discrete choice modelling. However, specifying the utility function of RUM models is not straightforward and has a considerable impact on the resulting interpretable outcomes and welfare measures. In this paper, we propose a new discrete choice model based on artificial neural networks (ANNs) named\"Alternative-Specific and Shared weights Neural Network (ASS-NN)\", which provides a further balance between flexible utility approximation from the data and consistency with two assumptions: RUM theory and fungibility of money (i.e.,\"one euro is one euro\"). Therefore, the ASS-NN can derive economically-consistent outcomes, such as marginal utilities or willingness to pay, without explicitly specifying the utility functional form. Using a Monte Carlo experiment and empirical data from the Swissmetro dataset, we show that ASS-NN outperforms (in terms of goodness of fit) conventional multinomial logit (MNL) models under different utility specifications. Furthermore, we show how the ASS-NN is used to derive marginal utilities and willingness to pay measures.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-23.json",
        "arxivId": "2404.13869",
        "category": "econ",
        "title": "A separate way to measure rate of return",
        "abstract": "Net profit is sometimes found from data for net operating surplus. We propose a way to find it from data for consumption, pay and market-value capital, and concomitantly to reveal the factor shares in consumption.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-23.json",
        "arxivId": "2404.14141",
        "category": "econ",
        "title": "Competition and Collaboration in Crowdsourcing Communities: What happens when peers evaluate each other?",
        "abstract": "Crowdsourcing has evolved as an organizational approach to distributed problem solving and innovation. As contests are embedded in online communities and evaluation rights are assigned to the crowd, community members face a tension: they find themselves exposed to both competitive motives to win the contest prize and collaborative participation motives in the community. The competitive motive suggests they may evaluate rivals strategically according to their self-interest, the collaborative motive suggests they may evaluate their peers truthfully according to mutual interest. Using field data from Threadless on 38 million peer evaluations of more than 150,000 submissions across 75,000 individuals over 10 years and two natural experiments to rule out alternative explanations, we answer the question of how community members resolve this tension. We show that as their skill level increases, they become increasingly competitive and shift from using self-promotion to sabotaging their closest competitors. However, we also find signs of collaborative behavior when high-skilled members show leniency toward those community members who do not directly threaten their chance of winning. We explain how the individual-level use of strategic evaluations translates into important organizational-level outcomes by affecting the community structure through individuals' long-term participation. While low-skill targets of sabotage are less likely to participate in future contests, high-skill targets are more likely. This suggests a feedback loop between competitive evaluation behavior and future participation. These findings have important implications for the literature on crowdsourcing design, and the evolution and sustainability of crowdsourcing communities.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2004.01623",
        "category": "econ",
        "title": "Estimation and Uniform Inference in Sparse High-Dimensional Additive Models",
        "abstract": "We develop a novel method to construct uniformly valid confidence bands for a nonparametric component $f_1$ in the sparse additive model $Y=f_1(X_1)+\\ldots + f_p(X_p) + \\varepsilon$ in a high-dimensional setting. Our method integrates sieve estimation into a high-dimensional Z-estimation framework, facilitating the construction of uniformly valid confidence bands for the target component $f_1$. To form these confidence bands, we employ a multiplier bootstrap procedure. Additionally, we provide rates for the uniform lasso estimation in high dimensions, which may be of independent interest. Through simulation studies, we demonstrate that our proposed method delivers reliable results in terms of estimation and coverage, even in small samples.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2104.14662",
        "category": "econ",
        "title": "Dynamic Population Games: A Tractable Intersection of Mean-Field Games and Population Games",
        "abstract": "In many real-world large-scale decision problems, self-interested agents have individual dynamics and optimize their own long-term payoffs. Important examples include the competitive access to shared resources (e.g., roads, energy, or bandwidth) but also non-engineering domains like epidemic propagation and control. These problems are natural to model as mean-field games. However, existing mathematical formulations of mean field games have had limited applicability in practice, since they require solving non-standard initial-terminal-value problems that are tractable only in limited special cases. In this letter, we propose a novel formulation, along with computational tools, for a practically relevant class of Dynamic Population Games (DPGs), which correspond to discrete-time, finite-state-and-action, stationary mean-field games. Our main contribution is a mathematical reduction of Stationary Nash Equilibria (SNE) in DPGs to standard Nash Equilibria (NE) in static population games. This reduction is leveraged to guarantee the existence of a SNE, develop an evolutionary dynamics-based SNE computation algorithm, and derive simple conditions that guarantee stability and uniqueness of the SNE. Additionally, DPGs enable us to tractably incorporate multiple agent types, which is of particular importance to assess fairness concerns in resource allocation problems. We demonstrate our results by computing the SNE in two complex application examples: fair resource allocation with heterogeneous agents and control of epidemic propagation. Open source software for SNE computation: https://gitlab.ethz.ch/elokdae/dynamic-population-games",
        "references": [
            {
                "arxivId": "2304.04282",
                "title": "Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium - Except When They Do",
                "abstract": "The framework of multi-agent learning explores the dynamics of how individual agent strategies evolve in response to the evolving strategies of other agents. Of particular interest is whether or not agent strategies converge to well known solution concepts such as Nash Equilibrium (NE). Most\"fixed order\"learning dynamics restrict an agent's underlying state to be its own strategy. In\"higher order\"learning, agent dynamics can include auxiliary states that can capture phenomena such as path dependencies. We introduce higher-order gradient play dynamics that resemble projected gradient ascent with auxiliary states. The dynamics are\"payoff based\"in that each agent's dynamics depend on its own evolving payoff. While these payoffs depend on the strategies of other agents in a game setting, agent dynamics do not depend explicitly on the nature of the game or the strategies of other agents. In this sense, dynamics are\"uncoupled\"since an agent's dynamics do not depend explicitly on the utility functions of other agents. We first show that for any specific game with an isolated completely mixed-strategy NE, there exist higher-order gradient play dynamics that lead (locally) to that NE, both for the specific game and nearby games with perturbed utility functions. Conversely, we show that for any higher-order gradient play dynamics, there exists a game with a unique isolated completely mixed-strategy NE for which the dynamics do not lead to NE. These results build on prior work that showed that uncoupled fixed-order learning cannot lead to NE in certain instances, whereas higher-order variants can. Finally, we consider the mixed-strategy equilibrium associated with coordination games. While higher-order gradient play can converge to such equilibria, we show such dynamics must be inherently internally unstable."
            },
            {
                "arxivId": "2212.14449",
                "title": "Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games",
                "abstract": "Mean-field games have been used as a theoretical tool to obtain an approximate Nash equilibrium for symmetric and anonymous $N$-player games. However, limiting applicability, existing theoretical results assume variations of a\"population generative model\", which allows arbitrary modifications of the population distribution by the learning algorithm. Moreover, learning algorithms typically work on abstract simulators with population instead of the $N$-player game. Instead, we show that $N$ agents running policy mirror ascent converge to the Nash equilibrium of the regularized game within $\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$ samples from a single sample trajectory without a population generative model, up to a standard $\\mathcal{O}(\\frac{1}{\\sqrt{N}})$ error due to the mean field. Taking a divergent approach from the literature, instead of working with the best-response map we first show that a policy mirror ascent map can be used to construct a contractive operator having the Nash equilibrium as its fixed point. We analyze single-path TD learning for $N$-agent games, proving sample complexity guarantees by only using a sample path from the $N$-agent simulator without a population generative model. Furthermore, we demonstrate that our methodology allows for independent learning by $N$ agents with finite sample guarantees."
            },
            {
                "arxivId": "2207.00495",
                "title": "A self-contained karma economy for the dynamic allocation of common resources",
                "abstract": null
            },
            {
                "arxivId": "2112.12313",
                "title": "Mean field game for modeling of COVID-19 spread",
                "abstract": null
            },
            {
                "arxivId": "2109.03182",
                "title": "A Dynamic Population Model of Strategic Interaction and Migration under Epidemic Risk",
                "abstract": "In this paper, we show how a dynamic population game can model the strategic interaction and migration decisions made by a large population of agents in response to epidemic prevalence. Specifically, we consider a modified susceptible-asymptomatic-infected-recovered (SAIR) epidemic model over multiple zones. Agents choose whether to activate (i.e., interact with others), how many other agents to interact with, and which zone to move to in a time-scale which is comparable with the epidemic evolution. We define and analyze the notion of equilibrium in this game, and investigate the transient behavior of the epidemic spread in a range of numerical case studies, providing insights on the effects of the agents\u2019 degree of future awareness, strategic migration decisions, as well as different levels of lockdown and other interventions. One of our key findings is that the strategic behavior of agents plays an important role in the progression of the epidemic and can be exploited in order to design suitable epidemic control measures."
            },
            {
                "arxivId": "2005.03797",
                "title": "Dissipativity Tools for Convergence to Nash Equilibria in Population Games",
                "abstract": "We analyze the stability of a nonlinear dynamical model describing the noncooperative strategic interactions among the agents of a finite collection of populations. Each agent selects one strategy at a time and revises it repeatedly according to a protocol that typically prioritizes strategies whose payoffs are either higher than those of the current strategy or exceed the population average. The model is predicated on well-established research in population and evolutionary games, and has two components. The first is the payoff dynamics model (PDM), which ascribes the payoff to each strategy according to the so-called social state vector whose entries are the proportions of every population adopting the available strategies. The second component is the evolutionary dynamics model (EDM) that accounts for the revision process. In our model, the social state at equilibrium is a best response to the strategies\u2019 payoffs, and can be viewed as a Nash-like solution that has predictive value when it is globally asymptotically stable (GAS). We present a systematic methodology that ascertains GAS by checking separately whether the EDM and PDM satisfy appropriately defined system-theoretic dissipativity properties. Our work generalizes pioneering methods based on notions of contractivity applicable to memoryless PDMs, and more general system-theoretic passivity conditions. As demonstrated with examples, the added flexibility afforded by our approach is particularly useful when the contraction properties of the PDM are unequal across populations."
            },
            {
                "arxivId": "2003.06069",
                "title": "A General Framework for Learning Mean-Field Games",
                "abstract": "This paper presents a general mean-field game (GMFG) framework for simultaneous learning and decision making in stochastic games with a large population. It first establishes the existence of a unique Nash equilibrium to this GMFG, and it demonstrates that naively combining reinforcement learning with the fixed-point approach in classical mean-field games yields unstable algorithms. It then proposes value-based and policy-based reinforcement learning algorithms (GMF-V and GMF-P, respectively) with smoothed policies, with analysis of their convergence properties and computational complexities. Experiments on an equilibrium product pricing problem demonstrate that two specific instantiations of GMF-V with Q-learning and GMF-P with trust region policy optimization\u2014GMF-V-Q and GMF-P-TRPO, respectively\u2014are both efficient and robust in the GMFG setting. Moreover, their performance is superior in convergence speed, accuracy, and stability when compared with existing algorithms for multiagent reinforcement learning in the N-player setting."
            },
            {
                "arxivId": "2010.09385",
                "title": "Stationary Equilibria of Mean Field Games with Finite State and Action Space",
                "abstract": null
            },
            {
                "arxivId": "1808.04464",
                "title": "On Passivity, Reinforcement Learning, and Higher Order Learning in Multiagent Finite Games",
                "abstract": "In this article, we propose a passivity-based methodology for the analysis and design of reinforcement learning dynamics and algorithms in multiagent finite games. Starting from a known, first-order reinforcement learning scheme, we show that convergence to a Nash distribution can be attained in a broader class of games than previously considered in the literature\u2014namely, in games characterized by the monotonicity property of their (negative) payoff vectors. We further exploit passivity techniques to design a class of higher order learning schemes that preserve the convergence properties of their first-order counterparts. Moreover, we show that the higher order schemes improve upon the rate of convergence and can even achieve convergence where the first-order scheme fails. We demonstrate these properties through numerical simulations for several representative games."
            },
            {
                "arxivId": "1612.07878",
                "title": "Markov-Nash equilibria in mean-field games with discounted cost",
                "abstract": "In this paper, we consider discrete-time dynamic games of the mean-field type with a finite number, N, of agents subject to an infinite-horizon discounted-cost optimality criterion. The state space of each agent is a locally compact Polish space. At each time, the agents are coupled through the empirical distribution of their states, which affects both the agents' individual costs and their state transition probabilities. We introduce the solution concept of Markov-Nash equilibrium, under which a policy is player-by-player optimal in the class of all Markov policies. Under mild assumptions, we demonstrate the existence of a mean-field equilibrium in the infinite-population limit, N \u2192 1, and then show that the policy obtained from the mean-field equilibrium is approximately Markov-Nash when the number of agents N is sufficiently large."
            },
            {
                "arxivId": "1511.06576",
                "title": "Two Numerical Approaches to Stationary Mean-Field Games",
                "abstract": null
            },
            {
                "arxivId": "1404.5741",
                "title": "Linear-Quadratic Mean Field Games",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2205.07950",
        "category": "econ",
        "title": "The Power of Tests for Detecting $p$-Hacking",
        "abstract": "$p$-Hacking undermines the validity of empirical studies. A flourishing empirical literature investigates the prevalence of $p$-hacking based on the distribution of $p$-values across studies. Interpreting results in this literature requires a careful understanding of the power of methods for detecting $p$-hacking. We theoretically study the implications of likely forms of $p$-hacking on the distribution of $p$-values to understand the power of tests for detecting it. Power depends crucially on the $p$-hacking strategy and the distribution of true effects. Publication bias can enhance the power for testing the joint null of no $p$-hacking and no publication bias.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2212.09715",
        "category": "econ",
        "title": "Liquid Democracy. Two Experiments on Delegation in Voting",
        "abstract": "Proponents of participatory democracy praise Liquid Democracy: decisions are taken by referendum, but voters delegate their votes freely. When better informed voters are present, delegation can increase the probability of a correct decision. However, delegation must be used sparely because it reduces the information aggregated through voting. In two different experiments, we find that delegation underperforms both universal majority voting and the simpler option of abstention. In a tightly controlled lab experiment where the subjects' precision of information is conveyed in precise mathematical terms and very salient, the result is due to overdelegation. In a perceptual task run online where the precision of information is not known precisely, delegation remains very high and again underperforms both majority voting and abstention. In addition, subjects substantially overestimate the precision of the better informed voters, underlining that Liquid Democracy is fragile to multiple sources of noise. The paper makes an innovative methodological contribution by combining two very different experimental procedures: the study of voting rules would benefit from complementing controlled experiments with known precision of information with tests under ambiguity, a realistic assumption in many voting situations.",
        "references": [
            {
                "arxivId": "1808.01906",
                "title": "The Fluid Mechanics of Liquid Democracy",
                "abstract": "Liquid democracy is the principle of making collective decisions by letting agents transitively delegate their votes. Despite its significant appeal, it has become apparent that a weakness of liquid democracy is that a small subset of agents may gain massive influence. To address this, we propose to change the current practice by allowing agents to specify multiple delegation options instead of just one. Much like in nature, where\u2014fluid mechanics teaches us\u2014liquid maintains an equal level in connected vessels, we seek to control the flow of votes in a way that balances influence as much as possible. Specifically, we analyze the problem of choosing delegations to approximately minimize the maximum number of votes entrusted to any agent by drawing connections to the literature on confluent flow. We also introduce a random graph model for liquid democracy and use it to demonstrate the benefits of our approach both theoretically and empirically."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2310.04176",
        "category": "econ",
        "title": "Approximating the set of Nash equilibria for convex games",
        "abstract": "In Feinstein and Rudloff (2023), it was shown that the set of Nash equilibria for any non-cooperative $N$ player game coincides with the set of Pareto optimal points of a certain vector optimization problem with non-convex ordering cone. To avoid dealing with a non-convex ordering cone, an equivalent characterization of the set of Nash equilibria as the intersection of the Pareto optimal points of $N$ multi-objective problems (i.e.\\ with the natural ordering cone) is proven. So far, algorithms to compute the exact set of Pareto optimal points of a multi-objective problem exist only for the class of linear problems, which reduces the possibility of finding the true set of Nash equilibria by those algorithms to linear games only. In this paper, we will consider the larger class of convex games. As, typically, only approximate solutions can be computed for convex vector optimization problems, we first show, in total analogy to the result above, that the set of $\\epsilon$-approximate Nash equilibria can be characterized by the intersection of $\\epsilon$-approximate Pareto optimal points for $N$ convex multi-objective problems. Then, we propose an algorithm based on results from vector optimization and convex projections that allows for the computation of a set that, on one hand, contains the set of all true Nash equilibria, and is, on the other hand, contained in the set of $\\epsilon$-approximate Nash equilibria. In addition to the joint convexity of the cost function for each player, this algorithm works provided the players are restricted by either shared polyhedral constraints or independent convex constraints.",
        "references": [
            {
                "arxivId": "2103.06613",
                "title": "On the approximation error for approximating convex bodies using multiobjective optimization",
                "abstract": "A polyhedral approximation of a convex body can be calculated by solving approximately an associated multiobjective convex program (MOCP). An MOCP can be solved approximately by Benson type algorithms, which compute outer and inner polyhedral approximations of the problem's upper image. Polyhedral approximations of a convex body can be obtained from polyhedral approximations of the upper image of the associated MOCP. We provide error bounds in terms of the Hausdorff distance for the polyhedral approximations of a convex body in dependence of the stopping criterion of the primal and dual Benson type algorithms which are applied to the associated MOCP."
            },
            {
                "arxivId": "2007.04388",
                "title": "Continuity and sensitivity analysis of parameterized Nash games",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2311.04374",
        "category": "econ",
        "title": "Common Knowledge, Regained",
        "abstract": "For common knowledge to arise in dynamic settings, all players must simultaneously come to know it has arisen. Consequently, common knowledge cannot arise in many realistic settings with timing frictions. This counterintuitive observation of Halpern and Moses (1990) was discussed by Arrow et al. (1987) and Aumann (1989), was called a paradox by Morris (2014), and has evaded satisfactory resolution for four decades. We resolve this paradox by proposing a new definition for common knowledge, which coincides with the traditional one in static settings but is more permissive in dynamic settings. Under our definition, common knowledge can arise without simultaneity, particularly in canonical examples of the Haplern-Moses paradox. We demonstrate its usefulness by deriving for it an agreement theorem \\`a la Aumann (1976), showing it arises in the setting of Geanakoplos and Polemarchakis (1982) with timing frictions added, and applying it to characterize equilibrium behavior in a dynamic coordination game.",
        "references": [
            {
                "arxivId": "1605.07354",
                "title": "Unbeatable Set Consensus via Topological and Combinatorial Reasoning",
                "abstract": "The set consensus problem has played an important role in the study of distributed systems for over two decades. Indeed, the search for lower bounds and impossibility results for this problem spawned the topological approach to distributed computing, which has given rise to new techniques in the design and analysis of protocols. The design of efficient solutions to set consensus has also proven to be challenging. In the synchronous crash failure model, the literature contains a sequence of solutions to set consensus, each improving upon the previous ones. This paper presents an unbeatable protocol for nonuniform k-set consensus in the synchronous crash failure model. This is an efficient protocol whose decision times cannot be improved upon. Moreover, the description of our protocol is extremely succinct. Proving unbeatability of this protocol is a nontrivial challenge. We provide two proofs for its unbeatability: one is a subtle constructive combinatorial proof, and the other is a topological proof of a new style. These two proofs provide new insight into the connection between topological reasoning and combinatorial reasoning about protocols, which has long been a subject of interest. In particular, our topological proof reasons in a novel way about subcomplexes of the protocol complex, and sheds light on an open question posed by Guerraoui and Pochon (2009). Finally, using the machinery developed in the design of this unbeatable protocol, we propose a protocol for uniform k-set consensus that beats all known solutions by a large margin."
            },
            {
                "arxivId": "1203.5399",
                "title": "Agent-Time Epistemics and Coordination",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2311.16333",
        "category": "econ",
        "title": "From Reactive to Proactive Volatility Modeling with Hemisphere Neural Networks",
        "abstract": "We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density forecasting through a novel neural network architecture with dedicated mean and variance hemispheres. Our architecture features several key ingredients making MLE work in this context. First, the hemispheres share a common core at the entrance of the network which accommodates for various forms of time variation in the error variance. Second, we introduce a volatility emphasis constraint that breaks mean/variance indeterminacy in this class of overparametrized nonlinear models. Third, we conduct a blocked out-of-bag reality check to curb overfitting in both conditional moments. Fourth, the algorithm utilizes standard deep learning software and thus handles large data sets - both computationally and statistically. Ergo, our Hemisphere Neural Network (HNN) provides proactive volatility forecasts based on leading indicators when it can, and reactive volatility based on the magnitude of previous prediction errors when it must. We evaluate point and density forecasts with an extensive out-of-sample experiment and benchmark against a suite of models ranging from classics to more modern machine learning-based offerings. In all cases, HNN fares well by consistently providing accurate mean/variance forecasts for all targets and horizons. Studying the resulting volatility paths reveals its versatility, while probabilistic forecasting evaluation metrics showcase its enviable reliability. Finally, we also demonstrate how this machinery can be merged with other structured deep learning models by revisiting Goulet Coulombe (2022)'s Neural Phillips Curve.",
        "references": [
            {
                "arxivId": "2306.05568",
                "title": "Maximally Machine-Learnable Portfolios",
                "abstract": "When it comes to stock returns, any form of predictability can bolster risk-adjusted profitability. We develop a collaborative machine learning algorithm that optimizes portfolio weights so that the resulting synthetic security is maximally predictable. Precisely, we introduce MACE, a multivariate extension of Alternating Conditional Expectations that achieves the aforementioned goal by wielding a Random Forest on one side of the equation, and a constrained Ridge Regression on the other. There are two key improvements with respect to Lo and MacKinlay's original maximally predictable portfolio approach. First, it accommodates for any (nonlinear) forecasting algorithm and predictor set. Second, it handles large portfolios. We conduct exercises at the daily and monthly frequency and report significant increases in predictability and profitability using very little conditioning information. Interestingly, predictability is found in bad as well as good times, and MACE successfully navigates the debacle of 2022."
            },
            {
                "arxivId": "2012.08155",
                "title": "Real-time inflation forecasting using non-linear dimension reduction techniques",
                "abstract": null
            },
            {
                "arxivId": "2008.12706",
                "title": "Nowcasting in a Pandemic Using Non-Parametric Mixed Frequency VARs",
                "abstract": null
            },
            {
                "arxivId": "2008.01714",
                "title": "Macroeconomic data transformations matter",
                "abstract": null
            },
            {
                "arxivId": "2006.16333",
                "title": "Inference in Bayesian additive vector autoregressive tree models",
                "abstract": "Vector autoregressive (VAR) models assume linearity between the endogenous variables and their lags. This linearity assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. As a solution, we propose combining VAR with Bayesian additive regression tree (BART) models. The resulting Bayesian additive vector autoregressive tree (BAVART) model is capable of capturing arbitrary non-linear relations between the endogenous variables and the covariates without much input from the researcher. Since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. Using synthetic and real data, we demonstrate the advantages of our methods. For Eurozone data, we show that our nonparametric approach improves upon commonly used forecasting models and that it produces impulse responses to an uncertainty shock that are consistent with established findings in the literature."
            },
            {
                "arxivId": "1809.09953",
                "title": "Deep Neural Networks for Estimation and Inference: Application to Causal Effects and Other Semiparametric Estimands",
                "abstract": "We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second\u2010step inference after first\u2010step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now\u2010common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed\u2010width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression\u2010type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing."
            },
            {
                "arxivId": "1802.06300",
                "title": "Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data",
                "abstract": "We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and accounts for potential serial dependence by including block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods. When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score."
            },
            {
                "arxivId": "1712.00504",
                "title": "A Neural Stochastic Volatility Model",
                "abstract": "\n \n In this paper, we show that the recent integration of statistical models with deep recurrent neural networks provides a new way of formulating volatility (the degree of variation of time series) models that have been widely used in time series analysis and prediction in finance. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observables. Our focus here is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Experiments on real-world stock price datasets demonstrate that the proposed model generates a better volatility estimation and prediction that outperforms mainstream methods, e.g., deterministic models such as GARCH and its variants, and stochastic models namely the MCMC-based stochvol as well as the Gaussian-process-based, on average negative log-likelihood.\n \n"
            },
            {
                "arxivId": "1708.02709",
                "title": "Recent Trends in Deep Learning Based Natural Language Processing",
                "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP."
            },
            {
                "arxivId": "1707.09461",
                "title": "Bayesian regression tree ensembles that adapt to smoothness and sparsity",
                "abstract": "Ensembles of decision trees are a useful tool for obtaining flexible estimates of regression functions. Examples of these methods include gradient\u2010boosted decision trees, random forests and Bayesian classification and regression trees. Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modifications to existing Bayesian additive regression tree algorithms."
            },
            {
                "arxivId": "1706.04599",
                "title": "On Calibration of Modern Neural Networks",
                "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
            },
            {
                "arxivId": "1704.04110",
                "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks",
                "abstract": null
            },
            {
                "arxivId": "1704.02971",
                "title": "A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction",
                "abstract": "The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction."
            },
            {
                "arxivId": "1612.01474",
                "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                "abstract": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2404.09180",
        "category": "econ",
        "title": "ge_gravity2: a command for solving universal gravity models",
        "abstract": "We describe an algorithm for computing counterfactual trade flows, prices, output, and welfare in a large class of general equilibrium trade models. We introduce a command called ge_gravity2 that allows users to perform these computations in Stata. This command extends the existing ge_gravity command by allowing users to compute the general equilibrium effects of changes in trade policy in positive supply elasticity models. It can be used to solve any model that falls into the class of universal gravity models as defined by Allen, Arkolakis, and Takahashi [Universal Gravity, Journal of Political Economy, 128(2), 2020, 393-433].",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2404.11705",
        "category": "econ",
        "title": "A Comparative Study of Electric, Internal Combustion Engine, and Hybrid Vehicles",
        "abstract": "Electric vehicles (EVs) are increasingly becoming popular as a viable means of transportation for the future. The use of EVs may help in providing better climatic conditions in urban areas with a pocket friendly cost for transportation to the consumers throughout its life. EVs enact as a boon to the society by providing zero tailpipe emissions, better comfort, low lifecycle cost and higher connectivity. The article aims to provide scientific information throughout the literature across various aspects of EVs in their lifetime and thus, assist the scholarly community and various organisations to understand the impact of EVs. In this study we have gathered information from the articles published in SCOPUS database and through grey literature with the focus of information post 2009. We have also used a hybrid methodology using Best-Worst Method (BWM) and technique for order preference by similarity to ideal solution (TOPSIS) for comparing EVs, internal combustion engine vehicles (ICEVs) and hybrid vehicles in various price segments. The study has helped us conclude that EVs should be preferred over ICEVs and hybrids by the users.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2404.14603",
        "category": "econ",
        "title": "Quantifying the Internal Validity of Weighted Estimands",
        "abstract": "In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects. The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework. Our focus is on answering two questions concerning weighted estimands. First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation? Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest? We argue that this upper bound provides a valuable diagnostic for empirical research. When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low. Our paper develops practical tools to quantify the internal validity of weighted estimands.",
        "references": [
            {
                "arxivId": "2202.02903",
                "title": "Difference-in-Differences with Time-Varying Covariates in the Parallel Trends Assumption",
                "abstract": "In this paper, we study difference-in-differences identification and estimation strategies where the parallel trends assumption holds after conditioning on time-varying covariates and/or time-invariant covariates. Our first main contribution is to point out a number of weaknesses of commonly used two-way fixed effects (TWFE) regressions in this context. In addition to issues related to multiple periods and variation in treatment timing that have been emphasized in the literature, we show that, even in the case with only two time periods, TWFE regressions are not generally robust to (i) paths of untreated potential outcomes depending on the level of time-varying covariates (as opposed to only the change in the covariates over time), (ii) paths of untreated potential outcomes depending on time-invariant covariates, and (iii) violations of linearity conditions for outcomes over time and/or the propensity score. Even in cases where none of the previous three issues hold, we show that TWFE regressions can suffer from negative weighting and weight-reversal issues. Thus, TWFE regressions can deliver misleading estimates of causal effect parameters in a number of empirically relevant cases. Second, we extend these arguments to the case of multiple periods and variation in treatment timing. Third, we provide simple diagnostics for assessing the extent of misspecification bias arising due to TWFE regressions. Finally, we propose alternative (and simple) estimation strategies that can circumvent these issues with two-way fixed regressions."
            },
            {
                "arxivId": "2106.05024",
                "title": "Contamination Bias in Linear Regressions",
                "abstract": "We study regressions with multiple treatments and a set of controls that is flexible enough to purge omitted variable bias. We show that these regressions generally fail to estimate convex averages of heterogeneous treatment effects -- instead, estimates of each treatment's effect are contaminated by non-convex averages of the effects of other treatments. We discuss three estimation approaches that avoid such contamination bias, including the targeting of easiest-to-estimate weighted average effects. A re-analysis of nine empirical applications finds economically and statistically meaningful contamination bias in observational studies; contamination bias in experimental studies is more limited due to idiosyncratic effect heterogeneity."
            },
            {
                "arxivId": "2011.06695",
                "title": "When Should We (Not) Interpret Linear IV Estimands as Late?",
                "abstract": "In this paper I revisit the interpretation of the linear instrumental variables (IV) estimand as a weighted average of conditional local average treatment effects (LATEs). I focus on a practically relevant situation in which additional covariates are required for identification but the reduced-form and first-stage regressions are possibly misspecified as a result of neglected heterogeneity in the effects of the instrument. If we also allow for conditional monotonicity, i.e. the existence of compliers but no defiers at some covariate values and the existence of defiers but no compliers elsewhere, then the weights on some conditional LATEs are negative and the IV estimand is no longer interpretable as a causal effect. Even if monotonicity holds unconditionally, the IV estimand is not interpretable as the unconditional LATE parameter unless the groups that are encouraged and not encouraged to get treated are roughly equal sized."
            },
            {
                "arxivId": "1404.1785",
                "title": "Balancing Covariates via Propensity Score Weighting",
                "abstract": "ABSTRACT Covariate balance is crucial for unconfounded descriptive or causal comparisons. However, lack of balance is common in observational studies. This article considers weighting strategies for balancing covariates. We define a general class of weights\u2014the balancing weights\u2014that balance the weighted distributions of the covariates between treatment groups. These weights incorporate the propensity score to weight each group to an analyst-selected target population. This class unifies existing weighting methods, including commonly used weights such as inverse-probability weights as special cases. General large-sample results on nonparametric estimation based on these weights are derived. We further propose a new weighting scheme, the overlap weights, in which each unit\u2019s weight is proportional to the probability of that unit being assigned to the opposite group. The overlap weights are bounded, and minimize the asymptotic variance of the weighted average treatment effect among the class of balancing weights. The overlap weights also possess a desirable small-sample exact balance property, based on which we propose a new method that achieves exact balance for means of any selected set of covariates. Two applications illustrate these methods and compare them with other approaches."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2404.14927",
        "category": "econ",
        "title": "Optimal Refund Mechanism with Consumer Learning",
        "abstract": "This paper studies the optimal refund mechanism when an uninformed buyer can privately acquire information about his valuation of a product over time. We consider a class of refund mechanisms based on stochastic return policies: if the buyer requests a return, the seller will issue a (partial) refund while allowing the buyer to keep the product with some probability. Such return policies can affect the buyer's learning process and thereby influence the return rate. Nevertheless, we show that the optimal refund mechanism is deterministic and takes a simple form: either the seller offers a sufficiently low price and disallows returns to deter buyer learning, or she offers a sufficiently high price with free returns to implement maximal buyer learning. The form of the optimal refund mechanism is non-monotone in the buyer's prior belief regarding his valuation.",
        "references": [
            {
                "arxivId": "1808.02233",
                "title": "Robust Pricing With Refunds",
                "abstract": "Before purchase, a buyer of an experience good learns about the product's fit using various information sources, including some of which the seller may be unaware of. The buyer, however, can conclusively learn the fit only after purchasing and trying out the product. We show that the seller can use a simple mechanism to best take advantage of the buyer's post-purchase learning to maximize his guaranteed-profit. We show that this mechanism combines a generous refund, which performs well when the buyer is relatively informed, with non-refundable random discounts, which work well when the buyer is relatively uninformed. JEL: D82, C79, D42"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2404.15158",
        "category": "econ",
        "title": "Blackwell-Monotone Information Costs",
        "abstract": "A Blackwell-monotone information cost function assigns higher costs to Blackwell more informative experiments. This paper provides simple necessary and sufficient conditions for Blackwell monotonicity over finite experiments. The key condition is a system of linear differential inequalities that are convenient to check given an arbitrary cost function. When the cost function is additively separable across signals, our characterization implies that Blackwell monotonicity is equivalent to sublinearity. This identifies a wide range of practical information cost functions. Finally, we apply our results to bargaining and persuasion problems with costly information.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-24.json",
        "arxivId": "2404.15226",
        "category": "econ",
        "title": "Revisiting Granular Models of Firm Growth",
        "abstract": "We revisit\"granular models of firm growth\"that have been proposed in the literature to explain the anomalously slow decrease of growth volatility with firms size and how this phenomenon shapes the distribution of their growth rates. In these models, firms' sales are viewed as collections of independent\"sub-units\", and these non-trivial statistical properties occur as a direct result of the fat-tailed distribution of the number or sizes of these sub-units. We present and discuss new theoretical results on the relation between firm size and growth rate statistics. Our results can be understood by noting that granular models imply the existence of three types of firms: well-diversified firms, with a size evenly distributed among several sub-units; firms with many sub-units but with their total size concentrated on only a handful of them, and lastly firms which are poorly diversified simply because they are made up of a small number of sub-units. We establish new empirical facts about growth rates and their relation with size. As predicted by the model, the distribution of growth rate volatilities is to a good approximation {independent of firm size}, once rescaled by the average size-conditioned volatility. However, the tail of this distribution is much too thin to be consistent with a granular mechanism. Moreover, the moments of growth volatility scale with size in a way that is at odds with theoretical predictions. We also find that the distribution of growth rates rescaled by firm-specific volatility, which is predicted to be Gaussian by all the models we consider, remains very fat-tailed in the data, even for large firms. This paper, in ruling out the granularity scenario, suggests that the overarching mechanisms underlying the growth of firms are not satisfactorily understood, and argues that they deserve further theoretical investigations.",
        "references": [
            {
                "arxivId": "2404.07935",
                "title": "Compositional Growth Models",
                "abstract": "We review models of compositional growth, which were introduced to explain the growth statistics of various quantities ranging from firm sizes to GDP. In these models, entities are decomposed into units that grow independently. Thus, the growth rate of the entity is the addition of the growth rates of the composing units, with possibly heterogeneous weights. We review such models and show that they can be understood through a unifying theoretical framework, explaining the resulting growth rate distributions using mixtures of Gaussians."
            },
            {
                "arxivId": "2210.10169",
                "title": "Expectations Formation with Fat-Tailed Processes: Evidence from Sales Forecasts",
                "abstract": "We empirically analyze a large sample of \ufb01rm sales growth expectations. We \ufb01nd that the relationship between forecast errors and lagged revision is non-linear. Forecasters underreact to typical (positive or negative) news about future sales, but overreact to very signi\ufb01cant news. To account for this non-linearity, we propose a simple framework, where (1) sales growth dynamics have a fat-tailed high frequency component and (2) forecasters use a simple linear rule. This framework qualitatively \ufb01ts several additional features of data on sales growth dynamics, forecast errors, and stock returns."
            },
            {
                "arxivId": "1004.5397",
                "title": "The cause of universality in growth fluctuations",
                "abstract": "Phenomena as diverse as breeding bird populations, the size of U.S. firms, money invested in mutual funds, the GDP of individual countries and the scientific output of universities all show unusual but remarkably similar growth fluctuations. The fluctuations display characteristic features, including double exponential scaling in the body of the distribution and power law scaling of the standard deviation as a function of size. To explain this we propose a remarkably simple additive replication model: At each step each individual is replaced by a new number of individuals drawn from the same replication distribution. If the replication distribution is sufficiently heavy tailed then the growth fluctuations are Levy distributed. We analyze the data from bird populations, firms, and mutual funds and show that our predictions match the data well, in several respects: Our theory results in a much better collapse of the individual distributions onto a single curve and also correctly predicts the scaling of the standard deviation with size. To illustrate how this can emerge from a collective microscopic dynamics we propose a model based on stochastic influence dynamics over a scale-free contact network and show that it produces results similar to those observed. We also extend the model to deal with correlations between individual elements. Our main conclusion is that the universality of growth fluctuations is driven by the additivity of growth processes and the action of the generalized central limit theorem."
            },
            {
                "arxivId": "cond-mat/0210479",
                "title": "Statistical Models for Company Growth",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0002374",
                "title": "Wealth condensation in a simple model of economy",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9804100",
                "title": "Universal features in the growth dynamics of complex organizations",
                "abstract": "We analyze the fluctuations in the gross domestic product (GDP) of 152 countries for the period 1950--1992. We find that (i) the distribution of annual growth rates for countries of a given GDP decays with ``fatter'' tails than for a Gaussian, and (ii) the width of the distribution scales as a power law of GDP with a scaling exponent $\\beta \\approx 0.15$. Both findings are in surprising agreement with results on firm growth. These results are consistent with the hypothesis that the evolution of organizations with complex structure is governed by similar growth mechanisms."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2110.11846",
        "category": "econ",
        "title": "Cycles to compute the full set of many-to-many stable matchings",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2302.08987",
        "category": "econ",
        "title": "Firm-level supply chains to minimize unemployment and economic losses in rapid decarbonization scenarios",
        "abstract": null,
        "references": [
            {
                "arxivId": "2302.11451",
                "title": "Estimating the loss of economic predictability from aggregating firm-level production networks",
                "abstract": "Abstract To estimate the reaction of economies to political interventions or external disturbances, input\u2013output (IO) tables\u2014constructed by aggregating data into industrial sectors\u2014are extensively used. However, economic growth, robustness, and resilience crucially depend on the detailed structure of nonaggregated firm-level production networks (FPNs). Due to nonavailability of data, little is known about how much aggregated sector-based and detailed firm-level-based model predictions differ. Using a nearly complete nationwide FPN, containing 243,399 Hungarian firms with 1,104,141 supplier\u2013buyer relations, we self-consistently compare production losses on the aggregated industry-level production network (IPN) and the granular FPN. For this, we model the propagation of shocks of the same size on both, the IPN and FPN, where the latter captures relevant heterogeneities within industries. In a COVID-19 inspired scenario, we model the shock based on detailed firm-level data during the early pandemic. We find that using IPNs instead of FPNs leads to an underestimation of economic losses of up to 37%, demonstrating a natural limitation of industry-level IO models in predicting economic outcomes. We ascribe the large discrepancy to the significant heterogeneity of firms within industries: we find that firms within one sector only sell 23.5% to and buy 19.3% from the same industries on average, emphasizing the strong limitations of industrial sectors for representing the firms they include. Similar error levels are expected when estimating economic growth, CO2 emissions, and the impact of policy interventions with industry-level IO models. Granular data are key for reasonable predictions of dynamical economic systems."
            },
            {
                "arxivId": "1707.04870",
                "title": "Environmental impact assessment for climate change policy with the simulation-based integrated assessment model E3ME-FTT-GENIE",
                "abstract": null
            },
            {
                "arxivId": "1401.8026",
                "title": "Elimination of systemic risk in financial networks by means of a systemic risk transaction tax",
                "abstract": "Financial markets are exposed to systemic risk (SR), the risk that a major fraction of the system ceases to function, and collapses. It has recently become possible to quantify SR in terms of underlying financial networks where nodes represent financial institutions, and links capture the size and maturity of assets (loans), liabilities and other obligations, such as derivatives. We demonstrate that it is possible to quantify the share of SR that individual liabilities within a financial network contribute to the overall SR. We use empirical data of nationwide interbank liabilities to show that the marginal contribution to overall SR of liabilities for a given size varies by a factor of a thousand. We propose a tax on individual transactions that is proportional to their marginal contribution to overall SR. If a transaction does not increase SR, it is tax-free. With an agent-based model (ABM) (CRISIS macro-financial model), we demonstrate that the proposed \u2018Systemic Risk Tax\u2019 (SRT) leads to a self-organized restructuring of financial networks that are practically free of SR. The SRT can be seen as an insurance for the public against costs arising from cascading failure. ABM predictions are shown to be in remarkable agreement with the empirical data and can be used to understand the relation of credit risk and SR."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2304.08432",
        "category": "econ",
        "title": "How Do Digital Advertising Auctions Impact Product Prices?",
        "abstract": "We present a model of digital advertising with three key features: (i) advertisers can reach consumers on and off a platform, (ii) additional data enhances the value of advertiser-consumer matches, and (iii) bidding follows auction-like mechanisms. We contrast data-augmented auctions, which leverage the platform's data advantage to improve match quality, and managed campaign mechanisms that automate match formation and price-setting. The platform-optimal mechanism is a managed campaign that conditions on-platform prices for sponsored products on the off-platform prices set by all advertisers. This mechanism yields the efficient on-platform allocation but inefficient off-platform allocations due to high product prices; it attains the vertical integration profit for the platform and advertisers; and it increases off-platform product prices and decreases consumer surplus, relative to data-augmented auctions.",
        "references": [
            {
                "arxivId": "1909.02156",
                "title": "Bidding strategies with gender nondiscrimination constraints for online ad auctions",
                "abstract": "Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings."
            },
            {
                "arxivId": "2004.03107",
                "title": "The Economics of Social Data",
                "abstract": "A data intermediary pays consumers for information about their preferences and sells the information so acquired to firms that use it to tailor their products and prices. The social dimension of the individual data---whereby an individual's data are predictive of the behavior of others---generates a data externality that reduces the intermediary's cost of acquiring information. We derive the intermediary's optimal data policy and show that it preserves the privacy of the consumers' identities while providing precise information about market demand to the firms. This enables the intermediary to capture the entire value of information as the number of consumers grows large."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2305.08268",
        "category": "econ",
        "title": "Bubble Necessity Theorem",
        "abstract": "Asset price bubbles are situations where asset prices exceed the fundamental values defined by the present value of dividends. This paper presents a conceptually new perspective: the necessity of bubbles. We establish the Bubble Necessity Theorem in a plausible general class of economic models: with faster long-run economic growth ($G$) than dividend growth ($G_d$) and counterfactual long-run autarky interest rate ($R$) below dividend growth, all equilibria are bubbly with non-negligible bubble sizes relative to the economy. This bubble necessity condition naturally arises in economies with sufficiently strong savings motives and multiple factors or sectors with uneven productivity growth.",
        "references": [
            {
                "arxivId": "2311.03638",
                "title": "Bubble economics",
                "abstract": null
            },
            {
                "arxivId": "2307.00349",
                "title": "Unbalanced Growth, Elasticity of Substitution, and Land Overvaluation",
                "abstract": "We study the long-run behavior of land prices when land plays the dual role of factor of production and store of value. In modern economies where technological progress is faster in non-land sectors, when the elasticity of substitution in production exceeds 1 at high input levels (which always holds if non-land factors do not fully depreciate), unbalanced growth occurs and land becomes overvalued on the long-run trend relative to the fundamental value defined by the present value of land rents. Around the trend, land prices exhibit recurrent stochastic fluctuations, with expansions and contractions in the size of land overvaluation."
            },
            {
                "arxivId": "2303.11365",
                "title": "A Theory of Rational Housing Bubbles with Phase Transitions",
                "abstract": "We analyze equilibrium housing prices in an overlapping generations model with perfect housing and rental markets. We prove that the economy exhibits a two-stage phase transition: as the income of home buyers rises, the equilibrium regime changes from fundamental only to coexistence of fundamental and bubbly equilibria. With even higher incomes, fundamental equilibria disappear and housing bubbles become inevitable. Even with low current incomes, housing bubbles may emerge if home buyers have access to credit or have high future income expectations. Contrary to widely-held beliefs, fundamental equilibria in the coexistence region are inefficient despite housing being a productive non-reproducible asset."
            },
            {
                "arxivId": "2211.13100",
                "title": "Leverage, Endogenous Unbalanced Growth, and Asset Price Bubbles",
                "abstract": "We present a general equilibrium macro-finance model with a positive feedback loop between capital investment and land price. As leverage is relaxed beyond a critical value, through the financial accelerator, a phase transition occurs from balanced growth where land prices reflect fundamentals (present value of rents) to unbalanced growth where land prices grow faster than rents, generating land price bubbles. Unbalanced growth dynamics and bubbles are associated with financial loosening and technological progress. In an analytically tractable two-sector large open economy model with unique equilibria, financial loosening simultaneously leads to low interest rates, asset overvaluation, and top-end wealth concentration."
            },
            {
                "arxivId": "1712.01431",
                "title": "Determination of Pareto Exponents in Economic Models Driven by Markov Multiplicative Processes",
                "abstract": "This article contains new tools for studying the shape of the stationary distribution of sizes in a dynamic economic system in which units experience random multiplicative shocks and are occasionally reset. Each unit has a Markov\u2010switching type, which influences their growth rate and reset probability. We show that the size distribution has a Pareto upper tail, with exponent equal to the unique positive solution to an equation involving the spectral radius of a certain matrix\u2010valued function. Under a nonlattice condition on growth rates, an eigenvector associated with the Pareto exponent provides the distribution of types in the upper tail of the size distribution."
            },
            {
                "arxivId": "0803.0442",
                "title": "Individual risk and Lebesgue extension without aggregate uncertainty",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2307.12104",
        "category": "econ",
        "title": "Sharing Credit for Joint Research",
        "abstract": "How can one efficiently share payoffs with collaborators when participating in risky research? First, I show that efficiency can be achieved by allocating payoffs asymmetrically between the researcher who makes a breakthrough (\"winner\") and the others, even if agents cannot observe others' effort. When the winner's identity is non-contractible, allocating credit based on effort at time of breakthrough also suffices to achieve efficiency; so the terminal effort profile, rather than the full history of effort, is a sufficient statistic. These findings suggest that simple mechanisms using minimal information are robust and effective in addressing inefficiencies in strategic experimentation.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2309.06546",
        "category": "econ",
        "title": "Not obviously manipulable allotment rules",
        "abstract": "In the problem of allocating a single non-disposable commodity among agents whose preferences are single-peaked, we study a weakening of strategy-proofness called not obvious manipulability (NOM). If agents are cognitively limited, then NOM is sufficient to describe their strategic behavior. We characterize a large family of own-peak-only rules that satisfy efficiency, NOM, and a minimal fairness condition. We call these rules\"simple\". In economies with excess demand, simple rules fully satiate agents whose peak amount is less than or equal to equal division and assign, to each remaining agent, an amount between equal division and his peak. In economies with excess supply, simple rules are defined symmetrically. These rules can be thought of as a two-step procedure that involves solving a claims problem. We also show that the single-plateaued domain is maximal for the characterizing properties of simple rules. Therefore, even though replacing strategy-proofness with NOM greatly expands the family of admissible rules, the maximal domain of preferences involved remains basically unaltered.",
        "references": [
            {
                "arxivId": "2306.17773",
                "title": "Obvious Manipulations in Matching with and without Contracts",
                "abstract": "In a many-to-one matching model, with or without contracts, where doctors' preferences are private information and hospitals' preferences are substitutable and public information, any stable matching rule could be manipulated for doctors. Since manipulations can not be completely avoided, we consider the concept of \\textit{obvious manipulations} and look for stable matching rules that prevent at least such manipulations (for doctors). For the model with contracts, we prove that: \\textit{(i)} the doctor-optimal matching rule is non-obviously manipulable and \\textit{(ii)} the hospital-optimal matching rule is obviously manipulable, even in the one-to-one model. In contrast to \\textit{(ii)}, for a many-to-one model without contracts, we prove that the hospital-optimal matching rule is not obviously manipulable.% when hospitals' preferences are substitutable. Furthermore, if we focus on quantile stable rules, then we prove that the doctor-optimal matching rule is the only non-obviously manipulable quantile stable rule."
            },
            {
                "arxivId": "2210.11627",
                "title": "Obvious manipulations of tops-only voting rules",
                "abstract": null
            },
            {
                "arxivId": "2206.11143",
                "title": "Fair and Efficient Allocations Without Obvious Manipulations",
                "abstract": "We consider the fundamental problem of allocating a set of indivisible goods among strategic agents with additive valuation functions. It is well known that, in the absence of monetary transfers, Pareto efficient and truthful rules are dictatorial, while there is no deterministic truthful mechanism that allocates all items and achieves envy-freeness up to one item (EF1), even for the case of two agents. In this paper, we investigate the interplay of fairness and efficiency under a relaxation of truthfulness called non-obvious manipulability (NOM), recently proposed by Troyan and Morrill. We show that this relaxation allows us to bypass the aforementioned negative results in a very strong sense. Specifically, we prove that there are deterministic and EF1 algorithms that are not obviously manipulable, and the algorithm that maximizes utilitarian social welfare (the sum of agents' utilities), which is Pareto efficient but not dictatorial, is not obviously manipulable for $n \\geq 3$ agents (but obviously manipulable for $n=2$ agents). At the same time, maximizing the egalitarian social welfare (the minimum of agents' utilities) or the Nash social welfare (the product of agents' utilities) is obviously manipulable for any number of agents and items. Our main result is an approximation preserving black-box reduction from the problem of designing EF1 and NOM mechanisms to the problem of designing EF1 algorithms. En route, we prove an interesting structural result about EF1 allocations, as well as new\"best-of-both-worlds\"results (for the problem without incentives), that might be of independent interest."
            },
            {
                "arxivId": "2111.01983",
                "title": "Obvious Manipulability of Voting Rules",
                "abstract": null
            },
            {
                "arxivId": "1908.02988",
                "title": "Obvious manipulations in cake-cutting",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-25.json",
        "arxivId": "2403.15262",
        "category": "econ",
        "title": "Strategic Responses to Technological Change: Evidence from ChatGPT and Upwork",
        "abstract": "AI technologies have the potential to affect labor market outcomes by both increasing worker productivity and reducing the demand for certain skills or tasks. Such changes may have important implications for the ways in which workers seek jobs and position themselves. In this project, we examine how freelancers change their strategic behavior on an online work platform following the launch of ChatGPT in December 2022. We present evidence suggesting that, in response to technological change, freelancers increase the concentration of job applications across specializations and differentiate themselves from both their past behavior and their peers. We show that such changes are shaped by the extent to which a freelancer experiences growth in demand or supply in the domains they specialize in. We document heterogeneity in this effect across freelancer characteristics and document how strategic repositioning can help mitigate the negative effects of technological change on freelancer performance on the platform.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-26.json",
        "arxivId": "1912.10488",
        "category": "econ",
        "title": "Efficient and Convergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete Games",
        "abstract": "We propose a new sequential Efficient Pseudo-Likelihood (EPL) estimator for structural economic models with an equality constraint, particularly dynamic discrete choice games of incomplete information. Each iteration in the EPL sequence is consistent and asymptotically efficient, and iterating to convergence improves finite sample performance. For dynamic single-agent models, we show that Aguirregabiria and Mira's (2002, 2007) Nested Pseudo-Likelihood (NPL) estimator arises as a special case of EPL. In dynamic games, EPL maintains its efficiency properties, although NPL does not. And a convenient change of variable in the equilibrium fixed point equation ensures EPL iterations have the same computational simplicity as NPL iterations. Furthermore, EPL iterations are stable and locally convergent to the finite-sample maximum likelihood estimator at a nearly-quadratic rate for all regular Markov perfect equilibria, including unstable equilibria where NPL encounters convergence problems. Monte Carlo simulations confirm the theoretical results and demonstrate EPL's good performance in finite samples.",
        "references": [
            {
                "arxivId": "1802.06665",
                "title": "On the Iterated Estimation of Dynamic Discrete Choice Games",
                "abstract": "\n We study the first-order asymptotic properties of a class of estimators of the structural parameters in dynamic discrete choice games. We consider $K$-stage policy iteration (PI) estimators, where $K$ denotes the number of PIs employed in the estimation. This class nests several estimators proposed in the literature. By considering a \u201cpseudo likelihood\u201d criterion function, our estimator becomes the $K$-pseudo maximum likelihood (PML) estimator in Aguirregabiria and Mira (2002, 2007). By considering a \u201cminimum distance\u201d criterion function, it defines a new $K$-minimum distance (MD) estimator, which is an iterative version of the estimators in Pesendorfer and Schmidt-Dengler (2008) and Pakes et al. (2007). First, we establish that the $K$-PML estimator is consistent and asymptotically normal for any $K \\in \\mathbb{N}$. This complements findings in Aguirregabiria and Mira (2007), who focus on $K=1$ and $K$ large enough to induce convergence of the estimator. Furthermore, we show under certain conditions that the asymptotic variance of the $K$-PML estimator can exhibit arbitrary patterns as a function of $K$. Second, we establish that the $K$-MD estimator is consistent and asymptotically normal for any $K \\in \\mathbb{N}$. For a specific weight matrix, the $K$-MD estimator has the same asymptotic distribution as the $K$-PML estimator. Our main result provides an optimal sequence of weight matrices for the $K$-MD estimator and shows that the optimally weighted $K$-MD estimator has an asymptotic distribution that is invariant to $K$. The invariance result is especially unexpected given the findings in Aguirregabiria and Mira (2007) for $K$-PML estimators. Our main result implies two new corollaries about the optimal $1$-MD estimator (derived by Pesendorfer and Schmidt-Dengler (2008)). First, the optimal $1$-MD estimator is efficient in the class of $K$-MD estimators for all $K \\in \\mathbb{N}$. In other words, additional PIs do not provide first-order efficiency gains relative to the optimal $1$-MD estimator. Second, the optimal $1$-MD estimator is more or equally efficient than any $K$-PML estimator for all $K \\in \\mathbb{N}$. Finally, the Appendix provides appropriate conditions under which the optimal $1$-MD estimator is efficient among regular estimators."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-26.json",
        "arxivId": "2312.05985",
        "category": "econ",
        "title": "Fused Extended Two-Way Fixed Effects for Difference-in-Differences with Staggered Adoptions",
        "abstract": "To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We demonstrate FETWFE in simulation studies and an empirical application.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-26.json",
        "arxivId": "2402.12200",
        "category": "econ",
        "title": "The matching problem with linear transfers is equivalent to a hide-and-seek game",
        "abstract": "Matching problems with linearly transferable utility (LTU) generalize the well-studied transferable utility (TU) case by relaxing the assumption that utility is transferred one-for-one within matched pairs. We show that LTU matching problems can be reframed as nonzero-sum games between two players, thus generalizing a result from von Neumann. The underlying linear programming structure of TU matching problems, however, is lost when moving to LTU. These results draw a new bridge between non-TU matching problems and the theory of bimatrix games, with consequences notably regarding the computation of stable outcomes.",
        "references": [
            {
                "arxivId": "2205.11196",
                "title": "Zero-Sum Games and Linear Programming Duality",
                "abstract": "The minimax theorem for zero-sum games is easily proved from the strong duality theorem of linear programming. For the converse direction, the standard proof by Dantzig is known to be incomplete. We explain and combine classical theorems about solving linear equations with nonnegative variables to give a correct alternative proof more directly than Adler. We also extend Dantzig\u2019s game so that any max-min strategy gives either an optimal LP solution or shows that none exists."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-26.json",
        "arxivId": "2404.11794",
        "category": "econ",
        "title": "Automated Social Science: Language Models as Scientist and Subjects",
        "abstract": "We present an approach for automatically generating and testing, in silico, social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.",
        "references": [
            {
                "arxivId": "2310.11501",
                "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "abstract": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature."
            },
            {
                "arxivId": "2307.14324",
                "title": "Evaluating the Moral Beliefs Encoded in LLMs",
                "abstract": "This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM\"making a choice\", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g.,\"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g.,\"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g.,\"do not kill\"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models\"choose\"actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other."
            },
            {
                "arxivId": "2306.03917",
                "title": "Turning large language models into cognitive models",
                "abstract": "Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole."
            },
            {
                "arxivId": "2301.07543",
                "title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?",
                "abstract": "Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution."
            },
            {
                "arxivId": "2212.03827",
                "title": "Discovering Latent Knowledge in Language Models Without Supervision",
                "abstract": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels."
            },
            {
                "arxivId": "2211.15006",
                "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
                "abstract": "Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single\"generic\"user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g.,\"should we raise taxes on the rich?\"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another."
            },
            {
                "arxivId": "2209.06899",
                "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
                "abstract": "Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one such tool\u2014the GPT-3 language model\u2014is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create \u201csilicon samples\u201d by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."
            },
            {
                "arxivId": "2208.10264",
                "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-26.json",
        "arxivId": "2404.16056",
        "category": "econ",
        "title": "Intelligent Machines and Incomplete Information",
        "abstract": "The distribution of efficient individuals in the economy and the efforts that they will put in if they are hired, there are two important concerns for a technologically advanced firm. wants to open a new branch. The firm does not have information about the exact level of efficiency of an individual when she is hired. We call this situation incomplete information. The standard principal agent models assume that employees know their efficiency levels. Hence these models design incentive-compatible mechanisms. An incentive-compatible mechanism ensures that a participant does not have the incentive to misreport her efficiency level. This paper does not assume that employees know how efficient they are. This paper assumes that the production technology of the firm is intelligent, that is, the output of the machine reveals the efficiency levels of employees. Employees marginal contributions to the total output of the intelligent machine, the probability distribution of the levels of efficiency and employees costs of efforts together define a game of incomplete information. A characterization of ex-ante Nash Equilibrium is established. The results of the characterization formalize the relationship between the distribution of efficiency levels and the distribution of output.",
        "references": [
            {
                "arxivId": "2104.12871",
                "title": "Why AI is harder than we think",
                "abstract": "Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (\"AI Spring\") and periods of disappointment, loss of confidence, and reduced funding (\"AI Winter\"). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this talk I will discuss some fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I will also speculate on what is needed for the grand challenge of making AI systems more robust, general, and adaptable --- in short, more intelligent."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2101.00009",
        "category": "econ",
        "title": "Adversarial Estimation of Riesz Representers",
        "abstract": "Many causal parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Our estimators are highly compatible with targeted and debiased machine learning with sample splitting; our guarantees directly verify general conditions for inference that allow mis-specification. We also use our guarantees to prove inference without sample splitting, based on stability or complexity. Our estimators achieve nominal coverage in highly nonlinear simulations where some previous methods break down. They shed new light on the heterogeneous effects of matching grants.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2104.13367",
        "category": "econ",
        "title": "A model of multiple hypothesis testing",
        "abstract": "Multiple hypothesis testing practices vary widely, without consensus on which are appropriate when. This paper provides an economic foundation for these practices designed to capture processes of scientific communication, such as regulatory approval on the basis of clinical trials. In studies of multiple treatments or sub-populations, adjustments may be appropriate depending on scale economies in the research production function, with control of classical notions of compound errors emerging in some but not all cases. In studies with multiple outcomes, indexing is appropriate and adjustments to test levels may be appropriate if the intended audience is heterogeneous. Data on actual costs in the drug approval process suggest both that some adjustment is warranted in that setting and that standard procedures are overly conservative.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2107.05936",
        "category": "econ",
        "title": "Testability of Reverse Causality Without Exogenous Variation",
        "abstract": "This paper shows that testability of reverse causality is possible even in the absence of exogenous variation, such as in the form of instrumental variables. Instead of relying on exogenous variation, we achieve testability by imposing relatively weak model restrictions and exploiting that a dependence of residual and purported cause is informative about the causal direction. Our main assumption is that the true functional relationship is nonlinear and that error terms are additively separable. We extend previous results by incorporating control variables and allowing heteroskedastic errors. We build on reproducing kernel Hilbert space (RKHS) embeddings of probability distributions to test conditional independence and demonstrate the efficacy in detecting the causal direction in both Monte Carlo simulations and an application to German survey data.",
        "references": [
            {
                "arxivId": "1909.10129",
                "title": "SPECIFICATION TESTING IN NONPARAMETRIC INSTRUMENTAL QUANTILE REGRESSION",
                "abstract": "There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model with endogenous regressors, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable variable. Under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. A failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. This article develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specified. Our test statistic is asymptotically normally distributed under correct specification and consistent against any alternative model. In addition, test statistics to justify the model simplification are established. Finite sample properties are examined in a Monte Carlo study and an empirical illustration is provided."
            },
            {
                "arxivId": "1907.07271",
                "title": "Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics",
                "abstract": "In this essay I discuss potential outcome and graphical approaches to causality, and their relevance for empirical work in economics. I review some of the work on directed acyclic graphs, including the recent The Book of Why (Pearl and Mackenzie 2018). I also discuss the potential outcome framework developed by Rubin and coauthors (e.g., Rubin 2006), building on work by Neyman (1990 [1923]). I then discuss the relative merits of these approaches for empirical work in economics, focusing on the questions each framework answers well, and why much of the the work in economics is closer in spirit to the potential outcome perspective. (JEL C31, C36, I26)"
            },
            {
                "arxivId": "1906.00232",
                "title": "Kernel Instrumental Variable Regression",
                "abstract": "Instrumental variable (IV) regression is a strategy for learning causal relationships in observational data. If measurements of input X and output Y are confounded, the causal relationship can nonetheless be identified if an instrumental variable Z is available that influences X directly, but is conditionally independent of Y given X and the unmeasured confounder. The classic two-stage least squares algorithm (2SLS) simplifies the estimation problem by modeling all relationships as linear functions. We propose kernel instrumental variable regression (KIV), a nonparametric generalization of 2SLS, modeling relations among X, Y, and Z as nonlinear functions in reproducing kernel Hilbert spaces (RKHSs). We prove the consistency of KIV under mild assumptions, and derive conditions under which convergence occurs at the minimax optimal rate for unconfounded, single-stage RKHS regression. In doing so, we obtain an efficient ratio between training sample sizes used in the algorithm's first and second stages. In experiments, KIV outperforms state of the art alternatives for nonparametric IV regression."
            },
            {
                "arxivId": "1702.03877",
                "title": "Approximate Kernel-Based Conditional Independence Tests for Fast Non-Parametric Causal Discovery",
                "abstract": "Abstract Constraint-based causal discovery (CCD) algorithms require fast and accurate conditional independence (CI) testing. The Kernel Conditional Independence Test (KCIT) is currently one of the most popular CI tests in the non-parametric setting, but many investigators cannot use KCIT with large datasets because the test scales at least quadratically with sample size. We therefore devise two relaxations called the Randomized Conditional Independence Test (RCIT) and the Randomized conditional Correlation Test (RCoT) which both approximate KCIT by utilizing random Fourier features. In practice, both of the proposed tests scale linearly with sample size and return accurate p-values much faster than KCIT in the large sample size context. CCD algorithms run with RCIT or RCoT also return graphs at least as accurate as the same algorithms run with KCIT but with large reductions in run time."
            },
            {
                "arxivId": "1607.03300",
                "title": "From Dependence to Causation",
                "abstract": "Machine learning is the science of discovering statistical dependencies in data, and the use of those dependencies to perform predictions. During the last decade, machine learning has made spectacular progress, surpassing human performance in complex tasks such as object recognition, car driving, and computer gaming. However, the central role of prediction in machine learning avoids progress towards general-purpose artificial intelligence. As one way forward, we argue that causal inference is a fundamental component of human intelligence, yet ignored by learning algorithms. \nCausal inference is the problem of uncovering the cause-effect relationships between the variables of a data generating system. Causal structures provide understanding about how these systems behave under changing, unseen environments. In turn, knowledge about these causal dynamics allows to answer \"what if\" questions, describing the potential responses of the system under hypothetical manipulations and interventions. Thus, understanding cause and effect is one step from machine learning towards machine reasoning and machine intelligence. But, currently available causal inference algorithms operate in specific regimes, and rely on assumptions that are difficult to verify in practice. \nThis thesis advances the art of causal inference in three different ways. First, we develop a framework for the study of statistical dependence based on copulas and random features. Second, we build on this framework to interpret the problem of causal inference as the task of distribution classification, yielding a family of novel causal inference algorithms. Third, we discover causal structures in convolutional neural network features using our algorithms. The algorithms presented in this thesis are scalable, exhibit strong theoretical guarantees, and achieve state-of-the-art performance in a variety of real-world benchmarks."
            },
            {
                "arxivId": "1605.09522",
                "title": "Kernel Mean Embedding of Distributions: A Review and Beyonds",
                "abstract": "A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original \"feature map\" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions."
            },
            {
                "arxivId": "1909.10133",
                "title": "Goodness-of-fit tests based on series estimators in nonparametric instrumental regression",
                "abstract": null
            },
            {
                "arxivId": "1412.3773",
                "title": "Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks",
                "abstract": "The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X,Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CAUSEEFFECTPAIRS that consists of data for 100 different causee ffect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the \"ground truth\" causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63 \u00b1 10 % and an AUC of 0.74 \u00b1 0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method."
            },
            {
                "arxivId": "1309.6779",
                "title": "Causal discovery with continuous additive noise models",
                "abstract": "We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation."
            },
            {
                "arxivId": "1205.4656",
                "title": "Conditional mean embeddings as regressors",
                "abstract": "We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS) embeddings of conditional distributions and vector-valued regressors. This connection introduces a natural regularized loss function which the RKHS embeddings minimise, providing an intuitive understanding of the embeddings and a justification for their use. Furthermore, the equivalence allows the application of vector-valued regression methods and results to the problem of learning conditional distributions. Using this link we derive a sparse version of the embedding by considering alternative formulations. Further, by applying convergence results for vector-valued regression to the embedding problem we derive minimax convergence rates which are O(\\log(n)/n) -- compared to current state of the art rates of O(n^{-1/4}) -- and are valid under milder and more intuitive assumptions. These minimax upper rates coincide with lower rates up to a logarithmic factor, showing that the embedding method achieves nearly optimal rates. We study our sparse embedding algorithm in a reinforcement learning task where the algorithm shows significant improvement in sparsity over an incomplete Cholesky decomposition."
            },
            {
                "arxivId": "1202.3775",
                "title": "Kernel-based Conditional Independence Test and Application in Causal Discovery",
                "abstract": "Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties."
            },
            {
                "arxivId": "1205.2599",
                "title": "On the Identifiability of the Post-Nonlinear Causal Model",
                "abstract": "By taking into account the nonlinear effect of the cause, the inner noise effect, and the measurement distortion effect in the observed variables, the post-nonlinear (PNL) causal model has demonstrated its excellent performance in distinguishing the cause from effect. However, its identifiability has not been properly addressed, and how to apply it in the case of more than two variables is also a problem. In this paper, we conduct a systematic investigation on its identifiability in the two-variable case. We show that this model is identifiable in most cases; by enumerating all possible situations in which the model is not identifiable, we provide sufficient conditions for its identifiability. Simulations are given to support the theoretical results. Moreover, in the case of more than two variables, we show that the whole causal structure can be found by applying the PNL causal model to each structure in the Markov equivalent class and testing if the disturbance is independent of the direct causes for each variable. In this way the exhaustive search over all possible causal structures is avoided."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2305.13956",
        "category": "econ",
        "title": "Nash implementation in a many-to-one matching market",
        "abstract": "In a many-to-one matching market, we analyze the matching game induced by a stable rule when firms' choice function satisfy substitutability. We show that any stable rule implements the individually rational correspondence in Nash equilibrium when both sides of the market play strategically. Moreover, when only workers play strategically and firms' choice functions satisfy the law of aggregate demand, we show that the firm-optimal stable rule implements the stable correspondence in Nash equilibrium.",
        "references": [
            {
                "arxivId": "2210.06549",
                "title": "General Manipulability Theorem for a Matching Model",
                "abstract": ": In a many-to-many matching model in which agents\u2019 preferences satisfy substitutability and the law of aggregate demand, we proof the General Manipulability Theorem. We result generalizes the presented in Sotomayor (1996 and 2012) for the many-to-one model. In addition, we show General Manipulability Theorem fail when agents\u2019 preferences satisfy only substitutability."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2309.15983",
        "category": "econ",
        "title": "What To Do (and Not to Do) with Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study",
        "abstract": "Two-way fixed effects (TWFE) models are ubiquitous in causal panel analysis in political science. However, recent methodological discussions challenge their validity in the presence of heterogeneous treatment effects (HTE) and violations of the parallel trends assumption (PTA). This burgeoning literature has introduced multiple estimators and diagnostics, leading to confusion among empirical researchers on two fronts: the reliability of existing results based on TWFE models and the current best practices. To address these concerns, we examined, replicated, and reanalyzed 37 articles from three leading political science journals that employed observational panel data with binary treatments. Using six newly introduced HTE-robust estimators, we find that although precision may be affected, the core conclusions derived from TWFE estimates largely remain unchanged. PTA violations and insufficient statistical power, however, continue to be significant obstacles to credible inferences. Based on these findings, we offer recommendations for improving practice in empirical research.",
        "references": [
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            },
            {
                "arxivId": "2108.12419",
                "title": "Revisiting event study designs: robust and efficient estimation",
                "abstract": "We develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Our method applies with time-varying controls, in triple-difference designs, and with certain non-binary treatments. We show the practical relevance of our results in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter - about half as large as benchmark estimates used to calibrate macroeconomic models - and predominantly occurs in the first month after the rebate."
            },
            {
                "arxivId": "2106.15074",
                "title": "Causal Inference with Panel Data under Temporal and Spatial Interference",
                "abstract": "Many social events and policy interventions generate treatment effects that persistently spill over into neighboring areas, resulting in a phenomenon statisticians refer to as\"interference\"both in time and space. In this paper, I put forward a design-based framework to identify and estimate these spillover effects in panel data with a spatial dimension, when temporal and spatial interference intertwine in intricate ways that are unknown to researchers. The framework defines estimands that enable researchers to measure the influence of each type of interference, and I propose estimators that are consistent and asymptotically normal under the assumption of sequential ignorability and mild regularity conditions. I show that fixed effects models in panel data analysis, such as the difference-in-differences (DID) estimator, can lead to significant biases in such scenarios. I test the method's performance on both simulated datasets and the replication of two empirical studies."
            },
            {
                "arxivId": "2006.15780",
                "title": "Treatment effects in interactive fixed effects models with a small number of time periods",
                "abstract": null
            },
            {
                "arxivId": "2107.00856",
                "title": "A Practical Guide to Counterfactual Estimators for Causal Inference with Time-Series Cross-Sectional Data",
                "abstract": "This paper introduces a unified framework of counterfactual estimation for time-series cross-sectional data, which estimates the average treatment effect on the treated by directly imputing treated counterfactuals. Its special cases include several newly developed methods, such as the fixed effects counterfactual estimator, interactive fixed effects counterfactual estimator, and matrix completion estimator. These estimators provide more reliable causal estimates than conventional two-way fixed effects models when the treatment effects are heterogeneous or unobserved time-varying confounders exist. Under this framework, we propose two sets of diagnostic tests, tests for (no) pre-trend and placebo tests, accompanied by visualization tools, to help researchers gauge the validity of the no-time-varying-confounder assumption. We illustrate these methods with two political economy examples and develop an open-source package, fect, in both R and Stata to facilitate implementation."
            },
            {
                "arxivId": "1910.06677",
                "title": "Matrix Completion, Counterfactuals, and Factor Analysis of Missing Data",
                "abstract": "Abstract This article proposes an imputation procedure that uses the factors estimated from a tall block along with the re-rotated loadings estimated from a wide block to impute missing values in a panel of data. Assuming that a strong factor structure holds for the full panel of data and its sub-blocks, it is shown that the common component can be consistently estimated at four different rates of convergence without requiring regularization or iteration. An asymptotic analysis of the estimation error is obtained. An application of our analysis is estimation of counterfactuals when potential outcomes have a factor structure. We study the estimation of average and individual treatment effects on the treated and establish a normal distribution theory that can be useful for hypothesis testing."
            },
            {
                "arxivId": "1909.09412",
                "title": "Double-Robust Identification for Causal Panel Data Models",
                "abstract": "\n We study identification and estimation of causal effects in settings with panel data. Traditionally researchers follow model-based identification strategies relying on assumptions governing the relation between the potential outcomes and the observed and unobserved confounders. We focus on a different, complementary approach to identification where assumptions are made about the connection between the treatment assignment and the unobserved confounders. Such strategies are common in cross-section\u00a0settings but rarely used with panel data. We introduce different sets of assumptions that follow the two paths to identification and develop a double robust approach. We propose estimation methods that build on these identification strategies."
            },
            {
                "arxivId": "1808.05293",
                "title": "Design-Based Analysis in Difference-in-Differences Settings with Staggered Adoption",
                "abstract": "In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is is an unbiased estimator of a particular weighted average causal effect. We characterize the proeperties of this estimand, and show that the standard variance estimator is conservative."
            },
            {
                "arxivId": "1804.05785",
                "title": "Estimating Dynamic Treatment Effects in Event Studies With Heterogeneous Treatment Effects",
                "abstract": "Event studies are frequently used to estimate average treatment effects on the treated (ATT). In estimating the ATT, researchers commonly use fixed effects models that implicitly assume constant treatment effects across cohorts. We show that this is not an innocuous assumption. In fixed effect models where the sole regressor is treatment status, the OLS coefficient is a non-convex average of the heterogeneous cohort-specific ATTs. When regressors containing lags and leads of treatment are added, the OLS coefficient corresponding to a given lead or lag picks up spurious terms consisting of treatment effects from other periods. Therefore, estimates from these commonly used models are not causally interpretable. We propose alternative estimators that identify certain convex averages of the cohort-specific ATTs, hence allowing for causal interpretation even under heterogeneous treatment effects. To illustrate the empirical content of our results, we show that the fixed effects estimators and our proposed estimators differ substantially in an application to the economic consequences of hospitalization."
            },
            {
                "arxivId": "1803.09015",
                "title": "Difference-in-Differences with Multiple Time Periods",
                "abstract": "Difference-in-Differences (DID) is one of the most important and popular designs for evaluating causal effects of policy changes. In its standard format, there are two time periods and two groups: in the first period no one is treated, and in the second period a \"treatment group\" becomes treated, whereas a \"control group\" remains untreated. However, many em- pirical applications of the DID design have more than two periods and variation in treatment timing. In this article, we consider identification and estimation of treatment effect parameters using DID with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the \"parallel trends assumption\" holds potentially only after conditioning on observed covariates. We propose a simple two-step estimation strategy, establish the asymptotic prop- erties of the proposed estimators, and prove the validity of a computationally convenient bootstrap procedure. Furthermore we propose a semiparametric data-driven testing procedure to assess the credibility of the DID design in our context. Finally, we analyze the effect of the minimum wage on teen employment from 2001-2007. By using our proposed methods we confront the challenges related to variation in the timing of the state-level minimum wage policy changes. Open-source software is available for implementing the proposed methods."
            },
            {
                "arxivId": "1803.08807",
                "title": "Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects",
                "abstract": "Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)"
            },
            {
                "arxivId": "1710.02926",
                "title": "When Should You Adjust Standard Errors for Clustering?",
                "abstract": "\n Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, e.g., by state but not by gender, and in observational studies, but not in completely randomized experiments? (ii) Why is conventional clustering an \u201call-or-nothing\u201d adjustment, while within-cluster correlations can be strong or extremely weak? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional cluster standard errors can be severely inflated, and propose new variance estimators that correct for this bias."
            },
            {
                "arxivId": "1510.01757",
                "title": "Fuzzy Differences-in-Differences",
                "abstract": "In many applications of the differences-in-differences (DID) method, the treatment increases more in the treatment group, but some units are also treated in the control group. In such fuzzy designs, a popular estimator of treatment effects is the DID of the outcome divided by the DID of the treatment, or OLS and 2SLS regressions with time and group fixed effects estimating weighted averages of this ratio across groups. We start by showing that when the treatment also increases in the control group, this ratio estimates a causal effect only if treatment effects are homogenous in the two groups. Even when the distribution of treatment is stable, it requires that the effect of time be the same on all counterfactual outcomes. As this assumption is not always applicable, we propose two alternative estimators. The first estimator relies on a generalization of common trends assumptions to fuzzy designs, while the second extends the changes-in-changes estimator of Athey & Imbens (2006). When the distribution of treatment changes in the control group, treatment effects are partially identified. Finally, we prove that our estimators are asymptotically normal and use them to revisit applied papers using fuzzy designs."
            },
            {
                "arxivId": "0811.1640",
                "title": "For objective causal inference, design trumps analysis",
                "abstract": "For obtaining causal inferences that are objective, and therefore have the best chance of revealing scientific truths, carefully designed and executed randomized experiments are generally considered to be the gold standard. Observational studies, in contrast, are generally fraught with problems that compromise any claim for objectivity of the resulting causal inferences. The thesis here is that observational studies have to be carefully designed to approximate randomized experiments, in particular, without examining any final outcome data. Often a candidate data set will have to be rejected as inadequate because of lack of data on key covariates, or because of lack of overlap in the distributions of key covariates between treatment and control groups, often revealed by careful propensity score analyses. Sometimes the template for the approximating randomized experiment will have to be altered, and the use of principal stratification can be helpful in doing this. These issues are discussed and illustrated using the framework of potential outcomes to define causal effects, which greatly clarifies critical issues. 1. Randomized experiments versus observational studies. 1.1. Historical dichotomy between randomized and nonrandomized studies for causal effects. For may years, causal inference based on randomized experiments, as described, for example, in classic texts by Fisher (1935), Kempthorne (1952), Cochran and Cox (1950 )a ndCox (1958), was an entirely distinct endeavor than causal inference based on observational data sets, described, for example, in texts by Blalock (1964), Kenny (1979), Campbell and Stanley (1963), Cook and Campbell (1979), Rothman (1986), Lilienfeld and Lilienfeld (1976), Maddala (1977 )a ndCochran (1983). This began to change in the 1970\u2019s when the use of potential outcomes, commonly used in the context of randomized experiments to define causal effects since Neyman (1923), was used to define causal effects in both randomized experiments and observational studies [Rubin (1974)]. This allowed the definition of assignment mechanisms [Rubin (1975)], with randomized experiments as special cases, thereby allowing"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2402.03922",
        "category": "econ",
        "title": "Competitive advantage of URLLC vs. eMBB for supporting timeliness-relevant services",
        "abstract": "5G specifications promise a common and flexible-enough network infrastructure capable of satisfying diverse requirements of both current and future use cases. Two service types standardized in 5G are eMBB, without stringent delay guarantee, and URLLC, with stringent delay guarantee. We focus on a use case where data timeliness is the relevant quality parameter. We provide an economic rationale for the support of data-based services, that is, from the point of view of the profits attained by the service providers and operators (SP). More specifically, we focus on data-based services the quality of which is related to the Age of Information, and we assess two alternatives for the support of this sort of services by means of a 5G network: one that is based on the eMBB service type, and one that is based on the URLLC service type. These assessment is conducted in a duopoly scenario. We conclude that URLLC support provides a competitive advantage to an SP against a competitor SP that supports its service offering on eMBB. And that there is a slightly better situation for the users when the URLLC QoS constraint is stringent.",
        "references": [
            {
                "arxivId": "2111.05811",
                "title": "Internet of Things (IoT) Connectivity in 6G: An Interplay of Time, Space, Intelligence, and Value",
                "abstract": "Internet of Things (IoT) connectivity has a prominent presence in the 5G wireless communication systems. As these systems are being deployed, there is a surge of research efforts and visions towards 6G wireless systems. In order to position the evolution of IoT within the 6G systems, this paper first takes a critical view on the way IoT connectivity is supported within 5G. Following that, the wireless IoT evolution is discussed through multiple dimensions: time, space, intelligence, and value. We also conjecture that the focus will broaden from IoT devices and their connections towards the emergence of complex IoT environments, seen as building blocks of the overall IoT ecosystem."
            },
            {
                "arxivId": "2007.08564",
                "title": "Age of Information: An Introduction and Survey",
                "abstract": "We summarize recent contributions in the broad area of age of information (AoI). In particular, we describe the current state of the art in the design and optimization of low-latency cyberphysical systems and applications in which sources send time-stamped status updates to interested recipients. These applications desire status updates at the recipients to be as timely as possible; however, this is typically constrained by limited system resources. We describe AoI timeliness metrics and present general methods of AoI evaluation analysis that are applicable to a wide variety of sources and systems. Starting from elementary single-server queues, we apply these AoI methods to a range of increasingly complex systems, including energy harvesting sensors transmitting over noisy channels, parallel server systems, queueing networks, and various single-hop and multi-hop wireless networks. We also explore how update age is related to MMSE methods of sampling, estimation and control of stochastic processes. The paper concludes with a review of efforts to employ age optimization in cyberphysical applications."
            },
            {
                "arxivId": "1912.04764",
                "title": "Competition Between Service Providers With Strategic Resource Allocation: Application to Network Slicing",
                "abstract": "We propose and analyze a business model for a set of operators that use the same physical network. Each operator is entitled to a share of a network operated by an Infrastructure Provider (InP) and uses network slicing mechanisms to request network resources as needed for service provision. The network operators become Network Slice Tenants (NSTs). The InP performs the resource allocation based on a vector of weights chosen selfishly by each NST. The weights distribute the NST\u2019s share of resources between its subscribers in each cell. We model this relationship as a game propose a solution for the Nash equilibrium in which each NST chooses weights equal to the product of its share by the ratio between the total number of subscribers in the cell and the total number of subscribers in the network. We characterize the proposed solution in terms of subscription ratios and fractions of subscribers, for different cell capacities and user sensitivities. The proposed solution provides the exact values for the Nash equilibrium if the cells are homogeneous in terms of normalized capacity, which is a measure of the total amount of resources available in the cell. Otherwise, if the cells are heterogeneous, it provides an accurate approximation. We quantify the deviation from the equilibrium and conclude that it is highly accurate."
            },
            {
                "arxivId": "1801.04067",
                "title": "Content Based Status Updates",
                "abstract": "Consider a stream of status updates generated by a source, where each update is of one of two types: priority or ordinary; these updates are to be transmitted through a network to a monitor. We analyze a transmission policy that treats updates depending on their content: ordinary updates are served in a first-come first-served fashion, whereas the priority updates receive preferential treatment. An arriving priority update discards and replaces any currently-in-service priority update, and preempts (with eventual resume) any ordinary update. We model the arrival processes of the two kinds of updates as independent Poisson processes and the service times as two (possibly different rate) exponentials. We find the arrival and service rates under which the system is stable and give closed-form expressions for average peak age and a lower bound on the average age of the ordinary stream. We give numerical results on the average age of both streams and observe the effect of each stream on the age of the other."
            },
            {
                "arxivId": "1608.08622",
                "title": "The Age of Information: Real-Time Status Updating by Multiple Sources",
                "abstract": "We examine multiple independent sources providing status updates to a monitor through simple queues. We formulate an age of information (AoI) timeliness metric and derive a general result for the AoI that is applicable to a wide variety of multiple source service systems. For first-come first-served and two types of last-come first-served systems with Poisson arrivals and exponential service times, we find the region of feasible average status ages for multiple updating sources. We then use these results to characterize how a service facility can be shared among multiple updating sources. A new simplified technique for evaluating the AoI in finite-state continuous-time queuing systems is also derived. Based on stochastic hybrid systems, this method makes AoI evaluation to be comparable in complexity to finding the stationary distribution of a finite-state Markov chain."
            },
            {
                "arxivId": "1504.05103",
                "title": "Optimizing age-of-information in a multi-class queueing system",
                "abstract": "We consider the age-of-information in a multi-class M/G/1 queueing system, where each class generates packets containing status information. Age of information is a relatively new metric that measures the amount of time that elapsed between status updates, thus accounting for both the queueing delay and the delay between packet generation. This gives rise to a tradeoff between frequency of status updates, and queueing delay. In this paper, we study this tradeoff in a system with heterogenous users modeled as a multi-class M/G/1 queue. To this end, we derive the exact peak age-of-Information (PAoI) profile of the system, which measures the \u201cfreshness\u201d of the status information. We then seek to optimize the age of information, by formulating the problem using quasiconvex optimization, and obtain structural properties of the optimal solution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2404.16961",
        "category": "econ",
        "title": "On the testability of common trends in panel data without placebo periods",
        "abstract": "We demonstrate and discuss the testability of the common trend assumption imposed in Difference-in-Differences (DiD) estimation in panel data when not relying on multiple pre-treatment periods for running placebo tests. Our testing approach involves two steps: (i) constructing a control group of non-treated units whose pre-treatment outcome distribution matches that of treated units, and (ii) verifying if this control group and the original non-treated group share the same time trend in average outcomes. Testing is motivated by the fact that in several (but not all) panel data models, a common trend violation across treatment groups implies and is implied by a common trend violation across pre-treatment outcomes. For this reason, the test verifies a sufficient, but (depending on the model) not necessary condition for DiD-based identification. We investigate the finite sample performance of a testing procedure that is based on double machine learning, which permits controlling for covariates in a data-driven manner, in a simulation study and also apply it to labor market data from the National Supported Work Demonstration.",
        "references": [
            {
                "arxivId": "2402.14416",
                "title": "Algorithm-agnostic significance testing in supervised learning with multimodal data",
                "abstract": "Valid statistical inference is crucial for decision-making but difficult to obtain in supervised learning with multimodal data, e.g., combinations of clinical features, genomic data, and medical images. Multimodal data often warrants the use of black-box algorithms, for instance, random forests or neural networks, which impede the use of traditional variable significance tests. We address this problem by proposing the use of COvariance Measure Tests (COMETs), which are calibrated and powerful tests that can be combined with any sufficiently predictive supervised learning algorithm. We apply COMETs to several high-dimensional, multimodal data sets to illustrate (i) variable significance testing for finding relevant mutations modulating drug-activity, (ii) modality selection for predicting survival in liver cancer patients with multiomics data, and (iii) modality selection with clinical features and medical imaging data. In all applications, COMETs yield results consistent with domain knowledge without requiring data-driven pre-processing which may invalidate type I error control. These novel applications with high-dimensional multimodal data corroborate prior results on the power and robustness of COMETs for significance testing. The comets R package and source code for reproducing all results is available at https://github.com/LucasKook/comets. All data sets used in this work are openly available."
            },
            {
                "arxivId": "2010.04814",
                "title": "When Is Parallel Trends Sensitive to Functional Form?",
                "abstract": "This paper assesses when the validity of difference\u2010in\u2010differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger \u201cparallel trends\u201d\u2010type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form."
            },
            {
                "arxivId": "2003.03191",
                "title": "Double Machine Learning Based Program Evaluation under Unconfoundedness",
                "abstract": "\n This paper reviews, applies and extends recently proposed methods based on Double Machine Learning (DML) with a focus on program evaluation under unconfoundedness. DML based methods leverage flexible prediction models to adjust for confounding variables in the estimation of (i) standard average effects, (ii) different forms of heterogeneous effects, and (iii) optimal treatment assignment rules. An evaluation of multiple programs of the Swiss Active Labour Market Policy illustrates how DML based methods enable a comprehensive program evaluation. Motivated by extreme individualised treatment effect estimates of the DR-learner, we propose the normalised DR-learner (NDR-learner) to address this issue. The NDR-learner acknowledges that individualised effect estimates can be stabilised by an individualised normalisation of inverse probability weights."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2404.17049",
        "category": "econ",
        "title": "Overidentification in Shift-Share Designs",
        "abstract": "This paper studies the testability of identifying restrictions commonly employed to assign a causal interpretation to two stage least squares (TSLS) estimators based on Bartik instruments. For homogeneous effects models applied to short panels, our analysis yields testable implications previously noted in the literature for the two major available identification strategies. We propose overidentification tests for these restrictions that remain valid in high dimensional regimes and are robust to heteroskedasticity and clustering. We further show that homogeneous effect models in short panels, and their corresponding overidentification tests, are of central importance by establishing that: (i) In heterogenous effects models, interpreting TSLS as a positively weighted average of treatment effects can impose implausible assumptions on the distribution of the data; and (ii) Alternative identifying strategies relying on long panels can prove uninformative in short panel applications. We highlight the empirical relevance of our results by examining the viability of Bartik instruments for identifying the effect of rising Chinese import competition on US local labor markets.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2404.17412",
        "category": "econ",
        "title": "Characterizing Public Debt Cycles: Don't Ignore the Impact of Financial Cycles",
        "abstract": "Based on the quarterly data from 26 advanced economies (AEs) and 18 emerging market economies (EMs) over the past two decades, this paper estimates the short- and medium-term impacts of financial cycles on the duration and amplitude of public debt cycles. The results indicate that public debt expansions are larger than their contractions in duration and amplitude, aligning with the\"deficit bias hypothesis\"and being more pronounced in EMs than in AEs. The impacts of various financial cycles are different. Specifically, credit cycles in EMs significantly impact the duration and amplitude of public debt cycles. Notably, short- and medium-term credit booms in EMs shorten the duration of public debt contractions and reduce the amplitude. Fast credit growth in AEs prolongs the duration of public debt expansions and increases the amplitude. However, credit cycles in AEs show no significant impact. For house price cycles, the overall impact is stronger in EMs than in AEs, differing between short- and medium-term cycles. Finally, the impact of equity price cycles is significant in the short term, but not in the medium term. Equity price busts are more likely to prolong the expansion of public debt in EMs while increasing the amplitude of public debt contractions in AEs. Uncovering the impacts of multiple financial cycles on public debt cycles provides implications for better debt policies under different financial conditions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2404.17497",
        "category": "econ",
        "title": "Merchants of Vulnerabilities: How Bug Bounty Programs Benefit Software Vendors",
        "abstract": "Software vulnerabilities enable exploitation by malicious hackers, compromising systems and data security. This paper examines bug bounty programs (BBPs) that incentivize ethical hackers to discover and responsibly disclose vulnerabilities to software vendors. Using game-theoretic models, we capture the strategic interactions between software vendors, ethical hackers, and malicious hackers. First, our analysis shows that software vendors can increase expected profits by participating in BBPs, explaining their growing adoption and the success of BBP platforms. Second, we find that vendors with BBPs will release software earlier, albeit with more potential vulnerabilities, as BBPs enable coordinated vulnerability disclosure and mitigation. Third, the optimal number of ethical hackers to invite to a BBP depends solely on the expected number of malicious hackers seeking exploitation. This optimal number of ethical hackers is lower than but increases with the expected malicious hacker count. Finally, higher bounties incentivize ethical hackers to exert more effort, thereby increasing the probability that they will discover severe vulnerabilities first while reducing the success probability of malicious hackers. These findings highlight BBPs' potential benefits for vendors beyond profitability. Earlier software releases are enabled by managing risks through coordinated disclosure. As cybersecurity threats evolve, BBP adoption will likely gain momentum, providing vendors with a valuable tool for enhancing security posture and stakeholder trust. Moreover, BBPs envelop vulnerability identification and disclosure into new market relationships and transactions, impacting software vendors' incentives regarding product security choices like release timing.",
        "references": [
            {
                "arxivId": "1812.00140",
                "title": "The Art, Science, and Engineering of Fuzzing: A Survey",
                "abstract": "Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-29.json",
        "arxivId": "2404.17551",
        "category": "econ",
        "title": "The Role of Marketing in Public Policy Decision Making: The Case of Fuel Subsidy Removal in Nigeria",
        "abstract": "Public policy decision making has become more complex and complicated in recent times. Some authors have attributed this to the fact that public policy decision makers now have more variables to consider in every decision more than ever before. Others have argued that the rate of civilization, globalization and information technology has made the public to be more enlightened and abreast with the activities of government and so can oppose government decisions if they are unfavourable. This tends to increase government need for more and better information in order to satisfy the public. Consequently, this paper examined the issue of fuel subsidy removal in Nigeria, the impact of the policy on the public as well as the country and the role marketing principles would have played if the Nigerian government had taken some time to investigate what should be done, how it should be done and when it should be done. It also proposed a roadmap for future policies that have direct implications for the general public. Keywordsdecision making, fuel subsidy, government, marketing, policy, public",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2210.16506",
        "category": "econ",
        "title": "Observable Perfect Equilibrium",
        "abstract": "While Nash equilibrium has emerged as the central game-theoretic solution concept, many important games contain several Nash equilibria and we must determine how to select between them in order to create real strategic agents. Several Nash equilibrium refinement concepts have been proposed and studied for sequential imperfect-information games, the most prominent being trembling-hand perfect equilibrium, quasi-perfect equilibrium, and recently one-sided quasi-perfect equilibrium. These concepts are robust to certain arbitrarily small mistakes, and are guaranteed to always exist; however, we argue that neither of these is the correct concept for developing strong agents in sequential games of imperfect information. We define a new equilibrium refinement concept for extensive-form games called observable perfect equilibrium in which the solution is robust over trembles in publicly-observable action probabilities (not necessarily over all action probabilities that may not be observable by opposing players). Observable perfect equilibrium correctly captures the assumption that the opponent is playing as rationally as possible given mistakes that have been observed (while previous solution concepts do not). We prove that observable perfect equilibrium is always guaranteed to exist, and demonstrate that it leads to a different solution than the prior extensive-form refinements in no-limit poker. We expect observable perfect equilibrium to be a useful equilibrium refinement concept for modeling many important imperfect-information games of interest in artificial intelligence.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2309.02072",
        "category": "econ",
        "title": "Data Scaling Effect of Deep Learning in Financial Time Series Forecasting",
        "abstract": "For many years, researchers have been exploring the use of deep learning in the forecasting of financial time series. However, they have continued to rely on the conventional econometric approach for model optimization, optimizing the deep learning models on individual assets. In this paper, we use the stock volatility forecast as an example to illustrate global training - optimizes the deep learning model across a wide range of stocks - is both necessary and beneficial for any academic or industry practitioners who is interested in employing deep learning to forecast financial time series. Furthermore, a pre-trained foundation model for volatility forecast is introduced, capable of making accurate zero-shot forecasts for any stocks.",
        "references": [
            {
                "arxivId": "2310.10688",
                "title": "A decoder-only foundation model for time-series forecasting",
                "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1503.08123",
                "title": "Higher order elicitability and Osband\u2019s principle",
                "abstract": "A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability for multi-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the `correct' quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2309.06305",
        "category": "econ",
        "title": "Sensitivity Analysis for Linear Estimators",
        "abstract": "We propose a novel sensitivity analysis framework for linear estimators with identification failures that can be viewed as seeing the wrong outcome distribution. Our approach measures the degree of identification failure through the change in measure between the observed distribution and a hypothetical target distribution that would identify the causal parameter of interest. The framework yields a sensitivity analysis that generalizes existing bounds for Average Potential Outcome (APO), Regression Discontinuity (RD), and instrumental variables (IV) exclusion failure designs. Our partial identification results extend results from the APO context to allow even unbounded likelihood ratios. Our proposed sensitivity analysis consistently estimates sharp bounds under plausible conditions and estimates valid bounds under mild conditions. We find that our method performs well in simulations even when targeting a discontinuous and nearly infinite bound.",
        "references": [
            {
                "arxivId": "2309.15391",
                "title": "A Risk-Ratio-Based Marginal Sensitivity Model for Causal Effects in Observational Studies",
                "abstract": "In observational studies, the identification of causal estimands depends on the no unmeasured confounding (NUC) assumption. As this assumption is not testable from observed data, sensitivity analysis plays an important role in observational studies to investigate the impact of unmeasured confounding on the causal conclusions. In this paper, we proposed a risk-ratio-based sensitivity analysis framework by introducing a modified marginal sensitivity model for observational studies with binary treatments. We further extended the proposed framework to the multivalued treatment setting.We then showed how the point estimate intervals and the corresponding percentile bootstrap confidence intervals can be constructed efficiently under the proposed framework. Simulation results suggested that the proposed framework of sensitivity analysis performs well in the presence of adequate overlap among the treatment groups. Lastly, we demonstrated our proposed sensitivity analysis framework by estimating the causal effect of maternal education on female fertility in Bangladesh."
            },
            {
                "arxivId": "2305.16988",
                "title": "Sharp Bounds for Generalized Causal Sensitivity Analysis",
                "abstract": "Causal inference from observational data is crucial for many disciplines such as medicine and economics. However, sharp bounds for causal effects under relaxations of the unconfoundedness assumption (causal sensitivity analysis) are subject to ongoing research. So far, works with sharp bounds are restricted to fairly simple settings (e.g., a single binary treatment). In this paper, we propose a unified framework for causal sensitivity analysis under unobserved confounding in various settings. For this, we propose a flexible generalization of the marginal sensitivity model (MSM) and then derive sharp bounds for a large class of causal effects. This includes (conditional) average treatment effects, effects for mediation analysis and path analysis, and distributional effects. Furthermore, our sensitivity model is applicable to discrete, continuous, and time-varying treatments. It allows us to interpret the partial identification problem under unobserved confounding as a distribution shift in the latent confounders while evaluating the causal effect of interest. In the special case of a single binary treatment, our bounds for (conditional) average treatment effects coincide with recent optimality results for causal sensitivity analysis. Finally, we propose a scalable algorithm to estimate our sharp bounds from observational data."
            },
            {
                "arxivId": "2210.08326",
                "title": "Distributionally Robust Causal Inference with Observational Data",
                "abstract": "We consider the estimation of average treatment effects in observational studies and propose a new framework of robust causal inference with unobserved confounders. Our approach is based on distributionally robust optimization and proceeds in two steps. We first specify the maximal degree to which the distribution of unobserved potential outcomes may deviate from that of observed outcomes. We then derive sharp bounds on the average treatment effects under this assumption. Our framework encompasses the popular marginal sensitivity model as a special case, and we demonstrate how the proposed methodology can address a primary challenge of the marginal sensitivity model that it produces uninformative results when unobserved confounders substantially affect treatment and outcome. Specifically, we develop an alternative sensitivity model, called the distributional sensitivity model, under the assumption that heterogeneity of treatment effect due to unobserved variables is relatively small. Unlike the marginal sensitivity model, the distributional sensitivity model allows for potential lack of overlap and often produces informative bounds even when unobserved variables substantially affect both treatment and outcome. Finally, we show how to extend the distributional sensitivity model to difference-in-differences designs and settings with instrumental variables. Through simulation and empirical studies, we demonstrate the applicability of the proposed methodology."
            },
            {
                "arxivId": "2209.11383",
                "title": "Model-assisted sensitivity analysis for treatment effects under unmeasured confounding via regularized calibrated estimation",
                "abstract": "Abstract. Consider sensitivity analysis for estimating average treatment effects under unmeasured confounding, assumed to satisfy a marginal sensitivity model. At the population level, we provide new representations for the sharp population bounds and doubly robust estimating functions, recently derived by Dorn, Guo, and Kallus. We also derive new, relaxed population bounds, depending on weighted linear outcome quantile regression. At the sample level, we develop new methods and theory for obtaining not only doubly robust point estimators for the relaxed population bounds with respect to misspecification of a propensity score model or an outcome mean regression model, but also model-assisted confidence intervals which are valid if the propensity score model is correctly specified, but the outcome quantile and mean regression models may be misspecified. The relaxed population bounds reduce to the sharp bounds if outcome quantile regression is correctly specified. For a linear outcome mean regression model, the confidence intervals are also doubly robust. Our methods involve regularized calibrated estimation, with Lasso penalties but carefully chosen loss functions, for fitting propensity score and outcome mean and quantile regression models. We present a simulation study and an empirical application to an observational study on the effects of right heart catheterization."
            },
            {
                "arxivId": "2112.11449",
                "title": "Doubly-Valid/Doubly-Sharp Sensitivity Analysis for Causal Inference with Unmeasured Confounding",
                "abstract": "We consider the problem of constructing bounds on the average treatment e\ufb00ect (ATE) when unmeasured confounders exist but have bounded in\ufb02uence. Speci\ufb01cally, we assume that omitted confounders could not change the odds of treatment for any unit by more than a \ufb01xed factor. We derive the sharp partial identi\ufb01cation bounds implied by this assumption by leveraging distributionally robust optimization, and we propose estimators of these bounds with several novel robustness properties. The \ufb01rst is double sharpness : our estimators consistently estimate the sharp ATE bounds when one of two nuisance parameters is misspeci\ufb01ed and achieve semiparametric e\ufb03ciency when all nuisance parameters are suit-ably consistent. The second is double validity : even when most nuisance parameters are misspeci\ufb01ed, our estimators still provide valid but possibly conservative bounds for the ATE and our Wald con\ufb01dence intervals remain valid even when our estimators are not asymptotically normal. As a result, our estimators provide a highly credible method for sensitivity analysis of causal inferences."
            },
            {
                "arxivId": "2102.04543",
                "title": "Sharp Sensitivity Analysis for Inverse Propensity Weighting via Quantile Balancing",
                "abstract": "Abstract Inverse propensity weighting (IPW) is a popular method for estimating treatment effects from observational data. However, its correctness relies on the untestable (and frequently implausible) assumption that all confounders have been measured. This article introduces a robust sensitivity analysis for IPW that estimates the range of treatment effects compatible with a given amount of unobserved confounding. The estimated range converges to the narrowest possible interval (under the given assumptions) that must contain the true treatment effect. Our proposal is a refinement of the influential sensitivity analysis by Zhao, Small, and Bhattacharya, which we show gives bounds that are too wide even asymptotically. This analysis is based on new partial identification results for Tan\u2019s marginal sensitivity model. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "2012.15716",
                "title": "Assessing Sensitivity to Unconfoundedness: Estimation and Inference",
                "abstract": "Abstract This article provides a set of methods for quantifying the robustness of treatment effects estimated using the unconfoundedness assumption. Specifically, we estimate and do inference on bounds for various treatment effect parameters, like the Average Treatment Effect (ATE) and the average effect of treatment on the treated (ATT), under nonparametric relaxations of the unconfoundedness assumption indexed by a scalar sensitivity parameter c. These relaxations allow for limited selection on unobservables, depending on the value of c. For large enough c, these bounds equal the no assumptions bounds. Using a nonstandard bootstrap method, we show how to construct confidence bands for these bound functions which are uniform over all values of c. We illustrate these methods with an empirical application to the National Supported Work Demonstration program. We implement these methods in the companion Stata module tesensitivity for easy use in practice."
            },
            {
                "arxivId": "2009.07551",
                "title": "Manipulation-Robust Regression Discontinuity Designs",
                "abstract": "We present a new identification condition for regression discontinuity designs. We replace the local randomization of Lee (2008) with two restrictions on its threat, namely, the manipulation of the running variable. Furthermore, we provide the first auxiliary assumption of McCrary's (2008) diagnostic test to detect manipulation. Based on our auxiliary assumption, we derive a novel expression of moments that immediately implies the worst-case bounds of Gerard, Rokkanen, and Rothe (2020) and an enhanced interpretation of their target parameters. We highlight two issues: an overlooked source of identification failure, and a missing auxiliary assumption to detect manipulation. In the case studies, we illustrate our solution to these issues using institutional details and economic theories."
            },
            {
                "arxivId": "1904.00989",
                "title": "Counterfactual Sensitivity and Robustness",
                "abstract": "We propose a framework for analyzing the sensitivity of counterfactuals to parametric assumptions about the distribution of latent variables in structural models. In particular, we derive bounds on counterfactuals as the distribution of latent variables spans nonparametric neighborhoods of a given parametric specification while other \u201cstructural\u201d features of the model are maintained. Our approach recasts the infinite\u2010dimensional problem of optimizing the counterfactual with respect to the distribution of latent variables (subject to model constraints) as a finite\u2010dimensional convex program. We also develop an MPEC version of our method to further simplify computation in models with endogenous parameters (e.g., value functions) defined by equilibrium constraints. We propose plug\u2010in estimators of the bounds and two methods for inference. We also show that our bounds converge to the sharp nonparametric bounds on counterfactuals as the neighborhood size becomes large. To illustrate the broad applicability of our procedure, we present empirical applications to matching models with transferable utility and dynamic discrete choice models."
            },
            {
                "arxivId": "1812.11598",
                "title": "Salvaging Falsified Instrumental Variable Models",
                "abstract": "What should researchers do when their baseline model is falsified? We recommend reporting the set of parameters that are consistent with minimally nonfalsified models. We call this the \n falsification adaptive set (FAS). This set generalizes the standard baseline estimand to account for possible falsification. Importantly, it does not require the researcher to select or calibrate sensitivity parameters. In the classical linear IV model with multiple instruments, we show that the FAS has a simple closed\u2010form expression that only depends on a few 2SLS coefficients. We apply our results to an empirical study of roads and trade. We show how the FAS complements traditional overidentification tests by summarizing the variation in estimates obtained from alternative nonfalsified models.\n"
            },
            {
                "arxivId": "1803.07951",
                "title": "Testing continuity of a density via g-order statistics in the regression discontinuity design",
                "abstract": "In the regression discontinuity design (RDD), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., McCrary (2008). In this paper we propose an approximate sign test for continuity of a density at a point based on the so-called g-order statistics, and study its properties under two complementary asymptotic frameworks. In the first asymptotic framework, the number q of observations local to the cut-off is fixed as the sample size n diverges to infinity, while in the second framework q diverges to infinity slowly as n diverges to infinity. Under both of these frameworks, we show that the test we propose is asymptotically valid in the sense that it has limiting rejection probability under the null hypothesis not exceeding the nominal level. More importantly, the test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, and exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity. In a simulation study, we find that the approximate sign test provides good control of the rejection probability under the null hypothesis while remaining competitive under the alternative hypothesis. We finally apply our test to the design in Lee (2008), a well-known application of the RDD to study incumbency advantage."
            },
            {
                "arxivId": "1711.11286",
                "title": "Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap",
                "abstract": "To identify the estimand in missing data problems and observational studies, it is common to base the statistical estimation on the \u2018missingness at random\u2019 and \u2018no unmeasured confounder\u2019 assumptions. However, these assumptions are unverifiable by using empirical data and pose serious threats to the validity of the qualitative conclusions of statistical inference. A sensitivity analysis asks how the conclusions may change if the unverifiable assumptions are violated to a certain degree. We consider a marginal sensitivity model which is a natural extension of Rosenbaum's sensitivity model that is widely used for matched observational studies. We aim to construct confidence intervals based on inverse probability weighting estimators, such that asymptotically the intervals have at least nominal coverage of the estimand whenever the data\u2010generating distribution is in the collection of marginal sensitivity models. We use a percentile bootstrap and a generalized minimax\u2013maximin inequality to transform this intractable problem into a linear fractional programming problem, which can be solved very efficiently. We illustrate our method by using a real data set to estimate the causal effect of fish consumption on blood mercury level."
            },
            {
                "arxivId": "1707.09563",
                "title": "Identification of Treatment Effects under Conditional Partial Independence",
                "abstract": "Conditional independence of treatment assignment from potential outcomes is a commonly used but nonrefutable assumption. We derive identified sets for various treatment effect parameters under nonparametric deviations from this conditional independence assumption. These deviations are defined via a conditional treatment assignment probability, which makes it straightforward to interpret. Our results can be used to assess the robustness of empirical conclusions obtained under the baseline conditional independence assumption."
            },
            {
                "arxivId": "1512.00120",
                "title": "Exact Bounds on the Inverse Mills Ratio and Its Derivatives",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2312.07881",
        "category": "econ",
        "title": "Efficiency of QMLE for dynamic panel data models with interactive effects",
        "abstract": "This paper derives the efficiency bound for estimating the parameters of dynamic panel data models in the presence of an increasing number of incidental parameters. We study the efficiency problem by formulating the dynamic panel as a simultaneous equations system, and show that the quasi-maximum likelihood estimator (QMLE) applied to the system achieves the efficiency bound. Comparison of QMLE with fixed effects estimators is made.",
        "references": [
            {
                "arxivId": "1601.00815",
                "title": "Semiparametric efficiency bounds for high-dimensional models",
                "abstract": "Asymptotic lower bounds for estimation play a fundamental role in assessing the quality of statistical procedures. In this paper we propose a framework for obtaining semi-parametric efficiency bounds for sparse high-dimensional models, where the dimension of the parameter is larger than the sample size. We adopt a semi-parametric point of view: we concentrate on one dimensional functions of a high-dimensional parameter. We follow two different approaches to reach the lower bounds: asymptotic Cramer-Rao bounds and Le Cam's type of analysis. Both these approaches allow us to define a class of asymptotically unbiased or \"regular\" estimators for which a lower bound is derived. Consequently, we show that certain estimators obtained by de-sparsifying (or de-biasing) an $\\ell_1$-penalized M-estimator are asymptotically unbiased and achieve the lower bound on the variance: thus in this sense they are asymptotically efficient. The paper discusses in detail the linear regression model and the Gaussian graphical model."
            },
            {
                "arxivId": "1205.6617",
                "title": "STATISTICAL ANALYSIS OF FACTOR MODELS OF HIGH DIMENSION",
                "abstract": "This paper considers the maximum likelihood estimation of factor models of high dimension, where the number of variables (N) is comparable with or even greater than the number of observations (T). An inferential theory is developed. We establish not only consistency but also the rate of convergence and the limiting distributions. Five different sets of identification conditions are considered. We show that the distributions of the MLE estimators depend on the identification restrictions. Unlike the principal components approach, the maximum likelihood estimator explicitly allows heteroskedasticities, which are jointly estimated with other parameters. Efficiency of MLE relative to the principal components method is also considered."
            },
            {
                "arxivId": "0909.0613",
                "title": "A Maximum Likelihood Method for the Incidental Parameter Problem",
                "abstract": "This paper uses the invariance principle to solve the incidental parameter problem. We seek group actions that preserve the structural parameter and yield a maximal invariant in the parameter space with fixed dimension. M-estimation from the likelihood of the maximal invariant statistic yields the maximum invariant likelihood estimator (MILE). We apply our method to (i) a stationary autoregressive model with fixed effects; (ii) an agent-specific monotonic transformation model; (iii) an instrumental variable (IV) model; and (iv) a dynamic panel data model with fixed effects. In the first two examples, there exist group actions that completely discard the incidental parameters. In a stationary autoregressive model with fixed effects, MILE coincides with existing conditional and integrated likelihood methods. The invariance principle also gives a new perspective to the marginal likelihood approach. In an agent-specific monotonic transformation model, our approach yields an estimator that is consistent and asymptotically normal when errors are Gaussian. In an instrumental variable (IV) model, this paper unifies asymptotic results under strong instruments (SIV) and many weak instruments (MWIV) frameworks. We obtain consistency, asymptotic normality, and optimality results for the limited information maximum likelihood estimator directly from the invariant likelihood. Our approach is parallel to M-estimation in problems in which the number of parameters does not change with the sample size. In a dynamic panel data model with N individuals and T time periods, MILE is consistent as long as NT goes to infinity. We obtain a large N, fixed T bound; this bound coincides with Hahn and Kuersteiner's (2002) bound when T goes to infinity. MILE reaches (i) our bound when N is large and T is fixed; and (ii) Hahn and Kuersteiner's (2002) bound when both N and T are large."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2402.00584",
        "category": "econ",
        "title": "Arellano-bond lasso estimator for dynamic linear panel models",
        "abstract": "The Arellano-Bond estimator can be severely biased when the time series dimension of the data, $T$, is long. The source of the bias is the large degree of overidentification. We propose a simple two-step approach to deal with this problem. The first step applies LASSO to the cross-section data at each time period to select the most informative moment conditions. The second step applies a linear instrumental variable estimator using the instruments constructed from the moment conditions selected in the first step. The two stages are combined using sample-splitting and cross-fitting to avoid overfitting bias. Using asymptotic sequences where the two dimensions of the panel grow with the sample size, we show that the new estimator is consistent and asymptotically normal under much weaker conditions on $T$ than the Arellano-Bond estimator. Our theory covers models with high dimensional covariates including multiple lags of the dependent variable, which are common in modern applications. We illustrate our approach with an application to the short and long-term effects of the opening of K-12 schools and other policies on the spread of COVID-19 using weekly county-level panel data from the United States.",
        "references": [
            {
                "arxivId": "2102.10453",
                "title": "The association of opening K\u201312 schools with the spread of COVID-19 in the United States: County-level panel data analysis",
                "abstract": "Significance This paper examines whether the opening of K\u201312 schools may lead to the spread of COVID-19. Analyzing how an increase of COVID-19 cases is related to the timing of opening K\u201312 schools in the United States, we find that counties that opened K\u201312 schools with in-person learning experienced an increase in the growth rate of cases by 5 percentage points on average, controlling for a variety of policies, past infection rates, and other factors. This association of K\u201312 school visits with case growth is stronger when mask wearing is not mandated for staff at school. These findings support policies that promote masking and other precautionary measures at schools and giving vaccine priority to education workers. This paper empirically examines how the opening of K\u201312 schools is associated with the spread of COVID-19 using county-level panel data in the United States. As preliminary evidence, our event-study analysis indicates that cases and deaths in counties with in-person or hybrid opening relative to those with remote opening substantially increased after the school opening date, especially for counties without any mask mandate for staff. Our main analysis uses a dynamic panel data model for case and death growth rates, where we control for dynamically evolving mitigation policies, past infection levels, and additive county-level and state-week \u201cfixed\u201d effects. This analysis shows that an increase in visits to both K\u201312 schools and colleges is associated with a subsequent increase in case and death growth rates. The estimates indicate that fully opening K\u201312 schools with in-person learning is associated with a 5 (SE = 2) percentage points increase in the growth rate of cases. We also find that the association of K\u201312 school visits or in-person school openings with case growth is stronger for counties that do not require staff to wear masks at schools. These findings support policies that promote masking and other precautionary measures at schools and giving vaccine priority to education workers."
            },
            {
                "arxivId": "2002.10761",
                "title": "Some notes on concentration for $\\alpha$-subexponential random variables.",
                "abstract": "We prove extensions of classical concentration inequalities for random variables which have $\\alpha$-subexponential tail decay for any $\\alpha \\in (0,2]$. This includes Hanson--Wright type and convex concentration inequalities. We also provide some applications of these results. This includes uniform Hanson--Wright inequalities and concentration results for simple random tensors in the spirit of previous work by Klochkov--Zhivotovskiy and Vershynin."
            },
            {
                "arxivId": "1901.03821",
                "title": "Mastering Panel Metrics: Causal Impact of Democracy on Growth",
                "abstract": "We revisit the panel data analysis of Acemoglu et al. (forthcoming) on the relationship between democracy and economic growth using state-of-the-art econometric methods. We argue that panel data settings are high-dimensional, resulting in estimators to be biased to a degree that invalidates statistical inference. We remove these biases by using simple analytical and sample-splitting methods, and thereby restore valid statistical inference. We find that debiased fixed effects and Arellano-Bond estimators produce higher estimates of the long-run effect of democracy on growth, providing even stronger support for the key hypothesis of Acemoglu et al."
            },
            {
                "arxivId": "1806.05081",
                "title": "LASSO-Driven Inference in Time and Space",
                "abstract": "We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak dependence. A sequence of large-scale regressions with LASSO is applied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure. Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors."
            },
            {
                "arxivId": "1806.01888",
                "title": "High-dimensional econometrics and regularized GMM",
                "abstract": "This chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. High-dimensional models are characterized by having a number of unknown parameters that is not vanishingly small relative to the sample size. We first present results in a framework where estimators of parameters of interest may be represented directly as approximate means. Within this context, we review fundamental results including high-dimensional central limit theorems, bootstrap approximation of high-dimensional limit distributions, and moderate deviation theory. We also review key concepts underlying inference when many parameters are of interest such as multiple testing with family-wise error rate or false discovery rate control. We then turn to a general high-dimensional minimum distance framework with a special focus on generalized method of moments problems where we present results for estimation and inference about model parameters. The presented results cover a wide array of econometric applications, and we discuss several leading special cases including high-dimensional linear regression and linear instrumental variables models to illustrate the general results."
            },
            {
                "arxivId": "1508.07036",
                "title": "Gaussian Approximation for High Dimensional Time Series",
                "abstract": "We consider the problem of approximating sums of high-dimensional stationary time series by Gaussian vectors, using the framework of functional dependence measure. The validity of the Gaussian approximation depends on the sample size $n$, the dimension $p$, the moment condition and the dependence of the underlying processes. We also consider an estimator for long-run covariance matrices and study its convergence properties. Our results allow constructing simultaneous confidence intervals for mean vectors of high-dimensional time series with asymptotically correct coverage probabilities. A Gaussian multiplier bootstrap method is proposed. A simulation study indicates the quality of Gaussian approximation with different $n$, $p$ under different moment and dependence conditions."
            },
            {
                "arxivId": "1501.00478",
                "title": "UNIFORM INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANEL DATA MODELS WITH APPROXIMATELY SPARSE FIXED EFFECTS",
                "abstract": "We establish oracle inequalities for a version of the Lasso in high-dimensional fixed effects dynamic panel data models. The inequalities are valid for the coefficients of the dynamic and exogenous regressors. Separate oracle inequalities are derived for the fixed effects. Next, we show how one can conduct uniformly valid inference on the parameters of the model and construct a uniformly valid estimator of the asymptotic covariance matrix which is robust to conditional heteroskedasticity in the error terms. Allowing for conditional heteroskedasticity is important in dynamic models as the conditional error variance may be nonconstant over time and depend on the covariates. Furthermore, our procedure allows for inference on high-dimensional subsets of the parameter vector of an increasing cardinality. We show that the confidence bands resulting from our procedure are asymptotically honest and contract at the optimal rate. This rate is different for the fixed effects than for the remaining parts of the parameter vector."
            },
            {
                "arxivId": "1309.3522",
                "title": "Tail bounds via generic chaining",
                "abstract": "We modify Talagrand's generic chaining method to obtain upper bounds for all p-th moments of the supremum of a stochastic process. These bounds lead to an estimate for the upper tail of the supremum with optimal deviation parameters. We apply our procedure to improve and extend some known deviation inequalities for suprema of unbounded empirical processes and chaos processes. As an application we give a significantly simplified proof of the restricted isometry property of the subsampled discrete Fourier transform."
            },
            {
                "arxivId": "1308.5732",
                "title": "High dimensional generalized empirical likelihood for moment restrictions with dependent data",
                "abstract": null
            },
            {
                "arxivId": "1001.0188",
                "title": "Least Squares After Model Selection in High-Dimensional Sparse Models",
                "abstract": "In this paper we study post-model selection estimators which apply ordinary least squares (ols) to the model selected by first-step penalized estimators, typically lasso. It is well known that lasso can estimate the non-parametric regression function at nearly the oracle rate, and is thus hard to improve upon. We show that ols post lasso estimator performs at least as well as lasso in terms of the rate of convergence, and has the advantage of a smaller bias. Remarkably, this performance occurs even if the lasso-based model selection \u201cfails\u201d in the sense of missing some components of the \u201ctrue\u201d regression model. By the \u201ctrue\u201d model we mean here the best s-dimensional approximation to the nonparametric regression function chosen by the oracle. Furthermore, ols post lasso estimator can perform strictly better than lasso, in the sense of a strictly faster rate of convergence, if the lasso-based model selection correctly includes all components of the \u201ctrue\u201d model as a subset and also achieves sufficient sparsity. In the extreme case, when lasso perfectly selects the \u201ctrue\u201d model, the ols post lasso estimator becomes the oracle estimator. An important ingredient in our analysis is a new sparsity bound on the dimension of the model selected by lasso which guarantees that this dimension is at most of the same order as the dimension of the \u201ctrue\u201d model. Our rate results are non-asymptotic and hold in both parametric and nonparametric models. Moreover, our analysis is not limited to the lasso estimator acting as selector in the first step, but also applies to any other estimator, for example various forms of thresholded lasso, with good rates and good sparsity properties. Our analysis covers both traditional thresholding and a new practical, data-driven thresholding scheme that induces maximal sparsity subject to maintaining a certain goodness-of-fit. The latter scheme has theoretical guarantees similar to those of lasso or ols post lasso, but it dominates these procedures as well as traditional thresholding in a wide variety of experiments. First arXiv version: December 2009."
            },
            {
                "arxivId": "0801.1095",
                "title": "SIMULTANEOUS ANALYSIS OF LASSO AND DANTZIG SELECTOR",
                "abstract": "We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the l p estimation loss for 1 \u2264 p \u2264 2 in the linear model when the number of variables can be much larger than the sample size."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2403.02342",
        "category": "econ",
        "title": "Entanglement: Balancing Punishment and Compensation, Repeated Dilemma Game-Theoretic Analysis of Maximum Compensation Problem for Bypass and Least Cost Paths in Fact-Checking, Case of Fake News with Weak Wallace's Law",
        "abstract": "This research note is organized with respect to a novel approach to solving problems related to the spread of fake news and effective fact-checking. Focusing on the least-cost routing problem, the discussion is organized with respect to the use of Metzler functions and Metzler matrices to model the dynamics of information propagation among news providers. With this approach, we designed a strategy to minimize the spread of fake news, which is detrimental to informational health, while at the same time maximizing the spread of credible information. In particular, through the punitive dominance problem and the maximum compensation problem, we developed and examined a path to reassess the incentives of news providers to act and to analyze their impact on the equilibrium of the information market. By applying the concept of entanglement to the context of information propagation, we shed light on the complexity of interactions among news providers and contribute to the formulation of more effective information management strategies. This study provides new theoretical and practical insights into issues related to fake news and fact-checking, and will be examined against improving informational health and public digital health.This paper is partially an attempt to utilize\"Generative AI\"and was written with educational intent. There are currently no plans for it to become a peer-reviewed paper.",
        "references": [
            {
                "arxivId": "2111.08551",
                "title": "Using classical bit-flip correction for error mitigation in quantum computations including 2-qubit correlations",
                "abstract": "We present an error mitigation scheme which corrects readout errors on Noisy Intermediate-Scale Quantum (NISQ) computers [1, 2]. After a short review of applying the method to one qubit, we proceed to discuss the case when correlations between di\ufb00erent qubits occur. We demonstrate how the readout error can be mitigated in this case. By performing experiments on IBMQ hardware, we show that"
            },
            {
                "arxivId": "2111.05026",
                "title": "Investigating the variance increase of readout error mitigation through classical bit-flip correction on IBM and Rigetti quantum computers",
                "abstract": "Constantia Alexandrou, Lena Funcke, Tobias Hartung, Karl Jansen, f Stefan K\u00fchn, Georgios Polykratis,a,b,\u2217 Paolo Stornati, f Xiaoyang Wang and Tom Weber Department of Physics, University of Cyprus, P.O. Box 20537, 1678 Nicosia, Cyprus Computation-Based Science and Technology Research Center, The Cyprus Institute, 20 Kavafi Street, 2121 Nicosia, Cyprus Center for Theoretical Physics, Co-Design Center for Quantum Advantage, and NSF AI Institute for Artificial Intelligence and Fundamental Interactions, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139, USA Perimeter Institute for Theoretical Physics, 31 Caroline Street North, Waterloo, ON N2L 2Y5, Canada Department of Mathematical Sciences University of Bath, Bath, United Kingdom f Deutsches Elektronen-Synchrotron DESY, Platanenallee 6, 15738 Zeuthen, Germany School of Physics, Peking University, 5 Yiheyuan Rd, Haidian District, Beijing 100871, China Department of Computer Science, Universit\u00e4t Hamburg, Vogt-K\u00f6lln-Str. 30, 22527 Hamburg, Germany E-mail: alexand@ucy.ac.cy, lfuncke@mit.edu, tobias.hartung@desy.de,"
            },
            {
                "arxivId": "2107.11398",
                "title": "Experimental demonstration of continuous quantum error correction",
                "abstract": null
            },
            {
                "arxivId": "2106.05800",
                "title": "Qubit readout error mitigation with bit-flip averaging",
                "abstract": "We present a scheme to characterize and mitigate readout errors on multi-qubit devices more efficiently using random bit-flips."
            },
            {
                "arxivId": "2102.06131",
                "title": "Removing leakage-induced correlated errors in superconducting quantum error correction",
                "abstract": null
            },
            {
                "arxivId": "2007.03663",
                "title": "Measurement error mitigation in quantum computers through classical bit-flip correction",
                "abstract": "We develop a classical bit-flip correction method to mitigate measurement errors on quantum computers. This method can be applied to any operator, any number of qubits, and any realistic bit-flip probability. We first demonstrate the successful performance of this method by correcting the noisy measurements of the ground-state energies of the longitudinal and transversal Ising models. We then generalize our results to arbitrary operators and test our method both numerically and experimentally on IBM quantum hardware. As a result, our correction method reduces the measurement error on the quantum hardware by up to one order of magnitude."
            },
            {
                "arxivId": "1907.11729",
                "title": "Exponential suppression of bit-flips in a qubit encoded in an oscillator",
                "abstract": null
            },
            {
                "arxivId": "1905.03919",
                "title": "On the Inevitability of Online Echo Chambers",
                "abstract": "While social media make it easy to connect with and access information from anyone, they also facilitate basic influence and unfriending mechanisms that may lead to segregated and polarized clusters known as \"echo chambers.\" Here we study the conditions in which such echo chambers emerge by introducing a simple model of information sharing in online social networks with the two ingredients of influence and unfriending. Users can change both opinions and social connections based on the information to which they are exposed through sharing. Model dynamics demonstrate that even with minimal amounts of influence and unfriending, the social network rapidly devolves into segregated, homogeneous communities. These predictions are consistent with empirical data from Twitter. Although our findings suggest that echo chambers are somewhat inevitable given the mechanisms at play in online social media, they also provide insights into possible mitigation strategies."
            },
            {
                "arxivId": "1812.11845",
                "title": "Opinion Dynamics Theory for Analysis of Consensus Formation and Division of Opinion on the Internet",
                "abstract": "The massive amount of text data on the web has facilitated research on the quantitative analysis of public opinion, which could not be visualized earlier. In this paper, we propose a new opinion dynamics theory. This theory that is intended to explain agreement formation and opinion breakup division in opinion exchanges on social media such as Twitter. With the popularization of the public network, we have become able to communicate with instantaneity and interactivity beyond the temporal and spatial constraints.Research on quantitatively analyzing the distribution of opinion on public opinion that has not been visualized so far utilizing massive web text data is progressing.Our model is based on the Bounded Confidence Model, that expresses opinions in as continuous quantity values. However, in the Bounded Confidence Model, it was assumed that people with different opinions move not in disregard but ignoring opinions. Furthermore, in our theory, it modeled so that it can expresser model incorporates the influence from of the external pressure outside and the phenomenon depending on the surrounding situation."
            },
            {
                "arxivId": "1802.09089",
                "title": "Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection",
                "abstract": "Neural networks have become an increasingly popular solution for network intrusion detection systems (NIDS). Their capability of learning complex patterns and behaviors make them a suitable solution for differentiating between normal traffic and network attacks. However, a drawback of neural networks is the amount of resources needed to train them. Many network gateways and routers devices, which could potentially host an NIDS, simply do not have the memory or processing power to train and sometimes even execute such models. More importantly, the existing neural network solutions are trained in a supervised manner. Meaning that an expert must label the network traffic and update the model manually from time to time. \nIn this paper, we present Kitsune: a plug and play NIDS which can learn to detect attacks on the local network, without supervision, and in an efficient online manner. Kitsune's core algorithm (KitNET) uses an ensemble of neural networks called autoencoders to collectively differentiate between normal and abnormal traffic patterns. KitNET is supported by a feature extraction framework which efficiently tracks the patterns of every network channel. Our evaluations show that Kitsune can detect various attacks with a performance comparable to offline anomaly detectors, even on a Raspberry PI. This demonstrates that Kitsune can be a practical and economic NIDS."
            },
            {
                "arxivId": "1801.01665",
                "title": "Political Discourse on Social Media: Echo Chambers, Gatekeepers, and the Price of Bipartisanship",
                "abstract": "Echo chambers, i.e., situations where one is exposed only to opinions that agree with their own, are an increasing concern for the political discourse in many democratic countries. This paper studies the phenomenon of political echo chambers on social media. We identify the two components in the phenomenon: the opinion that is shared, and the \u00bbchamber\u00bb (i.e., the social network) that allows the opinion to \u00bbecho\u00bb (i.e., be re-shared in the network) -- and examine closely at how these two components interact. We define a production and consumption measure for social-media users, which captures the political leaning of the content shared and received by them. By comparing the two, we find that Twitter users are, to a large degree, exposed to political opinions that agree with their own. We also find that users who try to bridge the echo chambers, by sharing content with diverse leaning, have to pay a \u00bbprice of bipartisanship\u00bb in terms of their network centrality and content appreciation. In addition, we study the role of \u00bbgatekeepers,\u00bb users who consume content with diverse leaning but produce partisan content (with a single-sided leaning), in the formation of echo chambers. Finally, we apply these findings to the task of predicting partisans and gatekeepers from social and content features. While partisan users turn out relatively easy to identify, gatekeepers prove to be more challenging."
            },
            {
                "arxivId": "1712.03141",
                "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
                "abstract": null
            },
            {
                "arxivId": "1605.06326",
                "title": "Opinion dynamics: models, extensions and external effects",
                "abstract": null
            },
            {
                "arxivId": "1209.5998",
                "title": "Biased assimilation, homophily, and the dynamics of polarization",
                "abstract": "We study the issue of polarization in society through a model of opinion formation. We say an opinion formation process is polarizing if it results in increased divergence of opinions. Empirical studies have shown that homophily, i.e., greater interaction between like-minded individuals, results in polarization. However, we show that DeGroot\u2019s well-known model of opinion formation based on repeated averaging can never be polarizing, even if individuals are arbitrarily homophilous. We generalize DeGroot\u2019s model to account for a phenomenon well known in social psychology as biased assimilation: When presented with mixed or inconclusive evidence on a complex issue, individuals draw undue support for their initial position, thereby arriving at a more extreme opinion. We show that in a simple model of homophilous networks, our biased opinion formation process results in polarization if individuals are sufficiently biased. In other words, homophily alone, without biased assimilation, is not sufficient to polarize society. Quite interestingly, biased assimilation also provides a framework to analyze the polarizing effect of Internet-based recommender systems that show us personalized content."
            },
            {
                "arxivId": "1203.5351",
                "title": "Activity driven modeling of time varying networks",
                "abstract": null
            },
            {
                "arxivId": "1203.2655",
                "title": "Control Centrality and Hierarchical Structure in Complex Networks",
                "abstract": "We introduce the concept of control centrality to quantify the ability of a single node to control a directed weighted network. We calculate the distribution of control centrality for several real networks and find that it is mainly determined by the network\u2019s degree distribution. We show that in a directed network without loops the control centrality of a node is uniquely determined by its layer index or topological position in the underlying hierarchical structure of the network. Inspired by the deep relation between control centrality and hierarchical structure in a general directed network, we design an efficient attack strategy against the controllability of malicious networks."
            },
            {
                "arxivId": "0905.3045",
                "title": "Qubit decoherence due to detector switching",
                "abstract": null
            },
            {
                "arxivId": "1003.2440",
                "title": "Stochastic games for security in networks with interdependent nodes",
                "abstract": "This paper studies a stochastic game theoretic approach to security and intrusion detection in communication and computer networks. Specifically, an Attacker and a Defender take part in a two-player game over a network of nodes whose security assets and vulnerabilities are correlated. Such a network can be modeled using weighted directed graphs with the edges representing the influence among the nodes. The game can be formulated as a non-cooperative zero-sum or nonzero-sum stochastic game. However, due to correlation among the nodes, if some nodes are compromised, the effective security assets and vulnerabilities of the remaining ones will not stay the same in general, which leads to complex system dynamics. We examine existence, uniqueness, and structure of the solution and also provide numerical examples to illustrate our model."
            },
            {
                "arxivId": "0811.3171",
                "title": "Quantum algorithm for linear systems of equations.",
                "abstract": "Solving linear systems of equations is a common problem that arises both on its own and as a subroutine in more complex problems: given a matrix A and a vector b(-->), find a vector x(-->) such that Ax(-->) = b(-->). We consider the case where one does not need to know the solution x(-->) itself, but rather an approximation of the expectation value of some operator associated with x(-->), e.g., x(-->)(dagger) Mx(-->) for some matrix M. In this case, when A is sparse, N x N and has condition number kappa, the fastest known classical algorithms can find x(-->) and estimate x(-->)(dagger) Mx(-->) in time scaling roughly as N square root(kappa). Here, we exhibit a quantum algorithm for estimating x(-->)(dagger) Mx(-->) whose runtime is a polynomial of log(N) and kappa. Indeed, for small values of kappa [i.e., poly log(N)], we prove (using some common complexity-theoretic assumptions) that any classical algorithm for this problem generically requires exponentially more time than our quantum algorithm."
            },
            {
                "arxivId": "0802.0235",
                "title": "Entanglement-breaking channels in infinite dimensions",
                "abstract": null
            },
            {
                "arxivId": "0710.3256",
                "title": "Statistical physics of social dynamics",
                "abstract": "Statistical physics has proven to be a fruitful framework to describe phenomena outside the realm of traditional physics. Recent years have witnessed an attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. A wide list of topics are reviewed ranging from opinion and cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, and social spreading. The connections between these problems and other, more traditional, topics of statistical physics are highlighted. Comparison of model results with empirical data from social systems are also emphasized."
            },
            {
                "arxivId": "quant-ph/0702225",
                "title": "Quantum entanglement",
                "abstract": "All our former experience with application of quantum theory seems to say: {\\it what is predicted by quantum formalism must occur in laboratory}. But the essence of quantum formalism - entanglement, recognized by Einstein, Podolsky, Rosen and Schr\\\"odinger - waited over 70 years to enter to laboratories as a new resource as real as energy. This holistic property of compound quantum systems, which involves nonclassical correlations between subsystems, is a potential for many quantum processes, including ``canonical'' ones: quantum cryptography, quantum teleportation and dense coding. However, it appeared that this new resource is very complex and difficult to detect. Being usually fragile to environment, it is robust against conceptual and mathematical tools, the task of which is to decipher its rich structure. This article reviews basic aspects of entanglement including its characterization, detection, distillation and quantifying. In particular, the authors discuss various manifestations of entanglement via Bell inequalities, entropic inequalities, entanglement witnesses, quantum cryptography and point out some interrelations. They also discuss a basic role of entanglement in quantum communication within distant labs paradigm and stress some peculiarities such as irreversibility of entanglement manipulations including its extremal form - bound entanglement phenomenon. A basic role of entanglement witnesses in detection of entanglement is emphasized."
            },
            {
                "arxivId": "quant-ph/0404068",
                "title": "A case for applying an abstracted quantum formalism to cognition",
                "abstract": null
            },
            {
                "arxivId": "physics/0403122",
                "title": "Sociophysics: a personal testimony",
                "abstract": null
            },
            {
                "arxivId": "quant-ph/0201149",
                "title": "Additivity of the classical capacity of entanglement-breaking quantum channels",
                "abstract": "We show that for the tensor product of an entanglement-breaking quantum channel with an arbitrary quantum channel, both the minimum entropy of an output of the channel and the Holevo\u2013Schumacher\u2013Westmoreland capacity are additive. In addition, for the tensor product of two arbitrary quantum channels, we give a bound involving entanglement of formation for the amount of subadditivity (for minimum entropy output) or superadditivity (for classical capacity) that can occur."
            },
            {
                "arxivId": "quant-ph/0102117",
                "title": "Computable measure of entanglement",
                "abstract": "We present a measure of entanglement that can be computed effectively for any mixed state of an arbitrary bipartite system. We show that it does not increase under local manipulations of the system, and use it to obtain a bound on the teleportation capacity and on the distillable entanglement of mixed states."
            },
            {
                "arxivId": "quant-ph/9811053",
                "title": "Conditions for a Class of Entanglement Transformations",
                "abstract": "Suppose Alice and Bob jointly possess a pure state, |\u03c8\u3009. Using local operations on their respective systems and classical communication it may be possible for Alice and Bob to transform |\u03c8\u3009 into another joint state |\u03c6\u3009. This Letter gives necessary and sufficient conditions for this process of entanglement transformation to be possible. These conditions reveal a partial ordering on the entangled states and connect quantum entanglement to the algebraic theory of majorization. As a consequence, we find that there exist essentially different types of entanglement for bipartite quantum systems."
            },
            {
                "arxivId": "quant-ph/9906129",
                "title": "Fault-Tolerant Quantum Computation with Constant Error Rate",
                "abstract": "This paper shows that quantum computation can be made fault-tolerant against errors and inaccuracies when $\\eta$, the probability for an error in a qubit or a gate, is smaller than a constant threshold $\\eta_c$. This result improves on Shor's result [Proceedings of the 37th Symposium on the Foundations of Computer Science, IEEE, Los Alamitos, CA, 1996, pp. 56-65], which shows how to perform fault-tolerant quantum computation when the error rate $\\eta$ decays polylogarithmically with the size of the computation, an assumption which is physically unreasonable. The cost of making the quantum circuit fault-tolerant in our construction is polylogarithmic in time and space. Our result holds for a very general local noise model, which includes probabilistic errors, decoherence, amplitude damping, depolarization, and systematic inaccuracies in the gates. Moreover, we allow exponentially decaying correlations between the errors both in space and in time. Fault-tolerant computation can be performed with any universal set of gates. The result also holds for quantum particles with $p>2$ states, namely, $p$-qudits, and is also generalized to one-dimensional quantum computers with only nearest-neighbor interactions. No measurements, or classical operations, are required during the quantum computation. We estimate the threshold of our construction to be $\\eta_c\\simeq 10^{-6}$, in the best case. By this we show that local noise is in principle not an obstacle for scalable quantum computation. The main ingredient of our proof is the computation on states encoded by a quantum error correcting code (QECC). To this end we introduce a special class of Calderbank-Shor-Steane (CSS) codes, called polynomial codes (the quantum analogue of Reed-Solomon codes). Their nice algebraic structure allows all of the encoded gates to be transversal. We also provide another version of the proof which uses more general CSS codes, but its encoded gates are slightly less elegant. To achieve fault tolerance, we encode the quantum circuit by another circuit by using one of these QECCs. This step is repeated polyloglog many times, each step slightly improving the effective error rate, to achieve the desired reliability. The resulting circuit exhibits a hierarchical structure, and for the analysis of its robustness we borrow terminology from Khalfin and Tsirelson [Found. Phys., 22 (1992), pp. 879-948] and Gacs [Advances in Computing Research: A Research Annual: Randomness and Computation, JAI Press, Greenwich, CT, 1989]. The paper is to a large extent self-contained. In particular, we provide simpler proofs for many of the known results we use, such as the fact that it suffices to correct for bit-flips and phase-flips, the correctness of CSS codes, and the fact that two-qubit gates are universal, together with their extensions to higher-dimensional particles. We also provide full proofs of the universality of the sets of gates we use (the proof of universality was missing in Shor's paper). This paper thus provides a self-contained and complete proof of universal fault-tolerant quantum computation in the presence of local noise."
            },
            {
                "arxivId": "quant-ph/9604024",
                "title": "Mixed-state entanglement and quantum error correction.",
                "abstract": "Entanglement purification protocols (EPPs) and quantum error-correcting codes (QECCs) provide two ways of protecting quantum states from interaction with the environment. In an EPP, perfectly entangled pure states are extracted, with some yield D, from a mixed state M shared by two parties; with a QECC, an arbitrary quantum state |\\ensuremath{\\xi}\u3009 can be transmitted at some rate Q through a noisy channel \\ensuremath{\\chi} without degradation. We prove that an EPP involving one-way classical communication and acting on mixed state M^(\\ensuremath{\\chi}) (obtained by sharing halves of Einstein-Podolsky-Rosen pairs through a channel \\ensuremath{\\chi}) yields a QECC on \\ensuremath{\\chi} with rate Q=D, and vice versa. We compare the amount of entanglement E(M) required to prepare a mixed state M by local actions with the amounts ${\\mathit{D}}_{1}$(M) and ${\\mathit{D}}_{2}$(M) that can be locally distilled from it by EPPs using one- and two-way classical communication, respectively, and give an exact expression for E(M) when M is Bell diagonal. While EPPs require classical communication, QECCs do not, and we prove Q is not increased by adding one-way classical communication. However, both D and Q can be increased by adding two-way communication. We show that certain noisy quantum channels, for example a 50% depolarizing channel, can be used for reliable transmission of quantum states if two-way communication is available, but cannot be used if only one-way communication is available. We exhibit a family of codes based on universal hashing able to achieve an asymptotic Q (or D) of 1-S for simple noise models, where S is the error entropy. We also obtain a specific, simple 5-bit single-error-correcting quantum block code. We prove that iff a QECC results in high fidelity for the case of no error then the QECC can be recast into a form where the encoder is the matrix inverse of the decoder. \\textcopyright{} 1996 The American Physical Society."
            },
            {
                "arxivId": "quant-ph/9508027",
                "title": "Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer",
                "abstract": "A digital computer is generally believed to be an efficient universal computing device; that is, it is believed to be able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems that are generally thought to be hard on classical computers and that have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, for example, the number of digits of the integer to be factored."
            },
            {
                "arxivId": "quant-ph/9503017",
                "title": "Conditional Quantum Dynamics and Logic Gates.",
                "abstract": "Quantum logic gates provide fundamental examples of conditional quantum dynamics. They could form the building blocks of general quantum information processing systems which have recently been shown to have many interesting nonclassical properties. We describe a simple quantum logic gate, the quantum controlled-NOT, and analyze some of its applications. We discuss two possible physical realizations of the gate, one based on Ramsey atomic interferometry and the other on the selective driving of optical resonances of two subsystems undergoing a dipole-dipole interaction."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2403.18837",
        "category": "econ",
        "title": "Repetitive Dilemma Games in Distribution Information Using Interplay of Droop Quota: Meek's Method in Impact of Maximum Compensation and Minimum Cost Routes in Information Role of Marginal Contribution in Two-Sided Matching Markets",
        "abstract": "This paper is a preliminary report of the research plan and a digest of the results and discussions. On research note explores the complex dynamics of fake news dissemination and fact-checking costs within the framework of information markets and analyzes the equilibrium between supply and demand using the concepts of droop quotas, Meek's method, and marginal contributions. By adopting a two-sided matching market perspective, we delve into scenarios in which markets are stable under the influence of fake news perceived as truth and those in which credibility prevails. Through the application of iterated dilemma game theory, we investigate the strategic choices of news providers affected by the costs associated with spreading fake news and fact-checking efforts. We further examine the maximum reward problem and strategies to minimize the cost path for spreading fake news, and consider a nuanced understanding of market segmentation into\"cheap\"and\"premium\"segments based on the nature of the information being spread. Our analysis uses mathematical models and computational processes to identify stable equilibrium points that ensure market stability in the face of deceptive information practices and provide insight into effective strategies to enhance the informational health of the market. Through this comprehensive approach, this paper aims for a more truthful and reliable perspective from which to observe information markets. This paper is partially an attempt to utilize\"Generative AI\"and was written with educational intent. There are currently no plans for it to become a peer-reviewed paper.",
        "references": [
            {
                "arxivId": "quant-ph/9605011",
                "title": "Fault-tolerant quantum computation",
                "abstract": "It has recently been realized that use of the properties of quantum mechanics might speed up certain computations dramatically. Interest in quantum computation has since been growing. One of the main difficulties in realizing quantum computation is that decoherence tends to destroy the information in a superposition of states in a quantum computer making long computations impossible. A further difficulty is that inaccuracies in quantum state transformations throughout the computation accumulate, rendering long computations unreliable. However, these obstacles may not be as formidable as originally believed. For any quantum computation with t gates, we show how to build a polynomial size quantum circuit that tolerates O(1/log/sup c/t) amounts of inaccuracy and decoherence per gate, for some constant c; the previous bound was O(1/t). We do this by showing that operations can be performed on quantum data encoded by quantum error-correcting codes without decoding this data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2404.15633",
        "category": "econ",
        "title": "Artificial Intelligence for Multi-Unit Auction design",
        "abstract": "Understanding bidding behavior in multi-unit auctions remains an ongoing challenge for researchers. Despite their widespread use, theoretical insights into the bidding behavior, revenue ranking, and efficiency of commonly used multi-unit auctions are limited. This paper utilizes artificial intelligence, specifically reinforcement learning, as a model free learning approach to simulate bidding in three prominent multi-unit auctions employed in practice. We introduce six algorithms that are suitable for learning and bidding in multi-unit auctions and compare them using an illustrative example. This paper underscores the significance of using artificial intelligence in auction design, particularly in enhancing the design of multi-unit auctions.",
        "references": [
            {
                "arxivId": "2205.09123",
                "title": "A2C is a special case of PPO",
                "abstract": "\u2014Advantage Actor-critic (A2C) and Proximal Policy Optimization (PPO) are popular deep reinforcement learning algorithms used for game AI in recent years. A common understanding is that A2C and PPO are separate algorithms because PPO\u2019s clipped objective appears signi\ufb01cantly different than A2C\u2019s objective. In this paper, however, we show A2C is a special case of PPO. We present theoretical justi\ufb01cations and pseudocode analysis to demonstrate why. To validate our claim, we conduct an empirical experiment using Stable-baselines3 , showing A2C and PPO produce the exact same models when other settings are controlled."
            },
            {
                "arxivId": "2202.05947",
                "title": "Artificial Intelligence and Auction Design",
                "abstract": "Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about the lowest bid to win, as introduced by Google at the time of the switch to first-price auctions, increases competitiveness of auctions."
            },
            {
                "arxivId": "1707.06347",
                "title": "Proximal Policy Optimization Algorithms",
                "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
            },
            {
                "arxivId": "1607.07684",
                "title": "The Price of Anarchy in Auctions",
                "abstract": "This survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings. This theory complements traditional economic techniques, which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings. \n \nWe highlight three user-friendly analytical tools: smoothness-type inequalities, which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies; extension theorems, which extend such guarantees to randomized strategies, no-regret learning outcomes, and incomplete-information settings; and composition theorems, which extend such guarantees from simpler to more complex auctions. Combining these tools yields tight worst-case approximation guarantees for the equilibria of many widely-used auction formats."
            },
            {
                "arxivId": "1604.03171",
                "title": "Learning Simple Auctions",
                "abstract": "We present a general framework for proving polynomial sample complexity bounds for the problem of learning from samples the best auction in a class of \"simple\" auctions. Our framework captures all of the most prominent examples of \"simple\" auctions, including anonymous and non-anonymous item and bundle pricings, with either a single or multiple buyers. The technique we propose is to break the analysis of auctions into two natural pieces. First, one shows that the set of allocation rules have large amounts of structure; second, fixing an allocation on a sample, one shows that the set of auctions agreeing with this allocation on that sample have revenue functions with low dimensionality. Our results effectively imply that whenever it's possible to compute a near-optimal simple auction with a known prior, it is also possible to compute such an auction with an unknown prior (given a polynomial number of samples)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2404.16061",
        "category": "econ",
        "title": "Dynamic Many-Valued Logic Systems in Theoretical Economics",
        "abstract": "This paper is an original attempt to understand the foundations of economic reasoning. It endeavors to rigorously define the relationship between subjective interpretations and objective valuations of such interpretations in the context of theoretical economics. This analysis is substantially expanded through a dynamic approach, where the truth of a valuation results in an updated interpretation or changes in the agent's subjective belief regarding the effectiveness of the selected action as well as the objective reality of the effectiveness of all other possible actions (i.e. consequence realization). Complications arise when the economic agent is presented with a set of actions that render ambiguous preference, or when the effectiveness of an action cannot be perceived upon its selection, thereby necessitating a different theory of choice and consequence realization.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_econ_2024-04-30.json",
        "arxivId": "2404.17700",
        "category": "econ",
        "title": "Decentralized Finance and Local Public Goods: A Bayesian Maximum Entropy Model of School District Spending in the U.S",
        "abstract": "This paper investigates the distribution of public school expenditures across U.S. school districts using a bayesian maximum entropy model. Covering the period 2000-2016, I explore how inter-jurisdictional competition and household choice influence spending patterns within the public education sector, providing a novel empirical treatment of the Tiebout hypothesis within a statistical equilibrium framework. The analysis reveals that these expenditures are characterized by sharply peaked and positively skewed distributions, suggesting significant socioeconomic stratification. Employing Bayesian inference and Markov Chain Monte Carlo (MCMC) sampling, I fit these patterns into a statistical equilibrium model to elucidate the roles of competition, as well as household mobility and arbitrage in shaping the distribution of educational spending. The analysis reveals how the scale parameters associated with competition and household choice critically shape the equilibrium outcomes. The model and analysis offer a statistical basis for shaping policy measures intended to affect distributional outcomes in scenarios characterized by the decentralized provision of local public goods.",
        "references": [
            {
                "arxivId": "1903.08008",
                "title": "Rank-Normalization, Folding, and Localization: An Improved R\u02c6 for Assessing Convergence of MCMC (with Discussion)",
                "abstract": "Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic $\\widehat{R}$ of Gelman and Rubin (1992) has serious flaws. Traditional $\\widehat{R}$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice."
            },
            {
                "arxivId": "1710.06068",
                "title": "Data Analysis Recipes: Using Markov Chain Monte Carlo",
                "abstract": "Markov Chain Monte Carlo (MCMC) methods for sampling probability density functions (combined with abundant computational resources) have transformed the sciences, especially in performing probabilistic inferences, or fitting models to data. In this primarily pedagogical contribution, we give a brief overview of the most basic MCMC method and some practical advice for the use of MCMC in real inference problems. We give advice on method choice, tuning for performance, methods for initialization, tests of convergence, troubleshooting, and use of the chain output to produce or report parameter estimates with associated uncertainties. We argue that autocorrelation time is the most important test for convergence, as it directly connects to the uncertainty on the sampling estimate of any quantity of interest. We emphasize that sampling is a method for doing integrals; this guides our thinking about how MCMC output is best used."
            },
            {
                "arxivId": "1507.08050",
                "title": "Probabilistic Programming in Python using PyMC",
                "abstract": "Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis."
            }
        ]
    }
]