[
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2312.14903",
        "category": "q-fin",
        "title": "Scalable Agent-Based Modeling for Complex Financial Market Simulations",
        "abstract": "In this study, we developed a computational framework for simulating large-scale agent-based financial markets. Our platform supports trading multiple simultaneous assets and leverages distributed computing to scale the number and complexity of simulated agents. Heterogeneous agents make decisions in parallel, and their orders are processed through a realistic, continuous double auction matching engine. We present a baseline model implementation and show that it captures several known statistical properties of real financial markets (i.e., stylized facts). Further, we demonstrate these results without fitting models to historical financial data. Thus, this framework could be used for direct applications such as human-in-the-loop machine learning or to explore theoretically exciting questions about market microstructure's role in forming the statistical regularities of real markets. To the best of our knowledge, this study is the first to implement multiple assets, parallel agent decision-making, a continuous double auction mechanism, and intelligent agent types in a scalable real-time environment.",
        "references": [
            {
                "arxivId": "2103.09692",
                "title": "Radical Complexity",
                "abstract": "This is an informal and sketchy review of five topical, somewhat unrelated subjects in quantitative finance and econophysics: (i) models of price changes; (ii) linear correlations and random matrix theory; (iii) non-linear dependence copulas; (iv) high-frequency trading and market stability; and finally\u2014but perhaps most importantly\u2014(v) \u201cradical complexity\u201d that prompts a scenario-based approach to macroeconomics heavily relying on Agent-Based Models. Some open questions and future research directions are outlined."
            },
            {
                "arxivId": "1904.12066",
                "title": "ABIDES: Towards High-Fidelity Market Simulation for AI Research",
                "abstract": "We introduce ABIDES, an Agent-Based Interactive Discrete Event Simulation environment. ABIDES is designed from the ground up to support AI agent research in market applications. While simulations are certainly available within trading firms for their own internal use, there are no broadly available high-fidelity market simulation environments. We hope that the availability of such a platform will facilitate AI research in this important area. ABIDES currently enables the simulation of tens of thousands of trading agents interacting with an exchange agent to facilitate transactions. It supports configurable pairwise network latencies between each individual agent as well as the exchange. Our simulator's message-based design is modeled after NASDAQ's published equity trading protocols ITCH and OUCH. We introduce the design of the simulator and illustrate its use and configuration with sample code, validating the environment with example trading scenarios. The utility of ABIDES is illustrated through experiments to develop a market impact model. We close with discussion of future experimental problems it can be used to explore, such as the development of ML-based trading algorithms."
            },
            {
                "arxivId": "1805.04750",
                "title": "Multifractal analysis of financial markets: a review",
                "abstract": "Multifractality is ubiquitously observed in complex natural and socioeconomic systems. Multifractal analysis provides powerful tools to understand the complex nonlinear nature of time series in diverse fields. Inspired by its striking analogy with hydrodynamic turbulence, from which the idea of multifractality originated, multifractal analysis of financial markets has bloomed, forming one of the main directions of econophysics. We review the multifractal analysis methods and multifractal models adopted in or invented for financial time series and their subtle properties, which are applicable to time series in other disciplines. We survey the cumulating evidence for the presence of multifractality in financial time series in different markets and at different time periods and discuss the sources of multifractality. The usefulness of multifractal analysis in quantifying market inefficiency, in supporting risk management and in developing other applications is presented. We finally discuss open problems and further directions of multifractal analysis."
            },
            {
                "arxivId": "1404.0243",
                "title": "Physics and financial economics (1776\u20132014): puzzles, Ising and agent-based models",
                "abstract": "This short review presents a selected history of the mutual fertilization between physics and economics\u2014from Isaac Newton and Adam Smith to the present. The fundamentally different perspectives embraced in theories developed in financial economics compared with physics are dissected with the examples of the volatility smile and of the excess volatility puzzle. The role of the Ising model of phase transitions to model social and financial systems is reviewed, with the concepts of random utilities and the logit model as the analog of the Boltzmann factor in statistical physics. Recent extensions in terms of quantum decision theory are also covered. A wealth of models are discussed briefly that build on the Ising model and generalize it to account for the many stylized facts of financial markets. A summary of the relevance of the Ising model and its extensions is provided to account for financial bubbles and crashes. The review would be incomplete if it did not cover the dynamical field of agent-based models (ABMs), also known as computational economic models, of which the Ising-type models are just special ABM implementations. We formulate the \u2018Emerging Intelligence Market Hypothesis\u2019 to reconcile the pervasive presence of \u2018noise traders\u2019 with the near efficiency of financial markets. Finally, we note that evolutionary biology, more than physics, is now playing a growing role to inspire models of financial markets."
            },
            {
                "arxivId": "0911.4679",
                "title": "Gain/loss asymmetry in time series of individual stock prices and its relationship to the leverage effect",
                "abstract": "Previous research has shown that for stock indices, the most likely time until a return of a particular size has been observed is longer for gains than for losses. We establish that this so-called gain/loss asymmetry is present also for individual stocks and show that the phenomenon is closely linked to the well-known leverage effect -- in the EGARCH model and a modified retarded volatility model, the same parameter that governs the magnitude of the leverage effect also governs the gain/loss asymmetry."
            },
            {
                "arxivId": "0809.0822",
                "title": "How Markets Slowly Digest Changes in Supply and Demand",
                "abstract": "In this article we revisit the classic problem of tatonnement in price formation from a microstructure point of view, reviewing a recent body of theoretical and empirical work explaining how fluctuations in supply and demand are slowly incorporated into prices. Because revealed market liquidity is extremely low, large orders to buy or sell can only be traded incrementally, over periods of time as long as months. As a result order flow is a highly persistent long-memory process. Maintaining compatibility with market efficiency has profound consequences on price formation, on the dynamics of liquidity, and on the nature of impact. We review a body of theory that makes detailed quantitative predictions about the volume and time dependence of market impact, the bid-ask spread, order book dynamics, and volatility. Comparisons to data yield some encouraging successes. This framework suggests a novel interpretation of financial information, in which agents are at best only weakly informed and all have a similar and extremely noisy impact on prices. Most of the processed information appears to come from supply and demand itself, rather than from external news. The ideas reviewed here are relevant to market microstructure regulation, agent-based models, cost-optimal execution strategies, and understanding market ecologies."
            },
            {
                "arxivId": "cond-mat/0309233",
                "title": "The Predictive Power of Zero Intelligence in Financial Markets",
                "abstract": "Standard models in economics stress the role of intelligent agents who maximize utility. However, there may be situations where constraints imposed by market institutions dominate strategic agent behavior. We use data from the London Stock Exchange to test a simple model in which minimally intelligent agents place orders to trade at random. The model treats the statistical mechanics of order placement, price formation, and the accumulation of revealed supply and demand within the context of the continuous double auction and yields simple laws relating order-arrival rates to statistical properties of the market. We test the validity of these laws in explaining cross-sectional variation for 11 stocks. The model explains 96% of the variance of the gap between the best buying and selling prices (the spread) and 76% of the variance of the price diffusion rate, with only one free parameter. We also study the market impact function, describing the response of quoted prices to the arrival of new orders. The nondimensional coordinates dictated by the model approximately collapse data from different stocks onto a single curve. This work is important from a practical point of view, because it demonstrates the existence of simple laws relating prices to order flows and, in a broader context, suggests there are circumstances where the strategic behavior of agents may be dominated by other considerations."
            },
            {
                "arxivId": "cond-mat/0203511",
                "title": "Statistical properties of stock order books: empirical results and models",
                "abstract": "Abstract We investigate several statistical properties of the order book of three liquid stocks of the Paris Bourse. The results are to a large degree independent of the stock studied. The most interesting features concern (i) the statistics of incoming limit order prices, which follows a power-law around the current price with a diverging mean; and (ii) the shape of the average order book, which can be quantitatively reproduced using a \u2018zero intelligence\u2019 numerical model and qualitatively predicted using a simple approximation."
            },
            {
                "arxivId": "cond-mat/0108023",
                "title": "Random matrix approach to cross correlations in financial data.",
                "abstract": "We analyze cross correlations between price fluctuations of different stocks using methods of random matrix theory (RMT). Using two large databases, we calculate cross-correlation matrices C of returns constructed from (i) 30-min returns of 1000 US stocks for the 2-yr period 1994-1995, (ii) 30-min returns of 881 US stocks for the 2-yr period 1996-1997, and (iii) 1-day returns of 422 US stocks for the 35-yr period 1962-1996. We test the statistics of the eigenvalues lambda(i) of C against a \"null hypothesis\"--a random correlation matrix constructed from mutually uncorrelated time series. We find that a majority of the eigenvalues of C fall within the RMT bounds [lambda(-),lambda(+)] for the eigenvalues of random correlation matrices. We test the eigenvalues of C within the RMT bound for universal properties of random matrices and find good agreement with the results for the Gaussian orthogonal ensemble of random matrices-implying a large degree of randomness in the measured cross-correlation coefficients. Further, we find that the distribution of eigenvector components for the eigenvectors corresponding to the eigenvalues outside the RMT bound display systematic deviations from the RMT prediction. In addition, we find that these \"deviating eigenvectors\" are stable in time. We analyze the components of the deviating eigenvectors and find that the largest eigenvalue corresponds to an influence common to all stocks. Our analysis of the remaining deviating eigenvectors shows distinct groups, whose identities correspond to conventionally identified business sectors. Finally, we discuss applications to the construction of portfolios of stocks that have a stable ratio of risk to return."
            },
            {
                "arxivId": "cond-mat/0103600",
                "title": "Agent-based simulation of a financial market",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9905305",
                "title": "Scaling of the distribution of fluctuations of financial market indices.",
                "abstract": "We study the distribution of fluctuations of the S&P 500 index over a time scale deltat by analyzing three distinct databases. Database (i) contains approximately 1 200 000 records, sampled at 1-min intervals, for the 13-year period 1984-1996, database (ii) contains 8686 daily records for the 35-year period 1962-1996, and database (iii) contains 852 monthly records for the 71-year period 1926-1996. We compute the probability distributions of returns over a time scale deltat, where deltat varies approximately over a factor of 10(4)-from 1 min up to more than one month. We find that the distributions for deltat<or= 4 d (1560 min) are consistent with a power-law asymptotic behavior, characterized by an exponent alpha approximately 3, well outside the stable L\u00e9vy regime 0<alpha<2. To test the robustness of the S&P result, we perform a parallel analysis on two other financial market indices. Database (iv) contains 3560 daily records of the NIKKEI index for the 14-year period 1984-1997, and database (v) contains 4649 daily records of the Hang-Seng index for the 18-year period 1980-1997. We find estimates of alpha consistent with those describing the distribution of S&P 500 daily returns. One possible reason for the scaling of these distributions is the long persistence of the autocorrelation function of the volatility. For time scales longer than (deltat)x approximately 4 d, our results are consistent with a slow convergence to Gaussian behavior."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.00949",
        "category": "q-fin",
        "title": "A Portfolio's Common Causal Conditional Risk-neutral PDE",
        "abstract": "Portfolio's optimal drivers for diversification are common causes of the constituents' correlations. A closed-form formula for the conditional probability of the portfolio given its optimal common drivers is presented, with each pair constituent-common driver joint distribution modelled by Gaussian copulas. A conditional risk-neutral PDE is obtained for this conditional probability as a system of copulas' PDEs, allowing for dynamical risk management of a portfolio as shown in the experiments. Implied conditional portfolio volatilities and implied weights are new risk metrics that can be dynamically monitored from the PDEs or obtained from their solution.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17047",
        "category": "q-fin",
        "title": "Determinants of well-being",
        "abstract": "Traditionally, European social policies have focused on material well-being and social justice, neglecting subjective indicators. This review systematically examines the scientific understanding of well-being, its indicators, and its relationship with governance. It suggests that political systems and institutions significantly impact well-being, and that subjective indicators should be incorporated into public policy decisions. The findings advocate for a more holistic approach to well-being measurement, encompassing both objective and subjective dimensions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17329",
        "category": "q-fin",
        "title": "Assessing public perception of car automation in Iran: Acceptance and willingness to pay for adaptive cruise control",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17334",
        "category": "q-fin",
        "title": "Efficient estimation of parameters in marginal in semiparametric multivariate models",
        "abstract": "We consider a general multivariate model where univariate marginal distributions are known up to a common parameter vector and we are interested in estimating that vector without assuming anything about the joint distribution, except for the marginals. If we assume independence between the marginals and maximize the resulting quasi-likelihood, we obtain a consistent but inefficient estimate. If we assume a parametric copula (other than independence) we obtain a full MLE, which is efficient but only under correct copula specification and badly biased if the copula is misspecified. Instead we propose a sieve MLE estimator which improves over OMLE but does not suffer the drawbacks of the full MLE. We model the unknown part of the joint distribution using the Bernstein-Kantorovich polynomial copula and assess the resulting improvement over QMLE and over misspecified FMLE in terms of relative efficiency and robustness. We derive the asymptotic distribution of the new estimator and show that it reaches the semiparametric efficiency bound. Simulations suggest that the sieve MLE can be almost as efficient as FMLE relative to QMLE provided there is enough dependence between the marginals. An application using insurance company loss and expense data demonstrates empirical relevance of the estimator.",
        "references": [
            {
                "arxivId": "1306.6658",
                "title": "Semiparametric Gaussian Copula Models: Geometry and Efficient Rank-Based Estimation",
                "abstract": "We propose, for multivariate Gaussian copula models with unknown margins and structured correlation matrices, a rank-based,semiparametrically efficient estimator for the Euclidean copula parameter. This estimator is defined as a one-step update of a rank-based pilot estimator in the direction of the efficient influence function, which is calculated explicitly. Moreover, finite-dimensional algebraic conditions are given that completely characterize efficiency of the pseudo-likelihood estimator and adaptivity of the model with respect to the unknown marginal distributions. For correlation matrices structured according to a factor model, the pseudo-likelihood estimator turns out to be semiparametrically efficient. On the other hand, for Toeplitz correlation matrices, the asymptotic relative efficiency of the pseudo-likelihood estimator can be as low as 20%. These findings are confirmed by Monte Carlo simulations. We indicate how our results can be extended to joint regression models."
            },
            {
                "arxivId": "1210.2043",
                "title": "Smooth nonparametric Bernstein vine copulas",
                "abstract": "We consider the problem of accurately modelling the distribution of the market risk of a multivariate financial portfolio. We employ a multivariate GARCH model in which the dependence structure between the assets is modelled via a vine copula. We address the problem of how the parametric pair-copulas in a vine copula should be chosen by proposing to use nonparametric Bernstein copulas as bivariate pair-copulas. An extensive simulation study illustrates that our smooth nonparametric vine copula model is able to match the results of a competing parametric vine model calibrated via Akaike\u2019s Information Criterion while at the same time significantly reducing model risk. Our empirical analysis of financial market data demonstrates that our proposed model yields Value-at-Risk forecasts that are significantly more accurate than those of a benchmark parametric model."
            },
            {
                "arxivId": "1110.3572",
                "title": "Information bounds for Gaussian copulas.",
                "abstract": "Often of primary interest in the analysis of multivariate data are the copula parameters describing the dependence among the variables, rather than the univariate marginal distributions. Since the ranks of a multivariate dataset are invariant to changes in the univariate marginal distributions, rank-based estimators are natural candidates for semiparametric copula estimation. Asymptotic information bounds for such estimators can be obtained from an asymptotic analysis of the rank likelihood, i.e. the probability of the multivariate ranks. In this article, we obtain limiting normal distributions of the rank likelihood for Gaussian copula models. Our results cover models with structured correlation matrices, such as exchangeable or circular correlation models, as well as unstructured correlation matrices. For all Gaussian copula models, the limiting distribution of the rank likelihood ratio is shown to be equal to that of a parametric likelihood ratio for an appropriately chosen multivariate normal model. This implies that the semiparametric information bounds for rank-based estimators are the same as the information bounds for estimators based on the full data, and that the multivariate normal distributions are least favorable."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17384",
        "category": "q-fin",
        "title": "Modeling how and why aquatic vegetation removal can free rural households from poverty-disease traps",
        "abstract": "Infectious disease can reduce labor productivity and incomes, trapping subpopulations in a vicious cycle of ill health and poverty. Efforts to boost African farmers' agricultural production through fertilizer use can inadvertently promote the growth of aquatic vegetation that hosts disease vectors. Recent trials established that removing aquatic vegetation habitat for snail intermediate hosts reduces schistosomiasis infection rates in children, while converting the harvested vegetation into compost boosts agricultural productivity and incomes. Our model illustrates how this ecological intervention changes the feedback between the human and natural systems, potentially freeing rural households from poverty-disease traps. We develop a bioeconomic model that interacts an analytical microeconomic model of agricultural households' behavior, health status and incomes over time with a dynamic model of schistosomiasis disease ecology. We calibrate the model with field data from northern Senegal. We show analytically and via simulation that local conversion of invasive aquatic vegetation to compost changes the feedbacks among interlinked disease, aquatic and agricultural systems, reducing schistosomiasis infection and increasing incomes relative to the current status quo, in which villagers rarely remove vegetation. Aquatic vegetation removal disrupts the poverty-disease trap by reducing habitat for snails that vector the infectious helminth and by promoting production of compost that returns to agricultural soils nutrients that currently leach into surface water from on-farm fertilizer applications. The result is healthier people, more productive labor, cleaner water, more productive agriculture, and higher incomes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17391",
        "category": "q-fin",
        "title": "Education Policy and Intergenerational Educational Persistence: Evidence from rural Benin",
        "abstract": "This paper employs a nonlinear difference-in-differences approach to empirically examine the Maximally Maintained Inequality (MMI) hypothesis in rural Benin. The findings of this study confirm the MMI hypothesis. In particular, it is observed that when 76% of educated parents choose to educate their daughters in the absence of educational programs, in contrast to only 37% among non-educated parents, the average impact of tuition fee subsidy on enrollment probability in primary schools stands at 3.8\\% for non-educated households and 0.27% for educated households. Conversely, in cases where only 27% of educated parents decide to educate their daughters without education programs, the average effect of tuition fee waivers on enrollment probability in primary schools increases to 19.64\\% for non-educated households and 24\\% for educated households. From the analysis of household education decisions influenced by a preference for education and budget constraints, three key conclusions emerge to explain the mechanism behind the MMI. Firstly, when the income advantage of educated households compared to non-educated households is significantly high, irrespective of the level of their preference advantage, reducing the financial cost of education induces a greater shift in education decisions among non-educated households. Secondly, in situations where educated households do not possess an income advantage relative to non-educated households, the reduction in education-related financial costs leads to a more pronounced change in education decisions among educated households. Lastly, for the low-income advantage of educated households, as the income advantage of educated households increases, non-educated households respond more to education policy than educated parents, if the preference advantage of educated households is relatively smaller.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17448",
        "category": "q-fin",
        "title": "What you know or who you know? The role of intellectual and social capital in opportunity recognition",
        "abstract": "The recognition of business opportunities is the first stage in the entrepreneurial process. This article analyses the effects of individuals\u2019 possession of and access to knowledge on the probability of recognizing good business opportunities in their area of residence. The authors use an eclectic theoretical framework, consisting of intellectual and social capital concepts. In particular, they analyse the role of individuals\u2019 educational level, their perception that they have the right knowledge and skills to start a business, whether they own and manage a firm, their contacts with other entrepreneurs, and whether they have been business angels. The hypotheses proposed here are tested using data collected for the GEM project in Spain in 2007. The results show that individuals\u2019 access to external knowledge through the social networks in which they participate, is fundamental for developing the capacity to recognize new business opportunities.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-01.json",
        "arxivId": "2401.17472",
        "category": "q-fin",
        "title": "Convergence of the deep BSDE method for stochastic control problems formulated through the stochastic maximum principle",
        "abstract": "It is well-known that decision-making problems from stochastic control can be formulated by means of forward-backward stochastic differential equation (FBSDE). Recently, the authors of Ji et al. 2022 proposed an efficient deep learning-based algorithm which was based on the stochastic maximum principle (SMP). In this paper, we provide a convergence result for this deep SMP-BSDE algorithm and compare its performance with other existing methods. In particular, by adopting a similar strategy as in Han and Long 2020, we derive a posteriori error estimate, and show that the total approximation error can be bounded by the value of the loss functional and the discretization error. We present numerical examples for high-dimensional stochastic control problems, both in case of drift- and diffusion control, which showcase superior performance compared to existing algorithms.",
        "references": [
            {
                "arxivId": "1812.05916",
                "title": "Deep Neural Networks Algorithms for Stochastic Control Problems on Finite Horizon: Numerical Applications",
                "abstract": null
            },
            {
                "arxivId": "1812.04300",
                "title": "Deep Neural Networks Algorithms for Stochastic Control Problems on Finite Horizon: Convergence Analysis",
                "abstract": "This paper develops algorithms for high-dimensional stochastic control problems based on deep learning and dynamic programming. Unlike classical approximate dynamic programming approaches, we first approximate the optimal policy by means of neural networks in the spirit of deep reinforcement learning, and then the value function by Monte Carlo regression. This is achieved in the dynamic programming recursion by performance or hybrid iteration, and regress now methods from numerical probabilities. We provide a theoretical justification of these algorithms. Consistency and rate of convergence for the control and value function estimates are analyzed and expressed in terms of the universal approximation error of the neural networks, and of the statistical error when estimating network function, leaving aside the optimization error. Numerical results on various applications are presented in a companion paper (arxiv.org/abs/1812.05916) and illustrate the performance of the proposed algorithms."
            },
            {
                "arxivId": "1707.02568",
                "title": "Solving high-dimensional partial differential equations using deep learning",
                "abstract": "Significance Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the \u201ccurse of dimensionality.\u201d This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships. Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the \u201ccurse of dimensionality.\u201d This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black\u2013Scholes equation, the Hamilton\u2013Jacobi\u2013Bellman equation, and the Allen\u2013Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships."
            },
            {
                "arxivId": "0801.3203",
                "title": "Time discretization and Markovian iteration for coupled FBSDEs",
                "abstract": "In this paper we lay the foundation for a numerical algorithm to simulate high-dimensional coupled FBSDEs under weak coupling or monotonicity conditions. In particular, we prove convergence of a time discretization and a Markovian iteration. The iteration differs from standard Picard iterations for FBSDEs in that the dimension of the underlying Markovian process does not increase with the number of iterations. This feature seems to be indispensable for an efficient iterative scheme from a numerical point of view. We finally suggest a fully explicit numerical algorithm and present some numerical examples with up to 10-dimensional state space."
            },
            {
                "arxivId": "math/0411248",
                "title": "The Rate of Convergence of Finite-Difference Approximations for Bellman Equations with Lipschitz Coefficients",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2109.02872",
        "category": "q-fin",
        "title": "Moment Matching Method for Pricing Spread Options with Mean-Variance Mixture L\\'evy Motions",
        "abstract": "The paper Borovkova et al. [4] uses moment matching method to obtain closed form formulas for spread and basket call option prices under log normal models. In this note, we also use moment matching method to obtain semi-closed form formulas for the price of spread options under exponential L\\'evy models with mean-variance mixture. Unlike the semi-closed form formulas in Caldana and Fusai [5], where spread prices were expressed by using Fourier inversion formula for general price dynamics, our formula expresses spread prices in terms of the mixing distribution. Numerical tests show that our formulas give accurate spread prices also",
        "references": [
            {
                "arxivId": "0902.3643",
                "title": "A Fourier Transform Method for Spread Option Pricing",
                "abstract": "Spread options are a fundamental class of derivative contracts written on multiple assets and are widely traded in a range of financial markets. There is a long history of approximation methods for computing such products, but as yet there is no preferred approach that is accurate, efficient, and flexible enough to apply in general asset models. The present paper introduces a new formula for general spread option pricing based on Fourier analysis of the payoff function. Our detailed investigation, including a flexible and general error analysis, proves the effectiveness of a fast Fourier transform implementation of this formula for the computation of spread option prices. It is found to be easy to implement, stable, efficient, and applicable in a wide variety of asset pricing models."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2109.05431",
        "category": "q-fin",
        "title": "A note on closed-form spread option valuation under log-normal models",
        "abstract": "In the papers Carmona and Durrleman [7] and Bjerksund and Stensland [1], closed form approximations for spread call option prices were studied under the log normal models. In this paper, we give an alternative closed form formula for the price of spread call options under the log-normal models also. Our formula can be seen as a generalization of the closed-form formula presented in Bjerksund and Stensland [1] as their formula can be obtained by selecting special parameter values to our formula. Numerical tests show that our formula performs better for certain range of model parameters than the closed-form formula presented in Bjerksund and Stensland [1].",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2205.07256",
        "category": "q-fin",
        "title": "Market-Based Asset Price Probability",
        "abstract": "We consider the randomness of market trade values and volumes as the origin of asset price stochasticity. We define the first four market-based price statistical moments that depend on statistical moments and correlations of market trade values and volumes. Market-based price statistical moments coincide with conventional frequency-based ones if all trade volumes are constant during the time averaging interval. We present approximations of market-based price probability by a finite number of price statistical moments. We consider the consequences of the use of market-based price statistical moments for asset-pricing models and Value-at-Risk. We show that the use of volume weighted average price results in zero price-volume correlations. We derive market-based correlations between price and squares of volume and between squares of price and volume. To forecast market-based price volatility at horizon T one should predict the first two statistical moments of market trade values and volumes and their correlations at the same horizon T.",
        "references": [
            {
                "arxivId": "1509.08503",
                "title": "Volume Weighted Average Price Optimal Execution",
                "abstract": "We study the problem of optimal execution of a trading order under Volume Weighted Average Price (VWAP) benchmark, from the point of view of a risk-averse broker. The problem consists in minimizing mean-variance of the slippage, with quadratic transaction costs. We devise multiple ways to solve it, in particular we study how to incorporate the information coming from the market during the schedule. Most related works in the literature eschew the issue of imperfect knowledge of the total market volume. We instead incorporate it in our model. We validate our method with extensive simulation of order execution on real NYSE market data. Our proposed solution, using a simple model for market volumes, reduces by 10% the VWAP deviation RMSE of the standard \"static\" solution (and can simultaneously reduce transaction costs)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2208.06549",
        "category": "q-fin",
        "title": "Exponential utility maximization in small/large financial markets",
        "abstract": "Obtaining utility maximizing optimal portfolios in closed form is a challenging issue when the return vector follows a more general distribution than the normal one. In this note, we give closed form expressions, in markets based on finitely many assets, for optimal portfolios that maximize the expected exponential utility when the return vector follows normal mean-variance mixture models. We then consider large financial markets based on normal mean-variance mixture models also and show that, under exponential utility, the optimal utilities based on small markets converge to the optimal utility in the large financial market. This result shows, in particular, that to reach optimal utility level investors need to diversify their portfolios to include infinitely many assets into their portfolio and with portfolios based on any set of only finitely many assets, they never be able to reach optimum level of utility. In this paper, we also consider portfolio optimization problems with more general class of utility functions and provide an easy-to-implement numerical procedure for locating optimal portfolios. Especially, our approach in this part of the paper reduces a high dimensional problem in locating optimal portfolio into a three dimensional problem for a general class of utility functions.",
        "references": [
            {
                "arxivId": "2202.02488",
                "title": "A discussion of stochastic dominance and mean-risk optimal portfolio problems based on mean-variance-mixture models",
                "abstract": "The classical Markowitz mean-variance model uses variance as a risk measure and calculates frontier portfolios in closed form by using standard optimization techniques. For general mean-risk models such closed form optimal portfolios are difficult to obtain. In this note, we obtain closed form expression for frontier portfolios under mean-risk criteria when risk is modelled by any finite law-invariant convex measures of risk and when return vectors follow the class of normal mean-variance mixture (NMVM) distributions. To achieve this goal, we first present necessary as well as sufficient conditions for stochastic dominance within the class of one dimensional NMVM models and then we apply them to portfolio optimization problems. Our main result in this paper states that when return vectors follow the class of NMVM distributions the associated mean-risk frontier portfolios can be obtained by optimizing a Markowitz mean-variance model with an appropriately adjusted return vector."
            },
            {
                "arxivId": "2111.04311",
                "title": "Portfolio analysis with mean-CVaR and mean-CVaR-skewness criteria based on mean-variance mixture models",
                "abstract": "The paper Zhao et al. (2015) shows that mean-CVaR-skewness portfolio optimization problems based on asymetric Laplace (AL) distributions can be transformed into quadratic optimization problems under which closed form solutions can be found. In this note, we show that such result also holds for mean-risk-skewness portfolio optimization problems when the underlying distribution is a larger class of normal mean-variance mixture (NMVM) models than the class of AL distributions. We then study the value at risk (VaR) and conditional value at risk (CVaR) risk measures on portfolios of returns with NMVM distributions. They have closed form expressions for portfolios of normal and more generally elliptically distributed returns as discussed in Rockafellar&Uryasev (2000) and in Landsman&Valdez (2003). When the returns have general NMVM distributions, these risk measures do not give closed form expressions. In this note, we give approximate closed form expressions for VaR and CVaR of portfolios of returns with NMVM distributions. Numerical tests show that our closed form formulas give accurate values for VaR and CVaR and shortens the computational time for portfolio optimization problems associated with VaR and CVaR considerably."
            },
            {
                "arxivId": "1110.6882",
                "title": "The Moore\u2013Penrose Pseudoinverse: A Tutorial Review of the Theory",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2209.04001",
        "category": "q-fin",
        "title": "Optimal Bubble Riding: A Mean Field Game with Varying Entry Times",
        "abstract": "Recent financial bubbles such as the emergence of cryptocurrencies and\"meme stocks\"have gained increasing attention from both retail and institutional investors. In this paper, we propose a game-theoretic model on optimal liquidation in the presence of an asset bubble. Our setup allows the influx of players to fuel the price of the asset. Moreover, traders will enter the market at possibly different times and take advantage of the uptrend at the risk of an inevitable crash. In particular, we consider two types of crashes: an endogenous burst which results from excessive selling, and an exogenous burst which cannot be anticipated and is independent from the actions of the traders. The popularity of asset bubbles suggests a large-population setting, which naturally leads to a mean field game (MFG) formulation. We introduce a class of MFGs with varying entry times. In particular, an equilibrium will depend on the entry-weighted average of conditional optimal strategies. To incorporate the exogenous burst time, we adopt the method of progressive enlargement of filtrations. We prove existence of MFG equilibria using the weak formulation in a generalized setup, and we show that the equilibrium strategy can be decomposed into before-and-after-burst segments, each part containing only the market information. We also perform numerical simulations of the solution, which allow us to provide some intriguing results on the relationship between the bubble burst and equilibrium strategies.",
        "references": [
            {
                "arxivId": "2105.00484",
                "title": "Non-asymptotic convergence rates for mean-field games: weak formulation and McKean-Vlasov BSDEs",
                "abstract": "This work is mainly concerned with the so-called limit theory for mean-field games. Adopting the weak formulation paradigm put forward by Carmona and Lacker, we consider a fully non-Markovian setting allowing for drift control and interactions through the joint distribution of players' states and controls. We provide first a characterisation of mean-field equilibria as arising from solutions to a novel kind of McKean-Vlasov backward stochastic differential equations, for which we provide a well-posedness theory. We incidentally obtain there unusual existence and uniqueness results for mean-field equilibria, which do not require short time horizon, separability assumptions on the coefficients, nor Lasry and Lions's monotonicity conditions, but rather smallness, or alternatively regularity, conditions on the terminal reward and a dissipativity condition on the drift. We then take advantage of this characterisation to provide non-asymptotic rates of convergence for the value functions and the Nash-equilibria of the N-player version to their mean-field counterparts, for general open-loop equilibria. An appropriate reformulation of our approach also allows us to treat closed-loop equilibria, and to obtain convergence results for the master equation associated to the problem."
            },
            {
                "arxivId": "2101.00327",
                "title": "The 2020 global stock market crash: Endogenous or exogenous?",
                "abstract": null
            },
            {
                "arxivId": "2004.08351",
                "title": "Convergence of Large Population Games to Mean Field Games with Interaction Through the Controls",
                "abstract": "This work considers stochastic differential games with a large number of players, whose costs and dynamics interact through the empirical distribution of both their states and their controls. We develop a framework to prove convergence of finite-player games to the asymptotic mean field game. Our approach is based on the concept of propagation of chaos for forward and backward weakly interacting particles which we investigate by fully probabilistic methods, and which appear to be of independent interest. These propagation of chaos arguments allow to derive moment and concentration bounds for the convergence of both Nash equilibria and social optima in non-cooperative and cooperative games, respectively. Incidentally, we also obtain convergence of a system of second order parabolic partial differential equations on finite dimensional spaces to a second order parabolic partial differential equation on the Wasserstein space."
            },
            {
                "arxivId": "2001.00925",
                "title": "McKean-Vlasov Optimal Control: Limit Theory and Equivalence Between Different Formulations",
                "abstract": "We study a McKean\u2013Vlasov optimal control problem with common noise in order to establish the corresponding limit theory as well as the equivalence between different formulations, including strong, weak, and relaxed formulations. In contrast to the strong formulation, in which the problem is formulated on a fixed probability space equipped with two Brownian filtrations, the weak formulation is obtained by considering a more general probability space with two filtrations satisfying an (H)-hypothesis type condition from the theory of enlargement of filtrations. When the common noise is uncontrolled, our relaxed formulation is obtained by considering a suitable controlled martingale problem. As for classic optimal control problems, we prove that the set of all relaxed controls is the closure of the set of all strong controls when considered as probability measures on the canonical space. Consequently, we obtain the equivalence of the different formulations of the control problem under additional mild regularity conditions on the reward functions. This is also a crucial technical step to prove the limit theory of the McKean\u2013Vlasov control problem, that is, proving that it consists in the limit of a large population control problem with common noise."
            },
            {
                "arxivId": "1810.01980",
                "title": "Nonexponential Sanov and Schilder theorems on Wiener space: BSDEs, Schr\u00f6dinger problems and control",
                "abstract": "We derive new limit theorems for Brownian motion, which can be seen as non-exponential analogues of the large deviation theorems of Sanov and Schilder in their Laplace principle forms. As a first application, we obtain novel scaling limits of backward stochastic differential equations and their related partial differential equations. As a second application, we extend prior results on the small-noise limit of the Schrodinger problem as an optimal transport cost, unifying the control-theoretic and probabilistic approaches initiated respectively by T. Mikami and C. Leonard. Lastly, our results suggest a new scheme for the computation of mean field optimal control problems, distinct from the conventional particle approximation. A key ingredient in our analysis is an extension of the classical variational formula (often attributed to Borell or Boue-Dupuis) for the Laplace transform of Wiener measure."
            },
            {
                "arxivId": "1804.08550",
                "title": "From the master equation to mean field game limit theory: Large deviations and concentration of measure",
                "abstract": "We study a sequence of symmetric $n$-player stochastic differential games driven by both idiosyncratic and common sources of noise, in which players interact with each other through their empirical distribution. The unique Nash equilibrium empirical measure of the $n$-player game is known to converge, as $n$ goes to infinity, to the unique equilibrium of an associated mean field game. Under suitable regularity conditions, in the absence of common noise, we complement this law of large numbers result with non-asymptotic concentration bounds for the Wasserstein distance between the $n$-player Nash equilibrium empirical measure and the mean field equilibrium. We also show that the sequence of Nash equilibrium empirical measures satisfies a weak large deviation principle, which can be strengthened to a full large deviation principle only in the absence of common noise. For both sets of results, we first use the master equation, an infinite-dimensional partial differential equation that characterizes the value function of the mean field game, to construct an associated McKean-Vlasov interacting $n$-particle system that is exponentially close to the Nash equilibrium dynamics of the $n$-player game for large $n$, by refining estimates obtained in our companion paper. Then we establish a weak large deviation principle for McKean-Vlasov systems in the presence of common noise. In the absence of common noise, we upgrade this to a full large deviation principle and obtain new concentration estimates for McKean-Vlasov systems. Finally, in two specific examples that do not satisfy the assumptions of our main theorems, we show how to adapt our methodology to establish large deviations and concentration results."
            },
            {
                "arxivId": "1804.06261",
                "title": "Dissection of Bitcoin\u2019s multiscale bubble history from January 2012 to February 2018",
                "abstract": "We present a detailed bubble analysis of the Bitcoin to US Dollar price dynamics from January 2012 to February 2018. We introduce a robust automatic peak detection method that classifies price time series into periods of uninterrupted market growth (drawups) and regimes of uninterrupted market decrease (drawdowns). In combination with the Lagrange Regularization Method for detecting the beginning of a new market regime, we identify three major peaks and 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin price during the analysed time period. We explain this classification of long and short bubbles by a number of quantitative metrics and graphs to understand the main socio-economic drivers behind the ascent of Bitcoin over this period. Then, a detailed analysis of the growing risks associated with the three long bubbles using the Log-Periodic Power-Law Singularity (LPPLS) model is based on the LPPLS Confidence Indicators, defined as the fraction of qualified fits of the LPPLS model over multiple time windows. Furthermore, for various fictitious \u2018present\u2019 times t2 before the crashes, we employ a clustering method to group the predicted critical times tc of the LPPLS fits over different time scales, where tc is the most probable time for the ending of the bubble. Each cluster is proposed as a plausible scenario for the subsequent Bitcoin price evolution. We present these predictions for the three long bubbles and the four short bubbles that our time scale of analysis was able to resolve. Overall, our predictive scheme provides useful information to warn of an imminent crash risk."
            },
            {
                "arxivId": "1702.05434",
                "title": "The Amazing Power of Dimensional Analysis: Quantifying Market Impact",
                "abstract": "This note complements the inspiring work on dimensional analysis and market microstructure by Kyle and Obizhaeva [18]. Following closely these authors, our main result shows by a similar argument as usually applied in physics the following remarkable fact. If the market impact of a meta-order only depends on four well-defined and financially meaningful variables, then -- up to a constant -- there is only one possible form of this dependence. In particular, the market impact is proportional to the square-root of the size of the meta-order. This theorem can be regarded as a special case of a more general result of Kyle and Obizhaeva. These authors consider five variables which might have an influence on the size of the market impact. In this case one finds a richer variety of possible functional relations which we precisely characterize. We also discuss the analogies to classical arguments from physics, such as the period of a pendulum."
            },
            {
                "arxivId": "1612.03816",
                "title": "$N$-player games and mean-field games with absorption",
                "abstract": "We introduce a simple class of mean field games with absorbing boundary over a finite time horizon. In the corresponding N-player games, the evolution of players\u2019 states is described by a system of weakly interacting Ito equations with absorption on first exit from a bounded open set. Once a player exits, her/his contribution is removed from the empirical measure of the system. Players thus interact through a renormalized empirical measure. In the definition of solution to the mean field game, the renormalization appears in form of a conditional law. We justify our definition of solution in the usual way, that is, by showing that a solution of the mean field game induces approximate Nash equilibria for the N-player games with approximation error tending to zero as N tends to infinity. This convergence is established provided the diffusion coefficient is non-degenerate. The degenerate case is more delicate and gives rise to counter-examples."
            },
            {
                "arxivId": "1610.09904",
                "title": "Mean field game of controls and an application to trade crowding",
                "abstract": null
            },
            {
                "arxivId": "1509.02505",
                "title": "The Master Equation and the Convergence Problem in Mean Field Games",
                "abstract": "This book describes the latest advances in the theory of mean field games, which are optimal control problems with a continuum of players, each of them interacting with the whole statistical distribution of a population. While it originated in economics, this theory now has applications in areas as diverse as mathematical finance, crowd phenomena, epidemiology, and cybersecurity. Because mean field games concern the interactions of infinitely many players in an optimal control framework, one expects them to appear as the limit for Nash equilibria of differential games with finitely many players as the number of players tends to infinity. The book rigorously establishes this convergence, which has been an open problem until now. The limit of the system associated with differential games with finitely many players is described by the so-called master equation, a nonlocal transport equation in the space of measures. After defining a suitable notion of differentiability in the space of measures, the authors provide a complete self-contained analysis of the master equation. Their analysis includes the case of common noise problems in which all the players are affected by a common Brownian motion. They then go on to explain how to use the master equation to prove the mean field limit. The book presents two important new results in mean field games that contribute to a unified theoretical framework for this exciting and fast-developing area of mathematics."
            },
            {
                "arxivId": "1308.1275",
                "title": "Dual Representation of Minimal Supersolutions of Convex BSDEs",
                "abstract": "We give a dual representation of minimal supersolutions of BSDEs with non-bounded, but integrable terminal conditions and under weak requirements on the generator which is allowed to depend on the value process of the equation. Conversely, we show that any dynamic risk measure satisfying such a dual representation stems from a BSDE. We also give a condition under which a supersolution of a BSDE is even a solution."
            },
            {
                "arxivId": "1307.1152",
                "title": "A probabilistic weak formulation of mean field games and applications",
                "abstract": "Mean field games are studied by means of the weak formulation of stochastic optimal control. This approach allows the mean field interactions to enter through both state and control processes and take a form which is general enough to include rank and nearest-neighbor effects. Moreover, the data may depend discontinuously on the state variable, and more generally its entire history. Existence and uniqueness results are proven, along with a procedure for identifying and constructing distributed strategies which provide approximate Nash equlibria for finite-player games. Our results are applied to a new class of multi-agent price impact models and a class of flocking models for which we prove existence of equilibria."
            },
            {
                "arxivId": "1210.5780",
                "title": "Probabilistic Analysis of Mean-Field Games",
                "abstract": "The purpose of this paper is to provide a complete probabilistic analysis of a large class of stochastic differential games with mean field interactions. We implement the Mean-Field Game strategy developed analytically by Lasry and Lions in a purely probabilistic framework, relying on tailor-made forms of the stochastic maximum principle. While we assume that the state dynamics are affine in the states and the controls, and the costs are convex, our assumptions on the nature of the dependence of all the coefficients upon the statistical distribution of the states of the individual players remains of a rather general nature. Our probabilistic approach calls for the solution of systems of forward-backward stochastic differential equations of a McKean--Vlasov type for which no existence result is known, and for which we prove existence and regularity of the corresponding value function. Finally, we prove that a solution of the Mean-Field Game problem as formulated by Lasry and Lions, does indeed provide appr..."
            },
            {
                "arxivId": "1210.5771",
                "title": "Control of McKean\u2013Vlasov dynamics versus mean field games",
                "abstract": null
            },
            {
                "arxivId": "1107.3171",
                "title": "Clarifications to questions and criticisms on the Johansen\u2013Ledoit\u2013Sornette financial bubble model",
                "abstract": null
            },
            {
                "arxivId": "1101.2815",
                "title": "Progressive enlargement of filtrations and Backward SDEs with jumps",
                "abstract": "This work deals with backward stochastic differential equation (BSDE) with random marked jumps, and their applications to default risk. We show that these BSDEs are linked with Brownian BSDEs through the decomposition of processes with respect to the progressive enlargement of filtrations. We show that the equations have solutions if the associated Brownian BSDEs have solutions. We also provide a uniqueness theorem for BSDEs with jumps by giving a comparison theorem based on the comparison for Brownian BSDEs. We give in particular some results for quadratic BDSEs. As applications, we study the pricing and the hedging of a European option in a complete market with a single jump, and the utility maximization problem in an incomplete market with a finite number of jumps."
            },
            {
                "arxivId": "1001.0206",
                "title": "Stochastic control under progressive enlargement of filtrations and applications to multiple defaults risk management",
                "abstract": null
            },
            {
                "arxivId": "0907.1221",
                "title": "CREDIT RISK PREMIA AND QUADRATIC BSDEs WITH A SINGLE JUMP",
                "abstract": "This paper is concerned with the determination of credit risk premia of defaultable contingent claims by means of indifference valuation principles. Assuming exponential utility preferences we derive representations of indifference premia of credit risk in terms of solutions of Backward Stochastic Differential Equations (BSDE). The class of BSDEs needed for that representation allows for quadratic growth generators and jumps at random times. Since the existence and uniqueness theory for this class of BSDEs has not yet been developed to the required generality, the first part of the paper is devoted to fill that gap. By using a simple constructive algorithm, and known results on continuous quadratic BSDEs, we provide sufficient conditions for the existence and uniqueness of quadratic BSDEs with discontinuities at random times."
            },
            {
                "arxivId": "cond-mat/0301543",
                "title": "Critical Market Crashes",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9903321",
                "title": "Predicting Financial Crashes Using Discrete Scale Invariance",
                "abstract": "We present a synthesis of all the available empirical evidence in the light of recent theoretical developments for the existence of characteristic log-periodic signatures of growing bubbles in a variety of markets including 8 unrelated crashes from 1929 to 1998 on stock markets as diverse as the US, Hong-Kong or the Russian market and on currencies. To our knowledge, no major financial crash preceded by an extended bubble has occurred in the past 2 decades without exhibiting such log-periodic signatures."
            },
            {
                "arxivId": "cond-mat/9810071",
                "title": "CRASHES AS CRITICAL POINTS",
                "abstract": "We study a rational expectation model of bubbles and crashes. The model has two components: (1) our key assumption is that a crash may be caused bylocalself-reinforcing imitation between noise traders. If the tendency for noise traders to imitate their nearest neighbors increases up to a certain point called the \"critical\" point, all noise traders may place the same order (sell) at the same time, thus causing a crash. The interplay between the progressive strengthening of imitation and the ubiquity of noise is characterized by the hazard rate, i.e. the probability per unit time that the crash will happen in the next instant if it has not happened yet. (2) Since the crash is not a certain deterministic outcome of the bubble, it remains rational for traders to remain invested provided they are compensated by a higher rate of growth of the bubble for taking the risk of a crash. Our model distinguishes between the end of the bubble and the time of the crash: the rational expectation constraint has the specific implication that the date of the crash must be random. The theoretical death of the bubble is not the time of the crash because the crash could happen at any time before, even though this is not very likely. The death of the bubble is the most probable time for the crash. There also exists a finite probability of attaining the end of the bubble without crash. Our model has specific predictions about the presence of certain critical log-periodic patterns in pre-crash prices, associated with the deterministic components of the bubble mechanism. We provide empirical evidence showing that these patterns were indeed present before the crashes of 1929, 1962 and 1987 on Wall Street and the 1997 crash on the Hong Kong Stock Exchange. These results are compared with statistical tests on synthetic data."
            },
            {
                "arxivId": "cond-mat/9510036",
                "title": "Stock Market Crashes, Precursors and Replicas",
                "abstract": "We present an analysis of the time behavior of the S&P 500 (Standard and Poors) New York stock exchange index before and after the October 1987 market crash and identify precursory patterns as well as aftershock signatures and characteristic oscillations of relaxation. Combined, they all suggest a picture of a kind of dynamical critical point, with characteristic log-periodic signatures, similar to what has been found recently for earthquakes. These observations are confirmed on other smaller crashes, and strengthen the view of the stockmarket as an example of a self-organizing cooperative system."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2309.05512",
        "category": "q-fin",
        "title": "Monte Carlo Simulation for Trading Under a L\u00e9vy-Driven Mean-Reverting Framework",
        "abstract": "We present a Monte Carlo approach to pairs trading on mean-reverting spreads modeled by L\\'evy-driven Ornstein-Uhlenbeck processes. Specifically, we focus on using a variance gamma driving process, an infinite activity pure jump process to allow for more flexible models of the price spread than is available in the classical model. However, this generalization comes at the cost of not having analytic formulas, so we apply Monte Carlo methods to determine optimal trading levels and develop a variance reduction technique using control variates. Within this framework, we numerically examine how the optimal trading strategies are affected by the parameters of the model. In addition, we extend our method to bivariate spreads modeled using a weak variance alpha-gamma driving process, and explore the effect of correlation on these trades.",
        "references": [
            {
                "arxivId": "2106.15452",
                "title": "The Variance Gamma++ Process and Applications to Energy Markets",
                "abstract": "The purpose of this article is to introduce a new L\u00b4evy process, termed Variance Gamma++ process, to model the dynamic of assets in illiquid markets. Such a process has the mathematical tractability of the Variance Gamma process and is obtained applying the self-decomposability of the gamma law. Compared to the Variance Gamma model, it has an additional parameter representing the measure of the trading activity. We give a full characterization of the Variance Gamma++ process in terms of its characteristic triplet, characteristic function and transition density. In addition, we provide e\ufb03cient path simulation algorithms, both forward and backward in time. We also obtain an e\ufb03cient \u201cintegral-free\u201d explicit pricing formula for European options. These results are instrumental to apply Fourier-based option pricing and maximum likelihood techniques for the parameter estimation. Finally, we apply our model to illiquid markets, namely to the calibration of European power future market data. We accordingly evaluate exotic derivatives using the Monte Carlo method and compare these values to those obtained using the Variance Gamma process and give an eco-nomic interpretation of the obtained results. Finally, we illustrate an extension to the multivariate framework."
            },
            {
                "arxivId": "2011.14542",
                "title": "Calibration for multivariate L\u00e9vy-driven Ornstein-Uhlenbeck processes with applications to weak subordination",
                "abstract": null
            },
            {
                "arxivId": "2004.06786",
                "title": "Exact Simulation of Variance Gamma-Related OU Processes: Application to the Pricing of Energy Derivatives",
                "abstract": "ABSTRACT In this study we use a three-step procedure that relates the self-decomposability of the stationary law of a generalized Ornstein-Uhlenbeck process to the transition law of such processes. Based on this procedure and the results of Qu, Dassios, and Zhao (2019), we derive the exact simulation, without numerical inversion, of the skeleton of a Variance Gamma and of a symmetric Variance Gamma driven Ornstein-Uhlenbeck process. Extensive numerical experiments are reported to demonstrate the accuracy and efficiency of our algorithms. These results are instrumental to simulate the spot price dynamics in energy markets and to price Asian options and gas storages by Monte Carlo simulations in a framework similar to the one discussed in Cummins, Kiely and Murphy (2017, 2018)."
            },
            {
                "arxivId": "1609.04481",
                "title": "Weak subordination of multivariate L\u00e9vy processes and variance generalised gamma convolutions",
                "abstract": "Subordinating a multivariate Levy process, the subordinate, with a univariate subordinator gives rise to a pathwise construction of a new Levy process, provided the subordinator and the subordinate are independent processes. The variance-gamma model in finance was generated accordingly from a Brownian motion and a gamma process. Alternatively, multivariate subordination can be used to create Levy processes, but this requires the subordinate to have independent components. In this paper, we show that there exists another operation acting on pairs $(T,X)$ of Levy processes which creates a Levy process $X\\odot T$. Here, $T$ is a subordinator, but $X$ is an arbitrary Levy process with possibly dependent components. We show that this method is an extension of both univariate and multivariate subordination and provide two applications. We illustrate our methods giving a weak formulation of the variance-$\\alpha$-gamma process that exhibits a wider range of dependence than using traditional subordination. Also, the variance generalised gamma convolution class of Levy processes formed by subordinating Brownian motion with Thorin subordinators is further extended using weak subordination."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2401.09718",
        "category": "q-fin",
        "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy",
        "abstract": "Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It also has the potential to help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity.",
        "references": [
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2402.00299",
        "category": "q-fin",
        "title": "Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction",
        "abstract": "Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.",
        "references": [
            {
                "arxivId": "2111.15367",
                "title": "A Review on Graph Neural Network Methods in Financial Applications",
                "abstract": "With multiple components and relations, financial data are often presented as graph data, since it could represent both the individual features and the complicated relations. Due to the complexity and volatility of the financial market, the graph constructed on the financial data is often heterogeneous or time-varying, which imposes challenges on modeling technology. Among the graph modeling technologies, graph neural network (GNN) models are able to handle the complex graph structure and achieve great performance and thus could be used to solve financial tasks. In this work, we provide a comprehensive review of GNN models in recent financial context. We first categorize the commonly-used financial graphs and summarize the feature processing step for each node. Then we summarize the GNN methodology for each graph type, application in each area, and propose some potential research areas."
            },
            {
                "arxivId": "2109.10119",
                "title": "mGNN: Generalizing the Graph Neural Networks to the Multilayer Case",
                "abstract": "Networks are a powerful tool to model complex systems, and the definition of many Graph Neural Networks (GNN), Deep Learning algorithms that can handle networks, has opened a new way to approach many real-world problems that would be hardly or even untractable. In this paper, we propose mGNN, a framework meant to generalize GNNs to the case of multi-layer networks, i.e., networks that can model multiple kinds of interactions and relations between nodes. Our approach is general (i.e., not task specific) and has the advantage of extending any type of GNN without any computational overhead. We test the framework into three different tasks (node and network classification, link prediction) to validate it."
            },
            {
                "arxivId": "2006.16904",
                "title": "Graph Clustering with Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. \nWe start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMoN is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results."
            },
            {
                "arxivId": "2005.12418",
                "title": "Evolution of Credit Risk Using a Personalized Pagerank Algorithm for Multilayer Networks",
                "abstract": "In this paper we present a novel algorithm to study the evolution of credit risk across complex multilayer networks. Pagerank-like algorithms allow for the propagation of an influence variable across single networks, and allow quantifying the risk single entities (nodes) are subject to given the connection they have to other nodes in the network. Multilayer networks, on the other hand, are networks where subset of nodes can be associated to a unique set (layer), and where edges connect elements either intra or inter networks. Our personalized PageRank algorithm for multilayer networks allows for quantifying how credit risk evolves across time and propagates through these networks. By using bipartite networks in each layer, we can quantify the risk of various components, not only the loans. We test our method in an agricultural lending dataset, and our results show how default risk is a challenging phenomenon that propagates and evolves through the network across time."
            },
            {
                "arxivId": "2005.07496",
                "title": "Foundations and Modeling of Dynamic Networks Using Dynamic Graph Neural Networks: A Survey",
                "abstract": "Dynamic networks are used in a wide range of fields, including social network analysis, recommender systems and epidemiology. Representing complex networks as structures changing over time allow network models to leverage not only structural but also temporal patterns. However, as dynamic network literature stems from diverse fields and makes use of inconsistent terminology, it is challenging to navigate. Meanwhile, graph neural networks (GNNs) have gained a lot of attention in recent years for their ability to perform well on a range of network science tasks, such as link prediction and node classification. Despite the popularity of graph neural networks and the proven benefits of dynamic network models, there has been little focus on graph neural networks for dynamic networks. To address the challenges resulting from the fact that this research crosses diverse fields as well as to survey dynamic graph neural networks, this work is split into two main parts. First, to address the ambiguity of the dynamic network terminology we establish a foundation of dynamic networks with consistent, detailed terminology and notation. Second, we present a comprehensive survey of dynamic graph neural network models using the proposed terminology."
            },
            {
                "arxivId": "1810.01405",
                "title": "GrAMME: Semisupervised Learning Using Multilayered Graph Attention Models",
                "abstract": "Modern data analysis pipelines are becoming increasingly complex due to the presence of multiview information sources. While graphs are effective in modeling complex relationships, in many scenarios, a single graph is rarely sufficient to succinctly represent all interactions, and hence, multilayered graphs have become popular. Though this leads to richer representations, extending solutions from the single-graph case is not straightforward. Consequently, there is a strong need for novel solutions to solve classical problems, such as node classification, in the multilayered case. In this article, we consider the problem of semisupervised learning with multilayered graphs. Though deep network embeddings, e.g., DeepWalk, are widely adopted for community discovery, we argue that feature learning with random node attributes, using graph neural networks, can be more effective. To this end, we propose to use attention models for effective feature learning and develop two novel architectures, GrAMME-SG and GrAMME-Fusion, that exploit the interlayer dependences for building multilayered graph embeddings. Using empirical studies on several benchmark data sets, we evaluate the proposed approaches and demonstrate significant performance improvements in comparison with the state-of-the-art network embedding strategies. The results also show that using simple random features is an effective choice, even in cases where explicit node attributes are not available."
            },
            {
                "arxivId": "1802.09691",
                "title": "Link Prediction Based on Graph Neural Networks",
                "abstract": "Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems."
            },
            {
                "arxivId": "1801.05407",
                "title": "Deep Canonically Correlated LSTMs",
                "abstract": "We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear transformations of variable length sequences and embed them into a correlated, fixed dimensional space. We use LSTMs to transform multi-view time-series data non-linearly while learning temporal relationships within the data. We then perform correlation analysis on the outputs of these neural networks to find a correlated subspace through which we get our final representation via projection. This work follows from previous work done on Deep Canonical Correlation (DCCA), in which deep feed-forward neural networks were used to learn nonlinear transformations of data while maximizing correlation."
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1704.06199",
                "title": "Dynamic Graph Convolutional Networks",
                "abstract": null
            },
            {
                "arxivId": "1703.03130",
                "title": "A Structured Self-attentive Sentence Embedding",
                "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks."
            },
            {
                "arxivId": "1601.06733",
                "title": "Long Short-Term Memory-Networks for Machine Reading",
                "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art."
            },
            {
                "arxivId": "1511.05298",
                "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs",
                "abstract": "Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatiotemporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks."
            },
            {
                "arxivId": "1502.06922",
                "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval",
                "abstract": "This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task."
            },
            {
                "arxivId": "1412.3555",
                "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM."
            },
            {
                "arxivId": "1411.4389",
                "title": "Long-term recurrent convolutional networks for visual recognition and description",
                "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \u201ctemporally deep\u201d, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
            },
            {
                "arxivId": "1409.1259",
                "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
                "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
            },
            {
                "arxivId": "1409.0473",
                "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
                "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2402.00445",
        "category": "q-fin",
        "title": "Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning",
        "abstract": "This paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model with a non-martingale asset price process having infinite active jumps. In our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai (2024). Moreover, the BNS model includes many variables, which makes the deep learning accuracy worse. Therefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically.",
        "references": [
            {
                "arxivId": "2306.05750",
                "title": "Monte Carlo simulation for Barndorff-Nielsen and Shephard model under change of measure",
                "abstract": null
            },
            {
                "arxivId": "1503.08589",
                "title": "Local risk-minimization for Barndorff-Nielsen and Shephard models",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2402.00543",
        "category": "q-fin",
        "title": "The extension of Pearson correlation coefficient, measuring noise, and selecting features",
        "abstract": "Not a matter of serious contention, Pearson's correlation coefficient is still the most important statistical association measure. Restricted to just two variables, this measure sometimes doesn't live up to users' needs and expectations. Specifically, a multivariable version of the correlation coefficient can greatly contribute to better assessment of the risk in a multi-asset investment portfolio. Needless to say, the correlation coefficient is derived from another concept: covariance. Even though covariance can be extended naturally by its mathematical formula, such an extension is to no use. Making matters worse, the correlation coefficient can never be extended based on its mathematical definition. In this article, we briefly explore random matrix theory to extend the notion of Pearson's correlation coefficient to an arbitrary number of variables. Then, we show that how useful this measure is at gauging noise, thereby selecting features particularly in classification.",
        "references": [
            {
                "arxivId": "2010.01768",
                "title": "Measuring Association on Topological Spaces Using Kernels and Geometric Graphs",
                "abstract": "In this paper we propose and study a class of simple, nonparametric, yet interpretable measures of association between two random variables $X$ and $Y$ taking values in general topological spaces. These nonparametric measures -- defined using the theory of reproducing kernel Hilbert spaces -- capture the strength of dependence between $X$ and $Y$ and have the property that they are 0 if and only if the variables are independent and 1 if and only if one variable is a measurable function of the other. Further, these population measures can be consistently estimated using the general framework of graph functionals which include $k$-nearest neighbor graphs and minimum spanning trees. Moreover, a sub-class of these estimators are also shown to adapt to the intrinsic dimensionality of the underlying distribution. Some of these empirical measures can also be computed in near linear time. Under the hypothesis of independence between $X$ and $Y$, these empirical measures (properly normalized) have a standard normal limiting distribution. Thus, these measures can also be readily used to test the hypothesis of mutual independence between $X$ and $Y$. In fact, as far as we are aware, these are the only procedures that possess all the above mentioned desirable properties. Furthermore, when restricting to Euclidean spaces, we can make these sample measures of association finite-sample distribution-free, under the hypothesis of independence, by using multivariate ranks defined via the theory of optimal transport. The recent correlation coefficient proposed in Dette et al. (2013), Chatterjee (2019), and Azadkia and Chatterjee (2019) can be seen as a special case of this general class of measures."
            },
            {
                "arxivId": "2008.11619",
                "title": "On the power of Chatterjee\u2019s rank correlation",
                "abstract": "\n Chatterjee (2021+) introduced a simple new rank correlation coefficient that has attracted much recent attention. The coefficient has the unusual appeal that it not only estimates a population quantity first proposed by Dette et al. (2013) that is zero if and only if the underlying pair of random variables is independent, but also is asymptotically normal under independence. This paper compares Chatterjee\u2019s new correlation coefficient to three established rank correlations that also facilitate consistent tests of independence, namely, Hoeffding\u2019s D, Blum\u2013Kiefer\u2013 Rosenblatt\u2019s R, and Bergsma\u2013Dassios\u2013Yanagimoto\u2019s \u03c4 *. We contrast their computational efficiency in light of recent advances, and investigate their power against local rotation and mixture alternatives. Our main results show that Chatterjee\u2019s coefficient is unfortunately rate sub-optimal compared to D, R, and \u03c4 *. The situation is more subtle for a related earlier estimator of Dette et al. (2013). These results favor D, R, and \u03c4 * over Chatterjee\u2019s new correlation coefficient for the purpose of testing independence."
            },
            {
                "arxivId": "2008.10177",
                "title": "Correlations with tailored extremal properties",
                "abstract": "Recently, Chatterjee has introduced a new coefficient of correlation which has several natural properties. In particular, the coefficient attains its maximal value if and only if one variable is a measurable function of the other variable. In this paper, we seek to define correlations which have a similar property, except now the measurable function must belong to a pre-specified class, which amounts to a shape restriction on the function. We will then look specifically at the correlation corresponding to the class of monotone nondecreasing functions, in which case we can prove various asymptotic results, as well as perform local power calculations."
            },
            {
                "arxivId": "1910.12327",
                "title": "A simple measure of conditional dependence",
                "abstract": "We propose a coefficient of conditional dependence between two random variables $Y$ and $Z$ given a set of other variables $X_1,\\ldots,X_p$, based on an i.i.d. sample. The coefficient has a long list of desirable properties, the most important of which is that under absolutely no distributional assumptions, it converges to a limit in $[0,1]$, where the limit is $0$ if and only if $Y$ and $Z$ are conditionally independent given $X_1,\\ldots,X_p$, and is $1$ if and only if $Y$ is equal to a measurable function of $Z$ given $X_1,\\ldots,X_p$. Using this statistic, we devise a new variable selection algorithm, called Feature Ordering by Conditional Independence (FOCI), which is model-free, has no tuning parameters, and is provably consistent under sparsity assumptions. A number of applications to synthetic and real datasets are worked out."
            },
            {
                "arxivId": "1909.10140",
                "title": "A New Coefficient of Correlation",
                "abstract": "Abstract\u2013Is it possible to define a coefficient of correlation which is (a) as simple as the classical coefficients like Pearson\u2019s correlation or Spearman\u2019s correlation, and yet (b) consistently estimates some simple and interpretable measure of the degree of dependence between the variables, which is 0 if and only if the variables are independent and 1 if and only if one is a measurable function of the other, and (c) has a simple asymptotic theory under the hypothesis of independence, like the classical coefficients? This article answers this question in the affirmative, by producing such a coefficient. No assumptions are needed on the distributions of the variables. There are several coefficients in the literature that converge to 0 if and only if the variables are independent, but none that satisfy any of the other properties mentioned above. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1010.3575",
                "title": "Introducing the discussion paper by Sz\\'{e}kely and Rizzo",
                "abstract": "I recall a great sense of excitement in the seminar room in Madison after Professor Sz\u00e9kely presented the astonishing findings about distance covariance (in the spring of 2008). It was one of the best statistics seminars I could remember. Since before computers, statisticians have held up Pearson\u2019s correlation coefficient as the most essential measure of association between quantitative variables. R. A. Fisher\u2019s reputation was sealed, in part, by solving the distribution of this statistic, and so much of linear-model methodology relates to it. And all the time we\u2019ve had to add the caveat about independence following zero correlation only if the data are jointly normal. Spearman\u2019s rank correlation has substantial practical utility in cases where normality is unreliable, but the goal to have a bona fide dependence measure seemed to have been beyond the scope of ordinary applied statistics. Some valid measures did exist, but being driven by empirical characteristic functions, they were too complicated to enter the toolkit of the applied statistician. Distance covariance not only provides a bona fide dependence measure, but it does so with a simplicity to satisfy Don Geman\u2019s elevator test (i.e., a method must be sufficiently simple that it can be explained to a colleague in the time it takes to go between floors on an elevator!). You take all pairwise distances between sample values of one variable, and do the same for the second variable. Then center the resulting distance matrices (so each has column and row means equal to zero) and average the entries of the matrix which holds componentwise products of the two centered distance matrices. That\u2019s the squared distance covariance between the two variables. The population quantity equals zero if and only if the variables are independent, whatever be the underlying distributions and whatever be the dimension of the two variables. The depth of the finding, the simplicity of the statistic, and the central role of statistical dependence make this an important story for our discipline."
            },
            {
                "arxivId": "0806.4441",
                "title": "Probability and Statistics: Essays in Honor of David A. Freedman",
                "abstract": "This volume is our tribute to David A. Freedman, whom we regard as one of the great statisticians of our time. He received his B.Sc. degree from McGill University and his Ph.D. from Princeton, and joined the Department of Statistics of the University of California, Berkeley, in 1962, where, apart from sabbaticals, he has been ever since. In a career of over 45 years, David has made many fine contributions to probability and statistical theory, and to the application of statistics. His early research was on Markov chains and martingales, and two topics with which he has had a lifelong fascination: exchangeability and De Finetti's theorem, and the consistency of Bayes estimates. His asymptotic theory for the bootstrap was also highly influential. David was elected to the American Academy of Arts and Sciences in 1991, and in 2003 he received the John J. Carty Award for the Advancement of Science from the U.S. National Academy of Sciences. In addition to his purely academic research, David has extensive experience as a consultant, including working for the Carnegie Commission, the City of San Francisco, and the Federal Reserve, as well as several Departments of the U.S. Government--Energy, Treasury, Justice, and Commerce. He has testified as an expert witness on statistics in a number of law cases, including Piva v. Xerox (employment discrimination), Garza v. County of Los Angeles (voting rights), and New York v. Department of Commerce (census adjustment). Lastly, he is an exceptionally good writer and teacher, and his many books and review articles are arguably his most important contribution to our subject."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2402.00787",
        "category": "q-fin",
        "title": "Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning",
        "abstract": "Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.",
        "references": [
            {
                "arxivId": "2306.04778",
                "title": "How to Evaluate Behavioral Models",
                "abstract": "Researchers building behavioral models, such as behavioral game theorists, use experimental data to evaluate predictive models of human behavior. However, there is little agreement about which loss function should be used in evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and squared L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions should be used for this task, formalizing axioms that we argue loss functions should satisfy. We construct a family of loss functions, which we dub ``diagonal bounded Bregman divergences'', that satisfy all of these axioms. These rule out many loss functions used in practice, but notably include squared L2 error; we thus recommend its use for evaluating behavioral models."
            },
            {
                "arxivId": "2210.08569",
                "title": "Limited or Biased: Modeling Sub-Rational Human Investors in Financial Markets",
                "abstract": "Human decision-making in real-life deviates significantly from the optimal decisions made by fully rational agents, primarily due to computational limitations or psychological biases. While existing studies in behavioral finance have discovered various aspects of human sub-rationality, there lacks a comprehensive framework to transfer these findings into an adaptive human model applicable across diverse financial market scenarios. In this study, we introduce a flexible model that incorporates five different aspects of human sub-rationality using reinforcement learning. Our model is trained using a high-fidelity multi-agent market simulator, which overcomes limitations associated with the scarcity of labeled data of individual investors. We evaluate the behavior of sub-rational human investors using hand-crafted market scenarios and SHAP value analysis, showing that our model accurately reproduces the observations in the previous studies and reveals insights of the driving factors of human behavior. Finally, we explore the impact of sub-rationality on the investor's Profit and Loss (PnL) and market quality. Our experiments reveal that bounded-rational and prospect-biased human behaviors improve liquidity but diminish price efficiency, whereas human behavior influenced by myopia, optimism, and pessimism reduces market liquidity."
            },
            {
                "arxivId": "2210.07184",
                "title": "Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations",
                "abstract": "We study a game between liquidity provider (LP) and liquidity taker agents interacting in an over\u2010the\u2010counter market, for which the typical example is foreign exchange. We show how a suitable design of parameterized families of reward functions coupled with shared policy learning constitutes an efficient solution to this problem. By playing against each other, our deep\u2010reinforcement\u2010learning\u2010driven agents learn emergent behaviors relative to a wide spectrum of objectives encompassing profit\u2010and\u2010loss, optimal execution, and market share. In particular, we find that LPs naturally learn to balance hedging and skewing, where skewing refers to setting their buy and sell prices asymmetrically as a function of their inventory. We further introduce a novel RL\u2010based calibration algorithm, which we found performed well at imposing constraints on the game equilibrium. On the theoretical side, we are able to show convergence rates for our multi\u2010agent policy gradient algorithm under a transitivity assumption, closely related to generalized ordinal potential\u00a0games."
            },
            {
                "arxivId": "2206.08781",
                "title": "Reinforcement Learning for Economic Policy: A New Frontier?",
                "abstract": "Agent-based computational economics is a field with a rich academic history, yet one which has struggled to enter mainstream policy design toolboxes, plagued by the challenges associated with representing a complex and dynamic reality. The field of Reinforcement Learning (RL), too, has a rich history, and has recently been at the centre of several exponential developments. Modern RL implementations have been able to achieve unprecedented levels of sophistication, handling previously unthinkable degrees of complexity. This review surveys the historical barriers of classical agent-based techniques in economic modelling, and contemplates whether recent developments in RL can overcome any of them."
            },
            {
                "arxivId": "2206.05568",
                "title": "Bounded strategic reasoning explains crisis emergence in multi-agent market games",
                "abstract": "The efficient market hypothesis (EMH), based on rational expectations and market equilibrium, is the dominant perspective for modelling economic markets. However, the most notable critique of the EMH is the inability to model periods of out-of-equilibrium dynamics without significant external news. When such dynamics emerge endogenously, the traditional economic frameworks prove insufficient. This work offers an alternate perspective explaining the endogenous emergence of punctuated out-of-equilibrium dynamics based on bounded rational agents. In a concise market entrance game, we show how boundedly rational strategic reasoning can lead to endogenously emerging crises, exhibiting fat tails in returns. We also show how other common stylized facts, such as clustered volatility, arise due to agent diversity (or lack thereof) and the varying learning updates across the agents. This work explains various stylized facts and crisis emergence in economic markets, in the absence of any external news, based on agent interactions and bounded rational reasoning."
            },
            {
                "arxivId": "2102.09180",
                "title": "A Maximum Entropy Model of Bounded Rational Decision-Making with Prior Beliefs and Market Feedback",
                "abstract": "Bounded rationality is an important consideration stemming from the fact that agents often have limits on their processing abilities, making the assumption of perfect rationality inapplicable to many real tasks. We propose an information-theoretic approach to the inference of agent decisions under Smithian competition. The model explicitly captures the boundedness of agents (limited in their information-processing capacity) as the cost of information acquisition for expanding their prior beliefs. The expansion is measured as the Kullblack\u2013Leibler divergence between posterior decisions and prior beliefs. When information acquisition is free, the homo economicus agent is recovered, while in cases when information acquisition becomes costly, agents instead revert to their prior beliefs. The maximum entropy principle is used to infer least biased decisions based upon the notion of Smithian competition formalised within the Quantal Response Statistical Equilibrium framework. The incorporation of prior beliefs into such a framework allowed us to systematically explore the effects of prior beliefs on decision-making in the presence of market feedback, as well as importantly adding a temporal interpretation to the framework. We verified the proposed model using Australian housing market data, showing how the incorporation of prior knowledge alters the resulting agent decisions. Specifically, it allowed for the separation of past beliefs and utility maximisation behaviour of the agent as well as the analysis into the evolution of agent beliefs."
            },
            {
                "arxivId": "2006.13085",
                "title": "Calibration of Shared Equilibria in General Sum Partially Observable Markov Games",
                "abstract": "Training multi-agent systems (MAS) to achieve realistic equilibria gives us a useful tool to understand and model real-world systems. We consider a general sum partially observable Markov game where agents of different types share a single policy network, conditioned on agent-specific information. This paper aims at i) formally understanding equilibria reached by such agents, and ii) matching emergent phenomena of such equilibria to real-world targets. Parameter sharing with decentralized execution has been introduced as an efficient way to train multiple agents using a single policy network. However, the nature of resulting equilibria reached by such agents is not yet understood: we introduce the novel concept of \\textit{Shared equilibrium} as a symmetric pure Nash equilibrium of a certain Functional Form Game (FFG) and prove convergence to the latter for a certain class of games using self-play. In addition, it is important that such equilibria satisfy certain constraints so that MAS are \\textit{calibrated} to real world data for practical use: we solve this problem by introducing a novel dual-Reinforcement Learning based approach that fits emergent behaviors of agents in a Shared equilibrium to externally-specified targets, and apply our methods to a $n$-player market example. We do so by calibrating parameters governing distributions of agent types rather than individual agents, which allows both behavior differentiation among agents and coherent scaling of the shared policy network to multiple agents."
            },
            {
                "arxivId": "1912.04941",
                "title": "Get real: realism metrics for robust limit order book market simulations",
                "abstract": "Market simulation is an increasingly important method for evaluating and training trading strategies and testing \"what if\" scenarios. The extent to which results from these simulations can be trusted depends on how realistic the environment is for the strategies being tested. As a step towards providing benchmarks for realistic simulated markets, we enumerate measurable stylized facts of limit order book (LOB) markets across multiple asset classes from the literature. We apply these metrics to data from real markets and compare the results to data originating from simulated markets. We illustrate their use in five different simulated market configurations: The first (market replay) is frequently used in practice to evaluate trading strategies; the other four are interactive agent based simulation (IABS) configurations which combine zero intelligence agents, and agents with limited strategic behavior. These simulated agents rely on an internal \"oracle\" that provides a fundamental value for the asset. In traditional IABS methods the fundamental originates from a mean reverting random walk. We show that markets exhibit more realistic behavior when the fundamental arises from historical market data. We further experimentally illustrate the effectiveness of IABS techniques as opposed to market replay."
            },
            {
                "arxivId": "1910.01913",
                "title": "If MaxEnt RL is the Answer, What is the Question?",
                "abstract": "Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods."
            },
            {
                "arxivId": "1909.07528",
                "title": "Emergent Tool Use From Multi-Agent Autocurricula",
                "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests."
            },
            {
                "arxivId": "1906.00729",
                "title": "Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games",
                "abstract": "We study the global convergence of policy optimization for finding the Nash equilibria (NE) in zero-sum linear quadratic (LQ) games. To this end, we first investigate the landscape of LQ games, viewing it as a nonconvex-nonconcave saddle-point problem in the policy space. Specifically, we show that despite its nonconvexity and nonconcavity, zero-sum LQ games have the property that the stationary point of the objective function with respect to the linear feedback control policies constitutes the NE of the game. Building upon this, we develop three projected nested-gradient methods that are guaranteed to converge to the NE of the game. Moreover, we show that all of these algorithms enjoy both globally sublinear and locally linear convergence rates. Simulation results are also provided to illustrate the satisfactory convergence properties of the algorithms. To the best of our knowledge, this work appears to be the first one to investigate the optimization landscape of LQ games, and provably show the convergence of policy optimization methods to the Nash equilibria. Our work serves as an initial step toward understanding the theoretical aspects of policy-based reinforcement learning algorithms for zero-sum Markov games in general."
            },
            {
                "arxivId": "1902.06865",
                "title": "Hyperbolic Discounting and Learning over Multiple Horizons",
                "abstract": "Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process. The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences. In this work we revisit the fundamentals of discounting in RL and bridge this disconnect by implementing an RL agent that acts via hyperbolic discounting. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL. Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over a strong value-based RL agent, Rainbow."
            },
            {
                "arxivId": "1901.10500",
                "title": "Discretizing Continuous Action Space for On-Policy Optimization",
                "abstract": "In this work, we show that discretizing action space for continuous control is a simple yet powerful technique for on-policy optimization. The explosion in the number of discrete actions can be efficiently addressed by a policy with factorized distribution across action dimensions. We show that the discrete policy achieves significant performance gains with state-of-the-art on-policy optimization algorithms (PPO, TRPO, ACKTR) especially on high-dimensional tasks with complex dynamics. Additionally, we show that an ordinal parameterization of the discrete distribution can introduce the inductive bias that encodes the natural ordering between discrete actions. This ordinal architecture further significantly improves the performance of PPO/TRPO."
            },
            {
                "arxivId": "1901.09216",
                "title": "Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning",
                "abstract": "Though limited in real-world decision making, most multi-agent reinforcement learning (MARL) models assume perfectly rational agents -- a property hardly met due to individual's cognitive limitation and/or the tractability of the decision problem. In this paper, we introduce generalized recursive reasoning (GR2) as a novel framework to model agents with different \\emph{hierarchical} levels of rationality; our framework enables agents to exhibit varying levels of ``thinking'' ability thereby allowing higher-level agents to best respond to various less sophisticated learners. We contribute both theoretically and empirically. On the theory side, we devise the hierarchical framework of GR2 through probabilistic graphical models and prove the existence of a perfect Bayesian equilibrium. Within the GR2, we propose a practical actor-critic solver, and demonstrate its convergent property to a stationary point in two-player games through Lyapunov analysis. On the empirical side, we validate our findings on a variety of MARL benchmarks. Precisely, we first illustrate the hierarchical thinking process on the Keynes Beauty Contest, and then demonstrate significant improvements compared to state-of-the-art opponent modeling baselines on the normal-form games and the cooperative navigation benchmark."
            },
            {
                "arxivId": "1801.01290",
                "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
            },
            {
                "arxivId": "0810.5306",
                "title": "Economics needs a scientific revolution",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-02.json",
        "arxivId": "2402.00855",
        "category": "q-fin",
        "title": "'Egalitarian pooling and sharing of longevity risk', a.k.a. 'The many ways to skin a tontine cat'",
        "abstract": "There is little disagreement among insurance actuaries and financial economists about the societal benefits of longevity-risk pooling in the form of life annuities, defined benefit pensions, self-annuitization funds, and even tontine schemes. Indeed, the discounted value or cost of providing an income for life is lower -- in other words, the amount of upfront capital required to generate a similar income stream with the same level of statistical safety is lower -- when participants pool their financial resources versus going it alone. Moreover, when participants' financial circumstances and lifespans are homogenous, there is consensus on how to share the\"winnings\"among survivors, namely by distributing them equally among survivors, a.k.a. a uniform rule. Alas, what is lesser-known and much more problematic is allocating the winnings in such a pool when participants differ in wealth (contributions) and health (longevity), especially when the pools are relatively small in size. The same problems arise when viewed from the dual perspective of decentralized risk sharing (DRS). The positive correlation between health and income and the fact that wealthier participants are likely to live longer is a growing concern among pension and retirement policymakers. With that motivation in mind, this paper offers a modelling framework for distributing longevity-risk pools' income and benefits (or tontine winnings) when participants are heterogeneous. Similar to the nascent literature on decentralized risk sharing, there are several equally plausible arrangements for sharing benefits (a.k.a.\"skinning the cat\") among survivors. Moreover, the selected rule depends on the extent of social cohesion within the longevity risk pool, ranging from solidarity and altruism to pure individualism. In sum, actuarial science cannot really offer or guarantee uniqueness, only a methodology.",
        "references": [
            {
                "arxivId": "2401.03328",
                "title": "Negatively dependent optimal risk sharing",
                "abstract": "We analyze the problem of optimally sharing risk using allocations that exhibit counter-monotonicity, the most extreme form of negative dependence. Counter-monotonic allocations take the form of either\"winner-takes-all\"lotteries or\"loser-loses-all\"lotteries, and we respectively refer to these (normalized) cases as jackpot or scapegoat allocations. Our main theorem, the counter-monotonic improvement theorem, states that for a given set of random variables that are either all bounded from below or all bounded from above, one can always find a set of counter-monotonic random variables such that each component is greater or equal than its counterpart in the convex order. We show that Pareto optimal allocations, if they exist, must be jackpot allocations when all agents are risk seeking. We essentially obtain the opposite when all agents have discontinuous Bernoulli utility functions, as scapegoat allocations maximize the probability of being above the discontinuity threshold. We also consider the case of rank-dependent expected utility (RDU) agents and find conditions which guarantee that RDU agents prefer jackpot allocations. We provide an application for the mining of cryptocurrencies and show that in contrast to risk-averse miners, RDU miners with small computing power never join a mining pool. Finally, we characterize the competitive equilibria with risk-seeking agents, providing a first and second fundamental theorem of welfare economics where all equilibrium allocations are jackpot allocations."
            },
            {
                "arxivId": "2208.07533",
                "title": "An axiomatic theory for anonymized risk sharing",
                "abstract": "We study an axiomatic framework for anonymized risk sharing. In contrast to traditional risk sharing settings, our framework requires no information on preferences, identities, private operations and realized losses from the individual agents, and thereby it is useful for modeling risk sharing in decentralized systems. Four axioms natural in such a framework -- actuarial fairness, risk fairness, risk anonymity, and operational anonymity -- are put forward and discussed. We establish the remarkable fact that the four axioms characterizes the conditional mean risk sharing rule, revealing the unique and prominent role of this popular risk sharing rule among all others in relevant applications of anonymized risk sharing. Several other properties and their relations to the four axioms are studied, as well as their implications in rationalizing the design of some sharing mechanisms in practice."
            },
            {
                "arxivId": "1903.05990",
                "title": "Modern tontine with bequest: Innovation in pooled annuity products",
                "abstract": null
            },
            {
                "arxivId": "1610.10078",
                "title": "Optimal retirement income tontines",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2104.12975",
        "category": "q-fin",
        "title": "An Empirical Assessment of Characteristics and Optimal Portfolios",
        "abstract": "\n We implement a dynamically regularized, bootstrapped two-stage out-of-sample parametric portfolio policy to evaluate characteristics\u2019 efficacy in the conditional stock return-generating process in the metric of expected power utility. Traditional characteristics, such as momentum and size afforded large utility gains before 1999. These opportunities have since vanished. Overfitting\u2014imprecision in weight estimation\u2014is correlated with the optimal portfolio\u2019s variance. Therefore, it is not a problem for power utility investors with coefficients of relative aversion greater than four. For more risk-tolerant investors, we successfully reduce estimation error by increasing the curvature of the loss function relative to the investor\u2019s utility function. (JEL L200; C110; C350)",
        "references": [
            {
                "arxivId": "1401.0212",
                "title": "Data-driven robust optimization",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2308.10974",
        "category": "q-fin",
        "title": "\"Guinea Pig Trials\" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion",
        "abstract": "Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate that, in the absence of communication, smart agents consistently reach tacit collusion, leading to prices converging at levels higher than the Bertrand equilibrium price but lower than monopoly or cartel prices. When communication is allowed, smart agents achieve a higher-level collusion with prices close to cartel prices. Collusion forms more quickly with communication, while price convergence is smoother without it. These results indicate that communication enhances trust between firms, encouraging frequent small price deviations to explore opportunities for a higher-level win-win situation and reducing the likelihood of triggering a price war. We also assigned different personas to firms to analyze behavioral differences and tested variant models under diverse market structures. The findings showcase the effectiveness and robustness of SABM and provide intriguing insights into competition and collusion.",
        "references": [
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2303.12712",
                "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
                "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions."
            },
            {
                "arxivId": "2206.14576",
                "title": "Using cognitive psychology to understand GPT-3",
                "abstract": "Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations."
            },
            {
                "arxivId": "2005.14165",
                "title": "Language Models are Few-Shot Learners",
                "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2308.12856",
        "category": "q-fin",
        "title": "Uncertainty Propagation and Dynamic Robust Risk Measures",
        "abstract": "We introduce a framework for quantifying propagation of uncertainty arising in a dynamic setting. Specifically, we define dynamic uncertainty sets designed explicitly for discrete stochastic processes over a finite time horizon. These dynamic uncertainty sets capture the uncertainty surrounding stochastic processes and models, accounting for factors such as distributional ambiguity. Examples of uncertainty sets include those induced by the Wasserstein distance and f-divergences. We further define dynamic robust risk measures as the supremum of all candidates' risks within the uncertainty set. In an axiomatic way, we discuss conditions on the uncertainty sets that lead to well-known properties of dynamic robust risk measures, such as convexity and coherence. Furthermore, we discuss the necessary and sufficient properties of dynamic uncertainty sets that lead to time-consistencies of dynamic robust risk measures. We find that uncertainty sets stemming from f-divergences lead to strong time-consistency while the Wasserstein distance results in a new time-consistent notion of weak recursiveness. Moreover, we show that a dynamic robust risk measure is strong time-consistent or weak recursive if and only if it admits a recursive representation of one-step conditional robust risk measures arising from static uncertainty sets.",
        "references": [
            {
                "arxivId": "2307.03447",
                "title": "Dynamic Return and Star-Shaped Risk Measures via BSDEs",
                "abstract": "This paper establishes characterization results for dynamic return and star-shaped risk measures induced via backward stochastic differential equations (BSDEs). We first characterize a general family of static star-shaped functionals in a locally convex Fr\\'echet lattice. Next, employing the Pasch-Hausdorff envelope, we build a suitable family of convex drivers of BSDEs inducing a corresponding family of dynamic convex risk measures of which the dynamic return and star-shaped risk measures emerge as the essential minimum. Furthermore, we prove that if the set of star-shaped supersolutions of a BSDE is not empty, then there exists, for each terminal condition, at least one convex BSDE with a non-empty set of supersolutions, yielding the minimal star-shaped supersolution. We illustrate our theoretical results in a few examples and demonstrate their usefulness in two applications, to capital allocation and portfolio choice."
            },
            {
                "arxivId": "2301.04971",
                "title": "Fully-dynamic risk measures: horizon risk, time-consistency, and relations with BSDEs and BSVIEs",
                "abstract": "In a dynamic framework, we identify a new concept associated with the risk of assessing the financial exposure by a measure that is not adequate to the actual time horizon of the position. This will be called horizon risk. We clarify that dynamic risk measures are subject to horizon risk, so we propose to use the fully-dynamic version. To quantify horizon risk, we introduce h-longevity as an indicator. We investigate these notions together with other properties of risk measures as normalization, restriction property, and different formulations of time-consistency. We also consider these concepts for fully-dynamic risk measures generated by backward stochastic differential equations (BSDEs), backward stochastic Volterra integral equations (BSVIEs), and families of these. Within this study, we provide new results for BSVIEs such as a converse comparison theorem and the dual representation of the associated risk measures."
            },
            {
                "arxivId": "1603.09030",
                "title": "A survey of time consistency of dynamic risk measures and dynamic performance measures in discrete time: LM-measure perspective",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2310.06000",
        "category": "q-fin",
        "title": "Towards Replication-Robust Analytics Markets",
        "abstract": "Many industries rely on data-driven analytics, yet useful datasets are often distributed amongst market competitors that are reluctant to collaborate and share information. Recent literature proposes analytics markets to provide monetary incentives for data sharing, however many of these market designs are vulnerable to malicious forms of replication -- whereby agents replicate their data and act under multiple identities to increase revenue. We develop a replication-robust analytics market, centering on supervised learning for regression. To allocate revenue, we use a Shapley value-based attribution policy, framing the features of agents as players and their interactions as a characteristic function game. We show that there are different ways to describe such a game, each with causal nuances that affect robustness to replication. Our proposal is validated using a real-world wind power forecasting case study.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2311.03638",
        "category": "q-fin",
        "title": "Bubble economics",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2312.13884",
        "category": "q-fin",
        "title": "Measures of Resilience to Cyber Contagion - An Axiomatic Approach for Complex Systems",
        "abstract": "We introduce a novel class of risk measures designed for the management of systemic risk in networks. In contrast to prevailing approaches, these risk measures target the topological configuration of the network in order to mitigate the propagation risk of contagious threats. While our discussion primarily revolves around the management of systemic cyber risks in digital networks, we concurrently draw parallels to risk management of other complex systems where analogous approaches may be adequate.",
        "references": [
            {
                "arxivId": "2311.15701",
                "title": "Cyber risk modeling using a two-phase Hawkes process with external excitation",
                "abstract": "With the growing digital transformation of the worldwide economy, cyber risk has become a major issue. As 1 % of the world's GDP (around $1,000 billion) is allegedly lost to cybercrime every year, IT systems continue to get increasingly interconnected, making them vulnerable to accumulation phenomena that undermine the pooling mechanism of insurance. As highlighted in the literature, Hawkes processes appear to be suitable models to capture contagion phenomena and clustering features of cyber events. This paper extends the standard Hawkes modeling of cyber risk frequency by adding external shocks, modelled by the publication of cyber vulnerabilities that are deemed to increase the likelihood of attacks in the short term. The aim of the proposed model is to provide a better quantification of contagion effects since, while the standard Hawkes model allocates all the clustering phenomena to self-excitation, our model allows to capture the external common factors that may explain part of the systemic pattern. We propose a Hawkes model with two kernels, one for the endogenous factor (the contagion from other cyber events) and one for the exogenous component (cyber vulnerability publications). We use parametric exponential specifications for both the internal and exogenous intensity kernels, and we compare different methods to tackle the inference problem based on public datasets containing features of cyber attacks found in the Hackmageddon database and cyber vulnerabilities from the Known Exploited Vulnerability database and the National Vulnerability Dataset. By refining the external excitation database selection, the degree of endogeneity of the model is nearly halved. We illustrate our model with simulations and discuss the impact of taking into account the external factor driven by vulnerabilities. Once an attack has occurred, response measures are implemented to limit the effects of an attack. These measures include patching vulnerabilities and reducing the attack's contagion. We use an augmented version of the model by adding a second phase modeling a reduction in the contagion pattern from the remediation measures. Based on this model, we explore various scenarios and quantify the effect of mitigation measures of an insurance company that aims to mitigate the effects of a cyber pandemic in its insured portfolio."
            },
            {
                "arxivId": "2303.12939",
                "title": "Managing cyber risk, a science in the making",
                "abstract": "Not a day goes by without news about a cyber attack. Fear spreads out and lots of wrong ideas circulate. This survey aims at showing how all these uncertainties about cyber can be transformed into manageable risk. After reviewing the main characteristics of cyber risk, we consider the three layers of cyber space: hardware, software and psycho-cognitive layer. We ask ourselves how is this risk different from others, how modelling has been tackled and needs to evolve, and what are the multi-facetted aspects of cyber risk management. This wide exploration pictures a science in the making and points out the questions to be solved for building a resilient society."
            },
            {
                "arxivId": "2211.04762",
                "title": "Building Resilience in Cybersecurity - An Artificial Lab Approach",
                "abstract": "Based on classical contagion models we introduce an artificial cyber lab: the digital twin of a complex cyber system in which possible cyber resilience measures may be implemented and tested. Using the lab, in numerical case studies, we identify two classes of measures to control systemic cyber risks: security- and topology-based interventions. We discuss the implications of our findings on selected real-world cybersecurity measures currently applied in the insurance and regulation practice or under discussion for future cyber risk control. To this end, we provide a brief overview of the current cybersecurity regulation and emphasize the role of insurance companies as private regulators. Moreover, from an insurance point of view, we provide first attempts to design systemic cyber risk obligations and to measure the systemic risk contribution of individual policyholders."
            },
            {
                "arxivId": "2209.02845",
                "title": "Building up Cyber Resilience by Better Grasping Cyber Risk Via a New Algorithm for Modelling Heavy-Tailed Data",
                "abstract": "Cyber security and resilience are major challenges in our modern economies; this is why they are top priorities on the agenda of governments, security and defense forces, management of companies and organizations. Hence, the need of a deep understanding of cyber risks to improve resilience. We propose here an analysis of the database of the cyber complaints filed at the {\\it Gendarmerie Nationale}. We perform this analysis with a new algorithm developed for non-negative asymmetric heavy-tailed data, which could become a handy tool in applied fields. This method gives a good estimation of the full distribution including the tail. Our study confirms the finiteness of the loss expectation, necessary condition for insurability. Finally, we draw the consequences of this model for risk management, compare its results to other standard EVT models, and lay the ground for a classification of attacks based on the fatness of the tail."
            },
            {
                "arxivId": "2209.07415",
                "title": "Modeling and Pricing Cyber Insurance -- Idiosyncratic, Systematic, and Systemic Risks",
                "abstract": "The paper provides a comprehensive overview of modeling and pricing cyber insurance and includes clear and easily understandable explanations of the underlying mathematical concepts. We distinguish three main types of cyber risks: idiosyncratic, systematic, and systemic cyber risks. While for idiosyncratic and systematic cyber risks, classical actuarial and \ufb01nancial mathematics appear to be well-suited, systemic cyber risks require more sophisticated approaches that capture both network and strategic interactions. In the context of pricing cyber insurance policies, issues of interdependence arise for both systematic and systemic cyber risks; classical actuarial valuation needs to be extended to include more complex methods, such as concepts of risk-neutral valuation and (set-valued) monetary risk measures."
            },
            {
                "arxivId": "2206.13594",
                "title": "Cyber Network Resilience against Self-Propagating Malware Attacks",
                "abstract": null
            },
            {
                "arxivId": "2204.04117",
                "title": "Finding shortest and nearly shortest path nodes in large substantially incomplete networks by hyperbolic mapping",
                "abstract": null
            },
            {
                "arxivId": "2110.10792",
                "title": "A Framework for Measures of Risk under Uncertainty",
                "abstract": "A risk analyst assesses potential financial losses based on multiple sources of information. Often, the assessment does not only depend on the specification of the loss random variable but also various economic scenarios. Motivated by this observation, we design a unified axiomatic framework for risk evaluation principles which quantifies jointly a loss random variable and a set of plausible probabilities. We call such an evaluation principle a generalized risk measure. We present a series of relevant theoretical results. The worst-case, coherent, and robust generalized risk measures are characterized via different sets of intuitive axioms. We establish the equivalence between a few natural forms of law invariance in our framework, and the technical subtlety therein reveals a sharp contrast between our framework and the traditional one. Moreover, coherence and strong law invariance are derived from a combination of other conditions, which provides additional support for coherent risk measures such as Expected Shortfall over Value-at-Risk, a relevant issue for risk management practice."
            },
            {
                "arxivId": "2104.01579",
                "title": "An expansion formula for Hawkes processes and application to cyber-insurance derivatives",
                "abstract": null
            },
            {
                "arxivId": "2012.12702",
                "title": "Systemic Risk in Financial Networks: A Survey",
                "abstract": "We provide an overview of the relationship between financial networks and systemic risk. We present a taxonomy of different types of systemic risk, differentiating between direct externalities between financial organizations (e.g., defaults, correlated portfolios, fire sales), and perceptions and feedback effects (e.g., bank runs, credit freezes). We also discuss optimal regulation and bailouts, measurements of systemic risk and financial centrality, choices by banks regarding their portfolios and partnerships, and the changing nature of financial networks."
            },
            {
                "arxivId": "1609.07903",
                "title": "Strongly consistent multivariate conditional risk measures",
                "abstract": null
            },
            {
                "arxivId": "1609.07897",
                "title": "Risk-consistent conditional systemic risk measures",
                "abstract": null
            },
            {
                "arxivId": "1503.06354",
                "title": "A unified approach to systemic risk measures via acceptance sets",
                "abstract": "We specify a general methodological framework for systemic risk measures via multidimensional acceptance sets and aggregation functions. Existing systemic risk measures can usually be interpreted as the minimal amount of cash needed to secure the system after aggregating individual risks. In contrast, our approach also includes systemic risk measures that can be interpreted as the minimal amount of cash that secures the aggregated system by allocating capital to the single institutions before aggregating the individual risks. An important feature of our approach is the possibility of allocating cash according to the future state of the system (scenario\u2010dependent allocation). We also provide conditions that ensure monotonicity, convexity, or quasi\u2010convexity of our systemic risk measures."
            },
            {
                "arxivId": "1502.07961",
                "title": "Measures of Systemic Risk",
                "abstract": "Systemic risk refers to the risk that the financial system is susceptible to failures due to the characteristics of the system itself. The tremendous cost of systemic risk requires the design and implementation of tools for the efficient macroprudential regulation of financial institutions. The current paper proposes a novel approach to measuring systemic risk. Key to our construction is a rigorous derivation of systemic risk measures from the structure of the underlying system and the objectives of a financial regulator. The suggested systemic risk measures express systemic risk in terms of capital endowments of the financial firms. Their definition requires two ingredients: A cash flow or value model that assigns to the capital allocations of the entities in the system a relevant stochastic outcome; and an acceptability criterion, i.e., a set of random outcomes that are acceptable to a regulatory authority. Systemic risk is measured by the set of allocations of additional capital that lead to acceptable..."
            },
            {
                "arxivId": "1408.2701",
                "title": "Epidemic processes in complex networks",
                "abstract": "In recent years the research community has accumulated overwhelming evidence for the emergence of complex and heterogeneous connectivity patterns in a wide range of biological and sociotechnical systems. The complex properties of real-world networks have a profound impact on the behavior of equilibrium and nonequilibrium phenomena occurring in various systems, and the study of epidemic spreading is central to our understanding of the unfolding of dynamical processes in complex networks. The theoretical analysis of epidemic spreading in heterogeneous networks requires the development of novel analytical frameworks, and it has produced results of conceptual and practical relevance. A coherent and comprehensive review of the vast research activity concerning epidemic processes is presented, detailing the successful theoretical approaches as well as making their limits and assumptions clear. Physicists, mathematicians, epidemiologists, computer, and social scientists share a common interest in studying epidemic spreading and rely on similar models for the description of the diffusion of pathogens, knowledge, and innovation. For this reason, while focusing on the main results and the paradigmatic models in infectious disease modeling, the major results concerning generalized social contagion processes are also presented. Finally, the research activity at the forefront in the study of epidemic spreading in coevolving, coupled, and time-varying networks is reported."
            },
            {
                "arxivId": "1109.2950",
                "title": "The Physics of Communicability in Complex Networks",
                "abstract": null
            },
            {
                "arxivId": "1112.5687",
                "title": "RESILIENCE TO CONTAGION IN FINANCIAL NETWORKS",
                "abstract": "We derive rigorous asymptotic results for the magnitude of contagion in a large counterparty network and give an analytical expression for the asymptotic fraction of defaults, in terms of network characteristics. Our results extend previous studies on contagion in random graphs to inhomogeneous\u2010directed graphs with a given degree sequence and arbitrary distribution of weights. We introduce a criterion for the resilience of a large financial network to the insolvency of a small group of financial institutions and quantify how contagion amplifies small shocks to the network. Our results emphasize the role played by \u201ccontagious links\u201d and show that institutions which contribute most to network instability have both large connectivity and a large fraction of contagious links. The asymptotic results show good agreement with simulations for networks with realistic sizes."
            },
            {
                "arxivId": "cond-mat/0503078",
                "title": "Self-similarity of complex networks",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0112110",
                "title": "Community structure in social and biological networks",
                "abstract": "A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known\u2014a collaboration network and a food web\u2014and find that it detects significant and informative community divisions in both cases."
            },
            {
                "arxivId": "cond-mat/0101396",
                "title": "Efficient behavior of small-world networks.",
                "abstract": "We introduce the concept of efficiency of a network as a measure of how efficiently it exchanges information. By using this simple measure, small-world networks are seen as systems that are both globally and locally efficient. This gives a clear physical meaning to the concept of \"small world,\" and also a precise quantitative analysis of both weighted and unweighted networks. We study neural networks and man-made communication and transportation systems and we show that the underlying general principle of their construction is in fact a small-world principle of high efficiency."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2402.01141",
        "category": "q-fin",
        "title": "Income distribution in Thailand is scale-invariant",
        "abstract": "This study examines whether income distribution in Thailand has a property of scale invariance or self-similarity across years. By using the data on income shares by quintile and by decile of Thailand from 1988 to 2021, the results from 306-pairwise Kolmogorov-Smirnov tests indicate that income distribution in Thailand is statistically scale-invariant or self-similar across years with p-values ranging between 0.988 and 1.000. Based on these empirical findings, this study would like to propose that, in order to change income distribution in Thailand whose pattern had been persisted for over three decades, the change itself cannot be gradual but has to be like a phase transition of substance in physics.",
        "references": [
            {
                "arxivId": "1801.03400",
                "title": "Scale-free networks are rare",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2402.01142",
        "category": "q-fin",
        "title": "A simple method for joint evaluation of skill in directional forecasts of multiple variables",
        "abstract": null,
        "references": [
            {
                "arxivId": "1903.05192",
                "title": "The Challenge of Machine Learning in Space Weather: Nowcasting and Forecasting",
                "abstract": "The numerous recent breakthroughs in machine learning make imperative to carefully ponder how the scientific community can benefit from a technology that, although not necessarily new, is today living its golden age. This Grand Challenge review paper is focused on the present and future role of machine learning in Space Weather. The purpose is twofold. On one hand, we will discuss previous works that use machine learning for Space Weather forecasting, focusing in particular on the few areas that have seen most activity: the forecasting of geomagnetic indices, of relativistic electrons at geosynchronous orbits, of solar flares occurrence, of coronal mass ejection propagation time, and of solar wind speed. On the other hand, this paper serves as a gentle introduction to the field of machine learning tailored to the Space Weather community and as a pointer to a number of open challenges that we believe the community should undertake in the next decade. The recurring themes throughout the review are the need to shift our forecasting paradigm to a probabilistic approach focused on the reliable assessment of uncertainties, and the combination of physics\u2010based and machine learning approaches, known as gray box."
            },
            {
                "arxivId": "1707.07903",
                "title": "Verification of operational solar flare forecast: Case of Regional Warning Center Japan",
                "abstract": "In this article, we discuss a verification study of an operational solar flare forecast in the Regional Warning Center (RWC) Japan. The RWC Japan has been issuing four-categorical deterministic solar flare forecasts for a long time. In this forecast verification study, we used solar flare forecast data accumulated over 16\u00a0years (from 2000 to 2015). We compiled the forecast data together with solar flare data obtained with the Geostationary Operational Environmental Satellites (GOES). Using the compiled data sets, we estimated some conventional scalar verification measures with 95% confidence intervals. We also estimated a multi-categorical scalar verification measure. These scalar verification measures were compared with those obtained by the persistence method and recurrence method. As solar activity varied during the 16\u00a0years, we also applied verification analyses to four subsets of forecast-observation pair data with different solar activity levels. We cannot conclude definitely that there are significant performance differences between the forecasts of RWC Japan and the persistence method, although a slightly significant difference is found for some event definitions. We propose to use a scalar verification measure to assess the judgment skill of the operational solar flare forecast. Finally, we propose a verification strategy for deterministic operational solar flare forecasting. For dichotomous forecast, a set of proposed verification measures is a frequency bias for bias, proportion correct and critical success index for accuracy, probability of detection for discrimination, false alarm ratio for reliability, Peirce skill score for forecast skill, and symmetric extremal dependence index for association. For multi-categorical forecast, we propose a set of verification measures as marginal distributions of forecast and observation for bias, proportion correct for accuracy, correlation coefficient and joint probability distribution for association, the likelihood distribution for discrimination, the calibration distribution for reliability and resolution, and the Gandin-Murphy-Gerrity score and judgment skill score for skill."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2402.01337",
        "category": "q-fin",
        "title": "Convergence rates for Backward SDEs driven by L\\'evy processes",
        "abstract": "We consider L\\'evy processes that are approximated by compound Poisson processes and, correspondingly, BSDEs driven by L\\'evy processes that are approximated by BSDEs driven by their compound Poisson approximations. We are interested in the rate of convergence of the approximate BSDEs to the ones driven by the L\\'evy processes. The rate of convergence of the L\\'evy processes depends on the Blumenthal--Getoor index of the process. We derive the rate of convergence for the BSDEs in the $\\mathbb L^2$-norm and in the Wasserstein distance, and show that, in both cases, this equals the rate of convergence of the corresponding L\\'evy process, and thus is optimal.",
        "references": [
            {
                "arxivId": "2107.11048",
                "title": "Stability of backward stochastic differential equations: the general Lipschitz case",
                "abstract": "In this paper, we obtain stability results for backward stochastic differential equations with jumps (BSDEs) in a very general framework. More specifically, we consider a convergent sequence of standard data, each associated to their own filtration, and we prove that the associated sequence of (unique) solutions is also convergent. The current result extends earlier contributions in the literature of stability of BSDEs and unifies several frameworks for numerical approximations of BSDEs and their implementations."
            },
            {
                "arxivId": "1908.01188",
                "title": "Donsker-type theorem for BSDEs: Rate of convergence",
                "abstract": "In this paper, we study in the Markovian case the rate of convergence in the Wasserstein distance of an approximation of the solution to a BSDE given by a BSDE which is driven by a scaled random walk as introduced in Briand, Delyon and M{\\'e}min (Electron. Comm. Probab. 6(2001),1-14)."
            },
            {
                "arxivId": "1807.05889",
                "title": "Mean square rate of convergence for random walk approximation of forward-backward SDEs",
                "abstract": "Abstract Let (Y, Z) denote the solution to a forward-backward stochastic differential equation (FBSDE). If one constructs a random walk $B^n$ from the underlying Brownian motion B by Skorokhod embedding, one can show $L_2$-convergence of the corresponding solutions $(Y^n,Z^n)$ to $(Y, Z).$ We estimate the rate of convergence based on smoothness properties, especially for a terminal condition function in $C^{2,\\alpha}$. The proof relies on an approximative representation of $Z^n$ and uses the concept of discretized Malliavin calculus. Moreover, we use growth and smoothness properties of the partial differential equation associated to the FBSDE, as well as of the finite difference equations associated to the approximating stochastic equations. We derive these properties by probabilistic methods."
            },
            {
                "arxivId": "1806.07674",
                "title": "Random walk approximation of BSDEs with H\u00f6lder continuous terminal condition",
                "abstract": "In this paper we consider the random walk approximation of the solution of a Markovian BSDE whose terminal condition is a locally H{\\\"o}lder continuous function of the Brownian motion. We state the rate of the L 2-convergence of the approximated solution to the true one. The proof relies in part on growth and smoothness properties of the solution u of the associated PDE. Here we improve existing results by showing some properties of the second derivative of u in space."
            },
            {
                "arxivId": "1607.04214",
                "title": "Existence and uniqueness results for BSDEs with jumps: the whole nine yards",
                "abstract": "This paper is devoted to obtaining a wellposedness result for multidimensional BSDEs with possibly unbounded random time horizon and driven by a general martingale in a filtration only assumed to satisfy the usual hypotheses, i.e. the filtration may be stochastically discontinuous. We show that for stochastic Lipschitz generators and unbounded, possibly infinite, time horizon, these equations admit a unique solution in appropriately weighted spaces. Our result allows in particular to obtain a wellposedness result for BSDEs driven by discrete--time approximations of general martingales."
            },
            {
                "arxivId": "1504.01150",
                "title": "Minimal supersolutions for BSDEs with singular terminal condition and application to optimal position targeting",
                "abstract": null
            },
            {
                "arxivId": "1502.02888",
                "title": "Reflected scheme for doubly reflected BSDEs with jumps and RCLL obstacles",
                "abstract": null
            },
            {
                "arxivId": "1406.7145",
                "title": "Convergence of BS\u0394Es driven by random walks to BSDEs: The case of (in)finite activity jumps with general driver",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2402.01354",
        "category": "q-fin",
        "title": "Forecasting Volatility of Oil-based Commodities: The Model of Dynamic Persistence",
        "abstract": "Time variation and persistence are crucial properties of volatility that are often studied separately in oil-based volatility forecasting models. Here, we propose a novel approach that allows shocks with heterogeneous persistence to vary smoothly over time, and thus model the two together. We argue that this is important because such dynamics arise naturally from the dynamic nature of shocks in oil-based commodities. We identify such dynamics from the data using localised regressions and build a model that significantly improves volatility forecasts. Such forecasting models, based on a rich persistence structure that varies smoothly over time, outperform state-of-the-art benchmark models and are particularly useful for forecasting over longer horizons.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-05.json",
        "arxivId": "2402.01441",
        "category": "q-fin",
        "title": "Learning the Market: Sentiment-Based Ensemble Trading Agents",
        "abstract": "We propose the integration of sentiment analysis and deep-reinforcement learning ensemble algorithms for stock trading, and design a strategy capable of dynamically altering its employed agent given concurrent market sentiment. In particular, we create a simple-yet-effective method for extracting news sentiment and combine this with general improvements upon existing works, resulting in automated trading agents that effectively consider both qualitative market factors and quantitative stock data. We show that our approach results in a strategy that is profitable, robust, and risk-minimal -- outperforming the traditional ensemble strategy as well as single agent algorithms and market metrics. Our findings determine that the conventional practice of switching ensemble agents every fixed-number of months is sub-optimal, and that a dynamic sentiment-based framework greatly unlocks additional performance within these agents. Furthermore, as we have designed our algorithm with simplicity and efficiency in mind, we hypothesize that the transition of our method from historical evaluation towards real-time trading with live data should be relatively simple.",
        "references": [
            {
                "arxivId": "1312.5602",
                "title": "Playing Atari with Deep Reinforcement Learning",
                "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "1812.07803",
        "category": "q-fin",
        "title": "Closed-form approximations with respect to the mixing solution for option pricing under stochastic volatility",
        "abstract": "We consider closed-form approximations for European put option prices within the Heston and GARCH diffusion stochastic volatility models with time-dependent parameters. Our methodology involves writing the put option price as an expectation of a Black-Scholes formula and performing a second-order Taylor expansion around the mean of its argument. The difficulties then faced are simplifying a number of expectations induced by the Taylor expansion. Under the assumption of piecewise-constant parameters, we derive closed-form pricing formulas and devise a fast calibration scheme. Furthermore, we perform a numerical error and sensitivity analysis to investigate the quality of our approximation and show that the errors are well within the acceptable range for application purposes. Lastly, we derive bounds on the remainder term generated by the Taylor expansion.",
        "references": [
            {
                "arxivId": "1908.07417",
                "title": "A Lognormal Type Stochastic Volatility Model With Quadratic Drift",
                "abstract": "This paper presents a novel one-factor stochastic volatility model where the instantaneous volatility of the asset log-return is a diffusion with a quadratic drift and a linear dispersion function. The instantaneous volatility mean reverts around a constant level, with a speed of mean reversion that is affine in the instantaneous volatility level. The steady-state distribution of the instantaneous volatility belongs to the class of Generalized Inverse Gaussian distributions. We show that the quadratic term in the drift is crucial to avoid moment explosions and to preserve the martingale property of the stock price process. Using a conveniently chosen change of measure, we relate the model to the class of polynomial diffusions. This remarkable relation allows us to develop a highly accurate option price approximation technique based on orthogonal polynomial expansions."
            },
            {
                "arxivId": "1809.08635",
                "title": "Exact Solutions for a GBM\u2010Type Stochastic Volatility Model Having a Stationery Distribution",
                "abstract": "We find various exact solutions for a new stochastic volatility (SV) model: the transition probability density, European-style option values, and (when it exists) the martingale defect. This may represent the first example of an SV model combining exact solutions, GBM-type volatility noise, and a stationary volatility density."
            },
            {
                "arxivId": "1507.02847",
                "title": "Switching to Non-Affine Stochastic Volatility: A Closed-Form Expansion for the Inverse Gamma Model",
                "abstract": "We examine the inverse gamma (IGa) stochastic volatility model with time-dependent parameters. This nonaffine model compares favorably in terms of volatility distribution and volatility paths to classical affine models such as the Heston model, while being as parsimonious (only four stochastic parameters). In practice, this means more robust calibration and better hedging, explaining its popularity among practitioners. Closed-form volatility-of-volatility expansions are obtained for the price of vanilla options, which allow for very fast pricing and calibration to market data. Specifically, the price of a European put option with IGa volatility is approximated by a Black\u2013Scholes price plus a weighted combination of Black\u2013Scholes Greeks, with weights depending only on the four time-dependent parameters of the model. The accuracy of the expansion is illustrated on several calibration tests on foreign exchange market data. This paper shows that the IGa model is as simple, more realistic, easier to implement and faster to calibrate than classical transform-based affine models. We therefore hope that the present work will foster further research on nonaffine models favored by practitioners such as the IGa model."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2003.06249",
        "category": "q-fin",
        "title": "Optimal Hedging of a Perpetual American Put with a Single Trade",
        "abstract": "It is well-known that using delta hedging to hedge financial options is not feasible in practice. Traders often rely on discrete-time hedging strategies based on fixed trading times or fixed trading prices (i.e., trades only occur if the underlying asset's price reaches some predetermined values). Motivated by this insight and with the aim of obtaining explicit solutions, we consider the seller of a perpetual American put option who can hedge her portfolio once until the underlying stock price leaves a certain range of values $(a,b)$. We determine optimal trading boundaries as functions of the initial stock holding, and an optimal hedging strategy for a bond/stock portfolio. Optimality here refers to the variance of the hedging error at the (random) time when the stock leaves the interval $(a,b)$. Our study leads to analytical expressions for both the optimal boundaries and the optimal stock holding, which can be evaluated numerically with no effort.",
        "references": [
            {
                "arxivId": "1006.4283",
                "title": "Stopping of functionals with discontinuity at the boundary of an open set",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2103.17255",
        "category": "q-fin",
        "title": "Subsidising Inclusive Insurance to Reduce Poverty",
        "abstract": "In this article, we assess the benefits of coordination and partnerships between governments and private insurers, and provide further evidence for microinsurance products as powerful and cost-effective tools for achieving poverty reduction. To explore these ideas, we model the capital of a household from a ruin-theoretic perspective to measure the impact of microinsurance on poverty dynamics and the governmental cost of social protection. We analyse the model under four frameworks: uninsured, insured (without subsidies), insured with subsidised constant premiums and insured with subsidised flexible premiums. Although insurance alone (without subsidies) may not be sufficient to reduce the likelihood of falling into the area of poverty for specific groups of households, since premium payments constrain their capital growth, our analysis suggests that subsidised schemes can provide maximum social benefits while reducing governmental costs.",
        "references": [
            {
                "arxivId": "1905.10398",
                "title": "An application of fractional differential equations to risk theory",
                "abstract": null
            },
            {
                "arxivId": "1403.6769",
                "title": "Semi-parametric inference for the absorption features of a growth-fragmentation model",
                "abstract": "In the present paper, we focus on semi-parametric methods for estimating the absorption probability and the distribution of the absorbing time of a growth-fragmentation model observed within a long time interval. We establish that the absorption probability is the unique solution in an appropriate space of a Fredholm equation of the second kind whose parameters are unknown. We estimate this important characteristic of the underlying process by solving numerically the estimated Fredholm equation. Even if the study has been conducted for a particular model, our method is quite general."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2104.08502",
        "category": "q-fin",
        "title": "The American put with finite\u2010time maturity and stochastic interest rate",
        "abstract": "In this paper, we study pricing of American put options on a nondividend\u2010paying stock in the Black and Scholes market with a stochastic interest rate and finite\u2010time maturity. We prove that the option value is a C1 function of the initial time, interest rate, and stock price. By means of It\u00f4 calculus, we rigorously derive the option value's early exercise premium formula and the associated hedging portfolio. We prove the existence of an optimal exercise boundary splitting the state space into continuation and stopping region. The boundary has a parametrization as a jointly continuous function of time and stock price, and it is the unique solution to an integral equation, which we compute numerically. Our results hold for a large class of interest rate models including CIR and Vasicek models. We show a numerical study of the option price and the optimal exercise boundary for Vasicek model.",
        "references": [
            {
                "arxivId": "1305.0479",
                "title": "A robust tree method for pricing American options with CIR stochastic interest rate",
                "abstract": "We propose a robust and stable lattice method which permits to obtain very accurate American option prices in presence of CIR stochastic interest rate without any numerical restriction on its parameters. Numerical results show the reliability and the accuracy of the proposed method."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2305.00585",
        "category": "q-fin",
        "title": "Prospects of BRICS currency dominance in international trade",
        "abstract": null,
        "references": [
            {
                "arxivId": "1808.10090",
                "title": "Hierarchical communities in the walnut structure of the Japanese production network",
                "abstract": "This paper studies the structure of the Japanese production network, which includes one million firms and five million supplier-customer links. This study finds that this network forms a tightly-knit structure with a core giant strongly connected component (GSCC) surrounded by IN and OUT components constituting two half-shells of the GSCC, which we call awalnut structure because of its shape. The hierarchical structure of the communities is studied by the Infomap method, and most of the irreducible communities are found to be at the second level. The composition of some of the major communities, including overexpressions regarding their industrial or regional nature, and the connections that exist between the communities are studied in detail. The findings obtained here cause us to question the validity and accuracy of using the conventional input-output analysis, which is expected to be useful when firms in the same sectors are highly connected to each other."
            },
            {
                "arxivId": "0710.3256",
                "title": "Statistical physics of social dynamics",
                "abstract": "Statistical physics has proven to be a fruitful framework to describe phenomena outside the realm of traditional physics. Recent years have witnessed an attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. A wide list of topics are reviewed ranging from opinion and cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, and social spreading. The connections between these problems and other, more traditional, topics of statistical physics are highlighted. Comparison of model results with empirical data from social systems are also emphasized."
            },
            {
                "arxivId": "cond-mat/0503607",
                "title": "Importance of Positive Feedbacks and Over-Confidence in a Self-Fulfilling Ising Model of Financial Markets",
                "abstract": "Following a long tradition of physicists who have noticed that the Ising model provides a general background to build realistic models of social interactions, we study a model of financial price dynamics resulting from the collective aggregate decisions of agents. This model incorporates imitation, the impact of external news and private information. It has the structure of a dynamical Ising model in which agents have two opinions (buy or sell) with coupling coefficients which evolve in time with a memory of how past news have explained realized market returns. We study two versions of the model, which differ on how the agents interpret the predictive power of news. We show that the stylized facts of financial markets are reproduced only when agents are over-confident and mis-attribute the success of news to predict return to herding effects, thereby providing positive feedbacks leading to the model functioning close to the critical point. Our model exhibits a rich multifractal structure characterized by a continuous spectrum of exponents of the power law relaxation of endogenous bursts of volatility, in good agreement with previous analytical predictions obtained with the multifractal random walk model and with empirical facts."
            },
            {
                "arxivId": "cond-mat/0401144",
                "title": "Model of macroeconomic evolution in stable regionally dependent economic fields",
                "abstract": null
            },
            {
                "arxivId": "nlin/0210041",
                "title": "Simple Model for the Dynamics of Correlations in the Evolution of Economic Entities Under Varying Economic Conditions",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2306.10224",
        "category": "q-fin",
        "title": "Bloated Disclosures: Can ChatGPT Help Investors Process Information?",
        "abstract": "Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are remarkably shorter compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). Importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information ``bloat.\"We show that bloated disclosure is associated with adverse capital market consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance. Collectively, our results indicate that generative AI adds considerable value for investors with information processing constraints.",
        "references": [
            {
                "arxivId": "2302.08081",
                "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization",
                "abstract": "Text summarization has been a crucial problem in natural language processing (NLP) for several decades. It aims to condense lengthy documents into shorter versions while retaining the most critical information. Various methods have been proposed for text summarization, including extractive and abstractive summarization. The emergence of large language models (LLMs) like GPT3 and ChatGPT has recently created significant interest in using these models for text summarization tasks. Recent studies \\cite{goyal2022news, zhang2023benchmarking} have shown that LLMs-generated news summaries are already on par with humans. However, the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored. To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores. Moreover, we highlight some unique differences between ChatGPT-generated summaries and human references, providing valuable insights into the superpower of ChatGPT for diverse text summarization tasks. Our findings call for new directions in this area, and we plan to conduct further research to systematically examine the characteristics of ChatGPT-generated summaries through extensive human evaluation."
            },
            {
                "arxivId": "2209.12356",
                "title": "News Summarization and Evaluation in the Era of GPT-3",
                "abstract": "The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting. To support further research, we release: (a) a corpus of 10K generated summaries from fine-tuned and prompt-based models across 4 standard summarization benchmarks, (b) 1K human preference judgments comparing different systems for generic- and keyword-based summarization."
            },
            {
                "arxivId": "2005.14165",
                "title": "Language Models are Few-Shot Learners",
                "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
            },
            {
                "arxivId": "1912.08777",
                "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
                "abstract": "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "cmp-lg/9505040",
                "title": "Text Chunking using Transformation-Based Learning",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2307.03927",
        "category": "q-fin",
        "title": "Fast Empirical Scenarios",
        "abstract": "We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.",
        "references": [
            {
                "arxivId": "2006.01757",
                "title": "A Randomized Algorithm to Reduce the Support of Discrete Measures",
                "abstract": "Given a discrete probability measure supported on $N$ atoms and a set of $n$ real-valued functions, there exists a probability measure that is supported on a subset of $n+1$ of the original $N$ atoms and has the same mean when integrated against each of the $n$ functions. If $ N \\gg n$ this results in a huge reduction of complexity. We give a simple geometric characterization of barycenters via negative cones and derive a randomized algorithm that computes this new measure by \"greedy geometric sampling\". We then study its properties, and benchmark it on synthetic and real-world data to show that it can be very beneficial in the $N\\gg n$ regime. A Python implementation is available at \\url{this https URL}."
            },
            {
                "arxivId": "1906.07832",
                "title": "Kernel quadrature with DPPs",
                "abstract": "We study quadrature rules for functions from an RKHS, using nodes sampled from a determinantal point process (DPP). DPPs are parametrized by a kernel, and we use a truncated and saturated version of the RKHS kernel. This link between the two kernels, along with DPP machinery, leads to relatively tight bounds on the quadrature error, that depends on the spectrum of the RKHS kernel. Finally, we experimentally compare DPPs to existing kernel-based quadratures such as herding, Bayesian quadrature, or leverage score sampling. Numerical results confirm the interest of DPPs, and even suggest faster rates than our bounds in particular cases."
            },
            {
                "arxivId": "1703.04641",
                "title": "Adaptive Restart of the Optimized Gradient Method for Convex Optimization",
                "abstract": null
            },
            {
                "arxivId": "1605.06423",
                "title": "Coresets for Scalable Bayesian Logistic Regression",
                "abstract": "The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it."
            },
            {
                "arxivId": "1512.07516",
                "title": "Exact Worst-Case Performance of First-Order Methods for Composite Convex Optimization",
                "abstract": "We provide a framework for computing the exact worst-case performance of any algorithm belonging to a broad class of oracle-based first-order methods for composite convex optimization, including those performing explicit, projected, proximal, conditional and inexact (sub)gradient steps. We simultaneously obtain tight worst-case guarantees and explicit instances of optimization problems on which the algorithm reaches this worst-case. We achieve this by reducing the compu- tation of the worst-case to solving a convex semidefinite program, generalizing previous works on performance estimation by Drori and Teboulle and the authors. We use these developments to obtain a tighter analysis of the proximal point algorithm and of several variants of fast proximal gradient, conditional gradient, subgradient and alternating projection methods. In particular, we present a new analytical worst-case guarantee for the proximal point algorithm that is twice better than previously known, and improve the standard worst-case guarantee for the conditional gradient method by more than a factor of two. We also show how the optimized gradient method proposed by Kim and Fessler can be extended by incorporating a projection or a proximal operator, which leads to an algorithm that converges in the worst-case twice as fast as the standard accelerated proximal gradient method."
            },
            {
                "arxivId": "1807.04518",
                "title": "Turning big data into tiny data: Constant-size coresets for k-means, PCA and projective clustering",
                "abstract": "@d can be approximated up to (1 + e)-factor, for an arbitrary small e > 0, using the O(k/e2)-rank approximation of A and a constant. This implies, for example, that the optimal k-means clustering of the rows of A is (1 + e)-approximated by an optimal k-means clustering of their projection on the O(k/e2) first right singular vectors (principle components) of A. \n \nA (j, k)-coreset for projective clustering is a small set of points that yields a (1 + e)-approximation to the sum of squared distances from the n rows of A to any set of k affine subspaces, each of dimension at most j. Our embedding yields (0, k)-coresets of size O(k) for handling k-means queries, (j, 1)-coresets of size O(j) for PCA queries, and (j, k)-coresets of size (log n)O(jk) for any j, k \u2265 1 and constant e e (0, 1/2). Previous coresets usually have a size which is linearly or even exponentially dependent of d, which makes them useless when d ~ n. \n \nUsing our coresets with the merge-and-reduce approach, we obtain embarrassingly parallel streaming algorithms for problems such as k-means, PCA and projective clustering. These algorithms use update time per point and memory that is polynomial in log n and only linear in d. \n \nFor cost functions other than squared Euclidean distances we suggest a simple recursive coreset construction that produces coresets of size"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2308.08630",
        "category": "q-fin",
        "title": "Cooperation and interdependence in global science funding",
        "abstract": "Investments in research and development are key to scientific and economic growth and to the well-being of society. Scientific research demands significant resources making national scientific investment a crucial driver of scientific production. As scientific production becomes increasingly multinational, it is critical to study how nations' scientific activities are funded both domestically and internationally. By tracing research grants acknowledged in scholarly publications, our study reveals a shifting duopoly of China and the United States in the global funding landscape, with a contrasting funding pattern; while China has surpassed the United States in publications with acknowledged domestic and international funding, the United States largely maintains its role as the most important global research partner. Our results also highlight the precarity of low- and middle-income countries to global funding disruptions. By revealing the complex interdependence and collaboration between countries in the global scientific enterprise, this work informs future studies investigating the national and global scientific enterprise and how funding leads to both productive cooperation and vulnerable dependencies.",
        "references": [
            {
                "arxivId": "1604.04896",
                "title": "Funding Data from Publication Acknowledgments: Coverage, Uses, and Limitations",
                "abstract": "This article contributes to the development of methods for analysing research funding systems by exploring the robustness and comparability of emerging approaches to generate funding landscapes useful for policy making. We use a novel data set of manually extracted and coded data on the funding acknowledgements of 7,510 publications representing UK cancer research in the year 2011 and compare these \u201creference data\u201d with funding data provided by Web of Science (WoS) and MEDLINE/PubMed. Findings show high recall (around 93%) of WoS funding data. By contrast, MEDLINE/PubMed data retrieved less than half of the UK cancer publications acknowledging at least one funder. Conversely, both databases have high precision (+90%): That is, few cases of publications with no acknowledgment to funders are identified as having funding data. Nonetheless, funders acknowledged in UK cancer publications were not correctly listed by MEDLINE/PubMed and WoS in around 75% and 32% of the cases, respectively. Reference data on the UK cancer research funding system are used as a case study to demonstrate the utility of funding data for strategic intelligence applications (e.g., mapping of funding landscape and co\u2010funding activity, comparison of funders' research portfolios)."
            },
            {
                "arxivId": "1604.04780",
                "title": "Characterization, description, and considerations for the use of funding acknowledgement data in Web of Science",
                "abstract": null
            },
            {
                "arxivId": "0911.1044",
                "title": "Macro-level indicators of the relations between research funding and research output",
                "abstract": null
            },
            {
                "arxivId": "0904.2389",
                "title": "Extracting the multiscale backbone of complex weighted networks",
                "abstract": "A large number of complex systems find a natural abstraction in the form of weighted networks whose nodes represent the elements of the system and the weighted edges identify the presence of an interaction and its relative strength. In recent years, the study of an increasing number of large-scale networks has highlighted the statistical heterogeneity of their interaction pattern, with degree and weight distributions that vary over many orders of magnitude. These features, along with the large number of elements and links, make the extraction of the truly relevant connections forming the network's backbone a very challenging problem. More specifically, coarse-graining approaches and filtering techniques come into conflict with the multiscale nature of large-scale systems. Here, we define a filtering method that offers a practical procedure to extract the relevant connection backbone in complex multiscale networks, preserving the edges that represent statistically significant deviations with respect to a null model for the local assignment of weights to edges. An important aspect of the method is that it does not belittle small-scale interactions and operates at all scales defined by the weight distribution. We apply our method to real-world network instances and compare the obtained results with alternative backbone extraction techniques."
            },
            {
                "arxivId": "0903.5254",
                "title": "Comparing Bibliometric Statistics Obtained from the Web of Science and Scopus",
                "abstract": "For more than 40 years, the Institute for Scientific Information (ISI, now part of Thomson Reuters) produced the only available bibliographic databases from which bibliometricians could compile large-scale bibliometric indicators. ISI's citation indexes, now regrouped under the Web of Science (WoS), were the major sources of bibliometric data until 2004, when Scopus was launched by the publisher Reed Elsevier. For those who perform bibliometric analyses and comparisons of countries or institutions, the existence of these two major databases raises the important question of the comparability and stability of statistics obtained from different data sources. This paper uses macro-level bibliometric indicators to compare results obtained from the WoS and Scopus. It shows that the correlations between the measures obtained with both databases for the number of papers and the number of citations received by countries, as well as for their ranks, are extremely high (R2 > .99). There is also a very high correlation when countries' papers are broken down by field. The paper thus provides evidence that indicators of scientific production and citations at the country level are stable and largely independent of the database."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2310.16424",
        "category": "q-fin",
        "title": "Pre-electoral coalition agreement from the Black\u2013Scholes point of view",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2401.05441",
        "category": "q-fin",
        "title": "An adaptive network-based approach for advanced forecasting of cryptocurrency values",
        "abstract": "This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in adaily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.",
        "references": [
            {
                "arxivId": "2312.17683",
                "title": "Malware Detection in IOT Systems Using Machine Learning Techniques",
                "abstract": "Malware detection in IoT environments necessitates robust methodologies. This study introduces a CNN-LSTM hybrid model for IoT malware identification and evaluates its performance against established methods. Leveraging K-fold cross-validation, the proposed approach achieved 95.5% accuracy, surpassing existing methods. The CNN algorithm enabled superior learning model construction, and the LSTM classifier exhibited heightened accuracy in classification. Comparative analysis against prevalent techniques demonstrated the efficacy of the proposed model, highlighting its potential for enhancing IoT security. The study advocates for future exploration of SVMs as alternatives, emphasizes the need for distributed detection strategies, and underscores the importance of predictive analyses for a more powerful IOT security. This research serves as a platform for developing more resilient security measures in IoT ecosystems."
            },
            {
                "arxivId": "2305.02426",
                "title": "Evaluating Bert and Parsbert for Analyzing Persian Advertisement Data",
                "abstract": "This paper discusses the impact of the Internet on modern trading and the importance of data generated from these transactions for organizations to improve their marketing efforts. The paper uses the example of Divar, an online marketplace for buying and selling products and services in Iran, and presents a competition to predict the percentage of a car sales ad that would be published on the Divar website. Since the dataset provides a rich source of Persian text data, the authors use the Hazm library, a Python library designed for processing Persian text, and two state-of-the-art language models, mBERT and ParsBERT, to analyze it. The paper's primary objective is to compare the performance of mBERT and ParsBERT on the Divar dataset. The authors provide some background on data mining, Persian language, and the two language models, examine the dataset's composition and statistical features, and provide details on their fine-tuning and training configurations for both approaches. They present the results of their analysis and highlight the strengths and weaknesses of the two language models when applied to Persian text data. The paper offers valuable insights into the challenges and opportunities of working with low-resource languages such as Persian and the potential of advanced language models like BERT for analyzing such data. The paper also explains the data mining process, including steps such as data cleaning and normalization techniques. Finally, the paper discusses the types of machine learning problems, such as supervised, unsupervised, and reinforcement learning, and the pattern evaluation techniques, such as confusion matrix. Overall, the paper provides an informative overview of the use of language models and data mining techniques for analyzing text data in low-resource languages, using the example of the Divar dataset."
            },
            {
                "arxivId": "2401.02537",
                "title": "Using Singular Value Decomposition in a Convolutional Neural Network to Improve Brain Tumor Segmentation Accuracy",
                "abstract": "A brain tumor consists of cells showing abnormal brain growth. The area of the brain tumor significantly affects choosing the type of treatment and following the course of the disease during the treatment. At the same time, pictures of Brain MRIs are accompanied by noise. Eliminating existing noises can significantly impact the better segmentation and diagnosis of brain tumors. In this work, we have tried using the analysis of eigenvalues. We have used the MSVD algorithm, reducing the image noise and then using the deep neural network to segment the tumor in the images. The proposed method's accuracy was increased by 2.4% compared to using the original images. With Using the MSVD method, convergence speed has also increased, showing the proposed method's effectiveness."
            },
            {
                "arxivId": "2009.04200",
                "title": "Rise of the machines? Intraday high-frequency trading patterns of cryptocurrencies",
                "abstract": "This research analyses high-frequency data of the cryptocurrency market in regards to intraday trading patterns related to algorithmic trading and its impact on the European cryptocurrency market. We study trading quantitatives such as returns, traded volumes, volatility periodicity, and provide summary statistics of return correlations to CRIX (CRyptocurrency IndeX), as well as respective overall high-frequency based market statistics with respect to temporal aspects. Our results provide mandatory insight into a market, where the grand scale employment of automated trading algorithms and the extremely rapid execution of trades might seem to be a standard based on media reports. Our findings on intraday momentum of trading patterns lead to a new quantitative view on approaching the predictability of economic value in this new digital market."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2401.06141",
        "category": "q-fin",
        "title": "The role of direct capital cash transfers towards poverty and extreme poverty alleviation - an omega risk process",
        "abstract": "Trapping refers to the event when a household falls into the area of poverty. Households that live or fall into the area of poverty are said to be in a poverty trap, where a poverty trap is a state of poverty from which it is difficult to escape without external help. Similarly, extreme poverty is considered as the most severe type of poverty, in which households experience severe deprivation of basic human needs. In this article, we consider an Omega risk process with deterministic growth and a multiplicative jump (collapse) structure to model the capital of a household. It is assumed that, when a household's capital level is above a certain capital barrier level that determines a household's eligibility for a capital cash transfer programme, its capital grows exponentially. As soon as its capital falls below the capital barrier level, the capital dynamics incorporate external support in the form of direct transfers (capital cash transfers) provided by donors or governments. Otherwise, once trapped, the capital grows only due to the capital cash transfers. Under this model, we first derive closed-form expressions for the trapping probability and then do the same for the probability of extreme poverty, which only depends on the current value of the capital given by some extreme poverty rate function. Numerical examples illustrate the role of capital cash transfers on poverty and extreme poverty dynamics.",
        "references": [
            {
                "arxivId": "2402.11715",
                "title": "The Gerber-Shiu Expected Discounted Penalty Function: An Application to Poverty Trapping",
                "abstract": "In this article, we consider a risk process with deterministic growth and prorated losses to model the capital of a household. Our work focuses on the analysis of the trapping time of such a process, where trapping occurs when a household's capital level falls into the poverty area, a region from which it is difficult to escape without external help. A function analogous to the classical Gerber-Shiu expected discounted penalty function is introduced, which incorporates information on the trapping time, the capital surplus immediately before trapping and the capital deficit at trapping. Given that the remaining proportion of capital upon experiencing a capital loss is $Beta(\\alpha,1)-$distributed, closed-form expressions are obtained for quantities typically studied in classical risk theory, including the Laplace transform of the trapping time and the distribution of the capital deficit at trapping. In particular, we derive a model belonging to the generalised beta (GB) distribution family that describes the distribution of the capital deficit at trapping given that trapping occurs. Affinities between the capital deficit at trapping and a class of poverty measures, known as the Foster-Greer-Thorbecke (FGT) index, are presented. The versatility of this model to estimate FGT indices is assessed using household microdata from Burkina Faso's Enqu\\^ete Multisectorielle Continue (EMC) 2014."
            },
            {
                "arxivId": "2310.09295",
                "title": "On the impact of insurance on households susceptible to random proportional losses: An analysis of poverty trapping",
                "abstract": "In this paper, we consider a risk process with deterministic growth and multiplicative jumps to model the capital of a low-income household. Reflecting the high-risk nature of the low-income environment, capital losses are assumed to be proportional to the level of accumulated capital at the jump time. Our aim is to derive the probability that a household falls below the poverty line, i.e. the trapping probability, where ``trapping\"occurs when the level of capital of a household holds falls below the poverty line, to an area from which it is difficult to escape without external help. Considering the remaining proportion of capital to be distributed as a special case of the beta distribution, closed-form expressions for the trapping probability are obtained via analysis of the Laplace transform of the infinitesimal generator of the process. To study the impact of insurance on this probability, introduction of an insurance product offering proportional coverage is presented. The infinitesimal generator of the insured process gives rise to non-local differential equations. To overcome this, we propose a recursive method for deriving a closed-form solution of the integro-differential equation associated with the infinitesimal generator of the insured process and provide a numerical estimation method for obtaining the trapping probability. Constraints on the rate parameters of the process that prevent certain trapping are derived in both the uninsured and insured cases using classical results from risk theory."
            },
            {
                "arxivId": "2203.10680",
                "title": "The Gerber-Shiu discounted penalty function: A review from practical perspectives",
                "abstract": null
            },
            {
                "arxivId": "2103.17255",
                "title": "Subsidising Inclusive Insurance to Reduce Poverty",
                "abstract": "In this article, we assess the benefits of coordination and partnerships between governments and private insurers, and provide further evidence for microinsurance products as powerful and cost-effective tools for achieving poverty reduction. To explore these ideas, we model the capital of a household from a ruin-theoretic perspective to measure the impact of microinsurance on poverty dynamics and the governmental cost of social protection. We analyse the model under four frameworks: uninsured, insured (without subsidies), insured with subsidised constant premiums and insured with subsidised flexible premiums. Although insurance alone (without subsidies) may not be sufficient to reduce the likelihood of falling into the area of poverty for specific groups of households, since premium payments constrain their capital growth, our analysis suggests that subsidised schemes can provide maximum social benefits while reducing governmental costs."
            },
            {
                "arxivId": "1310.3052",
                "title": "Power Identities for L\u00e9vy Risk Models Under Taxation and Capital Injections",
                "abstract": "In this paper we study a spectrally negative L\\'evy process which is refracted at its running maximum and at the same time reflected from below at a certain level. Such a process can for instance be used to model an insurance surplus process subject to tax payments according to a loss-carry-forward scheme together with the flow of minimal capital injections required to keep the surplus process non-negative. We characterize the first passage time over an arbitrary level and the cumulative amount of injected capital up to this time by their joint Laplace transform, and show that it satisfies a simple power relation to the case without refraction. It turns out that this identity can also be extended to a certain type of refraction from below. The net present value of tax collected before the cumulative injected capital exceeds a certain amount is determined, and a numerical illustration is provided."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2401.14553",
        "category": "q-fin",
        "title": "Analysis of an aggregate loss model in a Markov renewal regime",
        "abstract": null,
        "references": [
            {
                "arxivId": "2401.14561",
                "title": "Fitting procedure for the two-state Batch Markov modulated Poisson process",
                "abstract": null
            },
            {
                "arxivId": "1310.1756",
                "title": "High Frequency Trading and Asymptotics for Small Risk Aversion in a Markov Renewal Model",
                "abstract": "We study a an optimal high frequency trading problem within a market microstructure model designed to be a good compromise between accuracy and tractability. The stock price is driven by a Markov Renewal Process (MRP), while market orders arrive in the limit order book via a point process correlated with the stock price itself. In this framework, we can reproduce the adverse selection risk, appearing in two different forms: the usual one due to big market orders impacting the stock price and penalizing the agent, and the weak one due to small market orders and reducing the probability of a profitable execution. We solve the market making problem by stochastic control techniques in this semi-Markov model. In the no risk-aversion case, we provide explicit formula for the optimal controls and characterize the value function as a simple linear PDE. In the general case, we derive the optimal controls and the value function in terms of the previous result, and illustrate how the risk aversion influences the trader strategy and her expected gain. Finally, by using a perturbation method, approximate optimal controls for small risk aversions are explicitly computed in terms of two simple PDE's, reducing drastically the computational cost and enlightening the financial interpretation of the results."
            },
            {
                "arxivId": "1011.3411",
                "title": "Bayesian inference for double Pareto lognormal queues",
                "abstract": "In this article we describe a method for carrying out Bayesian estimation for the double Pareto lognormal (dPlN) distribution which has been proposed as a model for heavy-tailed phenomena. We apply our approach to estimate the dPlN/M/1 and M/dPlN/1 queueing systems. These systems cannot be analyzed using standard techniques due to the fact that the dPlN distribution does not possess a Laplace transform in closed form. This difficulty is overcome using some recent approximations for the Laplace transform of the interarrival distribution for the Pareto/M/1 system. Our procedure is illustrated with applications in internet traffic analysis and risk theory."
            },
            {
                "arxivId": "1008.1108",
                "title": "Calculation of aggregate loss distributions",
                "abstract": "Estimation of the operational risk capital under the Loss Distribution Approach requires evaluation of aggregate (compound) loss distributions which is one of the classic problems in risk theory. Closed-form solutions are not available for the distributions typically used in operational risk. However with modern computer processing power, these distributions can be calculated virtually exactly using numerical methods. This paper reviews numerical algorithms that can be successfully used to calculate the aggregate loss distributions. In particular Monte Carlo, Panjer recursion and Fourier transformation methods are presented and compared. Also, several closed-form approximations based on moment matching and asymptotic result for heavy-tailed distributions are reviewed."
            },
            {
                "arxivId": "0904.1805",
                "title": "Implementing Loss Distribution Approach for Operational Risk",
                "abstract": "In order to quantify the operational risk capital charge under the current regulatory framework for banking supervision, referred to as Basel II, many banks adopt the loss distribution approach. There are many modeling issues that should be resolved to use this approach in practice. In this paper we review the quantitative methods suggested in the literature for the implementation of the approach. In particular, the use of Bayesian inference that allows one to take expert judgement and parameter uncertainty into account, modeling dependence, and inclusion of insurance are discussed. Copyright \u00a9 2009 John Wiley & Sons, Ltd."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2401.14945",
        "category": "q-fin",
        "title": "Free public transport to the destination: A causal analysis of tourists' travel mode choice",
        "abstract": "In this paper, we assess the impact of a fare-free public transport policy for overnight guests on travel mode choice to a Swiss tourism destination. The policy directly targets domestic transport to and from a destination, the substantial contributor to the CO2 emissions of overnight trips. Based on a survey sample, we identify the effect with the help of the random element that the information on the offer from a hotelier to the guest varies in day-to-day business. We estimate a shift from private cars to public transport due to the policy of, on average, 14.8 and 11.6 percentage points, depending on the application of propensity score matching and causal forest. This knowledge is relevant for policy-makers to design future offers that include more sustainable travels to a destination. Overall, our paper exemplifies how such an effect of comparable natural experiments in the travel and tourism industry can be properly identified with a causal framework and underlying assumptions.",
        "references": [
            {
                "arxivId": "2105.01426",
                "title": "Business analytics meets artificial intelligence: Assessing the demand effects of discounts on Swiss train tickets",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2401.16030",
        "category": "q-fin",
        "title": "Does green innovation crowd out other innovation of firms? Based on the extended CDM model and unconditional quantile regressions",
        "abstract": "In the era of sustainability, firms grapple with the decision of how much to invest in green innovation and how it influences their economic trajectory. This study employs the Crepon, Duguet, and Mairesse (CDM) framework to examine the conversion of R&D funds into patents and their impact on productivity, effectively addressing endogeneity by utilizing predicted dependent variables at each stage to exclude unobservable factors. Extending the classical CDM model, this study contrasts green and non-green innovations' economic effects. The results show non-green patents predominantly drive productivity gains, while green patents have a limited impact in non-heavy polluting firms. However, in high-pollution and manufacturing sectors, both innovation types equally enhance productivity. Using unconditional quantile regression, I found green innovation's productivity impact follows an inverse U-shape, unlike the U-shaped pattern of non-green innovation. Significantly, in the 50th to 80th productivity percentiles of manufacturing and high-pollution firms, green innovation not only contributes to environmental sustainability but also outperforms non-green innovation economically.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.00515",
        "category": "q-fin",
        "title": "Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management",
        "abstract": "Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and proactive agent as the market observer is integrated into the MASA framework to provide some additional information on the estimated market trends as valuable feedbacks for multi-agent RL approach to quickly adapt to the ever-changing market conditions. The obtained empirical results clearly reveal the potential strengths of our proposed MASA framework based on the multi-agent RL approach against many well-known RL-based approaches on the challenging data sets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the past 10 years. More importantly, our proposed MASA framework shed lights on many possible directions for future investigation.",
        "references": [
            {
                "arxivId": "2008.13535",
                "title": "DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems",
                "abstract": "Learning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep & Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently. In light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google. Our code and tutorial are open-sourced as part of TensorFlow Recommenders (TFRS)1."
            },
            {
                "arxivId": "2008.07871",
                "title": "Fast Agent-Based Simulation Framework with Applications to Reinforcement Learning and the Study of Trading Latency Effects",
                "abstract": null
            },
            {
                "arxivId": "2002.05780",
                "title": "Reinforcement-Learning based Portfolio Management with Augmented Asset Movement Prediction States",
                "abstract": "Portfolio management (PM) is a fundamental financial planning task that aims to achieve investment goals such as maximal profits or minimal risks. Its decision process involves continuous derivation of valuable information from various data sources and sequential decision optimization, which is a prospective research direction for reinforcement learning (RL). In this paper, we propose SARL, a novel State-Augmented RL framework for PM. Our framework aims to address two unique challenges in financial PM: (1) data heterogeneity \u2013 the collected information for each asset is usually diverse, noisy and imbalanced (e.g., news articles); and (2) environment uncertainty \u2013 the financial market is versatile and non-stationary. To incorporate heterogeneous data and enhance robustness against environment uncertainty, our SARL augments the asset information with their price movement prediction as additional states, where the prediction can be solely based on financial data (e.g., asset prices) or derived from alternative sources such as news. Experiments on two real-world datasets, (i) Bitcoin market and (ii) HighTech stock market with 7-year Reuters news articles, validate the effectiveness of SARL over existing PM approaches, both in terms of accumulated profits and risk-adjusted profits. Moreover, extensive simulations are conducted to demonstrate the importance of our proposed state augmentation, providing new insights and boosting performance significantly over standard RL-based PM method and other baselines."
            },
            {
                "arxivId": "1808.08497",
                "title": "FinBrain: when finance meets AI 2.0",
                "abstract": "Artificial intelligence (AI) is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a \u201cfinancial brain.\u201d In this paper, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision-making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field."
            },
            {
                "arxivId": "1802.09477",
                "title": "Addressing Function Approximation Error in Actor-Critic Methods",
                "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested."
            },
            {
                "arxivId": "1709.04494",
                "title": "A Rewriting System for Convex Optimization Problems",
                "abstract": "We describe a modular rewriting system for translating optimization problems written in a domain-specific language to forms compatible with low-level solver interfaces. Translation is facilitated by reductions, which accept a category of problems and transform instances of that category to equivalent instances of another category. Our system proceeds in two key phases: analysis, in which we attempt to find a suitable solver for a supplied problem, and canonicalization, in which we rewrite the problem in the selected solver's standard form. We implement the described system in version 1.0 of CVXPY, a domain-specific language for mathematical and especially convex optimization. By treating reductions as first-class objects, our method makes it easy to match problems to solvers well-suited for them and to support solvers with a wide variety of standard forms."
            },
            {
                "arxivId": "1706.10059",
                "title": "A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem",
                "abstract": "Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days."
            },
            {
                "arxivId": "1603.00943",
                "title": "CVXPY: A Python-Embedded Modeling Language for Convex Optimization",
                "abstract": "CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at http://www.cvxpy.org/ under the GPL license, along with documentation and examples."
            },
            {
                "arxivId": "1212.2129",
                "title": "Online portfolio selection: A survey",
                "abstract": "Online portfolio selection is a fundamental problem in computational finance, which has been extensively studied across several research communities, including finance, statistics, artificial intelligence, machine learning, and data mining. This article aims to provide a comprehensive survey and a structural understanding of online portfolio selection techniques published in the literature. From an online machine learning perspective, we first formulate online portfolio selection as a sequential decision problem, and then we survey a variety of state-of-the-art approaches, which are grouped into several major categories, including benchmarks, Follow-the-Winner approaches, Follow-the-Loser approaches, Pattern-Matching--based approaches, and Meta-Learning Algorithms. In addition to the problem formulation and related algorithms, we also discuss the relationship of these algorithms with the capital growth theory so as to better understand the similarities and differences of their underlying trading ideas. This article aims to provide a timely and comprehensive survey for both machine learning and data mining researchers in academia and quantitative portfolio managers in the financial industry to help them understand the state of the art and facilitate their research and practical applications. We also discuss some open issues and evaluate some emerging new trends for future research."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.01007",
        "category": "q-fin",
        "title": "Municipal cyber risk modeling using cryptographic computing to inform cyber policymaking",
        "abstract": "Municipalities are vulnerable to cyberattacks with devastating consequences, but they lack key information to evaluate their own risk and compare their security posture to peers. Using data from 83 municipalities collected via a cryptographically secure computation platform about their security posture, incidents, security control failures, and losses, we build data-driven cyber risk models and cyber security benchmarks for municipalities. We produce benchmarks of the security posture in a sector, the frequency of cyber incidents, forecasted annual losses for organizations based on their defensive posture, and a weighting of cyber controls based on their individual failure rates and associated losses. Combined, these four items can help guide cyber policymaking by quantifying the cyber risk in a sector, identifying gaps that need to be addressed, prioritizing policy interventions, and tracking progress of those interventions over time. In the case of the municipalities, these newly derived risk measures highlight the need for continuous measured improvement of cybersecurity readiness, show clear areas of weakness and strength, and provide governments with some early targets for policy focus such as security education, incident response, and focusing efforts first on municipalities at the lowest security levels that have the highest risk reduction per security dollar invested.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.01766",
        "category": "q-fin",
        "title": "LLM Voting: Human Choices and AI Collective Decision Making",
        "abstract": "This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.",
        "references": [
            {
                "arxivId": "2306.01495",
                "title": "Accelerating science with human-aware artificial intelligence",
                "abstract": null
            },
            {
                "arxivId": "2304.13734",
                "title": "The Internal State of an LLM Knows When its Lying",
                "abstract": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\% to 83\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios."
            },
            {
                "arxivId": "2208.10264",
                "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."
            },
            {
                "arxivId": "1604.01529",
                "title": "Axiomatic Characterization of Committee Scoring Rules",
                "abstract": null
            },
            {
                "arxivId": "1506.02891",
                "title": "Properties of multiwinner voting rules",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.01820",
        "category": "q-fin",
        "title": "Signature Volatility Models: Pricing and Hedging With Fourier",
        "abstract": "We consider a stochastic volatility model where the dynamics of the volatility are given by a possibly infinite linear combination of the elements of the time extended signature of a Brownian motion. First, we show that the model is remarkably universal, as it includes, but is not limited to, the celebrated Stein-Stein, Bergomi, and Heston models, together with some path-dependent variants. Second, we derive the joint characteristic functional of the log-price and integrated variance provided that some infinite dimensional extended tensor algebra valued Riccati equation admits a solution. This allows us to price and (quadratically) hedge certain European and path-dependent options using Fourier inversion techniques. We highlight the efficiency and accuracy of these Fourier techniques in a comprehensive numerical study.",
        "references": [
            {
                "arxivId": "2401.02393",
                "title": "A PDE approach for solving the characteristic function of the generalised signature process",
                "abstract": "The signature of a path, as a fundamental object in Rough path theory, serves as a generating function for non-commutative monomials on path space. It transforms the path into a grouplike element in the tensor algebra space, summarising the path faithfully up to a generalised form of re-parameterisation (a negligible equivalence class in this context). Our paper concerns stochastic processes and studies the characteristic function of the path signature of the stochastic process. In contrast to the expected signature, it determines the law on the random signatures without any regularity condition. The computation of the characteristic function of the random signature offers potential applications in stochastic analysis and machine learning, where the expected signature plays an important role. In this paper, we focus on a time-homogeneous It\\^o diffusion process, and adopt a PDE approach to derive the characteristic function of its signature defined at any fixed time horizon. A key ingredient of our approach is the introduction of the generalised-signature process. This lifting enables us to establish the Feynman-Kac-type theorem for the characteristic function of the generalised-signature process by following the martingale approach. Moreover, as an application of our results, we present a novel derivation of the joint characteristic function of Brownian motion coupled with the L\\'evy area, leveraging the structure theorem of anti-symmetric matrices."
            },
            {
                "arxivId": "2301.13235",
                "title": "Joint calibration to SPX and VIX options with signature-based models",
                "abstract": "We consider a stochastic volatility model where the dynamics of the volatility are described by linear functions of the (time extended) signature of a primary underlying process, which is supposed to be some multidimensional continuous semimartingale. Under the additional assumption that this primary process is of polynomial type, we obtain closed form expressions for the VIX squared, exploiting the fact that the truncated signature of a polynomial process is again a polynomial process. Adding to such a primary process the Brownian motion driving the stock price, allows then to express both the log-price and the VIX squared as linear functions of the signature of the corresponding augmented process. This feature can then be e\ufb03ciently used for pricing and calibration purposes. Indeed, as the signature samples can be easily precomputed, the calibration task can be split into an o\ufb04ine sampling and a standard optimization. For both the SPX and VIX options we obtain highly accurate calibration results, showing that this model class allows to solve the joint calibration problem without adding jumps or rough volatility."
            },
            {
                "arxivId": "2212.10917",
                "title": "The Quintic Ornstein-Uhlenbeck Volatility Model that Jointly Calibrates SPX & VIX Smiles",
                "abstract": "The quintic Ornstein-Uhlenbeck volatility model is a stochastic volatility model where the volatility process is a polynomial function of degree five of a single Ornstein-Uhlenbeck process with fast mean reversion and large vol-of-vol. The model is able to achieve remarkable joint fits of the SPX-VIX smiles with only 6 effective parameters and an input curve that allows to match certain term structures. We provide several practical specifications of the input curve, study their impact on the joint calibration problem and consider additionally time-dependent parameters to help achieve better fits for longer maturities going beyond 1 year. Even better, the model remains very simple and tractable for pricing and calibration: the VIX squared is again polynomial in the Ornstein-Uhlenbeck process, leading to efficient VIX derivative pricing by a simple integration against a Gaussian density; simulation of the volatility process is exact; and pricing SPX products derivatives can be done efficiently and accurately by standard Monte Carlo techniques with suitable antithetic and control variates."
            },
            {
                "arxivId": "2212.08297",
                "title": "Joint SPX\u2013VIX Calibration With Gaussian Polynomial Volatility Models: Deep Pricing With Quantization Hints",
                "abstract": "We consider the joint SPX-VIX calibration within a general class of Gaussian polynomial volatility models in which the volatility of the SPX is assumed to be a polynomial function of a Gaussian Volterra process defined as a stochastic convolution between a kernel and a Brownian motion. By performing joint calibration to daily SPX-VIX implied volatility surface data between 2012 and 2022, we compare the empirical performance of different kernels and their associated Markovian and non-Markovian models, such as rough and non-rough path-dependent volatility models. In order to ensure an efficient calibration and a fair comparison between the models, we develop a generic unified method in our class of models for fast and accurate pricing of SPX and VIX derivatives based on functional quantization and Neural Networks. For the first time, we identify a \\textit{conventional one-factor Markovian continuous stochastic volatility model} that is able to achieve remarkable fits of the implied volatility surfaces of the SPX and VIX together with the term structure of VIX futures. What is even more remarkable is that our conventional one-factor Markovian continuous stochastic volatility model outperforms, in all market conditions, its rough and non-rough path-dependent counterparts with the same number of parameters."
            },
            {
                "arxivId": "2009.10972",
                "title": "The characteristic function of Gaussian stochastic volatility models: an analytic expression",
                "abstract": null
            },
            {
                "arxivId": "1905.00711",
                "title": "Non-parametric Pricing and Hedging of Exotic Derivatives",
                "abstract": "ABSTRACT In the spirit of Arrow\u2013Debreu, we introduce a family of financial derivatives that act as primitive securities in that exotic derivatives can be approximated by their linear combinations. We call these financial derivatives signature payoffs. We show that signature payoffs can be used to non-parametrically price and hedge exotic derivatives in the scenario where one has access to price data for other exotic payoffs. The methodology leads to a computationally tractable and accurate algorithm for pricing and hedging using market prices of a basket of exotic derivatives that has been tested on real and simulated market prices, obtaining good results."
            },
            {
                "arxivId": "1708.08796",
                "title": "Affine Volterra processes",
                "abstract": "We introduce affine Volterra processes, defined as solutions of certain stochastic convolution equations with affine coefficients. Classical affine diffusions constitute a special case, but affine Volterra processes are neither semimartingales, nor Markov processes in general. We provide explicit exponential-affine representations of the Fourier-Laplace functional in terms of the solution of an associated system of deterministic integral equations of convolution type, extending well-known formulas for classical affine diffusions. For specific state spaces, we prove existence, uniqueness, and invariance properties of solutions of the corresponding stochastic convolution equations. Our arguments avoid infinite-dimensional stochastic analysis as well as stochastic integration with respect to non-semimartingales, relying instead on tools from the theory of finite-dimensional deterministic convolution equations. Our findings generalize and clarify recent results in the literature on rough volatility models in finance."
            },
            {
                "arxivId": "1707.07124",
                "title": "A signature-based machine learning model for distinguishing bipolar disorder and borderline personality disorder",
                "abstract": null
            },
            {
                "arxivId": "1603.03788",
                "title": "A Primer on the Signature Method in Machine Learning",
                "abstract": "In these notes, we wish to provide an introduction to the signature method, focusing on its basic theoretical properties and recent numerical applications. \nThe notes are split into two parts. The first part focuses on the definition and fundamental properties of the signature of a path, or the path signature. We have aimed for a minimalistic approach, assuming only familiarity with classical real analysis and integration theory, and supplementing theory with straightforward examples. We have chosen to focus in detail on the principle properties of the signature which we believe are fundamental to understanding its role in applications. We also present an informal discussion on some of its deeper properties and briefly mention the role of the signature in rough paths theory, which we hope could serve as a light introduction to rough paths for the interested reader. \nThe second part of these notes discusses practical applications of the path signature to the area of machine learning. The signature approach represents a non-parametric way for extraction of characteristic features from data. The data are converted into a multi-dimensional path by means of various embedding algorithms and then processed for computation of individual terms of the signature which summarise certain information contained in the data. The signature thus transforms raw data into a set of features which are used in machine learning tasks. We will review current progress in applications of signatures to machine learning problems."
            },
            {
                "arxivId": "1502.04592",
                "title": "Hawkes Processes in Finance",
                "abstract": "In this paper we propose an overview of the recent academic literature devoted to the applications of Hawkes processes in finance. Hawkes processes constitute a particular class of multivariate point processes that has become very popular in empirical high-frequency finance this last decade. After a reminder of the main definitions and properties that characterize Hawkes processes, we review their main empirical applications to address many different problems in high-frequency finance. Because of their great flexibility and versatility, we show that they have been successfully involved in issues as diverse as estimating the volatility at the level of transaction data, estimating the market stability, accounting for systemic risk contagion, devising optimal execution strategies or capturing the dynamics of the full order book."
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.01951",
        "category": "q-fin",
        "title": "Sparse spanning portfolios and under-diversification with second-order stochastic dominance",
        "abstract": "We develop and implement methods for determining whether relaxing sparsity constraints on portfolios improves the investment opportunity set for risk-averse investors. We formulate a new estimation procedure for sparse second-order stochastic spanning based on a greedy algorithm and Linear Programming. We show the optimal recovery of the sparse solution asymptotically whether spanning holds or not. From large equity datasets, we estimate the expected utility loss due to possible under-diversification, and find that there is no benefit from expanding a sparse opportunity set beyond 45 assets. The optimal sparse portfolio invests in 10 industry sectors and cuts tail risk when compared to a sparse mean-variance portfolio. On a rolling-window basis, the number of assets shrinks to 25 assets in crisis periods, while standard factor models cannot explain the performance of the sparse portfolios.",
        "references": [
            {
                "arxivId": "1703.02100",
                "title": "Guarantees for Greedy Maximization of Non-submodular Functions with Applications",
                "abstract": "We investigate the performance of the standard GREEDY algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of GREEDY for maximizing submodular functions, there are few guarantees for non-submodular ones. However, GREEDY enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by a combination of the (generalized) curvature \u03b1 and the sub-modularity ratio \u03b3. In particular, we prove that GREEDY enjoys a tight approximation guarantee of 1/\u03b1 (1 - e-\u03b3\u03b1) for cardinality constrained maximization. In addition, we bound the submod-ularity ratio and curvature for several important real-world objectives, including the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications."
            },
            {
                "arxivId": "1102.3975",
                "title": "Submodular meets Spectral: Greedy Algorithms for Subset Selection, Sparse Approximation and Dictionary Selection",
                "abstract": "We study the problem of selecting a subset of k random variables from a large set, in order to obtain the best linear prediction of another variable of interest. This problem can be viewed in the context of both feature selection and sparse approximation. We analyze the performance of widely used greedy heuristics, using insights from the maximization of submodular functions and spectral analysis. We introduce the submodularity ratio as a key quantity to help understand why greedy algorithms perform well even when the variables are highly correlated. Using our techniques, we obtain the strongest known approximation guarantees for this problem, both in terms of the submodularity ratio and the smallest k-sparse eigenvalue of the covariance matrix. We further demonstrate the wide applicability of our techniques by analyzing greedy algorithms for the dictionary selection problem, and significantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; the experiments show that the submodularity ratio is a stronger predictor of the performance of greedy algorithms than other spectral parameters."
            },
            {
                "arxivId": "0708.0046",
                "title": "Sparse and stable Markowitz portfolios",
                "abstract": "We consider the problem of portfolio selection within the classical Markowitz mean-variance framework, reformulated as a constrained least-squares regression problem. We propose to add to the objective function a penalty proportional to the sum of the absolute values of the portfolio weights. This penalty regularizes (stabilizes) the optimization problem, encourages sparse portfolios (i.e., portfolios with only few active positions), and allows accounting for transaction costs. Our approach recovers as special cases the no-short-positions portfolios, but does allow for short positions in limited number. We implement this methodology on two benchmark data sets constructed by Fama and French. Using only a modest amount of training data, we construct portfolios whose out-of-sample performance, as measured by Sharpe ratio, is consistently and significantly better than that of the na\u00efve evenly weighted portfolio."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.02197",
        "category": "q-fin",
        "title": "Numerical solution to a Parabolic-ODE Solow model with spatial diffusion and technology-induced motility",
        "abstract": "This work studies a parabolic-ODE PDE's system which describes the evolution of the physical capital\"$k$\"and technological progress\"$A$\", using a meshless in one and two dimensional bounded domain with regular boundary. The well-known Solow model is extended by considering the spatial diffusion of both capital anf technology. Moreover, we study the case in which no spatial diffusion of the technology progress occurs. For such models, we propound schemes based on the Generalized Finite Difference method and proof the convergence of the numerical solution to the continuous one. Several examples show the dynamics of the model for a wide range of parameters. These examples illustrate the accuary of the numerical method.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.02315",
        "category": "q-fin",
        "title": "A Survey of Large Language Models in Finance (FinLLMs)",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services. Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges. Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets. In addition, we provide eight advanced financial NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency. To support AI research in finance, we compile a collection of accessible datasets and evaluation benchmarks on GitHub.",
        "references": [
            {
                "arxivId": "2310.04793",
                "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
                "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs)."
            },
            {
                "arxivId": "2311.10723",
                "title": "Large Language Models in Finance: A Survey",
                "abstract": "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI."
            },
            {
                "arxivId": "2309.13064",
                "title": "InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning",
                "abstract": "We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned using a small set of carefully curated instructions on a well-trained foundation model, which is consistent with the Superficial Alignment Hypothesis (Zhou et al., 2023). From a practical perspective, this work develops a state-of-the-art financial domain LLM with superior capability in understanding financial texts and providing helpful investment advice, potentially enhancing the work efficiency of financial professionals. We release the model parameters to the research community."
            },
            {
                "arxivId": "2304.13712",
                "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
                "abstract": "This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai."
            },
            {
                "arxivId": "2302.03241",
                "title": "Continual Pre-training of Language Models",
                "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method."
            },
            {
                "arxivId": "2301.09279",
                "title": "StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series",
                "abstract": "There has been growing interest in applying NLP techniques in the financial domain, however, resources are extremely limited. This paper introduces StockEmotions, a new dataset for detecting emotions in the stock market that consists of 10,000 English comments collected from StockTwits, a financial social media platform. Inspired by behavioral finance, it proposes 12 fine-grained emotion classes that span the roller coaster of investor emotion. Unlike existing financial sentiment datasets, StockEmotions presents granular features such as investor sentiment classes, fine-grained emotions, emojis, and time series data. To demonstrate the usability of the dataset, we perform a dataset analysis and conduct experimental downstream tasks. For financial sentiment/emotion classification tasks, DistilBERT outperforms other baselines, and for multivariate time series forecasting, a Temporal Attention LSTM model combining price index, text, and emotion features achieves the best performance than using a single feature."
            },
            {
                "arxivId": "2211.00083",
                "title": "When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain",
                "abstract": "Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface."
            },
            {
                "arxivId": "2210.12467",
                "title": "ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts",
                "abstract": "Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial documents, discussing facts and figures, have largely been unexplored, majorly due to the unavailability of suitable datasets. In this work, we present ECTSum, a new dataset with transcripts of earnings calls (ECTs), hosted by publicly traded companies, as documents, and experts-written short telegram-style bullet point summaries derived from corresponding Reuters articles. ECTs are long unstructured documents without any prescribed length limit or format. We benchmark our dataset with state-of-the-art summarization methods across various metrics evaluating the content quality and factual consistency of the generated summaries. Finally, we present a simple yet effective approach, ECT-BPS, to generate a set of bullet points that precisely capture the important facts discussed in the calls."
            },
            {
                "arxivId": "2210.03849",
                "title": "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering",
                "abstract": "With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA."
            },
            {
                "arxivId": "2109.00122",
                "title": "FinQA: A Dataset of Numerical Reasoning over Financial Data",
                "abstract": "The sheer volume of financial statements makes it difficult for humans to access and analyze a business\u2019s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset \u2013 the first of its kind \u2013 should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA."
            },
            {
                "arxivId": "2108.07258",
                "title": "On the Opportunities and Risks of Foundation Models",
                "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature."
            },
            {
                "arxivId": "2107.13586",
                "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist."
            },
            {
                "arxivId": "2105.12825",
                "title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading",
                "abstract": "In this paper, we introduce an event-driven trading strategy that predicts stock movements by detecting corporate events from news articles. Unlike existing models that utilize textual features (e.g., bag-of-words) and sentiments to directly make stock predictions, we consider corporate events as the driving force behind stock movements and aim to profit from the temporary stock mispricing that may occur when corporate events take place. The core of the proposed strategy is a bi-level event detection model. The low-level event detector identifies events' existences from each token, while the high-level event detector incorporates the entire article's representation and the low-level detected results to discover events at the article-level. We also develop an elaborately-annotated dataset EDT for corporate event detection and news-based stock prediction benchmark. EDT includes 9721 news articles with token-level event labels as well as 303893 news articles with minute-level timestamps and comprehensive stock price labels. Experiments on EDT indicate that the proposed strategy outperforms all the baselines in winning rate, excess returns over the market, and the average return on each transaction."
            },
            {
                "arxivId": "2012.02505",
                "title": "The Financial Document Causality Detection Shared Task (FinCausal 2020)",
                "abstract": "We present the FinCausal 2020 Shared Task on Causality Detection in Financial Documents and the associated FinCausal dataset, and discuss the participating systems and results. Two sub-tasks are proposed: a binary classification task (Task 1) and a relation extraction task (Task 2). A total of 16 teams submitted runs across the two Tasks and 13 of them contributed with a system description paper. This workshop is associated to the Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020), held at The 28th International Conference on Computational Linguistics (COLING\u20192020), Barcelona, Spain on September 12, 2020."
            },
            {
                "arxivId": "2006.08097",
                "title": "FinBERT: A Pretrained Language Model for Financial Communications",
                "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at this https URL. We hope this will be useful for practitioners and researchers working on financial NLP tasks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-06.json",
        "arxivId": "2402.02714",
        "category": "q-fin",
        "title": "Neural option pricing for rough Bergomi model",
        "abstract": "The rough Bergomi (rBergomi) model can accurately describe the historical and implied volatilities, and has gained much attention in the past few years. However, there are many hidden unknown parameters or even functions in the model. In this work, we investigate the potential of learning the forward variance curve in the rBergomi model using a neural SDE. To construct an efficient solver for the neural SDE, we propose a novel numerical scheme for simulating the volatility process using the modified summation of exponentials. Using the Wasserstein 1-distance to define the loss function, we show that the learned forward variance curve is capable of calibrating the price process of the underlying asset and the price of the European-style options simultaneously. Several numerical tests are provided to demonstrate its performance.",
        "references": [
            {
                "arxivId": "1801.10359",
                "title": "Multifactor Approximation of Rough Volatility Models",
                "abstract": "Rough volatility models are very appealing because of their remarkable fit of both historical and implied volatilities. However, due to the non-Markovian and non-semimartingale nature of the volatility process, there is no simple way to simulate efficiently such models, which makes risk management of derivatives an intricate task. In this paper, we design tractable multi-factor stochastic volatility models approximating rough volatility models and enjoying a Markovian structure. Furthermore, we apply our procedure to the specific case of the rough Heston model. This in turn enables us to derive a numerical method for solving fractional Riccati equations appearing in the characteristic function of the log-price in this setting."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-07.json",
        "arxivId": "2402.03338",
        "category": "q-fin",
        "title": "CNN-DRL with Shuffled Features in Finance",
        "abstract": "In prior methods, it was observed that the application of Convolutional Neural Networks agent in Deep Reinforcement Learning to financial data resulted in an enhanced reward. In this study, a specific permutation was applied to the feature vector, thereby generating a CNN matrix that strategically positions more pertinent features in close proximity. Our comprehensive experimental evaluations unequivocally demonstrate a substantial enhancement in reward attainment.",
        "references": [
            {
                "arxivId": "2211.03107",
                "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning",
                "abstract": "Finance is a particularly difficult playground for deep reinforcement learning. However, establishing high-quality market environments and benchmarks for financial reinforcement learning is challenging due to three major factors, namely, low signal-to-noise ratio of financial data, survivorship bias of historical data, and model overfitting in the backtesting stage. In this paper, we present an openly accessible FinRL-Meta library that has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we will provide hundreds of market environments through an automatic pipeline that collects dynamic datasets from real-world markets and processes them into gym-style market environments. Second, we reproduce popular papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, FinRL-Meta provides tens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. FinRL-Meta is available at: https://github.com/AI4Finance-Foundation/FinRL-Meta"
            },
            {
                "arxivId": "2110.06829",
                "title": "Towards a fully rl-based market simulator",
                "abstract": "We present a new financial framework where two families of RL-based agents representing the Liquidity Providers and Liquidity Takers learn simultaneously to satisfy their objective. Thanks to a parametrized reward formulation and the use of Deep RL, each group learns a shared policy able to generalize and interpolate over a wide range of behaviors. This is a step towards a fully RL-based market simulator replicating complex market conditions particularly suited to study the dynamics of the financial market under various scenarios."
            },
            {
                "arxivId": "1811.07522",
                "title": "Practical Deep Reinforcement Learning Approach for Stock Trading",
                "abstract": "Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns."
            },
            {
                "arxivId": "1707.06347",
                "title": "Proximal Policy Optimization Algorithms",
                "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2009.06350",
        "category": "q-fin",
        "title": "Upstreamness and downstreamness in input-output analysis from local and aggregate information",
        "abstract": "Ranking sectors and countries within global value chains is of paramount importance to estimate risks and forecast growth in large economies. However, this task is often non-trivial due to the lack of complete and accurate information on the flows of money and goods between sectors and countries, which are encoded in Input-Output (I-O) tables. In this work, we show that an accurate estimation of the role played by sectors and countries in supply chain networks can be achieved without full knowledge of the I-O tables, but only relying on local and aggregate information, e.g., the total intermediate demand per sector. Our method, based on a rank-$1$ approximation to the I-O table, shows consistently good performance in reconstructing rankings (i.e., upstreamness and downstreamness measures for countries and sectors) when tested on empirical data from the World Input-Output Database. Moreover, we connect the accuracy of our approximate framework with the spectral properties of the I-O tables, which ordinarily exhibit relatively large spectral gaps. Our approach provides a fast and analytical tractable framework to rank constituents of a complex economy without the need of matrix inversions and the knowledge of finer intersectorial details.",
        "references": [
            {
                "arxivId": "2106.02730",
                "title": "\"Spectrally gapped\" random walks on networks: a Mean First Passage Time formula",
                "abstract": "We derive an approximate but explicit formula for the Mean First\nPassage Time of a random walker between a source and a target node of a\ndirected and weighted network. The formula does not require any matrix\ninversion, and it takes as only input the transition probabilities into\nthe target node. It is derived from the calculation of the average\nresolvent of a deformed ensemble of random sub-stochastic matrices\nH=\\langle H\\rangle +\\delta HH=\u27e8H\u27e9+\u03b4H,\nwith \\langle H\\rangle\u27e8H\u27e9\nrank-11\nand non-negative. The accuracy of the formula depends on the spectral\ngap of the reduced transition matrix, and it is tested numerically on\nseveral instances of (weighted) networks away from the high sparsity\nregime, with an excellent agreement."
            },
            {
                "arxivId": "1901.09629",
                "title": "May's Instability in Large Economies",
                "abstract": "Will a large economy be stable? Building on Robert May's original argument for large ecosystems, we conjecture that evolutionary and behavioural forces conspire to drive the economy towards marginal stability. We study networks of firms in which inputs for production are not easily substitutable, as in several real-world supply chains. Relying on results from random matrix theory, we argue that such networks generically become dysfunctional when their size increases, when the heterogeneity between firms becomes too strong, or when substitutability of their production inputs is reduced. At marginal stability and for large heterogeneities, we find that the distribution of firm sizes develops a power-law tail, as observed empirically. Crises can be triggered by small idiosyncratic shocks, which lead to \"avalanches\" of defaults characterized by a power-law distribution of total output losses. This scenario would naturally explain the well-known \"small shocks, large business cycles\" puzzle, as anticipated long ago by Bak, Chen, Scheinkman, and Woodford."
            },
            {
                "arxivId": "1806.06941",
                "title": "Reconstruction methods for networks: The case of economic and financial systems",
                "abstract": null
            },
            {
                "arxivId": "1411.7613",
                "title": "Systemic Risk Analysis on Reconstructed Economic and Financial Networks",
                "abstract": null
            },
            {
                "arxivId": "1407.0225",
                "title": "World Input-Output Network",
                "abstract": "Production systems, traditionally analyzed as almost independent national systems, are increasingly connected on a global scale. Only recently becoming available, the World Input-Output Database (WIOD) is one of the first efforts to construct the global multi-regional input-output (GMRIO) tables. By viewing the world input-output system as an interdependent network where the nodes are the individual industries in different economies and the edges are the monetary goods flows between industries, we analyze respectively the global, regional, and local network properties of the so-called world input-output network (WION) and document its evolution over time. At global level, we find that the industries are highly but asymmetrically connected, which implies that micro shocks can lead to macro fluctuations. At regional level, we find that the world production is still operated nationally or at most regionally as the communities detected are either individual economies or geographically well defined regions. Finally, at local level, for each industry we compare the network-based measures with the traditional methods of backward linkages. We find that the network-based measures such as PageRank centrality and community coreness measure can give valuable insights into identifying the key industries."
            },
            {
                "arxivId": "1108.2590",
                "title": "A Network Analysis of Countries\u2019 Export Flows: Firm Grounds for the Building Blocks of the Economy",
                "abstract": "In this paper we analyze the bipartite network of countries and products from UN data on country production. We define the country-country and product-product projected networks and introduce a novel method of filtering information based on elements\u2019 similarity. As a result we find that country clustering reveals unexpected socio-geographic links among the most competing countries. On the same footings the products clustering can be efficiently used for a bottom-up classification of produced goods. Furthermore we mathematically reformulate the \u201creflections method\u201d introduced by Hidalgo and Hausmann as a fixpoint problem; such formulation highlights some conceptual weaknesses of the approach. To overcome such an issue, we introduce an alternative methodology (based on biased Markov chains) that allows to rank countries in a conceptually consistent way. Our analysis uncovers a strong non-linear interaction between the diversification of a country and the ubiquity of its products, thus suggesting the possible need of moving towards more efficient and direct non-linear fixpoint algorithms to rank countries and products in the global market."
            },
            {
                "arxivId": "0909.3890",
                "title": "The building blocks of economic complexity",
                "abstract": "For Adam Smith, wealth was related to the division of labor. As people and firms specialize in different activities, economic efficiency increases, suggesting that development is associated with an increase in the number of individual activities and with the complexity that emerges from the interactions between them. Here we develop a view of economic growth and development that gives a central role to the complexity of a country's economy by interpreting trade data as a bipartite network in which countries are connected to the products they export, and show that it is possible to quantify the complexity of a country's economy by characterizing the structure of this network. Furthermore, we show that the measures of complexity we derive are correlated with a country's level of income, and that deviations from this relationship are predictive of future growth. This suggests that countries tend to converge to the level of income dictated by the complexity of their productive structures, indicating that development efforts should focus on generating the conditions that would allow complexity to emerge to generate sustained growth and prosperity."
            },
            {
                "arxivId": "0708.2090",
                "title": "The Product Space Conditions the Development of Nations",
                "abstract": "Economies grow by upgrading the products they produce and export. The technology, capital, institutions, and skills needed to make newer products are more easily adapted from some products than from others. Here, we study this network of relatedness between products, or \u201cproduct space,\u201d finding that more-sophisticated products are located in a densely connected core whereas less-sophisticated products occupy a less-connected periphery. Empirically, countries move through the product space by developing goods close to those they currently produce. Most countries can reach the core only by traversing empirically infrequent distances, which may help explain why poor countries have trouble developing more competitive exports and fail to converge to the income levels of rich countries."
            },
            {
                "arxivId": "0705.0010",
                "title": "Critical phenomena in complex networks",
                "abstract": "The combination of the compactness of networks, featuring small diameters, and their complex architectures results in a variety of critical effects dramatically different from those in cooperative systems on lattices. In the last few years, important steps have been made toward understanding the qualitatively new critical phenomena in complex networks. The results, concepts, and methods of this rapidly developing field are reviewed. Two closely related classes of these critical phenomena are considered, namely, structural phase transitions in the network architectures and transitions in cooperative models on networks as substrates. Systems where a network and interacting agents on it influence each other are also discussed. A wide range of critical phenomena in equilibrium and growing networks including the birth of the giant connected component, percolation, $k$-core percolation, phenomena near epidemic thresholds, condensation transitions, critical phenomena in spin models placed on networks, synchronization, and self-organized criticality effects in interacting systems on networks are mentioned. Strong finite-size effects in these systems and open problems and perspectives are also discussed."
            },
            {
                "arxivId": "cond-mat/0405566",
                "title": "Statistical mechanics of networks.",
                "abstract": "We study the family of network models derived by requiring the expected properties of a graph ensemble to match a given set of measurements of a real-world network, while maximizing the entropy of the ensemble. Models of this type play the same role in the study of networks as is played by the Boltzmann distribution in classical statistical mechanics; they offer the best prediction of network properties subject to the constraints imposed by a given set of observations. We give exact solutions of models within this class that incorporate arbitrary degree distributions and arbitrary but independent edge probabilities. We also discuss some more complex examples with correlated edges that can be solved approximately or exactly by adapting various familiar methods, including mean-field theory, perturbation theory, and saddle-point expansions."
            },
            {
                "arxivId": "cond-mat/0204455",
                "title": "Mean field solution of the Ising model on a Barab\u00e1si\u2013Albert network",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2212.14259",
        "category": "q-fin",
        "title": "Bipolar Theorems for Sets of Non-negative Random Variables",
        "abstract": "This paper assumes a robust, in general not dominated, probabilistic framework and provides necessary and sufficient conditions for a bipolar representation of subsets of the set of all quasi-sure equivalence classes of non-negative random variables without any further conditions on the underlying measure space. This generalises and unifies existing bipolar theorems proved under stronger assumptions on the robust framework. Applications are in areas of robust financial modeling which we discuss throughout the paper.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2303.10019",
        "category": "q-fin",
        "title": "Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices",
        "abstract": "This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, considering dependencies between quantiles and marginals through a smoothing procedure that allows for online learning. We discuss two smoothing methods: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. The procedure uses horizontal aggregation, i.e., aggregation across quantiles. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. We apply the proposed methodology to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss the temporal evolution of the weights and hyperparameters and present the results of reduced versions of the preferred model. A fast C++ implementation of the proposed algorithm is provided in the open-source R-Package profoc on CRAN.",
        "references": [
            {
                "arxivId": "2002.09578",
                "title": "Scores for Multivariate Distributions and Level Sets",
                "abstract": "Evaluating Forecasts of Multivariate Probability Distributions Forecasts of multivariate probability distributions are required for a variety of applications. The availability of a score for a forecast is important for evaluating prediction accuracy, as well as estimating model parameters. In \u201cScores for Multivariate Distributions and Level Sets,\u201d X. Meng, J. W. Taylor, S. Ben Taieb, and S. Li propose a theoretical framework that encompasses several existing scores for multivariate distributions and can be used to generate new scores. In some multivariate contexts, a forecast of a level set is needed, such as a density level set for anomaly detection or the level set of the cumulative distribution, which can be used as a measure of risk. This motivates consideration of scores for level sets. The authors show that such scores can be obtained by decomposing the scores developed for multivariate distributions. A simple numerical algorithm is presented to compute the scores, and practical applications are provided in the contexts of conditional value-at-risk for financial data and the combination of expert macroeconomic forecasts."
            },
            {
                "arxivId": "0805.0056",
                "title": "Quantile tomography: using quantiles with multivariate data",
                "abstract": "The use of quantiles to obtain insights about multivariate data is ad- dressed. It is argued that incisive insights can be obtained by considering direc- tional quantiles, the quantiles of projections. Directional quantile envelopes are proposed as a way to condense this kind of information; it is demonstrated that they are essentially halfspace (Tukey) depth levels sets, coinciding for elliptic distri- butions (in particular multivariate normal) with density contours. Relevant ques- tions concerning their indexing, the possibility of the reverse retrieval of directional quantile information, invariance with respect to affine transformations, and approx- imation/asymptotic properties are studied. It is argued that analysis in terms of directional quantiles and their envelopes offers a straightforward probabilistic inter- pretation and thus conveys a concrete quantitative meaning; the directional defini- tion can be adapted to elaborate frameworks, like estimation of extreme quantiles and directional quantile regression, the regression of depth contours on covariates. The latter facilitates the construction of multivariate growth charts\u2014the question that motivated this development."
            },
            {
                "arxivId": "0704.3649",
                "title": "QUANTILE AND PROBABILITY CURVES WITHOUT CROSSING",
                "abstract": "This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural quantile functions using data on Vietnam veteran status and earnings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.01938",
        "category": "q-fin",
        "title": "Present Value of the Future Consumer Goods Multiplier",
        "abstract": "In this paper, we derive a formula for the present value of future consumer goods multiplier based on the assumption that a constant share of investment in the production of consumer goods is expected. The present value appears to be an infinite geometric sequence. Moreover, we investigate how the notion of the multiplier can help us in macroeconomic analysis of capital and investment dynamics and in understanding some general principles of capital market equilibrium. Using the concept of this multiplier, we build a macroeconomic model of capital market dynamics which is consistent with the implications of classical models and with the market equilibrium condition but gives additional quantitative and qualitative predictions regarding the dynamics of shares of investment into the production of consumer goods and the production of means of production. The investment volume is modeled as a function of the multiplier: investments adjust when the value of the multiplier fluctuates around its equilibrium value of one. In addition, we suggest possible connections between the investment volume and the multiplier value in the form of differential equations. We also present the formula for the rate of growth of the multiplier. Independently of the implications of capital market dynamics models, the formula for the multiplier itself can be applied for the evaluation of the present value of capital or the estimation of the macroeconomic impact of changes in investment volumes. Our findings show that both the exponential and hyperbolic discounting in combination with empirical evidence available lead to the value of the multiplier that is close to one.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.04429",
        "category": "q-fin",
        "title": "Meritocracy and Its Discontents: Long-run Effects of Repeated School Admission Reforms",
        "abstract": "What happens if selective colleges change their admission policies? We answer this question by analyzing the world's first introduction of nationally centralized meritocratic admissions in the early twentieth century. We find a persistent meritocracy-equity tradeoff. Compared to the decentralized system, the centralized system admitted more high-achieving applicants, producing a greater number of top elite bureaucrats in the long run. However, this impact came at the distributional cost of equal regional access to elite higher education and career advancement. Several decades later, the meritocratic centralization increased the number of urban-born career elites (e.g., top income earners) relative to rural-born ones.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.04472",
        "category": "q-fin",
        "title": "Healthcare Quality by Specialists under a Mixed Compensation System: an Empirical Analysis",
        "abstract": "We analyze the effects of a mixed compensation (MC) scheme for specialists on the quality of their healthcare services. To do so, we exploit a major reform that was implemented in Quebec (Canada) in 1999. The government introduced a payment mechanism combining a fixed per diem with a reduced fee per clinical service. Using panel patient-doctor data covering the period 1996-2016 and including 320,441 patients, we estimate a multi-state multi-spell hazard model with correlated heterogeneity, analogous to a difference-in-differences approach. We compute three output-based quality indicators from our model. Our results suggest that the reform reduced the quality of MC specialist services as measured by the risk of re-hospitalization within 30 days after discharge and the risk of mortality within one year after discharge. These effects vary depending upon the specialty of the treating doctor.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.04474",
        "category": "q-fin",
        "title": "The Role of Child Gender in the Formation of Parents' Social Networks",
        "abstract": "Social networks play an important role in various aspects of life. While extensive research has explored factors such as gender, race, and education in network formation, one dimension that has received less attention is the gender of one's child. Children tend to form friendships with same-gender peers, potentially leading their parents to interact based on their child's gender. Focusing on households with children aged 3-5, we leverage a rich dataset from rural Bangladesh to investigate the role of children's gender in parental network formation. We estimate an equilibrium model of network formation that considers a child's gender alongside other socioeconomic factors. Counterfactual analyses reveal that children's gender significantly shapes parents' network structure. Specifically, if all children share the same gender, households would have approximately 15% more links, with a stronger effect for families having girls. Importantly, the impact of children's gender on network structure is on par with or even surpasses that of factors such as income distribution, parental occupation, education, and age. These findings carry implications for debates surrounding coed versus single-sex schools, as well as policies that foster inter-gender social interactions among children.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.04662",
        "category": "q-fin",
        "title": "Token vs Equity for Startup Financing",
        "abstract": "Why would a blockchain-based startup and its venture capital investors choose to finance by issuing tokens instead of equity? What would be their rates of return for each asset? This paper focuses on the liquidity difference between the two fundraising methods. I build a three-period model of an entrepreneur, two types of investors, and users. Some investors have unforeseen liquidity needs in the middle period that can only be met with tokens. The entrepreneur obtains higher payoff by issuing tokens instead of equity, and the payoff difference increases with investors risk-aversion and need for liquidity in the middle period, as well as the depth of the token market.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.04740",
        "category": "q-fin",
        "title": "Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes",
        "abstract": "An extension of the Hawkes process, the Marked Hawkes process distinguishes itself by featuring variable jump size across each event, in contrast to the constant jump size observed in a Hawkes process without marks. While extensive literature has been dedicated to the non-parametric estimation of both the linear and non-linear Hawkes process, there remains a significant gap in the literature regarding the marked Hawkes process. In response to this, we propose a methodology for estimating the conditional intensity of the marked Hawkes process. We introduce two distinct models: \\textit{Shallow Neural Hawkes with marks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these approaches take the past arrival times and their corresponding marks as the input to obtain the arrival intensity. This approach is entirely non-parametric, preserving the interpretability associated with the marked Hawkes process. To validate the efficacy of our method, we subject the method to synthetic datasets with known ground truth. Additionally, we apply our method to model cryptocurrency order book data, demonstrating its applicability to real-world scenarios.",
        "references": [
            {
                "arxivId": "2401.09361",
                "title": "Neural Hawkes: Non-Parametric Estimation in High Dimension and Causality Analysis in Cryptocurrency Markets",
                "abstract": "We propose a novel approach to marked Hawkes kernel inference which we name the moment-based neural Hawkes estimation method. Hawkes processes are fully characterized by their first and second order statistics through a Fredholm integral equation of the second kind. Using recent advances in solving partial differential equations with physics-informed neural networks, we provide a numerical procedure to solve this integral equation in high dimension. Together with an adapted training pipeline, we give a generic set of hyperparameters that produces robust results across a wide range of kernel shapes. We conduct an extensive numerical validation on simulated data. We finally propose two applications of the method to the analysis of the microstructure of cryptocurrency markets. In a first application we extract the influence of volume on the arrival rate of BTC-USD trades and in a second application we analyze the causality relationships and their directions amongst a universe of 15 cryptocurrency pairs in a centralized exchange."
            },
            {
                "arxivId": "2303.03073",
                "title": "A neural network based model for multi-dimensional nonlinear Hawkes processes",
                "abstract": "This paper introduces the Neural Network for Nonlinear Hawkes processes (NNNH), a non-parametric method based on neural networks to fit nonlinear Hawkes processes. Our method is suitable for analyzing large datasets in which events exhibit both mutually-exciting and inhibitive patterns. The NNNH approach models the individual kernels and the base intensity of the nonlinear Hawkes process using feed forward neural networks and jointly calibrates the parameters of the networks by maximizing the log-likelihood function. We utilize Stochastic Gradient Descent to search for the optimal parameters and propose an unbiased estimator for the gradient, as well as an efficient computation method. We demonstrate the flexibility and accuracy of our method through numerical experiments on both simulated and real-world data, and compare it with state-of-the-art methods. Our results highlight the effectiveness of the NNNH method in accurately capturing the complexities of nonlinear Hawkes processes."
            },
            {
                "arxivId": "2209.04792",
                "title": "A point process model for rare event detection",
                "abstract": "Detecting rare events, those de\ufb01ned to give rise to high impact but have a low probability of occurring, is a challenge in a number of domains including meteorological, environmen-tal, \ufb01nancial and economic. The use of machine learning to detect such events is becoming increasingly popular, since they o\ufb00er an e\ufb00ective and scalable solution when compared to traditional signature-based detection methods. In this work, we begin by undertaking exploratory data analysis, and present techniques that can be used in a framework for employing machine learning methods for rare event detection. Strategies to deal with the imbalance of classes including the selection of performance metrics are also discussed. Despite their popularity, we believe the performance of conventional machine learning classi\ufb01ers could be further improved, since they are agnostic to the natural order over time in which the events occur. Stochastic processes on the other hand, model sequences of events by exploiting their temporal structure such as clustering and dependence between the di\ufb00erent types of events. We develop a model for classi\ufb01cation based on Hawkes processes and apply it to a dataset of e-commerce transactions, resulting in not only better predictive performance but also deriving inferences regarding the temporal dynamics of the data. In this paper, we present techniques that build towards a general framework for employing machine learning based models for rare event detection. Using a publicly available dataset of e-commerce transactions, we \ufb01rst illustrate how one can develop a better understanding of such data using a series of visualisations. Strategies to deal with the imbalance of the classes including the crucial selection of performance metrics for evaluating models are also discussed."
            },
            {
                "arxivId": "2006.02460",
                "title": "Shallow Neural Hawkes: Non-parametric kernel estimation for Hawkes processes",
                "abstract": null
            },
            {
                "arxivId": "1909.12127",
                "title": "Intensity-Free Learning of Temporal Point Processes",
                "abstract": "Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function. However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. We draw on the literature on normalizing flows to design models that are flexible and efficient. We additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form. The proposed models achieve state-of-the-art performance in standard prediction tasks and are suitable for novel applications, such as learning sequence embeddings and imputing missing data."
            },
            {
                "arxivId": "1907.12025",
                "title": "Marked Hawkes process modeling of price dynamics and volatility estimation",
                "abstract": null
            },
            {
                "arxivId": "1811.06321",
                "title": "Multivariate Spatiotemporal Hawkes Processes and Network Reconstruction",
                "abstract": "There is often latent network structure in spatial and temporal data and the tools of network analysis can yield fascinating insights into such data. In this paper, we develop a nonparametric method for network reconstruction from spatiotemporal data sets using multivariate Hawkes processes. In contrast to prior work on network reconstruction with point-process models, which has often focused on exclusively temporal information, our approach uses both temporal and spatial information and does not assume a specific parametric form of network dynamics. This leads to an effective way of recovering an underlying network. We illustrate our approach using both synthetic networks and networks constructed from real-world data sets (a location-based social media network, a narrative of crime events, and violent gang crimes). Our results demonstrate that, in comparison to using only temporal data, our spatiotemporal approach yields improved network reconstruction, providing a basis for meaningful subsequent analysis --- such as community structure and motif analysis --- of the reconstructed networks."
            },
            {
                "arxivId": "1802.09304",
                "title": "Marked self-exciting point process modelling of information diffusion on Twitter",
                "abstract": "Information diffusion occurs on microblogging platforms like Twitter as retweet cascades. When a tweet is posted, it may be retweeted and henceforth further retweeted, and the retweeting process continues iteratively and indefinitely. A natural measure of the popularity of a tweet is the number of retweets it generates. Accurate predictions of tweet popularity can assist Twitter to rank contents more effectively and facilitate the assessment of potential for marketing and campaigning strategies. In this paper, we propose a model called the Marked Self-Exciting Process with Time-Dependent Excitation Function, or MaSEPTiDE for short, to model the retweeting dynamics and to predict the tweet popularity. Our model does not require expensive feature engineering but is capable of leveraging the observed dynamics to accurately predict the future evolution of retweet cascades. We apply our proposed methodology on a large amount of Twitter data and report substantial improvement in prediction performance over existing approaches in the literature."
            },
            {
                "arxivId": "1705.08051",
                "title": "Wasserstein Learning of Deep Generative Point Process Models",
                "abstract": "Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones."
            },
            {
                "arxivId": "1603.09449",
                "title": "TiDeH: Time-Dependent Hawkes Process for Predicting Retweet Dynamics",
                "abstract": "\n \n Online social networking services allow their users to post content in the form of text, images or videos. The main mechanism driving content diffusion is the possibility for users to re-share the content posted by their social connections, which may then cascade across the system. A fundamental problem when studying information cascades is the possibility to develop sound mathematical models, whose parameters can be calibrated on empirical data, in order to predict the future course of a cascade after a window of observation. In this paper, we focus on Twitter and, in particular, on the temporal patterns of retweet activity for an original tweet. We model the system by Time-Dependent Hawkes process (TiDeH), which properly takes into account the circadian nature of the users and the aging of information. The input of the prediction model are observed retweet times and structural information about the underlying social network. We develop a procedure for parameter optimization and for predicting the future profiles of retweet activity at different time resolutions. We validate our methodology on a large corpus of Twitter data and demonstrate its systematic improvement over existing approaches in all the time regimes.\n \n"
            },
            {
                "arxivId": "1602.07663",
                "title": "The role of volume in order book dynamics: a multivariate Hawkes process analysis",
                "abstract": "Abstract We show that multivariate Hawkes processes coupled with the nonparametric estimation procedure first proposed in Bacry and Muzy [IEEE Trans. Inform. Theory, 2016, 62, 2184\u20132202] can be successfully used to study complex interactions between the time of arrival of orders and their size observed in a limit order book market. We apply this methodology to high-frequency order book data of futures traded at EUREX. Specifically, we demonstrate how this approach is amenable not only to analyse interplay between different order types (market orders, limit orders, cancellations) but also to include other relevant quantities, such as the order size, into the analysis, showing also that simple models assuming the independence between volume and time are not suitable to describe the data."
            },
            {
                "arxivId": "1211.5063",
                "title": "On the difficulty of training recurrent neural networks",
                "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."
            },
            {
                "arxivId": "1211.4157",
                "title": "Modeling First Line Of An Order Book With Multivariate Marked Point Processes",
                "abstract": "We introduce a new model in order to describe the fluctuation of tick-by-tick financial time series. Our model, based on marked point process, allows us to incorporate in a unique process the duration of the transaction and the corresponding volume of orders. The model is motivated by the fact that the \"excitation\" of the market is different in periods of time with low exchanged volume and high volume exchanged. We illustrate our result by numerical simulations on foreign exchange data sampling in millisecond. By checking the main stylized facts, we show that the model is consistent with the empirical data. We also find an interesting relation between the distribution of the volume of limited order and the volume of market orders. To conclude, we propose an application to risk management and we introduce a forecast procedure."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-08.json",
        "arxivId": "2402.04773",
        "category": "q-fin",
        "title": "Prioritizing Investments in Cybersecurity: Empirical Evidence from an Event Study on the Determinants of Cyberattack Costs",
        "abstract": "Along with the increasing frequency and severity of cyber incidents, understanding their economic implications is paramount. In this context, listed firms' reactions to cyber incidents are compelling to study since they (i) are a good proxy to estimate the costs borne by other organizations, (ii) have a critical position in the economy, and (iii) have their financial information publicly available. We extract listed firms' cyber incident dates and characteristics from newswire headlines. We use an event study over 2012--2022, using a three-day window around events and standard benchmarks. We find that the magnitude of abnormal returns around cyber incidents is on par with previous studies using newswire or alternative data to identify cyber incidents. Conversely, as we adjust the standard errors accounting for event-induced variance and residual cross-correlation, we find that the previously claimed significance of abnormal returns vanishes. Given these results, we run a horse race of specifications, in which we test for the marginal effects of type of cyber incidents, target firm sector, periods, and their interactions. Data breaches are the most detrimental incident type with an average loss of -1.3\\% or (USD -1.9 billion) over the last decade. The health sector is the most sensitive to cyber incidents, with an average loss of -5.21\\% (or USD -1.2 billion), and even more so when these are data breaches. Instead, we cannot show any time-varying effect of cyber incidents or a specific effect of the type of news as had previously been advocated.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-12.json",
        "arxivId": "2401.07689",
        "category": "q-fin",
        "title": "Impermanent Loss Conditions: An Analysis of Decentralized Exchange Platforms",
        "abstract": "Decentralized exchanges are widely used platforms for trading crypto assets. The most common types work with automated market makers (AMM), allowing traders to exchange assets without needing to find matching counterparties. Thereby, traders exchange against asset reserves managed by smart contracts. These assets are provided by liquidity providers in exchange for a fee. Static analysis shows that small price changes in one of the assets can result in losses for liquidity providers. Despite the success of AMMs, it is claimed that liquidity providers often suffer losses. However, the literature does not adequately consider the dynamic effects of fees over time. Therefore, we investigate the impermanent loss problem in a dynamic setting using Monte Carlo simulations. Our findings indicate that price changes do not necessarily lead to losses. Fees paid by traders and arbitrageurs are equally important. In this respect, we can show that an arbitrage-friendly environment benefits the liquidity provider. Thus, we suggest that AMM developers should promote an arbitrage-friendly environment rather than trying to prevent arbitrage.",
        "references": [
            {
                "arxivId": "2101.02778",
                "title": "Dynamic Curves for Decentralized Autonomous Cryptocurrency Exchanges",
                "abstract": "One of the exciting recent developments in decentralized finance (DeFi) has been the development of decentralized cryptocurrency exchanges that can autonomously handle conversion between different cryptocurrencies. Decentralized exchange protocols such as Uniswap, Curve and other types of Automated Market Makers (AMMs) maintain a liquidity pool (LP) of two or more assets constrained to maintain at all times a mathematical relation to each other, defined by a given function or curve. Examples of such functions are the constant-sum and constant-product AMMs. Existing systems however suffer from several challenges. They require external arbitrageurs to restore the price of tokens in the pool to match the market price. Such activities can potentially drain resources from the liquidity pool. In particular, dramatic market price changes can result in low liquidity with respect to one or more of the assets and reduce the total value of the LP. We propose in this work a new approach to constructing the AMM by proposing the idea of dynamic curves. It utilizes input from a market price oracle to modify the mathematical relationship between the assets so that the pool price continuously and automatically adjusts to be identical to the market price. This approach eliminates arbitrage opportunities and, as we show through simulations, maintains liquidity in the LP for all assets and the total value of the LP over a wide range of market prices."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-13.json",
        "arxivId": "2308.08760",
        "category": "q-fin",
        "title": "Semi-analytic pricing of American options in time-dependent jump-diffusion models with exponential jumps",
        "abstract": "In this paper we propose a semi-analytic approach to pricing American options for time-dependent jump-diffusions models with exponential jumps The idea of the method is to further generalize our approach developed for pricing barrier, [Itkin et al., 2021], and American, [Carr and Itkin, 2021; Itkin and Muravey, 2023], options in various time-dependent one factor and even stochastic volatility models. Our approach i) allows arbitrary dependencies of the model parameters on time; ii) reduces solution of the pricing problem for American options to a simpler problem of solving a system of an algebraic nonlinear equation for the exercise boundary and a linear Fredholm-Volterra equation for the the option price; iii) the options Greeks solve a similar Fredholm-Volterra linear equation obtained by just differentiating Eq. (25) by the required parameter. Once done, the American option price is presented in close form.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-13.json",
        "arxivId": "2309.03202",
        "category": "q-fin",
        "title": "Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio",
        "abstract": "This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depending on the stability of future market conditions. Future work is suggested, including experiments with updated Q-learning policies during testing and trading diverse individual stocks. Additionally, the exploration of alternative economic indicators for training the models is proposed.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-13.json",
        "arxivId": "2402.06638",
        "category": "q-fin",
        "title": "Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting",
        "abstract": "Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we propose attentive federated transformers for time series stock forecasting with better performance while preserving the privacy of participating enterprises. Empirical results on various stock data from the Yahoo! Finance website indicate the superiority of our proposed scheme in dealing with the above challenges and data heterogeneity in federated learning.",
        "references": [
            {
                "arxivId": "2202.07125",
                "title": "Transformers in Time Series: A Survey",
                "abstract": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance."
            },
            {
                "arxivId": "2106.13008",
                "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
                "abstract": "Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: \\url{https://github.com/thuml/Autoformer}."
            },
            {
                "arxivId": "2004.13408",
                "title": "Time-series forecasting with deep learning: a survey",
                "abstract": "Numerous deep learning architectures have been developed to accommodate the diversity of time-series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time-series forecasting\u2014describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time-series data. This article is part of the theme issue \u2018Machine learning for weather and climate modelling\u2019."
            },
            {
                "arxivId": "2104.04041",
                "title": "CLVSA: A Convolutional LSTM Based Variational Sequence-to-Sequence Model with Attention for Predicting Trends of Financial Markets",
                "abstract": "Financial markets are a complex dynamical system. The complexity comes from the interaction between a market and its participants, in other words, the integrated outcome of activities of the entire participants determines the markets trend, while the markets trend affects activities of participants. These interwoven interactions make financial markets keep evolving. Inspired by stochastic recurrent models that successfully capture variability observed in natural sequential data such as speech and video, we propose CLVSA, a hybrid model that consists of stochastic recurrent networks, the sequence-to-sequence architecture, the self- and inter-attention mechanism, and convolutional LSTM units to capture variationally underlying features in raw financial trading data. Our model outperforms basic models, such as convolutional neural network, vanilla LSTM network, and sequence-to-sequence model with attention, based on backtesting results of six futures from January 2010 to December 2017. Our experimental results show that, by introducing an approximate posterior, CLVSA takes advantage of an extra regularizer based on the Kullback-Leibler divergence to prevent itself from overfitting traps."
            },
            {
                "arxivId": "1908.02646",
                "title": "AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks",
                "abstract": "Recent years have witnessed the successful marriage of finance innovations and AI techniques in various finance applications including quantitative trading (QT). Despite great research efforts devoted to leveraging deep learning (DL) methods for building better QT strategies, existing studies still face serious challenges especially from the side of finance, such as the balance of risk and return, the resistance to extreme loss, and the interpretability of strategies, which limit the application of DL-based strategies in real-life financial markets. In this work, we propose AlphaStock, a novel reinforcement learning (RL) based investment strategy enhanced by interpretable deep attention networks, to address the above challenges. Our main contributions are summarized as follows: i) We integrate deep attention networks with a Sharpe ratio-oriented reinforcement learning framework to achieve a risk-return balanced investment strategy; ii) We suggest modeling interrelationships among assets to avoid selection bias and develop a cross-asset attention mechanism; iii) To our best knowledge, this work is among the first to offer an interpretable investment strategy using deep reinforcement learning models. The experiments on long-periodic U.S. and Chinese markets demonstrate the effectiveness and robustness of AlphaStock over diverse market states. It turns out that AlphaStock tends to select the stocks as winners with high long-term growth, low volatility, high intrinsic value, and being undervalued recently."
            },
            {
                "arxivId": "1907.05321",
                "title": "Time2Vec: Learning a Vector Representation of Time",
                "abstract": "Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, we take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be easily imported into many existing and future architectures and improve their performances. We show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model."
            },
            {
                "arxivId": "1812.07108",
                "title": "Learning Private Neural Language Modeling with Attentive Aggregation",
                "abstract": "Mobile keyboard suggestion is typically regarded as a word-level language modeling problem. Centralized machine learning techniques require the collection of massive user data for training purposes, which may raise privacy concerns in relation to users\u2019 sensitive data. Federated learning (FL) provides a promising approach to learning private language modeling for intelligent personalized keyboard suggestions by training models on distributed clients rather than training them on a central server. To obtain a global model for prediction, existing FL algorithms simply average the client models and ignore the importance of each client during model aggregation. Furthermore, there is no optimization for learning a well-generalized global model on the central server. To solve these problems, we propose a novel model aggregation with an attention mechanism considering the contribution of client models to the global model, together with an optimization technique during server aggregation. Our proposed attentive aggregation method minimizes the weighted distance between the server model and client models by iteratively updating parameters while attending to the distance between the server model and client models. Experiments on two popular language modeling datasets and a social media dataset show that our proposed method outperforms its counterparts in terms of perplexity and communication cost in most settings of comparison."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1602.05629",
                "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
                "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. \nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-13.json",
        "arxivId": "2402.07124",
        "category": "q-fin",
        "title": "Econometric analysis to estimate the impact of holidays on airfares",
        "abstract": "The number of air transportation passengers during the holidays in Brazil has grown notably since the late nineties. One of the reasons is greater competition in airfares made possible by economic liberalization. This paper presents an econometric model of airline pricing aiming at estimating the impacts of holiday periods on fares, with special emphasis on three-day holiday events. It makes use of a database with daily collected data from the internet between 2008 and 2010 for the major Brazilian city, Sao Paulo. The econometric panel data model employs a two-way error components \u201cwithin\u201d estimator, controlling for airline/airport-pair fixed effect along with quotation and departure months effects. The decomposition of time effects between quotation and departure month effects is the main methodological contribution of the paper. Results allow for a comparative analysis of the performance of Sao Paulo\u2019s downtown and international airports - respectively, Congonhas (CGH), and Guarulhos (GRU) airports. As a result, the price of tickets bought 60 days in advance for flights with two stops leaving from the downtown airport fell by most.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-13.json",
        "arxivId": "2402.07125",
        "category": "q-fin",
        "title": "Intermodal Competition in the Brazilian Interstate Travel Market",
        "abstract": "A method for forming a planarized interlevel dielectric layer without degradation due to the microloading effect from spin-on material etchback is described. A patterned first conducting layer is provided over an insulating layer on a semiconductor substrate. An improved interlevel dielectric layer is formed overlying the patterned first conducting layer by the following steps. A first oxide layer is deposited overlying the patterned first conducting layer and the insulating layer. A spin-on material layer is coated overlying the first oxide layer and etched back using O2 gas added to the CHF3/CF4 chemistry until the first oxide layer is exposed overlying the patterned first conducting layer wherein microloading effects from the etching back of the spin-on material layer are lower than microloading effects in a conventional interlevel dielectric layer. A second oxide layer is deposited to complete the interlevel dielectric layer. A second conducting layer is deposited over the interlevel dielectric layer and patterned to complete the fabrication of the integrated circuit device.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-13.json",
        "arxivId": "2402.07190",
        "category": "q-fin",
        "title": "Transporte Aereo: Economia e Politicas Publicas",
        "abstract": "This book, written in Portuguese, presents a comprehensive analysis of the air transport industry in Brazil, highlighting its vital importance to the country's economy. It explores the sector's complexity, from economic characteristics to interaction with the national aeronautical industry, through the specialization of the workforce and market demand analysis. The book delves into the economic regulation of air transport, tracing the evolution from periods of strict regulation to phases of liberalization and deregulation, and examines market dynamics, focusing on concentration and competitiveness. It also analyzes demand and supply through case studies, investigating everything from tourists traveling within Brazil to the coverage of the national territory and the prices of air tickets. Finally, the book proposes principles for the regulation and public policies of the air sector, emphasizing the priority of the passenger, the business environment, access to air transport, and economic efficiency, culminating in the advocacy for a free market, but with protection for competition and the consumer.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-14.json",
        "arxivId": "2303.14732",
        "category": "q-fin",
        "title": "Interdisciplinary Papers Supported by Disciplinary Grants Garner Deep and Broad Scientific Impact",
        "abstract": "Interdisciplinary research has emerged as a hotbed for innovation and a key approach to addressing complex societal challenges. The increasing dominance of grant-supported research in shaping scientific advances, coupled with growing interest in funding interdisciplinary work, raises fundamental questions about the effectiveness of interdisciplinary grants in fostering high-impact interdisciplinary research outcomes. Here, we quantify the interdisciplinarity of both research grants and publications, capturing 350,000 grants from 164 funding agencies across 26 countries and 1.3 million papers that acknowledged their support from 1985 to 2009. Our analysis uncovers two seemingly contradictory patterns: Interdisciplinary grants tend to produce interdisciplinary papers, which are generally associated with high impact. However, compared to disciplinary grants, interdisciplinary grants on average yield fewer papers and interdisciplinary papers they support tend to have substantially reduced impact. We demonstrate that the key to explaining this paradox lies in the power of disciplinary grants in propelling high-impact interdisciplinary research. Specifically, our results show that highly interdisciplinary papers supported by deeply disciplinary grants garner disproportionately more citations, both within their core disciplines and from broader fields. Moreover, disciplinary grants, particularly when combined with other similar grants, are more effective in producing high-impact interdisciplinary research. Amidst the rapid rise of support for interdisciplinary work across the sciences, these results highlight the hitherto unknown role of disciplinary grants in driving crucial interdisciplinary advances, suggesting that interdisciplinary research requires deep disciplinary expertise and investments.",
        "references": [
            {
                "arxivId": "1811.00286",
                "title": "Interdisciplinarity: A Nobel Opportunity",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-14.json",
        "arxivId": "2305.12179",
        "category": "q-fin",
        "title": "Commodity-specific triads in the Dutch inter-industry production network",
        "abstract": null,
        "references": [
            {
                "arxivId": "2303.02716",
                "title": "Deterministic, quenched, and annealed parameter estimation for heterogeneous network models.",
                "abstract": "At least two different approaches to define and solve statistical models for the analysis of economic systems exist: the typical, econometric one, interpreting the gravity model specification as the expected link weight of an arbitrary probability distribution, and the one rooted in statistical physics, constructing maximum-entropy distributions constrained to satisfy certain network properties. In a couple of recent companion papers, they have been successfully integrated within the framework induced by the constrained minimization of the Kullback-Leibler divergence: specifically, two broad classes of models have been devised, i.e., the integrated and conditional ones, defined by different, probabilistic rules to place links, load them with weights and turn them into proper, econometric prescriptions. Still, the recipes adopted by the two approaches to estimate the parameters entering into the definition of each model differ. In econometrics, a likelihood that decouples the binary and weighted parts of a model, treating a network as deterministic, is typically maximized; to restore its random character, two alternatives exist: either solving the likelihood maximization on each configuration of the ensemble and taking the average of the parameters afterwards or taking the average of the likelihood function and maximizing the latter one. The difference between these approaches lies in the order in which the operations of averaging and maximization are taken-a difference that is reminiscent of the quenched and annealed ways of averaging out the disorder in spin glasses. The results of the present contribution, devoted to comparing these recipes in the case of continuous, conditional network models, indicate that the annealed estimation recipe represents the best alternative to the deterministic one."
            },
            {
                "arxivId": "2302.11451",
                "title": "Estimating the loss of economic predictability from aggregating firm-level production networks",
                "abstract": "Abstract To estimate the reaction of economies to political interventions or external disturbances, input\u2013output (IO) tables\u2014constructed by aggregating data into industrial sectors\u2014are extensively used. However, economic growth, robustness, and resilience crucially depend on the detailed structure of nonaggregated firm-level production networks (FPNs). Due to nonavailability of data, little is known about how much aggregated sector-based and detailed firm-level-based model predictions differ. Using a nearly complete nationwide FPN, containing 243,399 Hungarian firms with 1,104,141 supplier\u2013buyer relations, we self-consistently compare production losses on the aggregated industry-level production network (IPN) and the granular FPN. For this, we model the propagation of shocks of the same size on both, the IPN and FPN, where the latter captures relevant heterogeneities within industries. In a COVID-19 inspired scenario, we model the shock based on detailed firm-level data during the early pandemic. We find that using IPNs instead of FPNs leads to an underestimation of economic losses of up to 37%, demonstrating a natural limitation of industry-level IO models in predicting economic outcomes. We ascribe the large discrepancy to the significant heterogeneity of firms within industries: we find that firms within one sector only sell 23.5% to and buy 19.3% from the same industries on average, emphasizing the strong limitations of industrial sectors for representing the firms they include. Similar error levels are expected when estimating economic growth, CO2 emissions, and the impact of policy interventions with industry-level IO models. Granular data are key for reasonable predictions of dynamical economic systems."
            },
            {
                "arxivId": "2210.01179",
                "title": "Reconciling econometrics with continuous maximum-entropy network models",
                "abstract": null
            },
            {
                "arxivId": "2111.15248",
                "title": "Reconstructing firm-level interactions in the Dutch input\u2013output network from production constraints",
                "abstract": null
            },
            {
                "arxivId": "2107.02650",
                "title": "Gravity models of networks: Integrating maximum-entropy and econometric approaches",
                "abstract": "The World Trade Web (WTW) is the network of international trade relationships among world countries. Characterizing both the local link weights (observed trade volumes) and the global network structure (large-scale topology) of the WTW via a single model is still an open issue. While the traditional Gravity Model (GM) successfully replicates the observed trade volumes by employing macroeconomic properties such as GDP and geographic distance, it unfortunately predicts a fully connected network, returning a completely unrealistic topology of the WTW. To overcome this problem, two different classes of models have been introduced in econometrics and statistical physics. Econometric approaches interpret the traditional GM as the expected value of a probability distribution that can be chosen largely arbitrarily and tested against alternative distributions. Statistical physics approaches construct maximum-entropy probability distributions of (weighted) graphs from a chosen set of measurable, structural constraints and test distributions resulting from different constraints. Here we compare and integrate the two approaches by considering a class of maximum-entropy models that can incorporate macroeconomic properties used in standard econometric models. We \ufb01nd that the integrated approach achieves an overall better performance than the purely econometric one. These results suggest that the maximum-entropy construction can serve as a viable econometric framework wherein extensive and intensive margins can be separately controlled for, by combining topological constraints and dyadic macroeconomic variables."
            },
            {
                "arxivId": "2104.07260",
                "title": "Quantifying firm-level economic systemic risk from nation-wide supply networks",
                "abstract": null
            },
            {
                "arxivId": "2103.15777",
                "title": "Functional Structure in Production Networks",
                "abstract": "Production networks are integral to economic dynamics, yet dis-aggregated network data on inter-firm trade is rarely collected and often proprietary. Here we situate company-level production networks within a wider space of networks that are different in nature, but similar in local connectivity structure. Through this lens, we study a regional and a national network of inferred trade relationships reconstructed from Dutch national economic statistics and re-interpret prior empirical findings. We find that company-level production networks have so-called functional structure, as previously identified in protein-protein interaction (PPI) networks. Functional networks are distinctive in their over-representation of closed squares, which we quantify using an existing measure called spectral bipartivity. Shared local connectivity structure lets us ferry insights between domains. PPI networks are shaped by complementarity, rather than homophily, and we use multi-layer directed configuration models to show that this principle explains the emergence of functional structure in production networks. Companies are especially similar to their close competitors, not to their trading partners. Our findings have practical implications for the analysis of production networks and give us precise terms for the local structural features that may be key to understanding their routine function, failure, and growth."
            },
            {
                "arxivId": "2103.05623",
                "title": "The physics of financial networks",
                "abstract": null
            },
            {
                "arxivId": "2101.07818",
                "title": "Simultaneous supply and demand constraints in input\u2013output networks: the case of Covid-19 in Germany, Italy, and Spain",
                "abstract": "Natural and anthropogenic disasters frequently affect both the supply and demand sides of an economy. A striking recent example is the Covid-19 pandemic which has created severe disruptions to economic output in most countries. These direct shocks to supply and demand will propagate downstream and upstream through production networks. Given the exogenous shocks, we derive a lower bound on total shock propagation. We find that even in this best case scenario network effects substantially amplify the initial shocks. To obtain more realistic model predictions, we study the propagation of shocks bottom-up by imposing different rationing rules on industries if they are not able to satisfy incoming demand. Our results show that economic impacts depend strongly on the emergence of input bottlenecks, making the rationing assumption a key variable in predicting adverse economic impacts. We further establish that the magnitude of initial shocks and network density heavily influence model predictions."
            },
            {
                "arxivId": "2012.02677",
                "title": "Reconstructing Networks",
                "abstract": "Complex networks datasets often come with the problem of missing information: interactions data that have not been measured or discovered, may be affected by errors, or are simply hidden because of privacy issues. This Element provides an overview of the ideas, methods and techniques to deal with this problem and that together define the field of network reconstruction. Given the extent of the subject, we shall focus on the inference methods rooted in statistical physics and information theory. The discussion will be organized according to the different scales of the reconstruction task, that is, whether the goal is to reconstruct the macroscopic structure of the network, to infer its mesoscale properties, or to predict the individual microscopic connections."
            },
            {
                "arxivId": "2012.01265",
                "title": "Weighted network motifs as random walk patterns",
                "abstract": "Over the last two decades, network theory has shown to be a fruitful paradigm in understanding the organization and functioning of real-world complex systems. One technique helpful to this endeavor is identifying functionally influential subgraphs, shedding light on underlying evolutionary processes. Such overrepresented subgraphs, motifs, have received much attention in simple networks, where edges are either on or off. However, for weighted networks, motif analysis is still undeveloped. Here, we proposed a novel methodology\u2014based on a random walker taking a fixed maximum number of steps\u2014to study weighted motifs of limited size. We introduce a sink node to balance the network and allow the detection of configurations within an a priori fixed number of steps for the random walker. We applied this approach to different real networks and selected a specific null model based on maximum-entropy to test the significance of weighted motifs occurrence. We found that identified similarities enable the classifications of systems according to functioning mechanisms associated with specific configurations: economic networks exhibit close patterns while differentiating from ecological systems without any a priori assumption."
            },
            {
                "arxivId": "2004.14485",
                "title": "Distress propagation on production networks: Coarse-graining and modularity of linkages",
                "abstract": null
            },
            {
                "arxivId": "2001.11125",
                "title": "Testing biological network motif significance with exponential random graph models",
                "abstract": null
            },
            {
                "arxivId": "1909.01274",
                "title": "In Search of Lost Edges: A Case Study on Reconstructing FInancial Networks",
                "abstract": "To capture the systemic complexity of international financial systems, network data is an important prerequisite. However, dyadic data is often not available, raising the need for methods that allow for reconstructing networks based on limited information. In this paper, we are reviewing different methods that are designed for the estimation of matrices from their marginals and potentially exogenous information. This includes a general discussion of the available methodology that provides edge probabilities as well as models that are focussed on the reconstruction of edge values. Besides summarizing the advantages, shortfalls and computational issues of the approaches, we put them into a competitive comparison using the SWIFT (Society for Worldwide Interbank Financial Telecommunication) MT 103 payment messages network (MT 103: Single Customer Credit Transfer). This network is not only economically meaningful but also fully observed which allows for an extensive competitive horse race of methods. The comparison concerning the binary reconstruction is divided into an evaluation of the edge probabilities and the quality of the reconstructed degree structures. Furthermore, the accuracy of the predicted edge values is investigated. To test the methods on different topologies, the application is split into two parts. The first part considers the full MT 103 network, being an illustration for the reconstruction of large, sparse financial networks. The second part is concerned with reconstructing a subset of the full network, representing a dense medium-sized network. Regarding substantial outcomes, it can be found that no method is superior in every respect and that the preferred model choice highly depends on the goal of the analysis, the presumed network structure and the availability of exogenous information."
            },
            {
                "arxivId": "1811.09829",
                "title": "A faster horse on a safer trail: generalized inference for the efficient reconstruction of weighted networks",
                "abstract": "Due to the interconnectedness of financial entities, estimating certain key properties of a complex financial system, including the implied level of systemic risk, requires detailed information about the structure of the underlying network of dependencies. However, since data about financial linkages are typically subject to confidentiality, network reconstruction techniques become necessary to infer both the presence of connections and their intensity. Recently, several \u2018horse races\u2019 have been conducted to compare the performance of the available financial network reconstruction methods. These comparisons were based on arbitrarily chosen metrics of similarity between the real network and its reconstructed versions. Here we establish a generalized maximum-likelihood approach to rigorously define and compare weighted reconstruction methods. Our generalization uses the maximization of a certain conditional entropy to solve the problem represented by the fact that the density-dependent constraints required to reliably reconstruct the network are typically unobserved and, therefore, cannot enter directly, as sufficient statistics, in the likelihood function. The resulting approach admits as input any reconstruction method for the purely binary topology and, conditionally on the latter, exploits the available partial information to infer link weights. We find that the most reliable method is obtained by \u2018dressing\u2019 the best-performing binary method with an exponential distribution of link weights having a properly density-corrected and link-specific mean value and propose two safe (i.e. unbiased in the sense of maximum conditional entropy) variants of it. While the one named CReMA is perfectly general (as a particular case, it can place optimal weights on a network if the bare topology is known), the one named CReMB is recommended both in case of full uncertainty about the network topology and if the existence of some links is certain. In these cases, the CReMB is faster and reproduces empirical networks with highest generalized likelihood among the considered competing models."
            },
            {
                "arxivId": "1810.07774",
                "title": "How production networks amplify economic growth",
                "abstract": "Significance Technological improvement is the most important cause of long-term economic growth. We study the effects of technology improvement in the setting of a production network, in which each producer buys input goods and converts them to other goods, selling the product to households or other producers. We show how this network amplifies the effects of technological improvements as they propagate along chains of production. Longer production chains for an industry bias it toward faster price reduction, and longer production chains for a country bias it toward faster growth. These predictions are in good agreement with data and improve with the passage of time, demonstrating a key influence of production chains in price change and output growth over the long term. Technological improvement is the most important cause of long-term economic growth. In standard growth models, technology is treated in the aggregate, but an economy can also be viewed as a network in which producers buy goods, convert them to new goods, and sell the production to households or other producers. We develop predictions for how this network amplifies the effects of technological improvements as they propagate along chains of production, showing that longer production chains for an industry bias it toward faster price reduction and that longer production chains for a country bias it toward faster growth. These predictions are in good agreement with data from the World Input Output Database and improve with the passage of time. The results show that production chains play a major role in shaping the long-term evolution of prices, output growth, and structural change."
            },
            {
                "arxivId": "1810.05095",
                "title": "The statistical physics of real-world networks",
                "abstract": null
            },
            {
                "arxivId": "1809.06057",
                "title": "Cumulative effects of triadic closure and homophily in social networks",
                "abstract": "Dynamic interplay of choice homophily and triadic closure amplifies homophily and creates core-peripheries in social networks. Social network structure has often been attributed to two network evolution mechanisms\u2014triadic closure and choice homophily\u2014which are commonly considered independently or with static models. However, empirical studies suggest that their dynamic interplay generates the observed homophily of real-world social networks. By combining these mechanisms in a dynamic model, we confirm the longheld hypothesis that choice homophily and triadic closure cause induced homophily. We estimate how much observed homophily in friendship and communication networks is amplified due to triadic closure. We find that cumulative effects of homophily amplification can also lead to the widely documented core-periphery structure of networks, and to memory of homophilic constraints (equivalent to hysteresis in physics). The model shows that even small individual bias may prompt network-level changes such as segregation or core group dominance. Our results highlight that individual-level mechanisms should not be analyzed separately without considering the dynamics of society as a whole."
            },
            {
                "arxivId": "1506.00348",
                "title": "Enhanced Gravity Model of Trade: Reconciling Macroeconomic and Network Models",
                "abstract": "The structure of the International Trade Network (ITN), whose nodes and links represent world countries and their trade relations respectively, affects key economic processes worldwide, including globalization, economic integration, industrial production, and the propagation of shocks and instabilities. Characterizing the ITN via a simple yet accurate model is an open problem. The traditional Gravity Model (GM) successfully reproduces the volume of trade between connected countries, using macroeconomic properties such as GDP, geographic distance, and possibly other factors. However, it predicts a network with complete or homogeneous topology, thus failing to reproduce the highly heterogeneous structure of the ITN. On the other hand, recent maximum-entropy network models successfully reproduce the complex topology of the ITN, but provide no information about trade volumes. Here we integrate these two currently incompatible approaches via the introduction of an Enhanced Gravity Model (EGM) of trade. The EGM is the simplest model combining the GM with the network approach within a maximum-entropy framework. Via a unified and principled mechanism that is transparent enough to be generalized to any economic network, the EGM provides a new econometric framework wherein trade probabilities and trade volumes can be separately controlled by any combination of dyadic and country-specific macroeconomic variables. The model successfully reproduces both the global topology and the local link weights of the ITN, parsimoniously reconciling the conflicting approaches. It also indicates that the probability that any two countries trade a certain volume should follow a geometric or exponential distribution with an additional point mass at zero volume."
            },
            {
                "arxivId": "1411.7613",
                "title": "Systemic Risk Analysis on Reconstructed Economic and Financial Networks",
                "abstract": null
            },
            {
                "arxivId": "1403.4460",
                "title": "Stationarity, non-stationarity and early warning signals in economic networks",
                "abstract": "Economic integration, globalization and financial crises represent examples of processes whose understanding requires the analysis of the underlying network structure. Of particular interest is establishing whether a real economic network is in a state of (quasi)stationary equilibrium, i.e. characterized by smooth structural changes rather than abrupt transitions. While in the former case the behaviour of the system can be reasonably controlled and predicted, in the latter case this is generally impossible. Here, we propose a method to assess whether a real economic network is in a quasi-stationary state by checking the consistency of its structural evolution with appropriate quasi-equilibrium maximum-entropy ensembles of graphs. As illustrative examples, we consider the International Trade Network (ITN) and the Dutch Interbank Network (DIN). We find that the ITN is an almost perfect example of quasi-equilibrium network, while the DIN is clearly out-of-equilibrium. In the latter, the entity of the deviation from quasi-stationarity contains precious information that allows us to identify remarkable early warning signals of the interbank crisis of 2008. These early warning signals involve certain dyadic and triadic topological properties, including dangerous 'debt loops' with different levels of interbank reciprocity."
            },
            {
                "arxivId": "1307.2104",
                "title": "Enhanced reconstruction of weighted networks from strengths and degrees",
                "abstract": "Network topology plays a key role in many phenomena, from the spreading of diseases to that of financial crises. Whenever the whole structure of a network is unknown, one must resort to reconstruction methods that identify the least biased ensemble of networks consistent with the partial information available. A challenging case, frequently encountered due to privacy issues in the analysis of interbank flows and Big Data, is when there is only local (node-specific) aggregate information available. For binary networks, the relevant ensemble is one where the degree (number of links) of each node is constrained to its observed value. However, for weighted networks the problem is much more complicated. While the na\u00efve approach prescribes to constrain the strengths (total link weights) of all nodes, recent counter-intuitive results suggest that in weighted networks the degrees are often more informative than the strengths. This implies that the reconstruction of weighted networks would be significantly enhanced by the specification of both strengths and degrees, a computationally hard and bias-prone procedure. Here we solve this problem by introducing an analytical and unbiased maximum-entropy method that works in the shortest possible time and does not require the explicit generation of reconstructed samples. We consider several real-world examples and show that, while the strengths alone give poor results, the additional knowledge of the degrees yields accurately reconstructed networks. Information-theoretic criteria rigorously confirm that the degree sequence, as soon as it is non-trivial, is irreducible to the strength sequence. Our results have strong implications for the analysis of motifs and communities and whenever the reconstructed ensemble is required as a null model to detect higher-order patterns."
            },
            {
                "arxivId": "1306.0112",
                "title": "Deciphering the global organization of clustering in real complex networks",
                "abstract": null
            },
            {
                "arxivId": "1302.2063",
                "title": "Early-warning signals of topological collapse in interbank networks",
                "abstract": null
            },
            {
                "arxivId": "1208.4208",
                "title": "Reciprocity of weighted networks",
                "abstract": null
            },
            {
                "arxivId": "1208.1188",
                "title": "Relations between allometric scalings and fluctuations in complex systems: The case of Japanese firms",
                "abstract": null
            },
            {
                "arxivId": "1201.1215",
                "title": "Triadic motifs and dyadic self-organization in the World Trade Network",
                "abstract": null
            },
            {
                "arxivId": "1103.1243",
                "title": "Randomizing world trade. I. A binary network analysis.",
                "abstract": "The international trade network (ITN) has received renewed multidisciplinary interest due to recent advances in network theory. However, it is still unclear whether a network approach conveys additional, nontrivial information with respect to traditional international-economics analyses that describe world trade only in terms of local (first-order) properties. In this and in a companion paper, we employ a recently proposed randomization method to assess in detail the role that local properties have in shaping higher-order patterns of the ITN in all its possible representations (binary or weighted, directed or undirected, aggregated or disaggregated by commodity) and across several years. Here we show that, remarkably, the properties of all binary projections of the network can be completely traced back to the degree sequence, which is therefore maximally informative. Our results imply that explaining the observed degree sequence of the ITN, which has not received particular attention in economic theory, should instead become one the main focuses of models of trade."
            },
            {
                "arxivId": "1103.0701",
                "title": "Analytical maximum-likelihood method to detect patterns in real networks",
                "abstract": "In order to detect patterns in real networks, randomized graph ensembles that preserve only part of the topology of an observed network are systematically used as fundamental null models. However, the generation of them is still problematic. Existing approaches are either computationally demanding and beyond analytic control or analytically accessible but highly approximate. Here, we propose a solution to this long-standing problem by introducing a fast method that allows one to obtain expectation values and standard deviations of any topological property analytically, for any binary, weighted, directed or undirected network. Remarkably, the time required to obtain the expectation value of any property analytically across the entire graph ensemble is as short as that required to compute the same property using the adjacency matrix of the single original network. Our method reveals that the null behavior of various correlation properties is different from what was believed previously, and is highly sensitive to the particular network considered. Moreover, our approach shows that important structural properties (such as the modularity used in community detection problems) are currently based on incorrect expressions, and provides the exact quantities that should replace them."
            },
            {
                "arxivId": "0908.1143",
                "title": "How small are building blocks of complex networks",
                "abstract": "Network motifs are small building blocks of complex networks. Statistically significant motifs often perform network-specific functions. However, the precise nature of the connection between motifs and the global structure and function of networks remains elusive. Here we show that the global structure of some real networks is statistically determined by the probability of connections within motifs of size at most 3, once this probability accounts for node degrees. The connectivity profiles of node triples in these networks capture all their local and global properties. This finding impacts methods relying on motif statistical significance, and enriches our understanding of the elementary forces that shape the structure of complex networks."
            },
            {
                "arxivId": "cond-mat/0403051",
                "title": "Fitness-dependent topological properties of the world trade web.",
                "abstract": "Among the proposed network models, the hidden variable (or good get richer) one is particularly interesting, even if an explicit empirical test of its hypotheses has not yet been performed on a real network. Here we provide the first empirical test of this mechanism on the world trade web, the network defined by the trade relationships between world countries. We find that the power-law distributed gross domestic product can be successfully identified with the hidden variable (or fitness) determining the topology of the world trade web: all previously studied properties up to third-order correlation structure (degree distribution, degree correlations, and hierarchy) are found to be in excellent agreement with the predictions of the model. The choice of the connection probability is such that all realizations of the network with the same degree sequence are equiprobable."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-14.json",
        "arxivId": "2307.13772",
        "category": "q-fin",
        "title": "Fragmentation and optimal liquidity supply on decentralized exchanges",
        "abstract": "We investigate how liquidity providers (LPs) choose between trading venues with high and low fees, in the face of a fixed common gas cost. Analyzing Uniswap data, we find that high-fee pools attract 58% of liquidity supply but execute only 21% of trading volume. Large LPs dominate low-fee pools, frequently adjusting positions in response to substantial trading volume. In contrast, small LPs converge to high-fee pools, accepting lower execution probabilities to mitigate smaller liquidity management costs. Fragmented liquidity dominates a single-fee market, as it encourages more liquidity providers to enter the market, while enhancing LP competition on the low-fee pool.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-14.json",
        "arxivId": "2311.02431",
        "category": "q-fin",
        "title": "The contribution of US broadband infrastructure subsidy and investment programs to GDP using input-output modeling",
        "abstract": "More than one-fifth of the US population does not subscribe to a fixed broadband service despite broadband being a recognized merit good. For example, less than 4% of citizens earning more than US \\$70k annually do not have broadband, compared to 26% of those earning below US \\$20k annually. To address this, the Biden Administration has undertaken one of the largest broadband investment programs ever via The Bipartisan Infrastructure Law, with the aim of addressing this disparity and expanding broadband connectivity to all citizens. We examine broadband availability, adoption, and need for each US state, and then construct an Input-Output model to explore the potential macroeconomic impacts of broadband spending to Gross Domestic Product (GDP) and supply chain linkages. Our analysis indicates that higher funding allocations do appear to be allocated to areas with poorer broadband. While this may be logical, as it illustrates funding going to areas most in need, this could not have been assumed a priori given politically-motivated funding is not always rationally allocated. In terms of macroeconomic impact, the total direct contribution to US GDP by the program could be as high as US \\$84.8 billion, \\$55.2 billion, and \\$5.99 billion for the BEAD program, ACP, and TBCP, respectively. Thus, overall, the broadband allocations could expand US GDP by \\$146 billion (0.13% of annual US GDP over the next five years). We contribute one of the first economic impact assessments of the US Bipartisan Infrastructure Law to the literature.",
        "references": [
            {
                "arxivId": "2309.02338",
                "title": "Sustainability assessment of Low Earth Orbit (LEO) satellite broadband mega-constellations",
                "abstract": "The growth of megaconstellations is rapidly increasing the number of rocket launches. While Low Earth Orbit (LEO) broadband satellites help to connect unconnected communities and achieve the Sustainable Development Goals (SDGs), there are also significant environmental emissions impacts from burning rocket fuels. We present sustainability analytics for phase 1 of the three main LEO constellations including Amazon Kuiper (3,236 satellites), Eutelsat Group`s OneWeb (648 satellites), and SpaceX Starlink (4,425 satellites). We find that LEO megaconstellations provide substantially improved broadband speeds for rural and remote communities, but are roughly 6-8 times more emissions intensive (250 kg CO2eq/subscriber/year) than comparative terrestrial mobile broadband. In the worst-case emissions scenario, this rises to 12-14 times more (469 kg CO2eq/subscriber/year). Policy makers must carefully consider the trade-off between connecting unconnected communities to further the SDGs and mitigating the growing space sector environmental footprint, particularly regarding phase 2 plans to launch an order-of-magnitude more satellites."
            },
            {
                "arxivId": "2308.14734",
                "title": "Crowdsourced data indicates broadband has a positive impact on local business creation",
                "abstract": null
            },
            {
                "arxivId": "2101.07820",
                "title": "Policy choices can help keep 4G and 5G universal broadband affordable",
                "abstract": "The United Nations Broadband Commission has committed the international community to accelerate universal broadband. However, the cost of meeting this objective, and the feasibility of doing so on a commercially viable basis, are not well understood. Using scenario analysis, this paper compares the global cost-effectiveness of different infrastructure strategies for the developing world to achieve universal 4G or 5G mobile broadband. Utilizing remote sensing and demand forecasting, least-cost network designs are developed for eight representative low and middle-income countries (Malawi, Uganda, Kenya, Senegal, Pakistan, Albania, Peru and Mexico), the results from which form the basis for aggregation to the global level. The cost of meeting a minimum 10 Mbps per user is estimated at USD 1.7 trillion using 5G Non-Standalone, approximately 0.6% of annual GDP for the developing world over the next decade. However, by creating a favorable regulatory environment, governments can bring down these costs by as much as three quarters, to USD 0.5 trillion (approximately 0.2% of annual GDP), and avoid the need for public subsidy. Providing governments make judicious choices, adopting fiscal and regulatory regimes conducive to lowering costs, universal broadband may be within reach of most developing countries over the next decade."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-14.json",
        "arxivId": "2402.06714",
        "category": "q-fin",
        "title": "Electricity Price Forecasting in the Irish Balancing Market",
        "abstract": "Short-term electricity markets are becoming more relevant due to less-predictable renewable energy sources, attracting considerable attention from the industry. The balancing market is the closest to real-time and the most volatile among them. Its price forecasting literature is limited, inconsistent and outdated, with few deep learning attempts and no public dataset. This work applies to the Irish balancing market a variety of price prediction techniques proven successful in the widely studied day-ahead market. We compare statistical, machine learning, and deep learning models using a framework that investigates the impact of different training sizes. The framework defines hyperparameters and calibration settings; the dataset and models are made public to ensure reproducibility and to be used as benchmarks for future works. An extensive numerical study shows that well-performing models in the day-ahead market do not perform well in the balancing one, highlighting that these markets are fundamentally different constructs. The best model is LEAR, a statistical approach based on LASSO, which outperforms more complex and computationally demanding approaches.",
        "references": [
            {
                "arxivId": "2205.11439",
                "title": "Probabilistic Forecasting of German Electricity Imbalance Prices",
                "abstract": "The imbalance market is very volatile and often exhibits extreme price spikes. This makes it very hard to model; however, if predicted correctly, one could make significant gains by participating on the right side of the market. In this manuscript, we conduct a very short-term probabilistic forecasting of imbalance prices, contributing to the scarce literature in this novel subject. The forecasting is performed 30 min before the delivery, so that the trader might still choose the trading place. The distribution of the imbalance prices is modelled and forecasted using methods well-known in the electricity price forecasting literature: lasso with bootstrap, gamlss, and probabilistic neural networks. The methods are compared with a naive benchmark in a meaningful rolling window study. The results provide evidence of the efficiency between the intraday and balancing markets as the sophisticated methods do not substantially overperform the intraday continuous price index. On the other hand, they significantly improve the empirical coverage. Therefore, the traders should avoid participating in the balancing market, which is inline with the objective and current regulations of the market. The analysis was conducted on the German market; however, it could be easily applied to any other market of a similar structure."
            },
            {
                "arxivId": "2101.05249",
                "title": "Day-ahead electricity price prediction applying hybrid models of LSTM-based deep learning methods and feature selection algorithms under consideration of market coupling",
                "abstract": null
            },
            {
                "arxivId": "2008.08004",
                "title": "Forecasting day-ahead electricity prices: A review of state-of-the-art algorithms, best practices and an open-access benchmark",
                "abstract": null
            },
            {
                "arxivId": "2007.15745",
                "title": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice",
                "abstract": null
            },
            {
                "arxivId": "2106.07361",
                "title": "Probabilistic Forecasting of Imbalance Prices in the Belgian Context",
                "abstract": "Forecasting imbalance prices is essential for strategic participation in the short-term energy markets. A novel two-step probabilistic approach is proposed, with a particular focus on the Belgian case. The first step consists in computing the net regulation volume state transition probabilities. It is modeled as a matrix computed using historical data. This matrix is then used to infer the imbalance prices, since the net regulation volume can be related to the level of reserves activated and the corresponding marginal prices for each activation level are published by the Belgian Transmission System Operator one day before electricity delivery. This approach is compared to a deterministic model, a multi-layer perceptron, and to a widely used probabilistic technique, Gaussian Processes."
            },
            {
                "arxivId": "1905.07886",
                "title": "Conformal prediction interval estimation and applications to day-ahead and intraday power markets",
                "abstract": null
            },
            {
                "arxivId": "1603.02754",
                "title": "XGBoost: A Scalable Tree Boosting System",
                "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
            },
            {
                "arxivId": "1201.0490",
                "title": "Scikit-learn: Machine Learning in Python",
                "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2205.08614",
        "category": "q-fin",
        "title": "Well Posedness of Utility Maximization Problems Under Partial Information in a Market with Gaussian Drift",
        "abstract": "This paper investigates well posedness of utility maximization problems for financial markets where stock returns depend on a hidden Gaussian mean reverting drift process. Since that process is potentially unbounded, well posedness cannot be guaranteed for utility functions which are not bounded from above. For power utility with relative risk aversion smaller than those of log-utility this leads to restrictions on the choice of model parameters such as the investment horizon and parameters controlling the variance of the asset price and drift processes. We derive sufficient conditions to the model parameters leading to bounded maximum expected utility of terminal wealth for models with full and partial information.",
        "references": [
            {
                "arxivId": "2308.02049",
                "title": "Portfolio Optimization in a Market with Hidden Gaussian Drift and Randomly Arriving Expert Opinions: Modeling and Theoretical Results",
                "abstract": "This paper investigates the optimal selection of portfolios for power utility maximizing investors in a financial market where stock returns depend on a hidden Gaussian mean reverting drift process. Information on the drift is obtained from returns and expert opinions in the form of noisy signals about the current state of the drift arriving randomly over time. The arrival dates are modeled as the jump times of a homogeneous Poisson process. Applying Kalman filter techniques we derive estimates of the hidden drift which are described by the conditional mean and covariance of the drift given the observations. The utility maximization problem is solved with dynamic programming methods. We derive the associated dynamic programming equation and study regularization arguments for a rigorous mathematical justification."
            },
            {
                "arxivId": "2301.06847",
                "title": "Power Utility Maximization with Expert Opinions at Fixed Arrival Times in a Market with Hidden Gaussian Drift",
                "abstract": "In this paper we study optimal trading strategies in a \ufb01nancial market in which stock returns depend on a hidden Gaussian mean reverting drift process. Investors obtain information on that drift by observing stock returns. Moreover, expert opinions in the form of signals about the current state of the drift arriving at \ufb01xed and known dates are included in the analysis. Drift estimates are based on Kalman \ufb01lter techniques. They are used to transform a power utility maximization problem under partial information into an optimization problem under full information where the state variable is the \ufb01lter of the drift. The dynamic programming equation for this problem is studied and closed-form solutions for the value function and the optimal trading strategy of an investor are derived. They allow to quantify the monetary value of information delivered by the expert opinions. We illustrate our theoretical \ufb01ndings by results of extensive numerical experiments."
            },
            {
                "arxivId": "2210.08422",
                "title": "Duality in optimal consumption--investment problems with alternative data",
                "abstract": "This study investigates an optimal consumption--investment problem in which the unobserved stock trend is modulated by a hidden Markov chain that represents different economic regimes. In the classical approach, the hidden state is estimated from historical asset prices, but recent advancements in technology enable investors to consider alternative data in their decision-making. These include social media commentary, expert opinions, COVID-19 pandemic data, and GPS data, which originate outside of the standard sources of market data but are considered useful for predicting stock trends. We develop a novel duality theory for this problem and consider a jump-diffusion process for the alternative data series. This theory helps investors in identifying ``useful'' alternative data for dynamic decision-making by offering conditions to the filter equation that permit the use of a control approach based on the dynamic programming principle. We demonstrate an application for proving a unique smooth solution for a constant relative risk-averse agent once the distributions of the signals generated from alternative data satisfy a bounded likelihood ratio condition. In doing so, we obtain an explicit consumption--investment strategy that takes advantage of different types of alternative data that have not been addressed in the literature."
            },
            {
                "arxivId": "1909.07837",
                "title": "The value of knowing the market price of risk",
                "abstract": "We study an optimal allocation problem in a financial market with one risk-free and one risky asset, when the market is driven by a stochastic market price of risk. The problem is set in continuous time, for an investor with a constant relative risk aversion utility, under two scenarios: when the market price of risk is observable (the full information case), and when it is not (the partial information case). The corresponding market models are complete in the partial information case and incomplete under full information. We study how the access to more accurate information on the market price of risk affects the optimal strategies and we determine the maximum price that the investor would be willing to pay to receive such information. In particular, we examine two cases of additional information, when an exact observation of the market price of risk is available either at time 0 only (the initial information case), or during the whole investment period (the dynamic information case)."
            },
            {
                "arxivId": "1812.03453",
                "title": "Asymptotic filter behavior for high-frequency expert opinions in a market with Gaussian drift",
                "abstract": "Abstract This paper investigates a financial market where stock returns depend on a hidden Gaussian mean reverting drift process. Information on the drift is obtained from returns and expert opinions in the form of noisy signals about the current state of the drift arriving at the jump times of a homogeneous Poisson process. Drift estimates are based on Kalman filter techniques and described by the conditional mean and covariance matrix of the drift given the observations. We study the filter asymptotics for increasing arrival intensity of expert opinions and prove that the conditional mean is a consistent drift estimator; it converges in the mean-square sense to the hidden drift. Thus, in the limit as the arrival intensity goes to infinity investors have full information about the drift."
            },
            {
                "arxivId": "1807.00568",
                "title": "Diffusion approximations for randomly arriving expert opinions in a financial market with Gaussian drift",
                "abstract": "Abstract This paper investigates a financial market where stock returns depend on an unobservable Gaussian mean reverting drift process. Information on the drift is obtained from returns and randomly arriving discrete-time expert opinions. Drift estimates are based on Kalman filter techniques. We study the asymptotic behavior of the filter for high-frequency experts with variances that grow linearly with the arrival intensity. The derived limit theorems state that the information provided by discrete-time expert opinions is asymptotically the same as that from observing a certain diffusion process. These diffusion approximations are extremely helpful for deriving simplified approximate solutions of utility maximization problems."
            },
            {
                "arxivId": "1608.08268",
                "title": "On the Market-Neutrality of Optimal Pairs-Trading Strategies",
                "abstract": "We consider the problem of optimal investment in a market with two cointegrated stocks and an agent with CRRA utility. We extend the findings of Liu and Timmermann [The Review of Financial Studies, 26(4):1048-1086, 2013] by paying special attention to when/if the associated stochastic control problem is well-posed and providing a verification result. Our new findings lead to a sharp well-posedness condition which is, surprisingly, also the necessary and sufficient condition for the optimal investment to be market-neutral (i.e. having offsetting long/short positions in the stocks). Hence, we provide a theoretical justification for market-neutral pairs-trading which, despite having a strong practical relevance, has been lacking a theoretical ground."
            },
            {
                "arxivId": "1601.08155",
                "title": "Expert Opinions and Logarithmic Utility Maximization for Multivariate Stock Returns with Gaussian Drift",
                "abstract": "This paper investigates optimal trading strategies in a financial market with multidimensional stock returns where the drift is an unobservable multivariate Ornstein-Uhlenbeck process. Information about the drift is obtained by observing stock returns and expert opinions. The latter provide unbiased estimates on the current state of the drift at discrete points in time. \nThe optimal trading strategy of investors maximizing expected logarithmic utility of terminal wealth depends on the filter which is the conditional expectation of the drift given the available information. We state filtering equations to describe its dynamics for different information settings. Between expert opinions this is the Kalman filter. The conditional covariance matrices of the filter follow ordinary differential equations of Riccati type. We rely on basic theory about matrix Riccati equations to investigate their properties. Firstly, we consider the asymptotic behaviour of the covariance matrices for an increasing number of expert opinions on a finite time horizon. Secondly, we state conditions for the convergence of the covariance matrices on an infinite time horizon with regularly arriving expert opinions. \nFinally, we derive the optimal trading strategy of an investor. The optimal expected logarithmic utility of terminal wealth, the value function, is a functional of the conditional covariance matrices. Hence, our analysis of the covariance matrices allows us to deduce properties of the value function."
            },
            {
                "arxivId": "1402.6313",
                "title": "Expert opinions and logarithmic utility maximization in a market with Gaussian drift",
                "abstract": "This paper investigates optimal portfolio strategies in a financial market where the drift of the stock returns is driven by an unobserved Gaussian mean reverting process. Information on this process is obtained from observing stock returns and expert opinions. The latter provide at discrete time points an unbiased estimate of the current state of the drift. Nevertheless, the drift can only be observed partially and the best estimate is given by the conditional expectation given the available information, i.e., by the filter. We provide the filter equations in the model with expert opinion and derive in detail properties of the conditional variance. For an investor who maximizes expected logarithmic utility of his portfolio, we derive the optimal strategy explicitly in different settings for the available information. The optimal expected utility, the value function of the control problem, depends on the conditional variance. The bounds and asymptotic results for the conditional variances are used to derive bounds and asymptotic properties for the value functions. The results are illustrated with numerical examples."
            },
            {
                "arxivId": "1303.2513",
                "title": "Portfolio Optimization under Partial Information with Expert Opinions: a Dynamic Programming Approach",
                "abstract": "This paper investigates optimal portfolio strategies in a market where the drift is driven by an unobserved Markov chain. Information on the state of this chain is obtained from stock prices and expert opinions in the form of signals at random discrete time points. As in Frey et al. (2012), Int. J. Theor. Appl. Finance, 15, No. 1, we use stochastic filtering to transform the original problem into an optimization problem under full information where the state variable is the filter for the Markov chain. The dynamic programming equation for this problem is studied with viscosity-solution techniques and with regularization arguments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2211.13100",
        "category": "q-fin",
        "title": "Leverage, Endogenous Unbalanced Growth, and Asset Price Bubbles",
        "abstract": "We present a general equilibrium macro-finance model with a positive feedback loop between capital investment and land price. As leverage is relaxed beyond a critical value, through the financial accelerator, a phase transition occurs from balanced growth where land prices reflect fundamentals (present value of rents) to unbalanced growth where land prices grow faster than rents, generating land price bubbles. Unbalanced growth dynamics and bubbles are associated with financial loosening and technological progress. In an analytically tractable two-sector large open economy model with unique equilibria, financial loosening simultaneously leads to low interest rates, asset overvaluation, and top-end wealth concentration.",
        "references": [
            {
                "arxivId": "2311.03638",
                "title": "Bubble economics",
                "abstract": null
            },
            {
                "arxivId": "2307.00349",
                "title": "Unbalanced Growth, Elasticity of Substitution, and Land Overvaluation",
                "abstract": "We study the long-run behavior of land prices when land plays the dual role of factor of production and store of value. In modern economies where technological progress is faster in non-land sectors, when the elasticity of substitution in production exceeds 1 at high input levels (which always holds if non-land factors do not fully depreciate), unbalanced growth occurs and land becomes overvalued on the long-run trend relative to the fundamental value defined by the present value of land rents. Around the trend, land prices exhibit recurrent stochastic fluctuations, with expansions and contractions in the size of land overvaluation."
            },
            {
                "arxivId": "2305.08268",
                "title": "Bubble Necessity Theorem",
                "abstract": "Asset price bubbles are situations where asset prices exceed the fundamental values defined by the present value of dividends. This paper presents a conceptually new perspective: the necessity of bubbles. We establish the Bubble Necessity Theorem in a plausible general class of economic models: with faster long-run economic growth ($G$) than dividend growth ($G_d$) and counterfactual long-run autarky interest rate ($R$) below dividend growth, all equilibria are bubbly with non-negligible bubble sizes relative to the economy. This bubble necessity condition naturally arises in economies with sufficiently strong savings motives and multiple factors or sectors with uneven productivity growth."
            },
            {
                "arxivId": "2201.10673",
                "title": "Robust comparative statics for the elasticity of intertemporal substitution",
                "abstract": "We study a general class of consumption\u2013savings problems with recursive preferences. We characterize the sign of the consumption response to arbitrary shocks in terms of the product of two sufficient statistics: the elasticity of intertemporal substitution (EIS) between contemporaneous consumption and continuation utility, and the relative elasticity of the marginal value of wealth (REMV). Under homotheticity, the REMV always equals 1, so the propensity of the agent to save or \u201cdis\u2010save\u201d is always signed by the relationship of the EIS with unity. We apply our results to derive comparative statics in classical problems of portfolio allocation, consumption\u2013savings with income risk, and entrepreneurial investment. Our results suggest empirical identification strategies for both the value of the EIS and its relationship with unity."
            },
            {
                "arxivId": "1712.01431",
                "title": "Determination of Pareto Exponents in Economic Models Driven by Markov Multiplicative Processes",
                "abstract": "This article contains new tools for studying the shape of the stationary distribution of sizes in a dynamic economic system in which units experience random multiplicative shocks and are occasionally reset. Each unit has a Markov\u2010switching type, which influences their growth rate and reset probability. We show that the size distribution has a Pareto upper tail, with exponent equal to the unique positive solution to an equation involving the spectral radius of a certain matrix\u2010valued function. Under a nonlattice condition on growth rates, an eigenvector associated with the Pareto exponent provides the distribution of types in the upper tail of the size distribution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2303.16266",
        "category": "q-fin",
        "title": "On-line reinforcement learning for optimization of real-life energy trading strategy",
        "abstract": "An increasing share of energy is produced from renewable sources by many small producers. The efficiency of those sources is volatile and, to some extent, random, exacerbating the problem of energy market balancing. In many countries, this balancing is done on the day-ahead (DA) energy markets. This paper considers automated trading on the DA energy market by a medium-sized prosumer. We model this activity as a Markov Decision Process and formalize a framework in which an applicable in real-life strategy can be optimized with off-line data. We design a trading strategy that is fed with the available environmental information that can impact future prices, including weather forecasts. We use state-of-the-art reinforcement learning (RL) algorithms to optimize this strategy. For comparison, we also synthesize simple parametric trading strategies and optimize them with an evolutionary algorithm. Results show that our RL-based strategy generates the highest market profits.",
        "references": [
            {
                "arxivId": "2210.03469",
                "title": "Algorithmic Trading Using Continuous Action Space Deep Reinforcement Learning",
                "abstract": "Price movement prediction has always been one of the traders' concerns in financial market trading. In order to increase their profit, they can analyze the historical data and predict the price movement. The large size of the data and complex relations between them lead us to use algorithmic trading and artificial intelligence. This paper aims to offer an approach using Twin-Delayed DDPG (TD3) and the daily close price in order to achieve a trading strategy in the stock and cryptocurrency markets. Unlike previous studies using a discrete action space reinforcement learning algorithm, the TD3 is continuous, offering both position and the number of trading shares. Both the stock (Amazon) and cryptocurrency (Bitcoin) markets are addressed in this research to evaluate the performance of the proposed algorithm. The achieved strategy using the TD3 is compared with some algorithms using technical analysis, reinforcement learning, stochastic, and deterministic strategies through two standard metrics, Return and Sharpe ratio. The results indicate that employing both position and the number of trading shares can improve the performance of a trading system based on the mentioned metrics."
            },
            {
                "arxivId": "2002.06063",
                "title": "Robust Reinforcement Learning via Adversarial training with Langevin Dynamics",
                "abstract": "We introduce a sampling perspective to tackle the challenging task of training robust Reinforcement Learning (RL) agents. Leveraging the powerful Stochastic Gradient Langevin Dynamics, we present a novel, scalable two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our algorithm consistently outperforms existing baselines, in terms of generalization across different training and testing conditions, on several MuJoCo environments. Our experiments also show that, even for objective functions that entirely ignore potential environmental shifts, our sampling approach remains highly robust in comparison to standard RL algorithms."
            },
            {
                "arxivId": "1906.07516",
                "title": "Robust Reinforcement Learning for Continuous Control with Model Misspecification",
                "abstract": "We provide a framework for incorporating robustness -- to perturbations in the transition dynamics which we refer to as model misspecification -- into continuous control Reinforcement Learning (RL) algorithms. We specifically focus on incorporating robustness into a state-of-the-art continuous control RL algorithm called Maximum a-posteriori Policy Optimization (MPO). We achieve this by learning a policy that optimizes for a worst case expected return objective and derive a corresponding robust entropy-regularized Bellman contraction operator. In addition, we introduce a less conservative, soft-robust, entropy-regularized objective with a corresponding Bellman operator. We show that both, robust and soft-robust policies, outperform their non-robust counterparts in nine Mujoco domains with environment perturbations. In addition, we show improved robust performance on a high-dimensional, simulated, dexterous robotic hand. Finally, we present multiple investigative experiments that provide a deeper insight into the robustness framework. This includes an adaptation to another continuous control RL algorithm as well as learning the uncertainty set from offline data. Performance videos can be found online at this https URL."
            },
            {
                "arxivId": "1812.02341",
                "title": "Quantifying Generalization in Reinforcement Learning",
                "abstract": "In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization."
            },
            {
                "arxivId": "1810.12282",
                "title": "Assessing Generalization in Deep Reinforcement Learning",
                "abstract": "Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but agents often fail to generalize beyond the environment they were trained in. As a result, deep RL algorithms that promote generalization are receiving increasing attention. However, works in this area use a wide variety of tasks and experimental setups for evaluation. The literature lacks a controlled assessment of the merits of different generalization schemes. Our aim is to catalyze community-wide progress on generalization in deep RL. To this end, we present a benchmark and experimental protocol, and conduct a systematic empirical study. Our framework contains a diverse set of environments, our methodology covers both in-distribution and out-of-distribution generalization, and our evaluation includes deep RL algorithms that specifically tackle generalization. Our key finding is that `vanilla' deep RL algorithms generalize better than specialized schemes that were proposed specifically to tackle generalization."
            },
            {
                "arxivId": "1806.07937",
                "title": "A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning",
                "abstract": "The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners."
            },
            {
                "arxivId": "1801.01290",
                "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
            },
            {
                "arxivId": "1707.06347",
                "title": "Proximal Policy Optimization Algorithms",
                "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
            },
            {
                "arxivId": "1703.02660",
                "title": "Towards Generalization and Simplicity in Continuous Control",
                "abstract": "This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video."
            },
            {
                "arxivId": "1604.00772",
                "title": "The CMA Evolution Strategy: A Tutorial",
                "abstract": "This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain."
            },
            {
                "arxivId": "1602.01783",
                "title": "Asynchronous Methods for Deep Reinforcement Learning",
                "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2307.10983",
        "category": "q-fin",
        "title": "Commitment and the Dynamics of Household Labor Supply",
        "abstract": "The extent to which individuals commit to their partner for life has important implications. This paper develops a lifecycle collective model of the household, through which it characterizes behavior in three prominent alternative types of commitment: full, limited, and no commitment. We propose a test that distinguishes between all three types based on how contemporaneous and historical news affect household behavior. Our test permits heterogeneity in the degree of commitment across households. Using recent data from the Panel Study of Income Dynamics, we reject full and no commitment, while we find strong evidence for limited commitment.",
        "references": [
            {
                "arxivId": "2102.07476",
                "title": "Personality Traits and the Marriage Market",
                "abstract": "Which and how many attributes are relevant for the sorting of agents in a matching market? This paper addresses these questions by constructing indices of mutual attractiveness that aggregate information about agents\u2019 attributes. The first k indices for agents on each side of the market provide the best approximation of the matching surplus by a k-dimensional model. The methodology is applied on a unique Dutch household survey containing information about education, height, body mass index, health, attitude toward risk, and personality traits of spouses."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2309.02970",
        "category": "q-fin",
        "title": "On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making",
        "abstract": "We study the effect of stochastic feeding costs on animal-based commodities with particular focus on aquaculture. More specifically, we use soybean futures to infer on the stochastic behaviour of salmon feed, which we assume to follow a Schwartz-2-factor model. We compare the decision of harvesting salmon using a decision rule assuming either deterministic or stochastic feeding costs, i.e. including feeding cost risk. We identify cases, where accounting for stochastic feeding costs leads to significant improvements as well as cases where deterministic feeding costs are a good enough proxy. Nevertheless, in all of these cases, the newly derived rules show superior performance, while the additional computational costs are negligible. From a methodological point of view, we demonstrate how to use Deep-Neural-Networks to infer on the decision boundary that determines harvesting or continuation, improving on more classical regression-based and curve-fitting methods. To achieve this we use a deep classifier, which not only improves on previous results but also scales well for higher dimensional problems, and in addition mitigates effects due to model uncertainty, which we identify in this article. effects due to model uncertainty, which we identify in this article.",
        "references": [
            {
                "arxivId": "2001.03984",
                "title": "Estimating the Competitive Storage Model with Stochastic Trends in Commodity Prices",
                "abstract": "We propose a State-Space Model (SSM) for commodity prices that combines the competitive storage model with a stochastic trend. This approach fits into the economic rationality of storage decisions and adds to previous deterministic trend specifications of the storage model. For a Bayesian posterior analysis of the SSM, which is nonlinear in the latent states, we used a Markov chain Monte Carlo algorithm based on the particle marginal Metropolis\u2013Hastings approach. An empirical application to four commodity markets showed that the stochastic trend SSM is favored over deterministic trend specifications. The stochastic trend SSM identifies structural parameters that differ from those for deterministic trend specifications. In particular, the estimated price elasticities of demand are typically larger under the stochastic trend SSM."
            },
            {
                "arxivId": "2001.01098",
                "title": "On the Stochastic Magnus Expansion and Its Application to SPDEs",
                "abstract": null
            },
            {
                "arxivId": "1908.01602",
                "title": "Solving high-dimensional optimal stopping problems using deep learning",
                "abstract": "Nowadays many financial derivatives, such as American or Bermudan options, are of early exercise type. Often the pricing of early exercise options gives rise to high-dimensional optimal stopping problems, since the dimension corresponds to the number of underlying assets. High-dimensional optimal stopping problems are, however, notoriously difficult to solve due to the well-known curse of dimensionality. In this work, we propose an algorithm for solving such problems, which is based on deep learning and computes, in the context of early exercise option pricing, both approximations of an optimal exercise strategy and the price of the considered option. The proposed algorithm can also be applied to optimal stopping problems that arise in other areas where the underlying stochastic process can be efficiently simulated. We present numerical results for a large number of example problems, which include the pricing of many high-dimensional American and Bermudan options, such as Bermudan max-call options in up to 5000 dimensions. Most of the obtained results are compared to reference values computed by exploiting the specific problem design or, where available, to reference values from the literature. These numerical results suggest that the proposed algorithm is highly effective in the case of many underlyings, in terms of both accuracy and speed."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2310.09181",
        "category": "q-fin",
        "title": "A generalization of the rational rough Heston approximation",
        "abstract": "Previously, in [GR19], we derived a rational approximation of the solution of the rough Heston fractional ODE in the special case \\lambda = 0, which corresponds to a pure power-law kernel. In this paper we extend this solution to the general case of the Mittag-Leffler kernel with \\lambda \\geq 0. We provide numerical evidence of the convergence of the solution.",
        "references": [
            {
                "arxivId": "2009.00557",
                "title": "The SINC way: a fast and accurate approach to Fourier pricing",
                "abstract": "The goal of this paper is to investigate the method outlined by one of us (P. R.) in Cherubini, U., Della Lunga, G., Mulinacci, S. and Rossi, P. [Fourier Transform Methods in Finance, 2009 (John Wiley & Sons Inc.).] to compute option prices. We name it the SINC approach. While the COS method by Fang, F. and Oosterlee, C.W. [A novel pricing method for european options based on Fourier-cosine series expansions. SIAM. J. Sci. Comput., 2009, 31(2), 826\u2013848.] leverages the Fourier-cosine expansion of truncated densities, the SINC approach builds on the Shannon Sampling Theorem revisited for functions with bounded support. We provide several results which were missing in the early derivation: (i) a rigorous proof of the convergence of the SINC formula to the correct option price when the support grows and the number of Fourier frequencies increases; (ii) ready to implement formulas for put, Cash-or-Nothing, and Asset-or-Nothing options; (iii) a systematic comparison with the COS formula for several log-price models; iv) a numerical challenge against alternative Fast Fourier specifications, such as Carr, P. and Madan, D. [Option valuation using the fast Fourier transform. J. Comput. Finance, 1999, 2(4), 61\u201373.] and Lewis, A.L. [Option Valuation Under Stochastic Volatility with Mathematica Code, 2000 (Newport Beach: Finance Press).]; (v) an extensive pricing exercise under the rough Heston model of Jaisson, T. and Rosenbaum, M. [Limit theorems for nearly unstable Hawkes processes. Ann. Appl. Probab., 2015, 25(2), 600\u2013631.]; (vi) formulas to evaluate numerically the moments of a truncated density. The advantages of the SINC approach are numerous. When compared to benchmark methodologies, SINC provides the most accurate and fast pricing computation. The method naturally lends itself to price all options in a smile concurrently by means of Fast Fourier techniques, boosting fast calibration. Pricing requires resorting only to odd moments in the Fourier space."
            },
            {
                "arxivId": "1609.02108",
                "title": "The characteristic function of rough Heston models",
                "abstract": "It has been recently shown that rough volatility models, where the volatility is driven by a fractional Brownian motion with small Hurst parameter, provide very relevant dynamics in order to reproduce the behavior of both historical and implied volatilities. However, due to the non\u2010Markovian nature of the fractional Brownian motion, they raise new issues when it comes to derivatives pricing. Using an original link between nearly unstable Hawkes processes and fractional volatility models, we compute the characteristic function of the log\u2010price in rough Heston models. In the classical Heston model, the characteristic function is expressed in terms of the solution of a Riccati equation. Here, we show that rough Heston models exhibit quite a similar structure, the Riccati equation being replaced by a fractional Riccati equation."
            },
            {
                "arxivId": "1504.03100",
                "title": "Rough fractional diffusions as scaling limits of nearly unstable heavy tailed Hawkes processes",
                "abstract": "We investigate the asymptotic behavior as time goes to infinity of Hawkes processes whose regression kernel has $L^1$ norm close to one and power law tail of the form $x^{-(1+\\alpha)}$, with $\\alpha\\in(0,1)$. We in particular prove that when $\\alpha\\in(1/2,1)$, after suitable rescaling, their law converges to that of a kind of integrated fractional Cox-Ingersoll-Ross process, with associated Hurst parameter $H=\\alpha-1/2$. This result is in contrast to the case of a regression kernel with light tail, where a classical Brownian CIR process is obtained at the limit. Interestingly, it shows that persistence properties in the point process can lead to an irregular behavior of the limiting process. This theoretical result enables us to give an agent-based foundation to some recent findings about the rough nature of volatility in financial markets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.08143",
        "category": "q-fin",
        "title": "Artificial intelligence and the transformation of higher education institutions",
        "abstract": "Artificial intelligence (AI) advances and the rapid adoption of generative AI tools like ChatGPT present new opportunities and challenges for higher education. While substantial literature discusses AI in higher education, there is a lack of a systemic approach that captures a holistic view of the AI transformation of higher education institutions (HEIs). To fill this gap, this article, taking a complex systems approach, develops a causal loop diagram (CLD) to map the causal feedback mechanisms of AI transformation in a typical HEI. Our model accounts for the forces that drive the AI transformation and the consequences of the AI transformation on value creation in a typical HEI. The article identifies and analyzes several reinforcing and balancing feedback loops, showing how, motivated by AI technology advances, the HEI invests in AI to improve student learning, research, and administration. The HEI must take measures to deal with academic integrity problems and adapt to changes in available jobs due to AI, emphasizing AI-complementary skills for its students. However, HEIs face a competitive threat and several policy traps that may lead to decline. HEI leaders need to become systems thinkers to manage the complexity of the AI transformation and benefit from the AI feedback loops while avoiding the associated pitfalls. We also discuss long-term scenarios, the notion of HEIs influencing the direction of AI, and directions for future research on AI transformation.",
        "references": [
            {
                "arxivId": "2306.10052",
                "title": "Assigning AI: Seven Approaches for Students, with Prompts",
                "abstract": "This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the\"human in the loop,\"the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms"
            },
            {
                "arxivId": "2306.03823",
                "title": "Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots",
                "abstract": null
            },
            {
                "arxivId": "2304.11771",
                "title": "Generative AI at Work",
                "abstract": "We study the staggered introduction of a generative AI-based conversational assistant using data from 5,000 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14 percent on average, with the greatest impact on novice and low-skilled workers, and minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the potentially tacit knowledge of more able workers and helps newer workers move down the experience curve. In addition, we show that AI assistance improves customer sentiment, reduces requests for managerial intervention, and improves employee retention."
            },
            {
                "arxivId": "2303.11146",
                "title": "On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?",
                "abstract": "In late 2022, OpenAI released a new version of ChatGPT, a sophisticated natural language processing system capable of holding natural conversations while preserving and responding to the context of the discussion. ChatGPT has exceeded expectations in its abilities, leading to extensive considerations of its potential applications and misuse. In this work, we evaluate the influence of ChatGPT on university education, with a primary focus on computer security-oriented specialization. We gather data regarding the effectiveness and usability of this tool for completing exams, programming assignments, and term papers. We evaluate multiple levels of tool misuse, ranging from utilizing it as a consultant to simply copying its outputs. While we demonstrate how easily ChatGPT can be used to cheat, we also discuss the potentially significant benefits to the educational system. For instance, it might be used as an aid (assistant) to discuss problems encountered while solving an assignment or to speed up the learning process. Ultimately, we discuss how computer science higher education should adapt to tools like ChatGPT."
            },
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            },
            {
                "arxivId": "2303.04226",
                "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
                "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC."
            },
            {
                "arxivId": "2303.01157",
                "title": "How will Language Modelers like ChatGPT Affect Occupations and Industries?",
                "abstract": "Recent dramatic increases in AI language modeling capabilities has led to many questions about the effect of these technologies on the economy. In this paper we present a methodology to systematically assess the extent to which occupations, industries and geographies are exposed to advances in AI language modeling capabilities. We find that the top occupations exposed to language modeling include telemarketers and a variety of post-secondary teachers such as English language and literature, foreign language and literature, and history teachers. We find the top industries exposed to advances in language modeling are legal services and securities, commodities, and investments. We also find a positive correlation between wages and exposure to AI language modeling."
            },
            {
                "arxivId": "2302.04536",
                "title": "ChatGPT-3.5 as writing assistance in students\u2019 essays",
                "abstract": null
            },
            {
                "arxivId": "2301.01602",
                "title": "Unpacking the \"Black Box\" of AI in Education",
                "abstract": ": Recent advances in Artificial Intelligence (AI) have sparked renewed interest in its potential to improve education. However, AI is a loose umbrella term that refers to a collection of methods, capabilities, and limitations\u2014many of which are often not explicitly articulated by researchers, education technology companies, or other AI developers. In this paper, we seek to clarify what \u201cAI\u201d is and the potential it holds to both advance and hamper educational opportunities that may improve the human condition. We offer a basic introduction to different methods and philosophies underpinning AI, discuss recent advances, explore applications to education, and highlight key limitations and risks. We conclude with a set of questions that educationalists may ask as they encounter AI in their research and practice. Our hope is to make often jargon-laden terms and concepts accessible, so that all are equipped to understand, interrogate, and ultimately shape the development of human-centered AI in education."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.08755",
        "category": "q-fin",
        "title": "LLM-driven Imitation of Subrational Behavior : Illusion or Reality?",
        "abstract": "Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework.",
        "references": [
            {
                "arxivId": "2307.14984",
                "title": "S3: Social-network Simulation System with Large Language Model-Empowered Agents",
                "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science."
            },
            {
                "arxivId": "2307.04986",
                "title": "Epidemic Modeling with Generative Agents",
                "abstract": "This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models. Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT. Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise. Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period. Moreover, the agents successfully flatten the epidemic curve. This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making."
            },
            {
                "arxivId": "2305.10782",
                "title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "abstract": "Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4<5$) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number representations of LLMs and their cognitive plausibility."
            },
            {
                "arxivId": "2305.01937",
                "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
                "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms.Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided.In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks.We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer.We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation."
            },
            {
                "arxivId": "2305.03514",
                "title": "Can Large Language Models Transform Computational Social Science?",
                "abstract": "Abstract Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers\u2019 gold references. We conclude that the performance of today\u2019s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2303.12712",
                "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
                "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions."
            },
            {
                "arxivId": "2303.11191",
                "title": "A Survey of Demonstration Learning",
                "abstract": "With the fast improvement of machine learning, reinforcement learning (RL) has been used to automate human tasks in different areas. However, training such agents is difficult and restricted to expert users. Moreover, it is mostly limited to simulation environments due to the high cost and safety concerns of interactions in the real world. Demonstration Learning is a paradigm in which an agent learns to perform a task by imitating the behavior of an expert shown in demonstrations. It is a relatively recent area in machine learning, but it is gaining significant traction due to having tremendous potential for learning complex behaviors from demonstrations. Learning from demonstration accelerates the learning process by improving sample efficiency, while also reducing the effort of the programmer. Due to learning without interacting with the environment, demonstration learning would allow the automation of a wide range of real world applications such as robotics and healthcare. This paper provides a survey of demonstration learning, where we formally introduce the demonstration problem along with its main challenges and provide a comprehensive overview of the process of learning from demonstrations from the creation of the demonstration data set, to learning methods from demonstrations, and optimization by combining demonstration learning with different machine learning methods. We also review the existing benchmarks and identify their strengths and limitations. Additionally, we discuss the advantages and disadvantages of the paradigm as well as its main applications. Lastly, we discuss our perspective on open problems and research directions for this rapidly growing field."
            },
            {
                "arxivId": "2303.00001",
                "title": "Reward Design with Language Models",
                "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning"
            },
            {
                "arxivId": "2302.11996",
                "title": "K-SHAP: Policy Clustering Algorithm for Anonymous Multi-Agent State-Action Pairs",
                "abstract": "Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a world-policy able to mimic all the agent behaviors upon different environmental states. We leverage the world-policy to explain each anonymous observation through an additive feature attribution method called SHAP (SHapley Additive exPlanations). Finally, by clustering the explanations we show that we are able to identify different agent policies and group observations accordingly. We evaluate our approach on simulated synthetic market data and a real-world financial dataset. We show that our proposal significantly and consistently outperforms the existing methods, identifying different agent strategies."
            },
            {
                "arxivId": "2301.07597",
                "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
                "abstract": "The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection."
            },
            {
                "arxivId": "2212.10403",
                "title": "Towards Reasoning in Large Language Models: A Survey",
                "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work."
            },
            {
                "arxivId": "2212.10071",
                "title": "Large Language Models Are Reasoning Teachers",
                "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model\u2019s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher."
            },
            {
                "arxivId": "2210.08569",
                "title": "Limited or Biased: Modeling Sub-Rational Human Investors in Financial Markets",
                "abstract": "Human decision-making in real-life deviates significantly from the optimal decisions made by fully rational agents, primarily due to computational limitations or psychological biases. While existing studies in behavioral finance have discovered various aspects of human sub-rationality, there lacks a comprehensive framework to transfer these findings into an adaptive human model applicable across diverse financial market scenarios. In this study, we introduce a flexible model that incorporates five different aspects of human sub-rationality using reinforcement learning. Our model is trained using a high-fidelity multi-agent market simulator, which overcomes limitations associated with the scarcity of labeled data of individual investors. We evaluate the behavior of sub-rational human investors using hand-crafted market scenarios and SHAP value analysis, showing that our model accurately reproduces the observations in the previous studies and reveals insights of the driving factors of human behavior. Finally, we explore the impact of sub-rationality on the investor's Profit and Loss (PnL) and market quality. Our experiments reveal that bounded-rational and prospect-biased human behaviors improve liquidity but diminish price efficiency, whereas human behavior influenced by myopia, optimism, and pessimism reduces market liquidity."
            },
            {
                "arxivId": "2210.07729",
                "title": "Model-Based Imitation Learning for Urban Driving",
                "abstract": "An accurate model of the environment and the dynamic agents acting in it offers great potential for improving motion planning. We present MILE: a Model-based Imitation LEarning approach to jointly learn a model of the world and a policy for autonomous driving. Our method leverages 3D geometry as an inductive bias and learns a highly compact latent space directly from high-resolution videos of expert demonstrations. Our model is trained on an offline corpus of urban driving data, without any online interaction with the environment. MILE improves upon prior state-of-the-art by 31% in driving score on the CARLA simulator when deployed in a completely new town and new weather conditions. Our model can predict diverse and plausible states and actions, that can be interpretably decoded to bird's-eye view semantic segmentation. Further, we demonstrate that it can execute complex driving manoeuvres from plans entirely predicted in imagination. Our approach is the first camera-only method that models static scene, dynamic scene, and ego-behaviour in an urban driving environment. The code and model weights are available at https://github.com/wayveai/mile."
            },
            {
                "arxivId": "2210.09897",
                "title": "Learning to simulate realistic limit order book markets from data as a World Agent",
                "abstract": "Multi-agent market simulators usually require careful calibration to emulate real markets, which includes the number and the type of agents. Poorly calibrated simulators can lead to misleading conclusions, potentially causing severe loss when employed by investment banks, hedge funds, and traders to study and evaluate trading strategies. In this paper, we propose a world model simulator that accurately emulates a limit order book market \u2013 it requires no agent calibration but rather learns the simulated market behavior directly from historical data. Traditional approaches fail short to learn and calibrate trader population, as historical labeled data with details on each individual trader strategy is not publicly available. Our approach proposes to learn a unique \"world\" agent from historical data. It is intended to emulate the overall trader population, without the need of making assumptions about individual market agent strategies. We implement our world agent simulator models as a Conditional Generative Adversarial Network (CGAN), as well as a mixture of parametric distributions, and we compare our models against previous work. Qualitatively and quantitatively, we show that the proposed approaches consistently outperform previous work, providing more realism and responsiveness."
            },
            {
                "arxivId": "2209.06899",
                "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
                "abstract": "Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one such tool\u2014the GPT-3 language model\u2014is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create \u201csilicon samples\u201d by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."
            },
            {
                "arxivId": "2208.10264",
                "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."
            },
            {
                "arxivId": "2208.04024",
                "title": "Social Simulacra: Creating Populated Prototypes for Social Computing Systems",
                "abstract": "Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer\u2019s description of a community\u2019s design\u2014goal, rules, and member personas\u2014and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of \u201cwhat if?\u201d scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models\u2019 training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra."
            },
            {
                "arxivId": "2207.14382",
                "title": "Large Language Models and the Reverse Turing Test",
                "abstract": "Abstract Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function."
            },
            {
                "arxivId": "2206.07682",
                "title": "Emergent Abilities of Large Language Models",
                "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
            },
            {
                "arxivId": "2205.11916",
                "title": "Large Language Models are Zero-Shot Reasoners",
                "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."
            },
            {
                "arxivId": "2201.11903",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
            },
            {
                "arxivId": "2111.06956",
                "title": "Human irrationality: both bad and good for reward inference",
                "abstract": "Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference. We find that wrongly modeling a systematically irrational human as noisy-rational performs a lot worse than correctly capturing these biases -- so much so that it can be better to skip inference altogether and stick to the prior! More importantly, we show that an irrational human, when correctly modelled, can communicate more information about the reward than a perfectly rational human can. That is, if a robot has the correct model of a human's irrationality, it can make an even stronger inference than it ever could if the human were rational. Irrationality fundamentally helps rather than hinder reward inference, but it needs to be correctly accounted for."
            },
            {
                "arxivId": "2111.01243",
                "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
                "abstract": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research."
            },
            {
                "arxivId": "2110.14378",
                "title": "Towards artificial general intelligence via a multimodal foundation model",
                "abstract": null
            },
            {
                "arxivId": "1803.10122",
                "title": "World Models",
                "abstract": "We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/"
            },
            {
                "arxivId": "1606.03476",
                "title": "Generative Adversarial Imitation Learning",
                "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.08905",
        "category": "q-fin",
        "title": "Time preference, wealth and utility inequality: A microeconomic interaction and dynamic macroeconomic model connection approach",
        "abstract": "Based on interactions between individuals and others and references to social norms, this study reveals the impact of heterogeneity in time preference on wealth distribution and inequality. We present a novel approach that connects the interactions between microeconomic agents that generate heterogeneity to the dynamic equations for capital and consumption in macroeconomic models. Using this approach, we estimate the impact of changes in the discount rate due to microeconomic interactions on capital, consumption and utility and the degree of inequality. The results show that intercomparisons with others regarding consumption significantly affect capital, i.e. wealth inequality. Furthermore, the impact on utility is never small and social norms can reduce this impact. Our supporting evidence shows that the quantitative results of inequality calculations correspond to survey data from cohort and cross-cultural studies. This study's micro-macro connection approach can be deployed to connect microeconomic interactions, such as exchange, interest and debt, redistribution, mutual aid and time preference, to dynamic macroeconomic models.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.09125",
        "category": "q-fin",
        "title": "Database for the meta-analysis of the social cost of carbon (v2024.0)",
        "abstract": "A new version of the database for the meta-analysis of estimates of the social cost of carbon is presented. New records were added, and new fields on the impact of climate change and the shape of the welfare function. The database was extended to co-author and citation networks.",
        "references": [
            {
                "arxivId": "1504.06909",
                "title": "The Social Cost of Carbon with Economic and Climate Risks",
                "abstract": "Uncertainty about future economic and climate conditions substantially affects the choice of policies for managing interactions between the climate and the economy. We develop a framework of dynamic stochastic integration of climate and economy, and show that the social cost of carbon is substantially affected by both economic and climate risks and is a stochastic process with significant variation. We examine a wide but plausible range of values for critical parameters with robust results and show that large-scale computing makes it possible to analyze policies in models substantially more complex and realistic than usually used in the literature."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.09129",
        "category": "q-fin",
        "title": "Optimal Automated Market Makers: Differentiable Economics and Strong Duality",
        "abstract": "The role of a market maker is to simultaneously offer to buy and sell quantities of goods, often a financial asset such as a share, at specified prices. An automated market maker (AMM) is a mechanism that offers to trade according to some predetermined schedule; the best choice of this schedule depends on the market maker's goals. The literature on the design of AMMs has mainly focused on prediction markets with the goal of information elicitation. More recent work motivated by DeFi has focused instead on the goal of profit maximization, but considering only a single type of good (traded with a numeraire), including under adverse selection (Milionis et al. 2022). Optimal market making in the presence of multiple goods, including the possibility of complex bundling behavior, is not well understood. In this paper, we show that finding an optimal market maker is dual to an optimal transport problem, with specific geometric constraints on the transport plan in the dual. We show that optimal mechanisms for multiple goods and under adverse selection can take advantage of bundling, both improved prices for bundled purchases and sales as well as sometimes accepting payment\"in kind.\"We present conjectures of optimal mechanisms in additional settings which show further complex behavior. From a methodological perspective, we make essential use of the tools of differentiable economics to generate conjectures of optimal mechanisms, and give a proof-of-concept for the use of such tools in guiding theoretical investigations.",
        "references": [
            {
                "arxivId": "2103.14769",
                "title": "Replicating market makers",
                "abstract": null
            },
            {
                "arxivId": "2003.01497",
                "title": "A Permutation-Equivariant Neural Network Architecture For Auction Design",
                "abstract": "Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. Theoretical approaches to the problem have hit some limits in the past decades and analytical solutions are known for only a few simple settings. Computational approaches to the problem through the use of LPs have their own set of limitations. Building on the success of deep learning, a new approach was recently proposed by Duetting et al. (2019) in which the auction is modeled by a feed-forward neural network and the design problem is framed as a learning problem. The neural architectures used in that work are general purpose and do not take advantage of any of the symmetries the problem could present, such as permutation equivariance. In this work, we consider auction design problems that have permutation-equivariant symmetry and construct a neural architecture that is capable of perfectly recovering the permutation-equivariant optimal mechanism, which we show is not possible with the previous architecture. We demonstrate that permutation-equivariant architectures are not only capable of recovering previous results, they also have better generalization properties."
            },
            {
                "arxivId": "1902.09413",
                "title": "Estimating Approximate Incentive Compatibility",
                "abstract": "In practice, most mechanisms for selling, buying, matching, voting, and so on are not incentive compatible. We present techniques for estimating how far a mechanism is from incentive compatible. Given samples from the agents' type distribution, we show how to estimate the extent to which an agent can improve his utility by misreporting his type. We do so by first measuring the maximum utility an agent can gain by misreporting his type on average over the samples, assuming his true and reported types are from a finite subset---which our technique constructs---of the type space. The challenge is that by measuring utility gains over a finite subset of the type space, we might miss pairs of types t and t' where an agent with type t can greatly improve his utility by reporting the type t'. Our technique discretizes the type space by constructing a learning-theoretic cover in a higher-dimensional space. The key technical contribution is proving that the maximum utility gain over this finite subset nearly matches the maximum utility gain overall, despite the volatility of the utility functions we study. We apply our tools to the single-item and combinatorial first-price auctions, generalized second-price auction, discriminatory auction, uniform-price auction, and second-price auction with spiteful bidders. To our knowledge, these are the first guarantees for estimating approximate incentive compatibility from the mechanism designer's perspective."
            },
            {
                "arxivId": "1805.03382",
                "title": "Automated Mechanism Design via Neural Networks",
                "abstract": "Using AI approaches to automatically design mechanisms has been a central research mission at the interface of AI and economics. Previous approaches that attempt to design revenue optimal auctions for the multi-dimensional settings fall short in at least one of the three aspects: 1) representation --- search in a space that probably does not even contain the optimal mechanism; 2) exactness --- finding a mechanism that is either not truthful or far from optimal; 3) domain dependence --- need a different design for different environment settings. To resolve the three difficulties, in this paper, we put forward a unified neural network based framework that automatically learns to design revenue optimal mechanisms. Our framework consists of a mechanism network that takes an input distribution for training and outputs a mechanism, as well as a buyer network that takes a mechanism as input and output an action. Such a separation in design mitigates the difficulty to impose incentive compatibility constraints on the mechanism, by making it a rational choice of the buyer. As a result, our framework easily overcomes the previously mentioned difficulty in incorporating IC constraints and always returns exactly incentive compatible mechanisms. We then applied our framework to a number of multi-item auction design settings, for a few of which the theoretically optimal mechanisms are unknown. We then go on to theoretically prove that the mechanisms found by our framework are indeed optimal."
            },
            {
                "arxivId": "1709.10065",
                "title": "An Axiomatic Study of Scoring Rule Markets",
                "abstract": "Prediction markets are well-studied in the case where predictions are probabilities or expectations of future random variables. In 2008, Lambert, et al. proposed a generalization, which we call \"scoring rule markets\" (SRMs), in which traders predict the value of arbitrary statistics of the random variables, provided these statistics can be elicited by a scoring rule. Surprisingly, despite active recent work on prediction markets, there has not yet been any investigation into the properties of more general SRMs. To initiate such a study, we ask the following question: in what sense are SRMs \"markets\"? We classify SRMs according to several axioms that capture potentially desirable qualities of a market, such as the ability to freely exchange goods (contracts) for money. Not all SRMs satisfy our axioms: once a contract is purchased in any market for prediction the median of some variable, there will not necessarily be any way to sell that contract back, even in a very weak sense. Our main result is a characterization showing that slight generalizations of cost-function-based markets are the only markets to satisfy all of our axioms for finite-outcome random variables. Nonetheless, we find that several SRMs satisfy weaker versions of our axioms, including a novel share-based market mechanism for ratios of expected values."
            },
            {
                "arxivId": "1706.03459",
                "title": "Optimal Auctions through Deep Learning: Advances in Differentiable Economics",
                "abstract": "Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981, but more than 40 years later, a full analytical understanding of the optimal design still remains elusive for settings with two or more items. In this work, we initiate the exploration of the use of tools from deep learning for the automated design of optimal auctions. We model an auction as a multi-layer neural network, frame optimal auction design as a constrained learning problem, and show how it can be solved using standard machine learning pipelines. In addition to providing generalization bounds, we present extensive experimental results, recovering essentially all known solutions that come from the theoretical analysis of optimal auction design problems and obtaining novel mechanisms for settings in which the optimal mechanism is unknown."
            },
            {
                "arxivId": "1606.04145",
                "title": "Sample Complexity of Automated Mechanism Design",
                "abstract": "The design of revenue-maximizing combinatorial auctions, i.e. multi-item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale. In the traditional economic models, it is assumed that the bidders' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution. Despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown. In recent years, automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions. The most scalable automated mechanism design algorithms take as input samples from the bidders' valuation distribution and then search for a high-revenue auction in a rich auction class. In this work, we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy. In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory. In particular, the hypothesis functions used in our contexts are defined through multi-stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning."
            },
            {
                "arxivId": "1606.02825",
                "title": "Arbitrage-Free Combinatorial Market Making via Integer Programming",
                "abstract": "We present a new combinatorial market maker that operates arbitrage-free combinatorial prediction markets specified by integer programs. Although the problem of arbitrage-free pricing, while maintaining a bound on the subsidy provided by the market maker, is #P-hard in the worst case, we posit that the typical case might be amenable to modern integer programming (IP) solvers. At the crux of our method is the Frank-Wolfe (conditional gradient) algorithm which is used to implement a Bregman projection aligned with the market maker's cost function, using an IP solver as an oracle. We demonstrate the tractability and improved accuracy of our approach on real-world prediction market data from combinatorial bets placed on the 2010 NCAA Men's Division I Basketball Tournament, where the outcome space is of size $2^{63}$. To our knowledge, this is the first implementation and empirical evaluation of an arbitrage-free combinatorial prediction market on this scale."
            },
            {
                "arxivId": "1606.01610",
                "title": "Optimal Auctions with Restricted Allocations",
                "abstract": "We study the problem of designing optimal auctions under restrictions on the set of permissible allocations. In addition to allowing us to restrict to deterministic mechanisms, we can also indirectly model non-additive valuations. We prove a strong duality result, extending a result due to Daskalakis et al. [2015], that guarantees the existence of a certificate of optimality for optimal restricted mechanisms. As a corollary of our result, we provide a new characterization of the set of allocations that the optimal mechanism may actually use. To illustrate our result we find and certify optimal mechanisms for four settings where previous frameworks do not apply, and provide new economic intuition about some of the tools that have previously been used to find optimal mechanisms."
            },
            {
                "arxivId": "1409.4150",
                "title": "Strong Duality for a Multiple-Good Monopolist",
                "abstract": "We provide a duality-based framework for revenue maximization in a multiple-good monopoly. Our framework shows that every optimal mechanism has a certificate of optimality, taking the form of an optimal transportation map between measures. Using our framework, we prove that grand-bundling mechanisms are optimal if and only if two stochastic dominance conditions hold between specific measures induced by the buyer's type distribution. This result strengthens several results in the literature, where only sufficient conditions for grand-bundling optimality have been provided. As a corollary of our tight characterization of grand-bundling optimality, we show that the optimal mechanism for n independent uniform items each supported on [c; c + 1] is a grand-bundling mechanism, as long as c is sufficiently large, extending Pavlov's result for 2 items [Pavlov 2011]. Surprisingly, our characterization also implies that, for all c and for all sufficiently large n, the optimal mechanism for n independent uniform items supported on [c; c + 1] is not a grand bundling mechanism. The necessary and sufficient condition for grand bundling optimality is a special case of our more general characterization result that provides necessary and sufficient conditions for the optimality of an arbitrary mechanism for an arbitrary type distribution."
            },
            {
                "arxivId": "1404.2329",
                "title": "Duality and optimality of auctions for uniform distributions",
                "abstract": "We derive exact optimal solutions for the problem of optimizing revenue in single-bidder multi-item auctions for uniform i.i.d. valuations. We give optimal auctions of up to 6 items; previous results were only known for up to three items. To do so, we develop a general duality framework for the general problem of maximizing revenue in many-bidders multi-item additive Bayesian auctions with continuous probability valuation distributions. The framework extends linear programming duality and complementarity to constraints with partial derivatives. The dual system reveals the geometric nature of the problem and highlights its connection with the theory of bipartite graph matchings. The duality framework is used not only for proving optimality, but perhaps more importantly, for deriving the optimal auction; as a result, the optimal auction is defined by natural geometric constraints."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.09194",
        "category": "q-fin",
        "title": "The Boosted Difference of Convex Functions Algorithm for Value-at-Risk Constrained Portfolio Optimization",
        "abstract": "A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios. Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market and operational risks. For a portfolio with a discrete return distribution and finitely many scenarios, a Difference of Convex (DC) functions representation of the VaR can be derived. Wozabal (2012) showed that this yields a solution to a VaR constrained Markowitz style portfolio selection problem using the Difference of Convex Functions Algorithm (DCA). A recent algorithmic extension is the so-called Boosted Difference of Convex Functions Algorithm (BDCA) which accelerates the convergence due to an additional line search step. It has been shown that the BDCA converges linearly for solving non-smooth quadratic problems with linear inequality constraints. In this paper, we prove that the linear rate of convergence is also guaranteed for a piecewise linear objective function with linear equality and inequality constraints using the Kurdyka-{\\L}ojasiewicz property. An extended case study under consideration of best practices for comparing optimization algorithms demonstrates the superiority of the BDCA over the DCA for real-world financial market data. We are able to show that the results of the BDCA are significantly closer to the efficient frontier compared to the DCA. Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR constrained portfolio selection problems in Python.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.09243",
        "category": "q-fin",
        "title": "Exact Simulation Scheme for the Ornstein-uhlenbeck Driven Stochastic Volatility Model With the Karhunen-lo\u00e8ve Expansions",
        "abstract": "This study proposes a new exact simulation scheme of the Ornstein-Uhlenbeck driven stochastic volatility model. With the Karhunen-Lo\\`eve expansions, the stochastic volatility path following the Ornstein-Uhlenbeck process is expressed as a sine series, and the time integrals of volatility and variance are analytically derived as the sums of independent normal random variates. The new method is several hundred times faster than Li and Wu [Eur. J. Oper. Res., 2019, 275(2), 768-779] that relies on computationally expensive numerical transform inversion. The simulation algorithm is further improved with the conditional Monte-Carlo method and the martingale-preserving control variate on the spot price.",
        "references": [
            {
                "arxivId": "1506.00697",
                "title": "Approximations Of Bond And Swaption Prices In A Black\u2013Karasi\u0144ski Model",
                "abstract": "We derive semi-analytic approximation formulae for bond and swaption prices in a Black\u2013Karasinski (BK) interest rate model. Approximations are obtained using a novel technique based on the Karhunen\u2013Loeve expansion. Formulas are easily computable and prove to be very accurate in numerical tests. This makes them useful for numerically efficient calibration of the model."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-15.json",
        "arxivId": "2402.09317",
        "category": "q-fin",
        "title": "Extended mean-field games with multi-dimensional singular controls and non-linear jump impact",
        "abstract": "We establish a probabilistic framework for analysing extended mean-field games with multi-dimensional singular controls and state-dependent jump dynamics and costs. Two key challenges arise when analysing such games: the state dynamics may not depend continuously on the control and the reward function may not be u.s.c.~Both problems can be overcome by restricting the set of admissible singular controls to controls that can be approximated by continuous ones. We prove that the corresponding set of admissible weak controls is given by the weak solutions to a Marcus-type SDE and provide an explicit characterisation of the reward function. The reward function will in general only be u.s.c.~To address the lack of continuity we introduce a novel class of MFGs with a broader set of admissible controls, called MFGs of parametrisations. Parametrisations are laws of state/control processes that continuously interpolate jumps. We prove that the reward functional is continuous on the set of parametrisations, establish the existence of equilibria in MFGs of parametrisations, and show that the set of Nash equilibria in MFGs of parametrisations and in the underlying MFG with singular controls coincide. This shows that MFGs of parametrisations provide a canonical framework for analysing MFGs with singular controls and non-linear jump impact.",
        "references": [
            {
                "arxivId": "2303.05783",
                "title": "Mean\u2010field liquidation games with market drop\u2010out",
                "abstract": "We consider a novel class of portfolio liquidation games with market drop\u2010out (\u201cabsorption\u201d). More precisely, we consider mean\u2010field and finite player liquidation games where a player drops out of the market when her position hits zero. In particular, round\u2010trips are not admissible. This can be viewed as a no statistical arbitrage condition. In a model with only sellers, we prove that the absorption condition is equivalent to a short selling constraint. We prove that equilibria (both in the mean\u2010field and the finite player game) are given as solutions to a nonlinear higher\u2010order integral equation\u00a0with endogenous terminal condition. We prove the existence of a unique solution to the integral equation\u00a0from which we obtain the existence of a unique equilibrium in the MFG and the existence of a unique equilibrium in the N\u2010player game. We establish the convergence of the equilibria in the finite player games to the obtained mean\u2010field equilibrium and illustrate the impact of the drop\u2010out constraint on equilibrium trading\u00a0rates."
            },
            {
                "arxivId": "2207.00446",
                "title": "A Mean-Field Control Problem of Optimal Portfolio Liquidation with Semimartingale Strategies",
                "abstract": "We consider a mean-field control problem with c\u00e0dl\u00e0g semimartingale strategies arising in portfolio liquidation models with transient market impact and self-exciting order flow. We show that the value function depends on the state process only through its law, and we show that it is of linear-quadratic form and that its coefficients satisfy a coupled system of nonstandard Riccati-type equations. The Riccati equations are obtained heuristically by passing to the continuous-time limit from a sequence of discrete-time models. A sophisticated transformation shows that the system can be brought into standard Riccati form, from which we deduce the existence of a global solution. Our analysis shows that the optimal strategy jumps only at the beginning and the end of the trading period. Funding: Financial support is through the National Natural Science Foundation of China [Grants 12101465 and 12101523], Hong Kong Research Grants Council (Early Career Scheme) [Grant 25215122], Hong Kong Polytechnic University [Internal Grant P0044694, Internal Grant P0045668, and Startup Grant P0035348], and the Hong Kong Research Centre for Quantitative Finance [Grant P0042708]."
            },
            {
                "arxivId": "2202.06835",
                "title": "Approximation of $ N $-player stochastic games with singular controls by mean field games",
                "abstract": "This paper establishes that a class of $N$-player stochastic games with singular controls, either of bounded velocity or of finite variation, can both be approximated by mean field games (MFGs) with singular controls of bounded velocity. More specifically, it shows (i) the optimal control to an MFG with singular controls of a bounded velocity $\\theta$ is shown to be an $\\epsilon_N$-NE to an $N$-player game with singular controls of the bounded velocity, with $\\epsilon_N = O(\\frac{1}{\\sqrt{N}})$, and (ii) the optimal control to this MFG is an $(\\epsilon_N + \\epsilon_{\\theta})$-NE to an $N$-player game with singular controls of finite variation, where $\\epsilon_{\\theta}$ is an error term that depends on $\\theta$. This work generalizes the classical result on approximation $N$-player games by MFGs, by allowing for discontinuous controls."
            },
            {
                "arxivId": "2104.11128",
                "title": "A Stochastic Model of Economic Growth in Time-Space",
                "abstract": "We deal with an infinite horizon, infinite dimensional stochastic optimal control problem arising in the study of economic growth in time-space. Such problem has been the object of various papers in deterministic cases when the possible presence of stochastic disturbances is ignored (see e.g. [8], [4], [12], [6]). Here we propose and solve a stochastic generalization of such models where the stochastic term, in line with the standard stochastic economic growth models (see e.g. the books [19, Chapter 3] and [21, Chapter 9]), is a multiplicative one, driven by a cylindrical Wiener process. The problem is studied using the Dynamic Programming approach. We find an explicit solution of the associated HJB equation and, using a verification type result, we prove that such solution is the value function and we find the optimal feedback strategies. Finally we use this result to study the asymptotic behavior of the optimal trajectories."
            },
            {
                "arxivId": "2011.03105",
                "title": "Optimal Incentives to Mitigate Epidemics: A Stackelberg Mean Field Game Approach",
                "abstract": "Motivated by models of epidemic control in large populations, we consider a Stackelberg mean field game model between a principal and a mean field of agents evolving on a finite state space. The agents play a non-cooperative game in which they can control their transition rates between states to minimize an individual cost. The principal can influence the resulting Nash equilibrium through incentives so as to optimize its own objective. We analyze this game using a probabilistic approach. We then propose an application to an epidemic model of SIR type in which the agents control their interaction rate and the principal is a regulator acting with non pharmaceutical interventions. To compute the solutions, we propose an innovative numerical approach based on Monte Carlo simulations and machine learning tools for stochastic optimization. We conclude with numerical experiments by illustrating the impact of the agents' and the regulator's optimal decisions in two models: a basic SIR model with semi-explicit solutions and in a complex model which incorporates more states."
            },
            {
                "arxivId": "1612.05425",
                "title": "Mean Field Games with Singular Controls",
                "abstract": "This paper establishes the existence of relaxed solutions to mean eld games (MFGs for short) with singular controls. As a by-product, we obtain an existence of relaxed solutions results for McKean-Vlasov stochastic singular control problems. Finally, we prove approximations of solutions results for a particular class of MFGs with singular controls by solutions, respectively control rules, for MFGs with purely regular controls. Our existence and approximation results strongly hinge on the use of the Skorokhod M1 topology on the space of cadlag functions."
            },
            {
                "arxivId": "1407.6181",
                "title": "Mean field games with common noise",
                "abstract": "A theory of existence and uniqueness is developed for general stochastic differential mean field games with common noise. The concepts of strong and weak solutions are introduced in analogy with the theory of stochastic differential equations, and existence of weak solutions for mean field games is shown to hold under very general assumptions. Examples and counter-examples are provided to enlighten the underpinnings of the existence theory. Finally, an analog of the famous result of Yamada and Watanabe is derived, and it is used to prove existence and uniqueness of a strong solution under additional assumptions."
            },
            {
                "arxivId": "1404.2642",
                "title": "Mean field games via controlled martingale problems: Existence of Markovian equilibria",
                "abstract": null
            },
            {
                "arxivId": "1308.2172",
                "title": "Mean Field Games and Systemic Risk",
                "abstract": "We propose a simple model of inter-bank borrowing and lending where the evolution of the log-monetary reserves of $N$ banks is described by a system of diffusion processes coupled through their drifts in such a way that stability of the system depends on the rate of inter-bank borrowing and lending. Systemic risk is characterized by a large number of banks reaching a default threshold by a given time horizon. Our model incorporates a game feature where each bank controls its rate of borrowing/lending to a central bank. The optimization reflects the desire of each bank to borrow from the central bank when its monetary reserve falls below a critical level or lend if it rises above this critical level which is chosen here as the average monetary reserve. Borrowing from or lending to the central bank is also subject to a quadratic cost at a rate which can be fixed by the regulator. We solve explicitly for Nash equilibria with finitely many players, and we show that in this model the central bank acts as a clearing house, adding liquidity to the system without affecting its systemic risk. We also study the corresponding Mean Field Game in the limit of large number of banks in the presence of a common noise."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2207.12255",
        "category": "q-fin",
        "title": "Implementing a Hierarchical Deep Learning Approach for Simulating multilevel Auction Data",
        "abstract": "We present a deep learning solution to address the challenges of simulating realistic synthetic first-price sealed-bid auction data. The complexities encountered in this type of auction data include high-cardinality discrete feature spaces and a multilevel structure arising from multiple bids associated with a single auction instance. Our methodology combines deep generative modeling (DGM) with an artificial learner that predicts the conditional bid distribution based on auction characteristics, contributing to advancements in simulation-based research. This approach lays the groundwork for creating realistic auction environments suitable for agent-based learning and modeling applications. Our contribution is twofold: we introduce a comprehensive methodology for simulating multilevel discrete auction data, and we underscore the potential ofDGMas a powerful instrument for refining simulation techniques and fostering the development of economic models grounded in generative AI.",
        "references": [
            {
                "arxivId": "1910.09504",
                "title": "CORRGAN: Sampling Realistic Financial Correlation Matrices Using Generative Adversarial Networks",
                "abstract": "We propose a novel approach for sampling realistic financial correlation matrices. This approach is based on generative adversarial networks. Experiments demonstrate that generative adversarial networks are able to recover most of the known stylized facts about empirical correlation matrices estimated on asset returns. This is the first time such results are documented in the literature. Practical financial applications range from trading strategies enhancement to risk and portfolio stress testing. Such generative models can also help ground empirical finance deeper into science by allowing for falsifiability of statements and more objective comparison of empirical methods."
            },
            {
                "arxivId": "1908.08796",
                "title": "Reinforcement Learning in Healthcare: A Survey",
                "abstract": "As a subfield of machine learning, reinforcement learning (RL) aims at optimizing decision making by using interaction samples of an agent with its environment and the potentially delayed feedbacks. In contrast to traditional supervised learning that typically relies on one-shot, exhaustive, and supervised reward signals, RL tackles sequential decision-making problems with sampled, evaluative, and delayed feedbacks simultaneously. Such a distinctive feature makes RL techniques a suitable candidate for developing powerful solutions in various healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged period with delayed feedbacks. By first briefly examining theoretical foundations and key methods in RL research, this survey provides an extensive overview of RL applications in a variety of healthcare domains, ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis, and many other control or scheduling problems that have infiltrated every aspect of the healthcare system. In addition, we discuss the challenges and open issues in the current research and highlight some potential solutions and directions for future research."
            },
            {
                "arxivId": "1907.03355",
                "title": "Improving Detection of Credit Card Fraudulent Transactions using Generative Adversarial Networks",
                "abstract": "In this study, we employ Generative Adversarial Networks as an oversampling method to generate artificial data to assist with the classification of credit card fraudulent transactions. GANs is a generative model based on the idea of game theory, in which a generator G and a discriminator D are trying to outsmart each other. The objective of the generator is to confuse the discriminator. The objective of the discriminator is to distinguish the instances coming from the generator and the instances coming from the original dataset. By training GANs on a set of credit card fraudulent transactions, we are able to improve the discriminatory power of classifiers. The experiment results show that the Wasserstein-GAN is more stable in training and produce more realistic fraudulent transactions than the other GANs. On the other hand, the conditional version of GANs in which labels are set by k-means clustering does not necessarily improve the non-conditional versions of GANs."
            },
            {
                "arxivId": "1905.09442",
                "title": "Causal Discovery with Cascade Nonlinear Additive Noise Models",
                "abstract": "Identification of causal direction between a causal-effect pair from observed data has recently attracted much attention. Various methods based on functional causal models have been proposed to solve this problem, by assuming the causal process satisfies some (structural) constraints and showing that the reverse direction violates such constraints. The nonlinear additive noise model has been demonstrated to be effective for this purpose, but the model class is not transitive--even if each direct causal relation follows this model, indirect causal influences, which result from omitted intermediate causal variables and are frequently encountered in practice, do not necessarily follow the model constraints; as a consequence, the nonlinear additive noise model may fail to correctly discover causal direction.\u00a0In this work, we propose a cascade nonlinear additive noise model to represent such causal influences--each direct causal relation follows the nonlinear additive noise model but we observe only the initial cause and final effect.\u00a0We further propose a method to estimate the model, including the unmeasured intermediate variables, from data, under the variational auto-encoder framework.\u00a0Our theoretical results show that with our model, causal direction is identifiable under suitable technical conditions on the data generation process. Simulation results illustrate the power of the proposed method in identifying indirect causal relations across various settings, and experimental results on real data suggest that the proposed model and method greatly extend the applicability of causal discovery based on functional causal models in nonlinear cases."
            },
            {
                "arxivId": "1903.10075",
                "title": "Machine Learning Methods That Economists Should Know About",
                "abstract": "We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models."
            },
            {
                "arxivId": "1901.06415",
                "title": "A bi-partite generative model framework for analyzing and simulating large scale multiple discrete-continuous travel behaviour data",
                "abstract": null
            },
            {
                "arxivId": "1803.05028",
                "title": "Decentralised Learning in Systems with Many, Many Strategic Agents",
                "abstract": "\n \n Although multi-agent reinforcement learning can tackle systems of strategically interacting entities, it currently fails in scalability and lacks rigorous convergence guarantees. Crucially, learning in multi-agent systems can become intractable due to the explosion in the size of the state-action space as the number of agents increases. In this paper, we propose a method for computing closed-loop optimal policies in multi-agent systems that scales independently of the number of agents. This allows us to show, for the first time, successful convergence to optimal behaviour in systems with an unbounded number of interacting adaptive learners. Studying the asymptotic regime of N-player stochastic games, we devise a learning protocol that is guaranteed to converge to equilibrium policies even when the number of agents is extremely large. Our method is model-free and completely decentralised so that each agent need only observe its local state information and its realised rewards. We validate these theoretical results by showing convergence to Nash-equilibrium policies in applications from economics and control theory with thousands of strategically interacting agents.\n \n"
            },
            {
                "arxivId": "1712.04086",
                "title": "PacGAN: The Power of Two Samples in Generative Adversarial Networks",
                "abstract": "Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why recently-proposed approaches mitigate mode collapse. We propose a principled approach to handle mode collapse called packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing\u2014in particular the seminal result of (Blackwell, 1953)\u2014to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well."
            },
            {
                "arxivId": "1711.10337",
                "title": "Are GANs Created Equal? A Large-Scale Study",
                "abstract": "Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \\cite{goodfellow2014generative}."
            },
            {
                "arxivId": "1705.08821",
                "title": "Causal Effect Inference with Deep Latent-Variable Models",
                "abstract": "Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects."
            },
            {
                "arxivId": "1704.00028",
                "title": "Improved Training of Wasserstein GANs",
                "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms."
            },
            {
                "arxivId": "1703.06490",
                "title": "Generating Multi-label Discrete Patient Records using Generative Adversarial Networks",
                "abstract": "Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN."
            },
            {
                "arxivId": "1702.08431",
                "title": "Boundary-Seeking Generative Adversarial Networks",
                "abstract": "Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training."
            },
            {
                "arxivId": "1701.07875",
                "title": "Wasserstein GAN",
                "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions."
            },
            {
                "arxivId": "1611.04076",
                "title": "Least Squares Generative Adversarial Networks",
                "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs."
            },
            {
                "arxivId": "1606.03657",
                "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
                "abstract": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods."
            },
            {
                "arxivId": "1606.03498",
                "title": "Improved Techniques for Training GANs",
                "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "1312.6114",
                "title": "Auto-Encoding Variational Bayes",
                "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2210.01214",
        "category": "q-fin",
        "title": "Statistical inference for rough volatility: Minimax Theory",
        "abstract": "Rough volatility models have gained considerable interest in the quantitative finance community in recent years. In this paradigm, the volatility of the asset price is driven by a fractional Brownian motion with a small value for the Hurst parameter $H$. In this work, we provide a rigorous statistical analysis of these models. To do so, we establish minimax lower bounds for parameter estimation and design procedures based on wavelets attaining them. We notably obtain an optimal speed of convergence of $n^{-1/(4H+2)}$ for estimating $H$ based on n sampled data, extending results known only for the easier case $H>1/2$ so far. We therefore establish that the parameters of rough volatility models can be inferred with optimal accuracy in all regimes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2305.16842",
        "category": "q-fin",
        "title": "Accounting statement analysis at industry level. A gentle introduction to the compositional approach",
        "abstract": "Compositional data are contemporarily defined as positive vectors, the ratios among whose elements are of interest to the researcher. Financial statement analysis by means of accounting ratios fulfils this definition to the letter. Compositional data analysis solves the major problems in statistical analysis of standard financial ratios at industry level, such as skewness, non-normality, non-linearity and dependence of the results on the choice of which accounting figure goes to the numerator and to the denominator of the ratio. In spite of this, compositional applications to financial statement analysis are still rare. In this article, we present some transformations within compositional data analysis that are particularly useful for financial statement analysis. We show how to compute industry or sub-industry means of standard financial ratios from a compositional perspective. We show how to visualise firms in an industry with a compositional biplot, to classify them with compositional cluster analysis and to relate financial and non-financial indicators with compositional regression models. We show an application to the accounting statements of Spanish wineries using DuPont analysis, and a step-by-step tutorial to the compositional freeware CoDaPack.",
        "references": [
            {
                "arxivId": "2210.11138",
                "title": "New Financial Ratios Based on the Compositional Data Methodology",
                "abstract": "Due to the type of mathematical construction, the use of standard financial ratios in studies analyzing the financial health of a group of firms leads to a series of statistical problems that can invalidate the results obtained. These problems originate from the asymmetry of financial ratios. The present article justifies the use of a new methodology using Compositional Data (CoDa) to analyze the financial statements of an industry, improving analyses using conventional ratios, since the new methodology enables statistical techniques to be applied without encountering any serious drawbacks, such as skewness and outliers, and without the results depending on the arbitrary choice as to which of the accounting figures is the numerator of the ratio and which is the denominator. An example with data on the wine industry is provided. The results show that when using CoDa, outliers and skewness are much reduced, and results are invariant to numerator and denominator permutation."
            },
            {
                "arxivId": "1805.05617",
                "title": "Aggregating multiple types of complex data in stock market prediction: A model-independent framework",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2306.05433",
        "category": "q-fin",
        "title": "Equilibrium in Functional Stochastic Games with Mean-Field Interaction",
        "abstract": "We consider a general class of finite-player stochastic games with mean-field interaction, in which the linear-quadratic cost functional includes linear operators acting on controls in $L^2$. We propose a novel approach for deriving the Nash equilibrium of the game semi-explicitly in terms of operator resolvents, by reducing the associated first order conditions to a system of stochastic Fredholm equations of the second kind and deriving their solution in semi-explicit form. Furthermore, by proving stability results for the system of stochastic Fredholm equations, we derive the convergence of the equilibrium of the $N$-player game to the corresponding mean-field equilibrium. As a by-product, we also derive an $\\varepsilon$-Nash equilibrium for the mean-field game, which is valuable in this setting as we show that the conditions for existence of an equilibrium in the mean-field limit are less restrictive than in the finite-player game. Finally, we apply our general framework to solve various examples, such as stochastic Volterra linear-quadratic games, models of systemic risk and advertising with delay, and optimal liquidation games with transient price impact.",
        "references": [
            {
                "arxivId": "2211.00447",
                "title": "Optimal Liquidation with Signals: the General Propagator Case",
                "abstract": "We consider a class of optimal liquidation problems where the agent\u2019s trans-actions create transient price impact driven by a Volterra-type propagator along with temporary price impact. We formulate these problems as minimization of a revenue-risk functionals, where the agent also exploits available information on a progressively measurable price predicting signal. By using an in\ufb01nite dimensional stochastic control approach, we characterize the value function in terms of a solution to a free-boundary L 2 -valued backward stochastic di\ufb00erential equation and an operator-valued Riccati equation. We then derive analytic solutions to these equations which yields an explicit expression for the optimal trading strategy. We show that our formulas can be implemented in a straightforward and e\ufb03cient way for a large class of price impact kernels with possible singularities such as the power-law kernel."
            },
            {
                "arxivId": "2204.10239",
                "title": "Linear-quadratic stochastic Volterra controls II: Optimal strategies and Riccati--Volterra equations",
                "abstract": "In this paper, we study linear-quadratic control problems for stochastic Volterra integral equations with singular and non-convolution-type coefficients. The weighting matrices in the cost functional are not assumed to be non-negative definite. From a new viewpoint, we formulate a framework of causal feedback strategies. The existence and the uniqueness of a causal feedback optimal strategy are characterized by means of the corresponding Riccati--Volterra equation. The causal feedback optimal strategy is explicitly written by a finite dimensional (matrix-valued) function which solves the Riccati--Volterra equation."
            },
            {
                "arxivId": "2204.08694",
                "title": "Linear-Quadratic Optimal Controls for Stochastic Volterra Integral Equations: Causal State Feedback and Path-Dependent Riccati Equations",
                "abstract": "A linear-quadratic optimal control problem for a forward stochastic Volterra integral equation (FSVIE, for short) is considered. Under the usual convexity conditions, open-loop optimal control exists, which can be characterized by the optimality system, a coupled system of an FSVIE and a Type-II backward SVIE (BSVIE, for short). To obtain a causal state feedback representation for the open-loop optimal control, a path-dependent Riccati equation for an operator-valued function is introduced, via which the optimality system can be decoupled. In the process of decoupling, a Type-III BSVIE is introduced whose adapted solution can be used to represent the adapted M-solution of the corresponding Type-II BSVIE. Under certain conditions, it is proved that the path-dependent Riccati equation admits a unique solution, which means that the decoupling field for the optimality system is found. Therefore a causal state feedback representation of the open-loop optimal control is constructed. An additional interesting finding is that when the control only appears in the diffusion term, not in the drift term of the state system, the causal state feedback reduces to a Markovian state feedback."
            },
            {
                "arxivId": "2112.02961",
                "title": "Closed\u2010loop Nash competition for liquidity",
                "abstract": "We study a multiplayer stochastic differential game, where agents interact through their joint price impact on an asset that they trade to exploit a common trading signal. In this context, we prove that a closed\u2010loop Nash equilibrium exists if the price impact parameter is small enough. Compared to the corresponding open\u2010loop Nash equilibrium, both the agents' optimal trading rates and their performance move towards the central\u2010planner solution, in that excessive trading due to lack of coordination is reduced. However, the size of this effect is modest for plausible parameter values."
            },
            {
                "arxivId": "2108.02992",
                "title": "Large population games with interactions through controls and common noise: convergence results and equivalence between open\u2013loop and closed\u2013loop controls",
                "abstract": "In the presence of a common noise, we study the convergence problems in mean field game (MFG) and mean field control (MFC) problem where the cost function and the state dynamics depend upon the joint conditional distribution of the controlled state and the control process. In the first part, we consider the MFG setting. We start by recalling the notions of $measure$--$valued$ MFG equilibria and of approximate $closed$--$loop$ Nash equilibria associated to the corresponding $N$--player game. Then, we show that all convergent sequences of approximate $closed$--$loop$ Nash equilibria, when $N \\to \\infty,$ converge to $measure$--$valued$ MFG equilibria. And conversely, any $measure$--$valued$ MFG equilibrium is the limit of a sequence of approximate $closed$--$loop$ Nash equilibria. In other words, $measure$--$valued$ MFG equilibria are the accumulation points of the approximate $closed$--$loop$ Nash equilibria. Previous work has shown that $measure$--$valued$ MFG equilibria are the accumulation points of the approximate $open$--$loop$ Nash equilibria. Therefore, we obtain that the limits of approximate\u00a0 $closed$--$loop$ Nash equilibria and approximate $open$--$loop$ Nash equilibria are the same. In the second part, we deal with the MFC setting. After recalling the $closed$--$loop$ and $open$--$loop$ formulations of the MFC problem, we prove that they are equivalent."
            },
            {
                "arxivId": "2107.03273",
                "title": "Closed-loop convergence for mean field games with common noise",
                "abstract": "This paper studies the convergence problem for mean field games with common noise. We define a suitable notion of weak mean field equilibria, which we prove captures all subsequential limit points, as $n\\to\\infty$, of closed-loop approximate equilibria from the corresponding $n$-player games. This extends to the common noise setting a recent result of the first author, while also simplifying a key step in the proof and allowing unbounded coefficients and non-i.i.d. initial conditions. Conversely, we show that every weak mean field equilibrium arises as the limit of some sequence of approximate equilibria for the $n$-player games, as long as the latter are formulated over a broader class of closed-loop strategies which may depend on an additional common signal."
            },
            {
                "arxivId": "2106.09267",
                "title": "Trading with the crowd",
                "abstract": "We formulate and solve a multi\u2010player stochastic differential game between financial agents who seek to cost\u2010efficiently liquidate their position in a risky asset in the presence of jointly aggregated transient price impact, along with taking into account a common general price predicting signal. The unique Nash\u2010equilibrium strategies reveal how each agent's liquidation policy adjusts the predictive trading signal to the aggregated transient price impact induced by all other agents. This unfolds a quantitative relation between trading signals and the order flow in crowded markets. We also formulate and solve the corresponding mean field game in the limit of infinitely many agents. We prove that the equilibrium trading speed and the value function of an agent in the finite N\u2010player game converges to the corresponding trading speed and value function in the mean field game at rate O(N\u22122)$O(N^{-2})$ . In addition, we prove that the mean field optimal strategy provides an approximate Nash\u2010equilibrium for the finite\u2010player game."
            },
            {
                "arxivId": "2011.05589",
                "title": "Portfolio liquidation games with self\u2010exciting order flow",
                "abstract": "We analyze novel portfolio liquidation games with self\u2010exciting order flow. Both the N\u2010player game and the mean\u2010field game (MFG) are considered. We assume that players' trading activities have an impact on the dynamics of future market order arrivals thereby generating an additional transient price impact. Given the strategies of her competitors each player solves a mean\u2010field control problem. We characterize open\u2010loop Nash equilibria in both games in terms of a novel mean\u2010field FBSDE system with unknown terminal condition. Under a weak interaction condition, we prove that the FBSDE systems have unique solutions. Using a novel sufficient maximum principle that does not require convexity of the cost function we finally prove that the solution of the FBSDE systems do indeed provide open\u2010loop Nash equilibria."
            },
            {
                "arxivId": "2009.10972",
                "title": "The characteristic function of Gaussian stochastic volatility models: an analytic expression",
                "abstract": null
            },
            {
                "arxivId": "2006.13539",
                "title": "Markowitz Portfolio Selection for Multivariate Affine and Quadratic Volterra Models",
                "abstract": "This paper concerns portfolio selection with multiple assets under rough covariance matrix. We investigate the continuous-time Markowitz mean-variance problem for a multivariate class of affine and quadratic Volterra models. In this incomplete non-Markovian and non-semimartingale market framework with unbounded random coefficients, the optimal portfolio strategy is expressed by means of a Riccati backward stochastic differential equation (BSDE). In the case of affine Volterra models, we derive explicit solutions to this BSDE in terms of multi-dimensional Riccati-Volterra equations. This framework includes multivariate rough Heston models and extends the results of \\cite{han2019mean}. In the quadratic case, we obtain new analytic formulae for the the Riccati BSDE and we establish their link with infinite dimensional Riccati equations. This covers rough Stein-Stein and Wishart type covariance models. Numerical results on a two dimensional rough Stein-Stein model illustrate the impact of rough volatilities and stochastic correlations on the optimal Markowitz strategy. In particular for positively correlated assets, we find that the optimal strategy in our model is a `buy rough sell smooth' one."
            },
            {
                "arxivId": "2004.08351",
                "title": "Convergence of Large Population Games to Mean Field Games with Interaction Through the Controls",
                "abstract": "This work considers stochastic differential games with a large number of players, whose costs and dynamics interact through the empirical distribution of both their states and their controls. We develop a framework to prove convergence of finite-player games to the asymptotic mean field game. Our approach is based on the concept of propagation of chaos for forward and backward weakly interacting particles which we investigate by fully probabilistic methods, and which appear to be of independent interest. These propagation of chaos arguments allow to derive moment and concentration bounds for the convergence of both Nash equilibria and social optima in non-cooperative and cooperative games, respectively. Incidentally, we also obtain convergence of a system of second order parabolic partial differential equations on finite dimensional spaces to a second order parabolic partial differential equation on the Wasserstein space."
            },
            {
                "arxivId": "2002.09549",
                "title": "Optimal Signal-Adaptive Trading with Temporary and Transient Price Impact",
                "abstract": "We study optimal liquidation in the presence of linear temporary and transient price impact along with taking into account a general price predicting finite-variation signal. We formulate this problem as minimization of a cost-risk functional over a class of absolutely continuous and signal-adaptive strategies. The stochastic control problem is solved by following a probabilistic and convex analytic approach. We show that the optimal trading strategy is given by a system of four coupled forward-backward SDEs, which can be solved explicitly. Our results reveal how the induced transient price distortion provides together with the predictive signal an additional predictor about future price changes. As a consequence, the optimal signal-adaptive trading rate trades off exploiting the predictive signal against incurring the transient displacement of the execution price from its unaffected level. This answers an open question from Lehalle and Neuman [27] as we show how to derive the unique optimal signal-adaptive liquidation strategy when price impact is not only temporary but also transient."
            },
            {
                "arxivId": "2001.00622",
                "title": "An FBSDE approach to market impact games with stochastic parameters",
                "abstract": "In this study, we have analyzed a market impact game between n risk-averse agents who compete for liquidity in a market impact model with a permanent price impact and additional slippage. Most market parameters, including volatility and drift, are allowed to vary stochastically. Our first main result characterizes the Nash equilibrium in terms of a fully coupled system of forward-backward stochastic differential equations (FBSDEs). Our second main result provides conditions under which this system of FBSDEs has a unique solution, resulting in a unique Nash equilibrium."
            },
            {
                "arxivId": "1911.05122",
                "title": "A two-player portfolio tracking game",
                "abstract": null
            },
            {
                "arxivId": "1911.01900",
                "title": "Linear-quadratic control for a class of stochastic Volterra equations: Solvability and approximation",
                "abstract": "We provide an exhaustive treatment of Linear-Quadratic control problems for a class of stochastic Volterra equations of convolution type, whose kernels are Laplace transforms of certain signed matrix measures which are not necessarily finite. These equations are in general neither Markovian nor semimartingales, and include the fractional Brownian motion with Hurst index smaller than $1/2$ as a special case. We establish the correspondence of the initial problem with a possibly infinite dimensional Markovian one in a Banach space, which allows us to identify the Markovian \ncontrolled state variables. Using a refined martingale verification argument combined with a squares completion technique, we prove that the value function is of linear quadratic form in these state variables with a linear optimal feedback control, depending on non-standard Banach space valued Riccati equations. Furthermore, we show that the value function of the stochastic Volterra optimization problem can be approximated by that of conventional finite dimensional Markovian Linear--Quadratic problems, which is of crucial importance for numerical implementation."
            },
            {
                "arxivId": "1905.11782",
                "title": "Many-player games of optimal consumption and investment under relative performance criteria",
                "abstract": null
            },
            {
                "arxivId": "1810.06101",
                "title": "Mean\u2010field games with differing beliefs for algorithmic trading",
                "abstract": "Even when confronted with the same data, agents often disagree on a model of the real world. Here, we address the question of how interacting heterogeneous agents, who disagree on what model the real world follows, optimize their trading actions. The market has latent factors that drive prices, and agents account for the permanent impact they have on prices. This leads to a large stochastic game, where each agents performance criteria are computed under a different probability measure. We analyze the mean\u2010field game (MFG) limit of the stochastic game and show that the Nash equilibrium is given by the solution to a nonstandard vector\u2010valued forward\u2013backward stochastic differential equation. Under some mild assumptions, we construct the solution in terms of expectations of the filtered states. Furthermore, we prove that the MFG strategy forms an \u03b5\u2010Nash equilibrium for the finite player game. Finally, we present a least square Monte Carlo based algorithm for computing the equilibria and show through simulations that increasing disagreement may increase price volatility and trading activity."
            },
            {
                "arxivId": "1808.02745",
                "title": "On the convergence of closed-loop Nash equilibria to the mean field game limit",
                "abstract": "This paper continues the study of the mean field game (MFG) convergence problem: In what sense do the Nash equilibria of $n$-player stochastic differential games converge to the mean field game as $n\\rightarrow\\infty$? Previous work on this problem took two forms. First, when the $n$-player equilibria are open-loop, compactness arguments permit a characterization of all limit points of $n$-player equilibria as weak MFG equilibria, which contain additional randomness compared to the standard (strong) equilibrium concept. On the other hand, when the $n$-player equilibria are closed-loop, the convergence to the MFG equilibrium is known only when the MFG equilibrium is unique and the associated \"master equation\" is solvable and sufficiently smooth. This paper adapts the compactness arguments to the closed-loop case, proving a convergence theorem that holds even when the MFG equilibrium is non-unique. Every limit point of $n$-player equilibria is shown to be the same kind of weak MFG equilibrium as in the open-loop case. Some partial results and examples are discussed for the converse question, regarding which of the weak MFG equilibria can arise as the limit of $n$-player (approximate) equilibria."
            },
            {
                "arxivId": "1807.04795",
                "title": "Mean Field Game with Delay: A Toy Model",
                "abstract": "We study a toy model of linear-quadratic mean field game with delay. We \u201clift\u201d the delayed dynamic into an infinite dimensional space, and recast the mean field game system which is made of a forward Kolmogorov equation and a backward Hamilton-Jacobi-Bellman equation. We identify the corresponding master equation. A solution to this master equation is computed, and we show that it provides an approximation to a Nash equilibrium of the finite player game."
            },
            {
                "arxivId": "1804.04911",
                "title": "A Mean Field Game of Optimal Portfolio Liquidation",
                "abstract": "We consider a mean field game (MFG) of optimal portfolio liquidation under asymmetric information. We prove that the solution to the MFG can be characterized in terms of a forward-backward stochastic differential equation (FBSDE) with a possibly singular terminal condition on the backward component or, equivalently, in terms of an FBSDE with a finite terminal value yet a singular driver. Extending the method of continuation to linear-quadratic FBSDEs with a singular driver, we prove that the MFG has a unique solution. Our existence and uniqueness result allows proving that the MFG with a possibly singular terminal condition can be approximated by a sequence of MFGs with finite terminal values."
            },
            {
                "arxivId": "1704.00847",
                "title": "Incorporating signals into optimal trading",
                "abstract": null
            },
            {
                "arxivId": "1703.07685",
                "title": "Mean field and n\u2010agent games for optimal investment under relative performance criteria",
                "abstract": "We analyze a family of portfolio management problems under relative performance criteria, for fund managers having CARA or CRRA utilities and trading in a common investment horizon in log\u2010normal markets. We construct explicit constant equilibrium strategies for both the finite population games and the corresponding mean field games, which we show are unique in the class of constant equilibria. In the CARA case, competition drives agents to invest more in the risky asset than they would otherwise, while in the CRRA case competitive agents may over\u2010 or underinvest, depending on their levels of risk tolerance."
            },
            {
                "arxivId": "1607.06373",
                "title": "Systemic Risk and Stochastic Games with Delay",
                "abstract": null
            },
            {
                "arxivId": "1509.02505",
                "title": "The Master Equation and the Convergence Problem in Mean Field Games",
                "abstract": "This book describes the latest advances in the theory of mean field games, which are optimal control problems with a continuum of players, each of them interacting with the whole statistical distribution of a population. While it originated in economics, this theory now has applications in areas as diverse as mathematical finance, crowd phenomena, epidemiology, and cybersecurity. Because mean field games concern the interactions of infinitely many players in an optimal control framework, one expects them to appear as the limit for Nash equilibria of differential games with finitely many players as the number of players tends to infinity. The book rigorously establishes this convergence, which has been an open problem until now. The limit of the system associated with differential games with finitely many players is described by the so-called master equation, a nonlocal transport equation in the space of measures. After defining a suitable notion of differentiability in the space of measures, the authors provide a complete self-contained analysis of the master equation. Their analysis includes the case of common noise problems in which all the players are affected by a common Brownian motion. They then go on to explain how to use the master equation to prove the mean field limit. The book presents two important new results in mean field games that contribute to a unified theoretical framework for this exciting and fast-developing area of mathematics."
            },
            {
                "arxivId": "1506.06013",
                "title": "Stochastic Optimal Control with Delay in the Control: solution through partial smoothing",
                "abstract": "Stochastic optimal control problems governed by delay equations with delay in the control are usually more difficult to study than the the ones when the delay appears only in the state. This is particularly true when we look at the associated Hamilton-Jacobi-Bellman (HJB) equation. Indeed, even in the simplified setting (introduced first by Vinter and Kwong [44] for the deterministic case) the HJB equation is an infinite dimensional second order semilinear Partial Differential Equation (PDE) that does not satisfy the so-called \u201cstructure condition\u201d which substantially means that \u201cthe noise enters the system with the control\u201d. The absence of such condition, together with the lack of smoothing properties which is a common feature of problems with delay, prevents the use of the known techniques (based on Backward Stochastic Differential Equations (BSDEs) or on the smoothing properties of the linear part) to prove the existence of regular solutions of this HJB equation and so no results on this direction have been proved till now. In this paper we provide a result on existence of regular solutions of such kind of HJB equations and we use it to solve completely the corresponding control problem finding optimal feedback controls also in the more difficult case of pointwise delay. The main tool used is a partial smoothing property that we prove for the transition semigroup associated to the uncontrolled problem. Such results holds for a specific class of equations and data which arises naturally in many applied problems."
            },
            {
                "arxivId": "1408.2708",
                "title": "A general characterization of the mean field limit for stochastic differential games",
                "abstract": null
            },
            {
                "arxivId": "1405.1345",
                "title": "On the connection between symmetric $N$-player games and mean field games",
                "abstract": "Mean field games are limit models for symmetric $N$-player games with interaction of mean field type as $N\\to\\infty$. The limit relation is often understood in the sense that a solution of a mean field game allows to construct approximate Nash equilibria for the corresponding $N$-player games. The opposite direction is of interest, too: When do sequences of Nash equilibria converge to solutions of an associated mean field game? In this direction, rigorous results are mostly available for stationary problems with ergodic costs. Here, we identify limit points of sequences of certain approximate Nash equilibria as solutions to mean field games for problems with It{o}-type dynamics and costs over a finite time horizon. Limits are studied through weak convergence of associated normalized occupation measures and identified using a probabilistic notion of solution for mean field games."
            },
            {
                "arxivId": "1401.1421",
                "title": "Linear-Quadratic N-person and Mean-Field Games with Ergodic Cost",
                "abstract": "We consider stochastic differential games with $N$ players, linear-Gaussian dynamics in arbitrary state-space dimension, and long-time-average cost with quadratic running cost. Admissible controls are feedbacks for which the system is ergodic. We first study the existence of affine Nash equilibria by means of an associated system of $N$ Hamilton-Jacobi-Bellman and $N$ Kolmogorov-Fokker-Planck partial differential equations. We give necessary and sufficient conditions for the existence and uniqueness of quadratic-Gaussian solutions in terms of the solvability of suitable algebraic Riccati and Sylvester equations. Under a symmetry condition on the running costs and for nearly identical players we study the large population limit, $N$ tending to infinity, and find a unique quadratic-Gaussian solution of the pair of Mean Field Game HJB-KFP equations. Examples of explicit solutions are given, in particular for consensus problems."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2309.03541",
        "category": "q-fin",
        "title": "Thiele's PIDE for unit-linked policies in the Heston-Hawkes stochastic volatility model",
        "abstract": "The main purpose of the paper is to derive Thiele's differential equation for unit-linked policies in the Heston-Hawkes stochastic volatility model introduced in arXiv:2210.15343. This model is an extension of the well-known Heston model that incorporates the volatility clustering feature by adding a compound Hawkes process in the volatility. Since the model is arbitrage-free, pricing unit-linked policies via the equivalence principle under a risk neutral probability measure is possible. Studying the moments of the variance and certain stochastic exponentials, a suitable family of risk neutral probability measures is found. The established and practical method to compute reserves in life insurance is by solving Thiele's equation, which is crucial to guarantee the solvency of the insurance company.",
        "references": [
            {
                "arxivId": "2012.15541",
                "title": "Life insurance policies with cash flows subject to random interest rate changes",
                "abstract": "The main purpose of this work is to derive a partial differential equation for the reserves of life insurance liabilities subject to stochastic interest rates where the benefits and premiums depend directly on changes in the interest rate curve. In particular, we allow the payment streams to depend on the performance of an overnight technical interest rate, making them stochastic as well. This opens up for considering new types of contracts based on the performance of the insurer's returns on their own investments. We provide explicit solutions for the reserves when the premiums and benefits vary according to interest rate levels or averages under the Vasicek model and conduct some simulations computing reserve surfaces numerically. We also give an example of a reinsurance treaty taking over pension payments when the insurer's average returns fall under some specified threshold."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2402.09563",
        "category": "q-fin",
        "title": "ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents",
        "abstract": "We introduce a multi-agent simulator for economic systems comprised of heterogeneous Households, heterogeneous Firms, Central Bank and Government agents, that could be subjected to exogenous, stochastic shocks. The interaction between agents defines the production and consumption of goods in the economy alongside the flow of money. Each agent can be designed to act according to fixed, rule-based strategies or learn their strategies using interactions with others in the simulator. We ground our simulator by choosing agent heterogeneity parameters based on economic literature, while designing their action spaces in accordance with real data in the United States. Our simulator facilitates the use of reinforcement learning strategies for the agents via an OpenAI Gym style environment definition for the economic system. We demonstrate the utility of our simulator by simulating and analyzing two hypothetical (yet interesting) economic scenarios. The first scenario investigates the impact of heterogeneous household skills on their learned preferences to work at different firms. The second scenario examines the impact of a positive production shock to one of two firms on its pricing strategy in comparison to the second firm. We aspire that our platform sets a stage for subsequent research at the intersection of artificial intelligence and economics.",
        "references": [
            {
                "arxivId": "2311.17252",
                "title": "Analyzing the Impact of Tax Credits on Households in Simulated Economic Systems with Learning Agents",
                "abstract": "In economic modeling, there has been an increasing investigation into multi-agent simulators. Nevertheless, state-of-the-art studies establish the model based on reinforcement learning (RL) exclusively for specific agent categories, e.g., households, firms, or the government. It lacks concerns over the resulting adaptation of other pivotal agents, thereby disregarding the complex interactions within a real-world economic system. Furthermore, we pay attention to the vital role of the government policy in distributing tax credits. Instead of uniform distribution considered in state-of-the-art, it requires a well-designed strategy to reduce disparities among households and improve social welfare. To address these limitations, we propose an expansive multi-agent economic model comprising reinforcement learning agents of numerous types. Additionally, our research comprehensively explores the impact of tax credit allocation on household behavior and captures the spectrum of spending patterns that can be observed across diverse households. Further, we propose an innovative government policy to distribute tax credits, strategically leveraging insights from tax credit spending patterns. Simulation results illustrate the efficacy of the proposed government strategy in ameliorating inequalities across households."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2302.13971",
                "title": "LLaMA: Open and Efficient Foundation Language Models",
                "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
            },
            {
                "arxivId": "2210.08569",
                "title": "Limited or Biased: Modeling Sub-Rational Human Investors in Financial Markets",
                "abstract": "Human decision-making in real-life deviates significantly from the optimal decisions made by fully rational agents, primarily due to computational limitations or psychological biases. While existing studies in behavioral finance have discovered various aspects of human sub-rationality, there lacks a comprehensive framework to transfer these findings into an adaptive human model applicable across diverse financial market scenarios. In this study, we introduce a flexible model that incorporates five different aspects of human sub-rationality using reinforcement learning. Our model is trained using a high-fidelity multi-agent market simulator, which overcomes limitations associated with the scarcity of labeled data of individual investors. We evaluate the behavior of sub-rational human investors using hand-crafted market scenarios and SHAP value analysis, showing that our model accurately reproduces the observations in the previous studies and reveals insights of the driving factors of human behavior. Finally, we explore the impact of sub-rationality on the investor's Profit and Loss (PnL) and market quality. Our experiments reveal that bounded-rational and prospect-biased human behaviors improve liquidity but diminish price efficiency, whereas human behavior influenced by myopia, optimism, and pessimism reduces market liquidity."
            },
            {
                "arxivId": "2206.08781",
                "title": "Reinforcement Learning for Economic Policy: A New Frontier?",
                "abstract": "Agent-based computational economics is a field with a rich academic history, yet one which has struggled to enter mainstream policy design toolboxes, plagued by the challenges associated with representing a complex and dynamic reality. The field of Reinforcement Learning (RL), too, has a rich history, and has recently been at the centre of several exponential developments. Modern RL implementations have been able to achieve unprecedented levels of sophistication, handling previously unthinkable degrees of complexity. This review surveys the historical barriers of classical agent-based techniques in economic modelling, and contemplates whether recent developments in RL can overcome any of them."
            },
            {
                "arxivId": "2201.01163",
                "title": "Analyzing Micro-Founded General Equilibrium Models with Many Agents using Deep Reinforcement Learning",
                "abstract": "Real economies can be modeled as a sequential imperfect-information game with many heterogeneous agents, such as consumers, firms, and governments. Dynamic general equilibrium (DGE) models are often used for macroeconomic analysis in this setting. However, finding general equilibria is challenging using existing theoretical or computational methods, especially when using microfoundations to model individual agents. Here, we show how to use deep multi-agent reinforcement learning (MARL) to find $\\epsilon$-meta-equilibria over agent types in microfounded DGE models. Whereas standard MARL fails to learn non-trivial solutions, our structured learning curricula enable stable convergence to meaningful solutions. Conceptually, our approach is more flexible and does not need unrealistic assumptions, e.g., continuous market clearing, that are commonly used for analytical tractability. Furthermore, our end-to-end GPU implementation enables fast real-time convergence with a large number of RL economic agents. We showcase our approach in open and closed real-business-cycle (RBC) models with 100 worker-consumers, 10 firms, and a social planner who taxes and redistributes. We validate the learned solutions are $\\epsilon$-meta-equilibria through best-response analyses, show that they align with economic intuitions, and show our approach can learn a spectrum of qualitatively distinct $\\epsilon$-meta-equilibria in open RBC models. As such, we show that hardware-accelerated MARL is a promising framework for modeling the complexity of economies based on microfoundations."
            },
            {
                "arxivId": "2110.14771",
                "title": "ABIDES-gym: gym environments for multi-agent discrete event simulation and application to financial markets",
                "abstract": "Model-free Reinforcement Learning (RL) requires the ability to sample trajectories by taking actions in the original problem environment or a simulated version of it. Breakthroughs in the field of RL have been largely facilitated by the development of dedicated open source simulators with easy to use frameworks such as OpenAI Gym and its Atari environments. In this paper we propose to use the OpenAI Gym framework on discrete event time based Discrete Event Multi-Agent Simulation (DEMAS). We introduce a general technique to wrap a DEMAS simulator into the Gym framework. We expose the technique in detail and implement it using the simulator ABIDES as a base. We apply this work by specifically using the markets extension of ABIDES, ABIDES-Markets, and develop two benchmark financial markets OpenAI Gym environments for training daily investor and execution agents.1 As a result, these two environments describe classic financial problems with a complex interactive market behavior response to the experimental agent's action."
            },
            {
                "arxivId": "2104.09368",
                "title": "Deep Reinforcement Learning in a Monetary Model",
                "abstract": "We propose using deep reinforcement learning to solve dynamic stochastic general equilibrium models. Agents are represented by deep arti\ufb01cial neural networks and learn to solve their dynamic optimisation problem by interacting with the model environment, of which they have no a priori knowledge. Deep reinforcement learning o\ufb00ers a \ufb02exible yet principled way to model bounded rationality within this general class of models. We apply our proposed approach to a classical model from the adaptive learning literature in macroeconomics which looks at the interaction of monetary and \ufb01scal policy. We \ufb01nd that, contrary to adaptive learning, the arti\ufb01cially intelligent household can solve the model in all policy regimes."
            },
            {
                "arxivId": "2103.16977",
                "title": "Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning",
                "abstract": "General equilibrium macroeconomic models are a core tool used by policymakers to understand a nation's economy. They represent the economy as a collection of forward-looking actors whose behaviours combine, possibly with stochastic effects, to determine global variables (such as prices) in a dynamic equilibrium. However, standard semi-analytical techniques for solving these models make it difficult to include the important effects of heterogeneous economic actors. The COVID-19 pandemic has further highlighted the importance of heterogeneity, for example in age and sector of employment, in macroeconomic outcomes and the need for models that can more easily incorporate it. We use techniques from reinforcement learning to solve such models incorporating heterogeneous agents in a way that is simple, extensible, and computationally efficient. We demonstrate the method's accuracy and stability on a toy problem for which there is a known analytical solution, its versatility by solving a general equilibrium problem that includes global stochasticity, and its flexibility by solving a combined macroeconomic and epidemiological model to explore the economic and health implications of a pandemic. The latter successfully captures plausible economic behaviours induced by differential health risks by age."
            },
            {
                "arxivId": "2005.14165",
                "title": "Language Models are Few-Shot Learners",
                "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
            },
            {
                "arxivId": "1904.12066",
                "title": "ABIDES: Towards High-Fidelity Market Simulation for AI Research",
                "abstract": "We introduce ABIDES, an Agent-Based Interactive Discrete Event Simulation environment. ABIDES is designed from the ground up to support AI agent research in market applications. While simulations are certainly available within trading firms for their own internal use, there are no broadly available high-fidelity market simulation environments. We hope that the availability of such a platform will facilitate AI research in this important area. ABIDES currently enables the simulation of tens of thousands of trading agents interacting with an exchange agent to facilitate transactions. It supports configurable pairwise network latencies between each individual agent as well as the exchange. Our simulator's message-based design is modeled after NASDAQ's published equity trading protocols ITCH and OUCH. We introduce the design of the simulator and illustrate its use and configuration with sample code, validating the environment with example trading scenarios. The utility of ABIDES is illustrated through experiments to develop a market impact model. We close with discussion of future experimental problems it can be used to explore, such as the development of ML-based trading algorithms."
            },
            {
                "arxivId": "1707.06347",
                "title": "Proximal Policy Optimization Algorithms",
                "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "cond-mat/0409312",
                "title": "An Introduction to Agent Based Modelling and Simulation of Social Processes",
                "abstract": "The paper provides an introduction to agent-based modelling and simulation of social processes. Reader is introduced to the worldview underlying agent-based models, some basic terminology, basic properties of agent-based models, as well as to what one can and what cannot expect from such models, particularly when they are applied to social-scientific investigation. Special attention is given to the issues of validation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-16.json",
        "arxivId": "2402.09985",
        "category": "q-fin",
        "title": "Semi-parametric financial risk forecasting incorporating multiple realized measures (preprint)",
        "abstract": "A semi-parametric joint Value-at-Risk (VaR) and Expected Shortfall (ES) forecasting framework employing multiple realized measures is developed. The proposed framework extends the quantile regression using multiple realized measures as exogenous variables to model the VaR. Then, the information from realized measures is used to model the time-varying relationship between VaR and ES. Finally, a measurement equation that models the contemporaneous dependence between the quantile and realized measures is used to complete the model. A quasi-likelihood, built on the asymmetric Laplace distribution, enables the Bayesian inference for the proposed model. An adaptive Markov Chain Monte Carlo method is used for the model estimation. The empirical section evaluates the performance of the proposed framework with six stock markets from January 2000 to June 2022, covering the period of COVID-19. Three realized measures, including 5-minute realized variance, bi-power variation, and realized kernel, are incorporated and evaluated in the proposed framework. One-step ahead VaR and ES forecasting results of the proposed model are compared to a range of parametric and semi-parametric models, lending support to the effectiveness of the proposed framework.",
        "references": [
            {
                "arxivId": "1707.02587",
                "title": "Dynamic quantile function models",
                "abstract": "Motivated by the need for effectively summarising, modelling, and forecasting the distributional characteristics of intra-daily returns, as well as the recent work on forecasting histogram-valued time-series in the area of symbolic data analysis, we develop a time-series model for forecasting quantile-function-valued (QF-valued) daily summaries for intra-daily returns. We call this model the dynamic quantile function (DQF) model. Instead of a histogram, we propose to use a g-and-h quantile function to summarise the distribution of intra-daily returns. We work with a Bayesian formulation of the DQF model in order to make statistical inference while accounting for parameter uncertainty; an efficient MCMC algorithm is developed for sampling-based posterior inference. Using ten international market indices and approximately 2000 days of out-of-sample data from each market, the performance of the DQF model compares favourably, in terms of forecasting VaR of intra-daily returns, against the interval-valued and histogram-valued time-series models. Additionally, we demonstrate that the QF-valued forecasts can be used to forecast VaR measures at the daily timescale via a simple quantile regression model on daily returns (QR-DQF). In certain markets, the resulting QR-DQF model is able to provide competitive VaR forecasts for daily returns."
            },
            {
                "arxivId": "1503.08123",
                "title": "Higher order elicitability and Osband\u2019s principle",
                "abstract": "A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability for multi-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the `correct' quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-17.json",
        "arxivId": "2402.09746",
        "category": "q-fin",
        "title": "Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment",
        "abstract": "Recently, we introduced a new paradigm for alpha mining in the realm of quantitative investment, developing a new interactive alpha mining system framework, Alpha-GPT. This system is centered on iterative Human-AI interaction based on large language models, introducing a Human-in-the-Loop approach to alpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0 \\footnote{Draft. Work in progress}, a quantitative investment framework that further encompasses crucial modeling and analysis phases in quantitative investment. This framework emphasizes the iterative, interactive research between humans and AI, embodying a Human-in-the-Loop strategy throughout the entire quantitative investment pipeline. By assimilating the insights of human researchers into the systematic alpha research process, we effectively leverage the Human-in-the-Loop approach, enhancing the efficiency and precision of quantitative investment research.",
        "references": [
            {
                "arxivId": "2308.00016",
                "title": "Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment",
                "abstract": "One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments."
            },
            {
                "arxivId": "2306.12964",
                "title": "Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning",
                "abstract": "In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning (RL) to better explore the vast search space of formulaic alphas. The contribution to the combination models' performance is assigned to be the return used in the RL process. This return drives the alpha generator to find better alphas that improve upon the current set. Experimental evaluations on real-world stock market data demonstrate both the effectiveness and the efficiency of our framework for stock trend forecasting. The investment simulation results show that our framework is able to achieve higher returns compared to previous approaches."
            },
            {
                "arxivId": "1908.00709",
                "title": "AutoML: A Survey of the State-of-the-Art",
                "abstract": null
            },
            {
                "arxivId": "1601.00991",
                "title": "101 Formulaic Alphas",
                "abstract": "We present explicit formulas - that are also computer code - for 101 real-life quantitative trading alphas. Their average holding period approximately ranges 0.6-6.4 days. The average pair-wise correlation of these alphas is low, 15.9%. The returns are strongly correlated with volatility, but have no significant dependence on turnover, directly confirming an earlier result based on a more indirect empirical analysis. We further find empirically that turnover has poor explanatory power for alpha correlations."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2211.03638",
        "category": "q-fin",
        "title": "On Pricing of Discrete Asian and Lookback Options under the Heston Model",
        "abstract": "We propose a new, data-driven approach for efficient pricing of - fixed- and float-strike - discrete arithmetic Asian and Lookback options when the underlying process is driven by the Heston model dynamics. The method proposed in this article constitutes an extension of our previous work, where the problem of sampling from time-integrated stochastic bridges was addressed. The model relies on the Seven-League scheme, where artificial neural networks are employed to\"learn\"the distribution of the random variable of interest utilizing stochastic collocation points. The method results in a robust procedure for Monte Carlo pricing. Furthermore, semi-analytic formulae for option pricing are provided in a simplified, yet general, framework. The model guarantees high accuracy and a reduction of the computational time up to thousands of times compared to classical Monte Carlo pricing schemes.",
        "references": [
            {
                "arxivId": "2111.13901",
                "title": "Fast sampling from time-integrated bridges using deep learning",
                "abstract": null
            },
            {
                "arxivId": "2009.03202",
                "title": "The Seven-League Scheme: Deep learning for large time step Monte Carlo simulations of stochastic differential equations",
                "abstract": "We propose an accurate data-driven numerical scheme to solve stochastic differential equations (SDEs), by taking large time steps. The SDE discretization is built up by means of the polynomial chaos expansion method, on the basis of accurately determined stochastic collocation (SC) points. By employing an artificial neural network to learn these SC points, we can perform Monte Carlo simulations with large time steps. Basic error analysis indicates that this data-driven scheme results in accurate SDE solutions in the sense of strong convergence, provided the learning methodology is robust and accurate. With a method variant called the compression\u2013decompression collocation and interpolation technique, we can drastically reduce the number of neural network functions that have to be learned, so that computational speed is enhanced. As a proof of concept, 1D numerical experiments confirm a high-quality strong convergence error when using large time steps, and the novel scheme outperforms some classical numerical SDE discretizations. Some applications, here in financial option valuation, are also presented."
            },
            {
                "arxivId": "1811.03378",
                "title": "Activation Functions: Comparison of trends in Practice and Research for Deep Learning",
                "abstract": "Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date."
            },
            {
                "arxivId": "1610.01145",
                "title": "Error bounds for approximations with deep ReLU networks",
                "abstract": null
            },
            {
                "arxivId": "1505.04648",
                "title": "Chebyshev interpolation for parametric option pricing",
                "abstract": null
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "0906.4456",
                "title": "Path integral approach to Asian options in the Black-Scholes model",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2303.06603",
        "category": "q-fin",
        "title": "Correlation between upstreamness and downstreamness in random global value chains",
        "abstract": "This paper is concerned with upstreamness and downstreamness of industries and countries in global value chains. Upstreamness and downstreamness measure respectively the average distance of an industrial sector from final consumption and from primary inputs, and they are computed from based on the most used global Input-Output tables databases, e.g., the World Input-Output Database (WIOD). Recently, Antr\\`as and Chor reported a puzzling and counter-intuitive finding in data from the period 1995-2011, namely that (at country level) upstreamness appears to be positively correlated with downstreamness, with a correlation slope close to $+1$. This effect is stable over time and across countries, and it has been confirmed and validated by later analyses. We first analyze a simple model of random Input/Output tables, and we show that, under minimal and realistic structural assumptions, there is a natural positive correlation emerging between upstreamness and downstreamness of the same industrial sector/country, with correlation slope equal to $+1$. This effect is robust against changes in the randomness of the entries of the I/O table and different aggregation protocols. Secondly, we perform experiments by randomly reshuffling the entries of the empirical I/O table where these puzzling correlations are detected, in such a way that the global structural constraints are preserved. Again, we find that the upstreamness and downstreamness of the same industrial sector/country are positively correlated with slope close to $+1$, even though the random reshuffling has destroyed any underlying economic information about inter-sectorial connections and trends. Our results strongly suggest that the empirically observed puzzling correlation may rather be a necessary consequence of the few structural constraints that Input/Output tables must meet.",
        "references": [
            {
                "arxivId": "2208.09430",
                "title": "Statistics of the largest eigenvalues and singular values of low-rank random matrices with non-negative entries",
                "abstract": "We compute analytically the distribution and moments of the largest eigenvalues/singular values and resolvent statistics for random matrices with (i) non-negative entries, (ii) small rank, and (iii) prescribed sums of rows/columns. Applications are discussed in the context of Mean First Passage Time of random walkers on networks, and the calculation of network\"influence\"metrics. The analytical results are corroborated by numerical simulations."
            },
            {
                "arxivId": "2106.02730",
                "title": "\"Spectrally gapped\" random walks on networks: a Mean First Passage Time formula",
                "abstract": "We derive an approximate but explicit formula for the Mean First\nPassage Time of a random walker between a source and a target node of a\ndirected and weighted network. The formula does not require any matrix\ninversion, and it takes as only input the transition probabilities into\nthe target node. It is derived from the calculation of the average\nresolvent of a deformed ensemble of random sub-stochastic matrices\nH=\\langle H\\rangle +\\delta HH=\u27e8H\u27e9+\u03b4H,\nwith \\langle H\\rangle\u27e8H\u27e9\nrank-11\nand non-negative. The accuracy of the formula depends on the spectral\ngap of the reduced transition matrix, and it is tested numerically on\nseveral instances of (weighted) networks away from the high sparsity\nregime, with an excellent agreement."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2306.15585",
        "category": "q-fin",
        "title": "Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning",
        "abstract": null,
        "references": [
            {
                "arxivId": "2106.01345",
                "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
                "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks."
            },
            {
                "arxivId": "2005.14658",
                "title": "Super-App Behavioral Patterns in Credit Risk Models: Financial, Statistical and Regulatory Implications",
                "abstract": null
            },
            {
                "arxivId": "2003.08964",
                "title": "The value of text for small business default prediction: A deep learning approach",
                "abstract": null
            },
            {
                "arxivId": "1908.02646",
                "title": "AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks",
                "abstract": "Recent years have witnessed the successful marriage of finance innovations and AI techniques in various finance applications including quantitative trading (QT). Despite great research efforts devoted to leveraging deep learning (DL) methods for building better QT strategies, existing studies still face serious challenges especially from the side of finance, such as the balance of risk and return, the resistance to extreme loss, and the interpretability of strategies, which limit the application of DL-based strategies in real-life financial markets. In this work, we propose AlphaStock, a novel reinforcement learning (RL) based investment strategy enhanced by interpretable deep attention networks, to address the above challenges. Our main contributions are summarized as follows: i) We integrate deep attention networks with a Sharpe ratio-oriented reinforcement learning framework to achieve a risk-return balanced investment strategy; ii) We suggest modeling interrelationships among assets to avoid selection bias and develop a cross-asset attention mechanism; iii) To our best knowledge, this work is among the first to offer an interpretable investment strategy using deep reinforcement learning models. The experiments on long-periodic U.S. and Chinese markets demonstrate the effectiveness and robustness of AlphaStock over diverse market states. It turns out that AlphaStock tends to select the stocks as winners with high long-term growth, low volatility, high intrinsic value, and being undervalued recently."
            },
            {
                "arxivId": "1905.12567",
                "title": "MQLV: Optimal Policy of Money Management in Retail Banking with Q-Learning",
                "abstract": null
            },
            {
                "arxivId": "2002.09931",
                "title": "The value of big data for credit scoring: Enhancing financial inclusion using mobile phone data and social network analytics",
                "abstract": null
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1606.01540",
                "title": "OpenAI Gym",
                "abstract": "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            {
                "arxivId": "1106.1813",
                "title": "SMOTE: Synthetic Minority Over-sampling Technique",
                "abstract": "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2307.12776",
        "category": "q-fin",
        "title": "Assessing Large Language Models' ability to predict how humans balance self-interest and the interest of others",
        "abstract": "Generative artificial intelligence (AI) holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. By leveraging generative AI, humans can benefit from data-driven insights and predictions, enhancing their ability to make informed decisions that consider a wide array of factors and potential outcomes. However, as many decisions carry social implications, for AI to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. We investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 108 experiments with human participants from 12 countries. We find that only GPT-4 (not Bard nor Bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. Nonetheless, GPT-4 consistently underestimates self-interest and inequity-aversion, while overestimating altruistic behavior. This bias has significant implications for AI developers and users, as overly optimistic expectations about human altruism may lead to disappointment, frustration, suboptimal decisions in public policy or business contexts, and even social conflict.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2311.18164",
        "category": "q-fin",
        "title": "The Paradox Of Just-in-Time Liquidity in Decentralized Exchanges: More Providers Can Sometimes Mean Less Liquidity",
        "abstract": "We study Just-in-time (JIT) liquidity provision in blockchain-based decentralized exchanges. A JIT liquidity provider (LP) monitors pending swap orders in public mempools of blockchains to sandwich orders of their choice with liquidity, depositing right before and withdrawing right after the order. Our game-theoretic model with asymmetrically informed agents reveals that a JIT LP's presence does not always enhance liquidity pool depth, as one might expect. While passive LPs face adverse selection by informed arbitrageurs, a JIT LP's ability to detect pending orders for toxic order flow prior to liquidity provision lets them avoid being adversely selected. JIT LPs thus only provide liquidity to uninformed orders and crowd out passive LPs when order volume is not sufficiently elastic to pool depth, possibly reducing overall market liquidity. We show that using a two-tiered fee structure which transfers a part of a JIT LP's fee revenue to passive LPs or allowing for JIT LPs to compete \\`{a} la Cournot are potential solutions to mitigate the negative effects of JIT liquidity.",
        "references": [
            {
                "arxivId": "2208.06046",
                "title": "Automated Market Making and Loss-Versus-Rebalancing",
                "abstract": "Automated market making (AMM) protocols such as Uniswap have recently emerged as an alternative to the most common market structure for electronic trading, the limit order book. Relative to limit order books, AMMs are both more computationally efficient and do not require the participation of active market making intermediaries. As such, AMMs have emerged as the dominant market mechanism for trust-less decentralized exchanges (DEXs). We develop a model the underlying economics of AMMs from the perspective of their passive liquidity providers (LPs). Our central contribution is a\"Black-Scholes formula for AMMs\". Like the Black-Scholes formula, we consider the return to LPs once market risk has been hedged. We identify the main adverse selection cost incurred by LPs, which we call\"loss-versus-rebalancing\"(LVR, pronounced\"lever\"). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. In a continuous time Black-Scholes setting, we are able to derive closed-form expressions for this adverse selection cost, for all automated market makers, including constant function market makers and those featuring concentrated liquidity (e.g., Uniswap v3). Qualitatively, we highlight the main forces that drive AMM LP returns, including asset characteristics (volatility) and AMM characteristics (curvature/marginal liquidity). Quantitatively, we illustrate how our model's expressions for LP returns match actual LP returns for the Uniswap v2 WETH-USDC trading pair. Our model provides tradable insight into both the ex ante and ex post assessment of AMM LP investment decisions. LVR can also inform the design of the next generation of DEX market mechanisms -- in fact, in the short time since our work has been released,\"LVR mitigation\"has already emerged as the dominant challenge among practitioners in the AMM protocol designer community."
            },
            {
                "arxivId": "2112.07386",
                "title": "On The Quality Of Cryptocurrency Markets: Centralized Versus Decentralized Exchanges",
                "abstract": "We compare the market quality of centralized crypto exchanges (CEXs) such as Binance and Kraken to decentralized blockchain-based venues (DEXs) such as Uniswap v2 and v3. After discussing the microstructure of such exchanges, we analyze two key aspects of market quality: transaction costs and deviations from the no-arbitrage condition. We find that CEXs and DEXs operate on roughly equal footing in terms of transaction costs, particularly in light of recent innovations in DEX protocols. Moreover, while CEXs provide superior price efficiency, DEXs eliminate custodian risk. These complementary advantages may explain why both market structures coexist."
            },
            {
                "arxivId": "2103.08842",
                "title": "The Adoption of Blockchain-based Decentralized Exchanges",
                "abstract": "We investigate the market microstructure of Automated Market Makers (AMMs), the most prominent type of blockchain-based decentralized exchanges. We show that the order execution mechanism yields token value loss for liquidity providers if token exchange rates are volatile. AMMs are adopted only if their token pairs are highly correlated, or of high personal use for investors. A pricing curve with higher curvature makes the arbitrage problem less severe but also decreases investors\u2019 surplus. Pooling multiple tokens exacerbates the arbitrage problem. We provide statistical support for our main model implications using transaction-level data of AMMs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2401.04521",
        "category": "q-fin",
        "title": "Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient Liquidity",
        "abstract": "The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised Proof of Stake (PoS) consensus-based blockchains that incorporate intrinsic DeFi applications, aims to support sustainable liquidity bootstrapping and network security. This concept seeks to efficiently utilise budgeted staking rewards to attract and sustain liquidity through a risk-structuring engine and incentive allocation strategy, both of which are designed to maximise capital efficiency. The proposed protocol serves the dual objective of: (i) capital creation by attracting risk capital efficiently and maximising its operational utility for intrinsic DeFi applications, thereby asserting sustainability; and (ii) enhancing the adopting blockchain network's economic security by augmenting their staking (PoS) mechanism with a harmonious layer seeking to attract a diversity of digital assets. Finally, the protocol's conceptual framework, as detailed in the appendix, is extended to encompass service fee credits. This extension capitalises on the network's auxiliary services to disperse incentives and attract liquidity, ensuring the network achieves and maintains the critical usage threshold essential for its sustained operational viability and progressive growth.",
        "references": [
            {
                "arxivId": "2302.12319",
                "title": "Age and market capitalization drive large price variations of cryptocurrencies",
                "abstract": null
            },
            {
                "arxivId": "2105.13891",
                "title": "SoK: Yield Aggregators in DeFi",
                "abstract": "Yield farming has been an immensely popular activity for cryptocurrency holders since the explosion of Decentralized Finance (DeFi) in the summer of 2020. In this Systematization of Knowledge (SoK), we study a general framework for yield farming strategies with empirical analysis. First, we summarize the fundamentals of yield farming by focusing on the protocols and tokens used by aggregators. We then examine the sources of yield and translate those into three example yield farming strategies, followed by the simulations of yield farming performance, based on these strategies. We further compare four major yield aggregators\u2014Idle, Pickle, Harvest and Yearn\u2014in the ecosystem, along with brief introductions of others. We systematize their strategies and revenue models, and conduct an empirical analysis with on-chain data from example vaults, to find a plausible connection between data anomalies and historical events. Finally, we discuss the benefits and risks of yield aggregators."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2402.10215",
        "category": "q-fin",
        "title": "The Mean Field Market Model Revisited",
        "abstract": "In this paper, we present an alternative perspective on the mean-field LIBOR market model introduced by Desmettre et al. in arXiv:2109.10779. Our novel approach embeds the mean-field model in a classical setup, but retains the crucial feature of controlling the term rate's variances over large time horizons. This maintains the market model's practicability, since calibrations and simulations can be carried out efficiently without nested simulations. In addition, we show that our framework can be directly applied to model term rates derived from SOFR, ESTR or other nearly risk-free overnight short-term rates -- a crucial feature since many IBOR rates are gradually being replaced. These results are complemented by a calibration study and some theoretical arguments which allow to estimate the probability of unrealistically high rates in the presented market models.",
        "references": [
            {
                "arxivId": "2109.10779",
                "title": "A Mean-Field Extension of the LIBOR Market Model",
                "abstract": "We introduce a mean-field extension of the LIBOR market model (LMM) which preserves the basic features of the original model. Among others, these features are the martingale property, a directly implementable calibration and an economically reasonable parametrization of the classical LMM. At the same time, the mean-field LIBOR market model (MF-LMM) is designed to reduce the probability of exploding scenarios, arising in particular in the market-consistent valuation of long-term guarantees. To this end, we prove existence and uniqueness of the corresponding MF-LMM and investigate its practical aspects, including a Black '76-type formula. Moreover, we present an extensive numerical analysis of the MF-LMM. The corresponding Monte Carlo method is based on a suitable interacting particle system which approximates the underlying mean-field equation."
            },
            {
                "arxivId": "2101.06077",
                "title": "ESTIMATION OF FUTURE DISCRETIONARY BENEFITS IN TRADITIONAL LIFE INSURANCE",
                "abstract": "Abstract In the context of life insurance with profit participation, the future discretionary benefits (FDB), which are a central item for Solvency II reporting, are generally calculated by computationally expensive Monte Carlo algorithms. We derive analytic formulas to estimate lower and upper bounds for the FDB. This yields an estimation interval for the FDB, and the average of lower and upper bound is a simple estimator. These formulae are designed for real world applications, and we compare the results to publicly available reporting data."
            },
            {
                "arxivId": "1802.07009",
                "title": "Analytical validation formulas for best estimate calculation in traditional life insurance",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2402.10253",
        "category": "q-fin",
        "title": "The Famous American Economist H. Markowitz and Mathematical Overview of his Portfolio Selection Theory",
        "abstract": "This survey article is dedicated to the life of the famous American economist H. Markowitz (1927--2023). We do revisit the main statements of the portfolio selection theory in terms of mathematical completeness including all the necessary auxiliary details.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2402.10481",
        "category": "q-fin",
        "title": "Emoji Driven Crypto Assets Market Reactions",
        "abstract": "In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuanced perspective on the interplay between digital communication and market dynamics in an academic context.",
        "references": [
            {
                "arxivId": "2303.08774",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
            },
            {
                "arxivId": "2009.11007",
                "title": "Pricing Cryptocurrency Options",
                "abstract": "Cryptocurrencies, especially Bitcoin (BTC), which comprise a new digital asset class, have drawn extraordinary worldwide attention. The characteristics of the cryptocurrency/BTC include a high level of speculation, extreme volatility and price discontinuity. We propose a pricing mechanism based on a stochastic volatility with a correlated jump (SVCJ) model and compare it to a flexible co-jump model by Bandi and Reno (2016). The estimation results of both models confirm the impact of jumps and co-jumps on options obtained via simulation and an analysis of the implied volatility curve. We show that a sizeable proportion of price jumps are significantly and contemporaneously anti-correlated with jumps in volatility. Our study comprises pioneering research on pricing BTC options. We show how the proposed pricing mechanism underlines the importance of jumps in cryptocurrency markets."
            },
            {
                "arxivId": "2009.09782",
                "title": "CRIX an Index for cryptocurrencies",
                "abstract": null
            },
            {
                "arxivId": "1807.07961",
                "title": "Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM",
                "abstract": "Sentiment analysis on large-scale social media data is important to bridge the gaps between social media contents and real world activities including political election prediction, individual and public emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive emoji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter sentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under positive and negative sentimental tweets individually, and then train a sentiment classifier by attending on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM). Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embeddings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a more robust understanding of the semantics and sentiments."
            },
            {
                "arxivId": "1708.00524",
                "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
                "abstract": "NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches."
            },
            {
                "arxivId": "1609.08359",
                "title": "emoji2vec: Learning Emoji Representations from their Description",
                "abstract": "Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard. The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation."
            },
            {
                "arxivId": "2009.10392",
                "title": "Distillation of News Flow Into Analysis of Stock Reactions",
                "abstract": "The gargantuan plethora of opinions, facts, and tweets on financial business offers the opportunity to test and analyze the influence of such text sources on future directions of stocks. It also creates though the necessity to distill via statistical technology the informative elements of this prodigious and indeed colossal data source. Using mixed text sources from professional platforms, blog fora, and stock message boards, we distill via different lexica sentiment variables. These are employed for an analysis of stock reactions: volatility, volume, and returns. An increased sentiment, especially for those with negative prospection, will influence volatility as well as volume. This influence is contingent on the lexical projection and different across Global Industry Classification Standard (GICS) sectors. Based on review articles on 100 S&P 500 constituents for the period of October 20, 2009, to October 13, 2014, we project into BL, MPQA, LM lexica and use the distilled sentiment variables to forecast individual stock indicators in a panel context. Exploiting different lexical projections to test different stock reaction indicators we aim at answering the following research questions: Are the lexica consistent in their analytic ability? To which degree is there an asymmetric response given the sentiment scales (positive v.s. negative)? Are the news of high attention firms diffusing faster and result in more timely and efficient stock reaction? Is there a sector specific reaction from the distilled sentiment measures? We find that there is significant incremental information in the distilled news flow and the sentiment effect is characterized as an asymmetric, attention-specific, and sector-specific response of stock reactions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2402.10760",
        "category": "q-fin",
        "title": "RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction",
        "abstract": "Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose a novel model, RAGIC, which introduces sequence generation for stock interval prediction to quantify uncertainty more effectively. Our approach leverages a Generative Adversarial Network (GAN) to produce future price sequences infused with randomness inherent in financial markets. RAGIC's generator includes a risk module, capturing the risk perception of informed investors, and a temporal module, accounting for historical price trends and seasonality. This multi-faceted generator informs the creation of risk-sensitive intervals through statistical inference, incorporating horizon-wise insights. The interval's width is carefully adjusted to reflect market volatility. Importantly, our approach relies solely on publicly available data and incurs only low computational overhead. RAGIC's evaluation across globally recognized broad-based indices demonstrates its balanced performance, offering both accuracy and informativeness. Achieving a consistent 95% coverage, RAGIC maintains a narrow interval width. This promising outcome suggests that our approach effectively addresses the challenges of stock market prediction while incorporating vital risk considerations.",
        "references": [
            {
                "arxivId": "2302.14164",
                "title": "Stock Broad-Index Trend Patterns Learning via Domain Knowledge Informed Generative Network",
                "abstract": "Predicting the Stock movement attracts much attention from both industry and academia. Despite such significant efforts, the results remain unsatisfactory due to the inherently complicated nature of the stock market driven by factors including supply and demand, the state of the economy, the political climate, and even irrational human behavior. Recently, Generative Adversarial Networks (GAN) have been extended for time series data; however, robust methods are primarily for synthetic series generation, which fall short for appropriate stock prediction. This is because existing GANs for stock applications suffer from mode collapse and only consider one-step prediction, thus underutilizing the potential of GAN. Furthermore, merging news and market volatility are neglected in current GANs. To address these issues, we exploit expert domain knowledge in finance and, for the first time, attempt to formulate stock movement prediction into a Wasserstein GAN framework for multi-step prediction. We propose Index GAN, which includes deliberate designs for the inherent characteristics of the stock market, leverages news context learning to thoroughly investigate textual information and develop an attentive seq2seq learning network that captures the temporal dependency among stock prices, news, and market sentiment. We also utilize the critic to approximate the Wasserstein distance between actual and predicted sequences and develop a rolling strategy for deployment that mitigates noise from the financial market. Extensive experiments are conducted on real-world broad-based indices, demonstrating the superior performance of our architecture over other state-of-the-art baselines, also validating all its contributing components."
            },
            {
                "arxivId": "2108.04443",
                "title": "AdaRNN: Adaptive Learning and Forecasting of Time Series",
                "abstract": "Time series has wide applications in the real world and is known to be difficult to forecast. Since its statistical properties change over time, its distribution also changes temporally, which will cause severe distribution shift problem to existing methods. However, it remains unexplored to model the time series in the distribution perspective. In this paper, we term this as Temporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to tackle the TCS problem by building an adaptive model that generalizes well on the unseen test data. AdaRNN is sequentially composed of two novel algorithms. First, we propose Temporal Distribution Characterization to better characterize the distribution information in the TS. Second, we propose Temporal Distribution Matching to reduce the distribution mismatch in TS to learn the adaptive TS model. AdaRNN is a general framework with flexible distribution distances integrated. Experiments on human activity recognition, air quality prediction, and financial analysis show that AdaRNN outperforms the latest methods by a classification accuracy of 2.6% and significantly reduces the RMSE by 9.0%. We also show that the temporal distribution matching algorithm can be extended in Transformer structure to boost its performance."
            },
            {
                "arxivId": "2106.12950",
                "title": "Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport",
                "abstract": "Successful quantitative investment usually relies on precise predictions of the future movement of the stock price. Recently, machine learning based solutions have shown their capacity to give more accurate stock prediction and become indispensable components in modern quantitative investment systems. However, the i.i.d. assumption behind existing methods is inconsistent with the existence of diverse trading patterns in the stock market, which inevitably limits their ability to achieve better stock prediction performance. In this paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to empower existing stock prediction models with the ability to model multiple stock trading patterns. Essentially, TRA is a lightweight module that consists of a set of independent predictors for learning multiple patterns as well as a router to dispatch samples to different predictors. Nevertheless, the lack of explicit pattern identifiers makes it quite challenging to train an effective TRA-based model. To tackle this challenge, we further design a learning algorithm based on Optimal Transport (OT) to obtain the optimal sample to predictor assignment and effectively optimize the router with such assignment through an auxiliary loss term. Experiments on the real-world stock ranking task show that compared to the state-of-the-art baselines, e.g., Attention LSTM and Transformer, the proposed method can improve information coefficient (IC) from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used in this work are publicly available2: https://github.com/microsoft/qlib."
            },
            {
                "arxivId": "2007.00254",
                "title": "Construction of Confidence Interval for a Univariate Stock Price Signal Predicted Through Long Short Term Memory Network",
                "abstract": null
            },
            {
                "arxivId": "2003.01859",
                "title": "Applications of deep learning in stock market prediction: recent progress",
                "abstract": null
            },
            {
                "arxivId": "1912.10806",
                "title": "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News",
                "abstract": "Stock price prediction is important for value investments in the stock market. In particular, short-term prediction that exploits financial news articles is promising in recent years. In this paper, we propose a novel deep neural network DP-LSTM for stock price prediction, which incorporates the news articles as hidden information and integrates difference news sources through the differential privacy mechanism. First, based on the autoregressive moving average model (ARMA), a sentiment-ARMA is formulated by taking into consideration the information of financial news articles in the model. Then, an LSTM-based deep neural network is designed, which consists of three components: LSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM scheme can reduce prediction errors and increase the robustness. Extensive experiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32% improvement in mean MPA of prediction result, and (ii) for the prediction of the market index S&P 500, we achieve up to 65.79% improvement in MSE."
            },
            {
                "arxivId": "1912.03193",
                "title": "Risk-Averse Trust Region Optimization for Reward-Volatility Reduction",
                "abstract": "The use of reinforcement learning in algorithmic trading is of growing interest, since it offers the opportunity of making profit through the development of autonomous artificial traders, that do not depend on hard-coded rules. In such a framework, keeping uncertainty under control is as important as maximizing expected returns. Risk aversion has been addressed in reinforcement learning through measures related to the distribution of returns. However, in trading it is essential to keep under control the risk of portfolio positions in the intermediate steps. In this paper, we define a novel measure of risk, which we call reward volatility, consisting of the variance of the rewards under the state-occupancy measure. This new risk measure is shown to bound the return variance so that reducing the former also constrains the latter. We derive a policy gradient theorem with a new objective function that exploits the mean-volatility relationship. Furthermore, we adapt TRPO, the well-known policy gradient algorithm with monotonic improvement guarantees, in a risk-averse manner. Finally, we test the proposed approach in two financial environments using real market data."
            },
            {
                "arxivId": "1912.02757",
                "title": "Deep Ensembles: A Loss Landscape Perspective",
                "abstract": "Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis."
            },
            {
                "arxivId": "1907.06673",
                "title": "Quant GANs: deep generation of financial time series",
                "abstract": "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
            },
            {
                "arxivId": "1803.01271",
                "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
                "abstract": "For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL ."
            },
            {
                "arxivId": "1711.11053",
                "title": "A Multi-Horizon Quantile Recurrent Forecaster",
                "abstract": "We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Recurrent Neural Networks, the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme for recurrent nets is designed to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load."
            },
            {
                "arxivId": "1709.06298",
                "title": "MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment",
                "abstract": "\n \n Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/.\n \n"
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1704.02971",
                "title": "A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction",
                "abstract": "The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction."
            },
            {
                "arxivId": "1611.06624",
                "title": "Temporal Generative Adversarial Nets with Singular Value Clipping",
                "abstract": "In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods."
            },
            {
                "arxivId": "1609.05473",
                "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
                "abstract": "\n \n As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.\n \n"
            },
            {
                "arxivId": "1506.02142",
                "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
                "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."
            },
            {
                "arxivId": "1010.3003",
                "title": "Twitter mood predicts the stock market",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-19.json",
        "arxivId": "2402.10803",
        "category": "q-fin",
        "title": "Modelling crypto markets by multi-agent reinforcement learning",
        "abstract": "Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent reinforcement learning (MARL) model simulating crypto markets, which is calibrated to the Binance's daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022. Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with reinforcement learning (RL) techniques in order to model crypto markets. This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era. A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the market prices themselves, and the approximation of the crypto assets fundamental values beyond what those market prices are. Our MAS calibration against real market data allows for an accurate emulation of crypto markets microstructure and probing key market behaviors, in both the bearish and bullish regimes of that particular time period.",
        "references": [
            {
                "arxivId": "1803.06917",
                "title": "Universal features of price formation in financial markets: perspectives from deep learning",
                "abstract": "Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary relation between order flow history and the direction of price moves. The universal price formation model exhibits a remarkably stable out-of-sample accuracy across a wide range of stocks and time periods. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. The universal model\u2014trained on data from all stocks\u2014outperforms asset-specific models trained on time series of any given stock. This weighs in favor of pooling together financial data from various stocks, rather than designing asset- or sector-specific models, as is currently commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations improves forecast accuracy, indicating that there is path-dependence in price dynamics."
            },
            {
                "arxivId": "1704.02638",
                "title": "A fractional reaction\u2013diffusion description of supply and demand",
                "abstract": null
            },
            {
                "arxivId": "1611.01796",
                "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
                "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them\u2014specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level sub-goals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks."
            },
            {
                "arxivId": "1404.1052",
                "title": "An agent-based computational model for China's stock market and stock index futures market",
                "abstract": "This study presents an agent-based computational cross-market model for Chinese equity market structure, which includes both stocks and CSI 300 index futures. In this model, we design several stocks and one index futures to simulate this structure. This model allows heterogeneous investors to make investment decisions with restrictions including wealth, market trading mechanism, and risk management. Investors' demands and order submissions are endogenously determined. Our model successfully reproduces several key features of the Chinese financial markets including spot-futures basis distribution, bid-ask spread distribution, volatility clustering and long memory in absolute returns. Our model can be applied in cross-market risk control, market mechanism design and arbitrage strategies analysis."
            },
            {
                "arxivId": "1310.0762",
                "title": "Agent-Based Stock Market Model with Endogenous Agents' Impact",
                "abstract": "The three-state agent-based 2D model of financial markets as proposed by Giulia Iori has been extended by introducing increasing trust in the correctly predicting agents, a more realistic consultation procedure as well as a formal validation mechanism. This paper shows that such a model correctly reproduces the three fundamental stylised facts: fat-tail log returns, power-law volatility autocorrelation decay in time and volatility clustering."
            },
            {
                "arxivId": "1002.2171",
                "title": "Reverse Engineering Financial Markets with Majority and Minority Games Using Genetic Algorithms",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0009287",
                "title": "Money and Goldstone modes",
                "abstract": "Why is 'worthless' fiat money generally accepted as payment for goods and services? In equilibrium theory, the value of money is generally not determined: the number of equations is one less than the number of unknowns, so only relative prices are determined. In the language of mathematics, the equations are 'homogeneous of order one'. Using the language of physics, this represents a continuous 'Goldstone' symmetry. However, the continuous symmetry is often broken by the dynamics of the system, thus fixing the value of the otherwise undetermined variable. In economics, the value of money is a strategic variable which each agent must determine at each transaction by estimating the effect of future interactions with other agents. This idea is illustrated by a simple network model of monopolistic vendors and buyers, with bounded rationality. We submit that dynamical, spontaneous symmetry breaking is the fundamental principle for fixing the value of money. Perhaps the continuous symmetry representing the lack of restoring force is also the fundamental reason for large fluctuations in stock markets."
            },
            {
                "arxivId": "cond-mat/0008026",
                "title": "Power, L\u00e9vy, exponential and Gaussian-like regimes in autocatalytic financial systems",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0002138",
                "title": "Modelling an imperfect market",
                "abstract": null
            },
            {
                "arxivId": "adap-org/9909001",
                "title": "Social percolation models",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9906298",
                "title": "Self-organization of value and demand",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9811094",
                "title": "Dynamics of money.",
                "abstract": "We present a dynamical many-body theory of money in which the value of money is a time dependent \"strategic variable\" that is chosen by the individual agents. The value of money in equilibrium is not fixed by the equations, and thus represents a continuous symmetry. The dynamics breaks this continuous symmetry by fixating the value of money at a level which depends on initial conditions. The fluctuations around the equilibrium, for instance in the presence of noise, are governed by the \"Goldstone modes\" associated with the broken symmetry. The idea is illustrated by a simple network model of monopolistic vendors and buyers."
            },
            {
                "arxivId": "cond-mat/9712318",
                "title": "HERD BEHAVIOR AND AGGREGATE FLUCTUATIONS IN FINANCIAL MARKETS",
                "abstract": "We present a simple model of a stock market where a random communication structure between agents generically gives rise to heavy tails in the distribution of stock price variations in the form of an exponentially truncated power law, similar to distributions observed in recent empirical studies of high-frequency market data. Our model provides a link between two well-known market phenomena: the heavy tails observed in the distribution of stock market returns on one hand and herding behavior in financial markets on the other hand. In particular, our study suggests a relation between the excess kurtosis observed in asset returns, the market order flow, and the tendency of market participants to imitate each other."
            },
            {
                "arxivId": "adap-org/9607001",
                "title": "POWER LAWS ARE LOGARITHMIC BOLTZMANN LAWS",
                "abstract": "Multiplicative random processes in (not necessarily equilibrium or steady state) stochastic systems with many degrees of freedom lead to Boltzmann distributions when the dynamics is expressed in terms of the logarithm of the elementary variables. In terms of the original variables this gives a power-law distribution. This mechanism implies certain relations between the constraints of the system, the power of the distribution and the dispersion law of the fluctuations. These predictions are validated by Monte Carlo simulations and experimental data. We speculate that stochastic multiplicative dynamics might be the natural origin for the emergence of criticality and scale hierarchies without fine-tuning."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2011.10166",
        "category": "q-fin",
        "title": "Retirement decision with addictive habit persistence in a jump diffusion market",
        "abstract": "This paper investigates the optimal retirement decision, investment, and consumption strategies in a market with jump diffusion, taking into account habit persistence and stock-wage correlation. Our analysis considers multiple stocks and a finite time framework, intending to determine the retirement boundary of the ``wealth-habit-wage\"triplet $(x, h, w)$. To achieve this, we use the habit reduction method and a duality approach to obtain the retirement boundary of the primal variables and feedback forms of optimal strategies. { When dealing with the dual problem, we address technical challenges in the proof of integral equation characterization of optimal retirement boundary using a $C^1$ version of It$\\hat{\\rm o}$'s formula.} Our results show that when the so-called ``de facto wealth\"exceeds a critical proportion of wage, an immediate retirement is the optimal choice for the agent. Additionally, we find that the introduction of jump risks allows for the possibility of discontinuous investment strategies within the working region, which is a novel and insightful finding. Our numerical results effectively illustrate these findings by varying the parameters.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2105.12915",
        "category": "q-fin",
        "title": "Ordered Reference Dependent Choice",
        "abstract": "This paper studies how violations of structural assumptions like expected utility and exponential discounting can be connected to basic rationality violations, even though these assumptions are typically regarded as independent building blocks in decision theory. A reference-dependent generalization of behavioral postulates captures preference shifts in various choice domains. When reference points are fixed, canonical models hold; otherwise, reference-dependent preference parameters (e.g., CARA coefficients, discount factors) give rise to\"non-standard\"behavior. The framework allows us to study risk, time, and social preferences collectively, where seemingly independent anomalies are interconnected through the lens of reference-dependent choice.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2107.06544",
        "category": "q-fin",
        "title": "Winners and Losers of Immigration",
        "abstract": "We aim to identify winners and losers of a sudden inflow of low-skilled immigrants using a general equilibrium search and matching model in which employees, either native or nonnative, are heterogeneous with respect to their skill level and produce different types of goods. We estimate the short-term impact of this shock for Italy in the period 2008-2017 to be sizeable and highly asymmetric. In 2017, the real wages of low-skilled and high-skilled employees were 8% lower and 4% higher, respectively, compared to a counter-factual scenario with no non-natives. Similarly, employers working in the low-skilled market experienced a drop in profits of comparable magnitude, while the opposite happened to employers operating in the high-skilled market. Finally, the presence of non-natives led to a 10% increase in GDP and to an increment of approximately 70 billions \u20ac in Government revenues and 18 billions \u20ac in social security contributions. We argue that these results help rationalise the recent surge of anti-immigrant sentiments among the low-income segment of the Italian population.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2305.00231",
        "category": "q-fin",
        "title": "Historical trend in educational homophily: U-shaped or not U-shaped? Or, how to set a criterion to choose a criterion?",
        "abstract": "Measuring changes in overall inequality between different educational groups is often performed by quantifying variations in educational marital homophily across consecutive generations. However, this task becomes challenging when the education level of marriageable individuals is generation-specific. To address this challenge, various indicators have been proposed in the assortative mating literature. In this paper, we review a set of criteria that indicators must satisfy to be considered as suitable measures of homophily and inequality. Our analytical criteria include robustness to the number of educational categories distinguished and the negative association between intergenerational mobility and homophily. Additionally, we also impose an empirical criterion on the identified qualitative historical trend in homophily between 1960 and 2010 in the US at the national and sub-national levels. Our analysis reveals that while a specific cardinal indicator meets all three criteria, many commonly applied indices do not. We propose the application of this well-performing indicator to quantify the trend in overall inequality in any country, including European countries, with available population data on couples' education level.",
        "references": [
            {
                "arxivId": "2303.04905",
                "title": "Direct comparison or indirect comparison via a series of counterfactual decompositions?",
                "abstract": "We illustrate the point with an empirical analysis of assortative mating in the US, namely, that the outcome of comparing two distant groups can be sensitive to whether comparing the groups directly, or indirectly via a series of counterfactual decompositions involving the groups' comparisons to some intermediate groups. We argue that the latter approach is typically more fit for its purpose."
            },
            {
                "arxivId": "2103.06991",
                "title": "A new method for identifying what Cupid's invisible hand is doing. Is it spreading color blindness while turning us more\"picky'' about spousal education?",
                "abstract": "We develop a method suitable for detecting whether racial homophily is on the rise and also whether the economic divide (i.e., the gap between individuals with different education levels and thereby with different abilities to generate income) is growing in a society. We identify these changes with the changing aggregate marital preferences over the partners' race and education level through their effects on the share of inter-racial couples and the share of educationally homogamous couples. These shares are shaped not only by preferences, but also by the distributions of marriageable men and women by traits. The method proposed is designed to control for changes in the trait distributions from one generation to another. By applying the method, we find the economic divide in the US to display a U-curve pattern between 1960 and 2010 followed by its slightly negative trend between 2010 and 2015. The identified trend of racial homophily suggests that the American society has become more and more permissive towards racial intermarriages since 1970. Finally, we refute the aggregate version of the status-cast exchange hypothesis based on the joint dynamics of the economic divide and the racial homophily."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2306.15807",
        "category": "q-fin",
        "title": "Liquidity Premium, Liquidity-Adjusted Return and Volatility, and Extreme Liquidity",
        "abstract": "We establish innovative liquidity premium measures, and construct liquidity-adjusted return and volatility to model assets with extreme liquidity, represented by a portfolio of selected crypto assets, and upon which we develop a set of liquidity-adjusted ARMA-GARCH/EGARCH models. We demonstrate that these models produce superior predictability at extreme liquidity to their traditional counterparts. We provide empirical support by comparing the performances of a series of Mean Variance portfolios.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2401.16752",
        "category": "q-fin",
        "title": "Enhancing Urban Traffic Safety: An Evaluation of Taipei\u2019s Neighborhood Traffic Environment Improvement Program",
        "abstract": "In densely populated urban areas, where interactions between pedestrians, vehicles, and motorcycles are frequent and complex, traffic safety is a critical concern. This paper evaluates the Neighborhood Traffic Environment Improvement Program in Taipei, which involved painting green pedestrian paths, adjusting no-parking red/yellow lines, and painting speed limit and stop/slow signs on lanes and alleys. Exploiting staggered rollout of policy implementation and administrative traffic accident data, we found that the program reduced daytime traffic accidents by 5 percent and injuries by 8 percent, while having no significant impact on nighttime incidents. The effectiveness of the program during the day is mainly attributed to the painted green sidewalks, with adequate sunlight playing a part in the program's success. Our findings indicate that cost-effective strategies like green pedestrian lanes can be effective in areas with dense populations and high motorcycle traffic, as they improve safety by encouraging pedestrians to use marked areas and deterring vehicles from these zones.",
        "references": [
            {
                "arxivId": "2212.06080",
                "title": "Logs with Zeros? Some Problems and Solutions",
                "abstract": "\n When studying an outcome Y that is weakly positive but can equal zero (e.g. earnings), researchers frequently estimate an average treatment effect (ATE) for a \u201clog-like\u201d transformation that behaves like log\u2009(Y) for large Y but is defined at zero (e.g. log\u2009(1 + Y), $\\operatorname{arcsinh}(Y)$). We argue that ATEs for log-like transformations should not be interpreted as approximating percentage effects, since unlike a percentage, they depend on the units of the outcome. In fact, we show that if the treatment affects the extensive margin, one can obtain a treatment effect of any magnitude simply by rescaling the units of Y before taking the log-like transformation. This arbitrary unit-dependence arises because an individual-level percentage effect is not well-defined for individuals whose outcome changes from zero to nonzero when receiving treatment, and the units of the outcome implicitly determine how much weight the ATE for a log-like transformation places on the extensive margin. We further establish a trilemma: when the outcome can equal zero, there is no treatment effect parameter that is an average of individual-level treatment effects, unit invariant, and point identified. We discuss several alternative approaches that may be sensible in settings with an intensive and extensive margin, including (i) expressing the ATE in levels as a percentage (e.g. using Poisson regression), (ii) explicitly calibrating the value placed on the intensive and extensive margins, and (iii) estimating separate effects for the two margins (e.g. using Lee bounds). We illustrate these approaches in three empirical applications."
            },
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            },
            {
                "arxivId": "2108.12419",
                "title": "Revisiting event study designs: robust and efficient estimation",
                "abstract": "We develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Our method applies with time-varying controls, in triple-difference designs, and with certain non-binary treatments. We show the practical relevance of our results in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter - about half as large as benchmark estimates used to calibrate macroeconomic models - and predominantly occurs in the first month after the rebate."
            },
            {
                "arxivId": "1803.09015",
                "title": "Difference-in-Differences with Multiple Time Periods",
                "abstract": "Difference-in-Differences (DID) is one of the most important and popular designs for evaluating causal effects of policy changes. In its standard format, there are two time periods and two groups: in the first period no one is treated, and in the second period a \"treatment group\" becomes treated, whereas a \"control group\" remains untreated. However, many em- pirical applications of the DID design have more than two periods and variation in treatment timing. In this article, we consider identification and estimation of treatment effect parameters using DID with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the \"parallel trends assumption\" holds potentially only after conditioning on observed covariates. We propose a simple two-step estimation strategy, establish the asymptotic prop- erties of the proposed estimators, and prove the validity of a computationally convenient bootstrap procedure. Furthermore we propose a semiparametric data-driven testing procedure to assess the credibility of the DID design in our context. Finally, we analyze the effect of the minimum wage on teen employment from 2001-2007. By using our proposed methods we confront the challenges related to variation in the timing of the state-level minimum wage policy changes. Open-source software is available for implementing the proposed methods."
            },
            {
                "arxivId": "1803.08807",
                "title": "Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects",
                "abstract": "Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)"
            },
            {
                "arxivId": "0806.1354",
                "title": "Analysis of the Effect of Speed Limit Increases on Accident-Injury Severities",
                "abstract": "The influence of speed limits on roadway safety has been a subject of continuous debate in the State of Indiana and nationwide. In Indiana, highway-related accidents result in about 900 fatalities and forty thousand injuries annually and place an incredible social and economic burden on the state. Still, speed limits posted on highways and other roads are routinely exceeded as individual drivers try to balance safety, mobility (speed), and the risks and penalties associated with law enforcement efforts. The speed-limit/safety issue has been a matter of considerable concern in Indiana since the state raised its speed limits on rural interstates and selected multilane highways on July 1, 2005. In this paper, the influence of the posted speed limit on the severity of vehicle accidents is studied using Indiana accident data from 2004 (the year before speed limits were raised) and 2006 (the year after speed limits were raised on rural interstates and some multi-lane non-interstate routes). Statistical models of the injury severity of different types of accidents on various roadway classes were estimated. The results of the model estimations showed that, for the speed limit ranges currently used, speed limits did not have a statistically significant effect on the severity of accidents on interstate highways. However, for some non-interstate highways, higher speed limits were found to be associated with higher accident severities - suggesting that future speed limit changes, on non-interstate highways in particular, need to be carefully assessed on a case-by-case basis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2401.16754",
        "category": "q-fin",
        "title": "AI Oversight and Human Mistakes: Evidence from Centre Court",
        "abstract": "Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because of these costs, umpires cared twice as much about Type II errors under AI oversight.",
        "references": [
            {
                "arxivId": "2402.11157",
                "title": "The Value of Context: Human versus Black Box Evaluators",
                "abstract": "Evaluations once solely within the domain of human experts (e.g., medical diagnosis by doctors) can now also be carried out by machine learning algorithms. This raises a new conceptual question: what is the difference between being evaluated by humans and algorithms, and when should an individual prefer one form of evaluation over the other? We propose a theoretical framework that formalizes one key distinction between the two forms of evaluation: Machine learning algorithms are standardized, fixing a common set of covariates by which to assess all individuals, while human evaluators customize which covariates are acquired to each individual. Our framework defines and analyzes the advantage of this customization -- the value of context -- in environments with very high-dimensional data. We show that unless the agent has precise knowledge about the joint distribution of covariates, the value of more covariates exceeds the value of context."
            },
            {
                "arxivId": "2110.15310",
                "title": "On the Fairness of Machine-Assisted Human Decisions",
                "abstract": "When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.05152",
        "category": "q-fin",
        "title": "Is the Price Right? Reconceptualizing Price and Income Elasticity to Anticipate Price Perception Issues",
        "abstract": "Price perception by consumers represents a challenge to the ability of a business to correctly and profitably price and sell their products or services in a given market and any new target market. Complicating the perception of prices is the dynamics of price and income elasticity, both of which are key for estimating demand. This article proposes a novel and elegant identity that conceptualizes elasticity as a means of quantifying the potential for price perception problems in a market. Elasticity studies from 1990 to 2023 (n=30) were sampled to evaluate the relationship between price and income elasticity for various consumer commodities using the identity. The results suggest that, given known price and income elasticity values, a business can anticipate pricing perception problems in advance and address the potential for damaging distortion of their value proposition. Further, the business can use this insight to correctly choose a strategic pricing approach.",
        "references": [
            {
                "arxivId": "1602.08644",
                "title": "Relationship between the Uncompensated Price Elasticity and the Income Elasticity of Demand under Conditions of Additive Preferences",
                "abstract": "Income and price elasticity of demand quantify the responsiveness of markets to changes in income and in prices, respectively. Under the assumptions of utility maximization and preference independence (additive preferences), mathematical relationships between income elasticity values and the uncompensated own and cross price elasticity of demand are here derived using the differential approach to demand analysis. Key parameters are: the elasticity of the marginal utility of income, and the average budget share. The proposed method can be used to forecast the direct and indirect impact of price changes and of financial instruments of policy using available estimates of the income elasticity of demand."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.09495",
        "category": "q-fin",
        "title": "On the Potential of Network-Based Features for Fraud Detection",
        "abstract": "Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consistent feature distributions across training and test datasets.",
        "references": [
            {
                "arxivId": "2302.00775",
                "title": "Model Monitoring and Robustness of In-Use Machine Learning Models: Quantifying Data Distribution Shifts Using Population Stability Index",
                "abstract": "Safety goes first. Meeting and maintaining industry safety standards for robustness of artificial intelligence (AI) and machine learning (ML) models require continuous monitoring for faults and performance drops. Deep learning models are widely used in industrial applications, e.g., computer vision, but the susceptibility of their performance to environment changes (e.g., noise) \\emph{after deployment} on the product, are now well-known. A major challenge is detecting data distribution shifts that happen, comparing the following: {\\bf (i)} development stage of AI and ML models, i.e., train/validation/test, to {\\bf (ii)} deployment stage on the product (i.e., even after `testing') in the environment. We focus on a computer vision example related to autonomous driving and aim at detecting shifts that occur as a result of adding noise to images. We use the population stability index (PSI) as a measure of presence and intensity of shift and present results of our empirical experiments showing a promising potential for the PSI. We further discuss multiple aspects of model monitoring and robustness that need to be analyzed \\emph{simultaneously} to achieve robustness for industry safety standards. We propose the need for and the research direction toward \\emph{categorizations} of problem classes and examples where monitoring for robustness is required and present challenges and pointers for future work from a \\emph{practical} perspective."
            },
            {
                "arxivId": "1903.04881",
                "title": "ROC and AUC with a Binary Predictor: a Potentially Misleading Metric",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.09820",
        "category": "q-fin",
        "title": "Utilizing Deep Learning for Enhancing Network Resilience in Finance",
        "abstract": "In the age of the Internet, people's lives are increasingly dependent on today's network technology. Maintaining network integrity and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown risks causes the detection accuracy of the original model to decline. With this in mind, this paper uses deep learning for advanced threat detection to improve protective measures in the financial industry. Many network researchers have shifted their focus to exception-based intrusion detection techniques. The detection technology mainly uses statistical machine learning methods - collecting normal program and network behavior data, extracting multidimensional features, and training decision machine learning models on this basis (commonly used include naive Bayes, decision trees, support vector machines, random forests, etc.).",
        "references": [
            {
                "arxivId": "2312.12872",
                "title": "Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms",
                "abstract": "This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies. Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images. The successful experiences in the field of computer vision provide strong support for training deep learning algorithms. The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection. In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements. Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems. This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11066",
        "category": "q-fin",
        "title": "Towards Financially Inclusive Credit Products Through Financial Time Series Clustering",
        "abstract": "Financial inclusion ensures that individuals have access to financial products and services that meet their needs. As a key contributing factor to economic growth and investment opportunity, financial inclusion increases consumer spending and consequently business development. It has been shown that institutions are more profitable when they provide marginalised social groups access to financial services. Customer segmentation based on consumer transaction data is a well-known strategy used to promote financial inclusion. While the required data is available to modern institutions, the challenge remains that segment annotations are usually difficult and/or expensive to obtain. This prevents the usage of time series classification models for customer segmentation based on domain expert knowledge. As a result, clustering is an attractive alternative to partition customers into homogeneous groups based on the spending behaviour encoded within their transaction data. In this paper, we present a solution to one of the key challenges preventing modern financial institutions from providing financially inclusive credit, savings and insurance products: the inability to understand consumer financial behaviour, and hence risk, without the introduction of restrictive conventional credit scoring techniques. We present a novel time series clustering algorithm that allows institutions to understand the financial behaviour of their customers. This enables unique product offerings to be provided based on the needs of the customer, without reliance on restrictive credit practices.",
        "references": [
            {
                "arxivId": "2110.11769",
                "title": "Clustering of Bank Customers using LSTM-based encoder-decoder and Dynamic Time Warping",
                "abstract": "Clustering is an unsupervised data mining technique that can be employed to segment customers. The efficient clustering of customers enables banks to design and make offers based on the features of the target customers. The present study uses a real-world financial dataset (Berka, 2000) to cluster bank customers by an encoder-decoder network and the dynamic time warping (DTW) method. The customer features required for clustering are obtained in four ways: Dynamic Time Warping (DTW), Recency Frequency and Monetary (RFM), LSTM encoder-decoder network, and our proposed hybrid method. Once the LSTM model was trained by customer transaction data, a feature vector of each customer was automatically extracted by the encoder.Moreover, the distance between pairs of sequences of transaction amounts was obtained using DTW. Another vector feature was calculated for customers by RFM scoring. In the hybrid method, the feature vectors are combined from the encoder-decoder output, the DTW distance, and the demographic data (e.g., age and gender). Finally, feature vectors were introduced as input to the k-means clustering algorithm, and we compared clustering results with Silhouette and Davies-Bouldin index. As a result, the clusters obtained from the hybrid approach are more accurate and meaningful than those derived from individual clustering techniques. In addition, the type of neural network layers had a substantial effect on the clusters, and high network error does not necessarily worsen clustering performance."
            },
            {
                "arxivId": "2011.11829",
                "title": "RTFN: A Robust Temporal Feature Network for Time Series Classification",
                "abstract": null
            },
            {
                "arxivId": "1910.13051",
                "title": "ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels",
                "abstract": null
            },
            {
                "arxivId": "1802.03426",
                "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
                "abstract": "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning."
            },
            {
                "arxivId": "1802.03145",
                "title": "Relational autoencoder for feature extraction",
                "abstract": "Feature extraction becomes increasingly important as data grows high dimensional. Autoencoder as a neural network based feature extraction method achieves great success in generating abstract features of high dimensional data. However, it fails to consider the relationships of data samples which may affect experimental results of using original and new features. In this paper, we propose a Relation Autoencoder model considering both data features and their relationships. We also extend it to work with other major autoencoder models including Sparse Autoencoder, Denoising Autoencoder and Variational Autoencoder. The proposed relational autoencoder models are evaluated on a set of benchmark datasets and the experimental results show that considering data relationships can generate more robust features which achieve lower construction loss and then lower error rate in further classification compared to the other variants of autoencoders."
            },
            {
                "arxivId": "1704.06327",
                "title": "Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization",
                "abstract": "In this paper, we propose a new clustering model, called DEeP Embedded Regularized ClusTering (DEPICT), which efficiently maps data into a discriminative embedding subspace and precisely predicts cluster assignments. DEPICT generally consists of a multinomial logistic regression function stacked on top of a multi-layer convolutional autoencoder. We define a clustering objective function using relative entropy (KL divergence) minimization, regularized by a prior for the frequency of cluster assignments. An alternating strategy is then derived to optimize the objective by updating parameters and estimating cluster assignments. Furthermore, we employ the reconstruction loss functions in our autoencoder, as a data-dependent regularization term, to prevent the deep embedding function from overfitting. In order to benefit from end-to-end optimization and eliminate the necessity for layer-wise pre-training, we introduce a joint learning framework to minimize the unified clustering and reconstruction loss functions together and train all network layers simultaneously. Experimental results indicate the superiority and faster running time of DEPICT in real-world clustering tasks, where no labeled data is available for hyper-parameter tuning."
            },
            {
                "arxivId": "1611.06455",
                "title": "Time series classification from scratch with deep neural networks: A strong baseline",
                "abstract": "We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics."
            },
            {
                "arxivId": "1511.06335",
                "title": "Unsupervised Deep Embedding for Clustering Analysis",
                "abstract": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods."
            },
            {
                "arxivId": "1312.6114",
                "title": "Auto-Encoding Variational Bayes",
                "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11072",
        "category": "q-fin",
        "title": "Awareness of self-control",
        "abstract": "Economists modeled self-control problems in decisions of people with the time-inconsistence preferences model. They argued that the source of self-control problems could be uncertainty and temptation. This paper uses an experimental test offered to individuals instantaneous reward and future rewards to measure awareness of self-control problems in a tempting condition and also measure the effect of commitment and flexibility cost on their welfare. The quasi-hyperbolic discounting model with time discount factor and present bias at the same time was used for making a model for measuring awareness and choice reversal conditions. The test showed 66% awareness of self-control (partially naive behaviors) in individuals. The welfare implication for individuals increased with commitment and flexibility costs. The result can be useful in marketing and policy-making fields design precisely offers for customers and society.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11136",
        "category": "q-fin",
        "title": "Interbank network reconstruction enforcing density and reciprocity",
        "abstract": "Networks of financial exposures are the key propagators of risk and distress among banks, but their empirical structure is not publicly available because of confidentiality. This limitation has triggered the development of methods of network reconstruction from partial, aggregate information. Unfortunately, even the best methods available fail in replicating the number of directed cycles, which on the other hand play a crucial role in determining graph spectra and hence the degree of network stability and systemic risk. Here we address this challenge by exploiting the hypothesis that the statistics of higher-order cycles is strongly constrained by that of the shortest ones, i.e. by the amount of dyads with reciprocated links. First, we provide a detailed analysis of link reciprocity on the e-MID dataset of Italian banks, finding that correlations between reciprocal links systematically increase with the temporal resolution, typically changing from negative to positive around a timescale of up to 50 days. Then, we propose a new network reconstruction method capable of enforcing, only from the knowledge of aggregate interbank assets and liabilities, both a desired sparsity and a desired link reciprocity. We confirm that the addition of reciprocity dramatically improves the prediction of several structural and spectral network properties, including the largest real eigenvalue and the eccentricity of the elliptical distribution of the other eigenvalues in the complex plane. These results illustrate the importance of correctly addressing the temporal resolution and the resulting level of reciprocity in the reconstruction of financial networks.",
        "references": [
            {
                "arxivId": "1912.13275",
                "title": "Systemic liquidity contagion in the European interbank market",
                "abstract": null
            },
            {
                "arxivId": "1909.01274",
                "title": "In Search of Lost Edges: A Case Study on Reconstructing FInancial Networks",
                "abstract": "To capture the systemic complexity of international financial systems, network data is an important prerequisite. However, dyadic data is often not available, raising the need for methods that allow for reconstructing networks based on limited information. In this paper, we are reviewing different methods that are designed for the estimation of matrices from their marginals and potentially exogenous information. This includes a general discussion of the available methodology that provides edge probabilities as well as models that are focussed on the reconstruction of edge values. Besides summarizing the advantages, shortfalls and computational issues of the approaches, we put them into a competitive comparison using the SWIFT (Society for Worldwide Interbank Financial Telecommunication) MT 103 payment messages network (MT 103: Single Customer Credit Transfer). This network is not only economically meaningful but also fully observed which allows for an extensive competitive horse race of methods. The comparison concerning the binary reconstruction is divided into an evaluation of the edge probabilities and the quality of the reconstructed degree structures. Furthermore, the accuracy of the predicted edge values is investigated. To test the methods on different topologies, the application is split into two parts. The first part considers the full MT 103 network, being an illustration for the reconstruction of large, sparse financial networks. The second part is concerned with reconstructing a subset of the full network, representing a dense medium-sized network. Regarding substantial outcomes, it can be found that no method is superior in every respect and that the preferred model choice highly depends on the goal of the analysis, the presumed network structure and the availability of exogenous information."
            },
            {
                "arxivId": "1908.07092",
                "title": "Linear stability analysis of large dynamical systems on random directed graphs",
                "abstract": "We present a linear stability analysis of stationary states (or fixed points) in large dynamical systems defined on random directed graphs with a prescribed distribution of indegrees and outdegrees. We obtain two remarkable results for such dynamical systems: First, infinitely large systems on directed graphs can be stable even when the degree distribution has unbounded support; this result is surprising since their counterparts on nondirected graphs are unstable when system size is large enough. Second, we show that the phase transition between the stable and unstable phase is universal in the sense that it depends only on a few parameters, such as, the mean degree and a degree correlation coefficient. In addition, in the unstable regime we characterize the nature of the destabilizing mode, which also exhibits universal features. These results follow from an exact theory for the leading eigenvalue of infinitely large graphs that are locally tree-like and oriented, as well as, for the right and left eigenvectors associated with the leading eigenvalue. We corroborate analytical results for infinitely large graphs with numerical experiments on random graphs of finite size. We discuss how the presented theory can be extended to graphs with diagonal disorder and to graphs that contain nondirected links. Finally, we discuss the influence of small cycles and how they can destabilize large dynamical systems when they induce strong enough feedback loops."
            },
            {
                "arxivId": "1806.06941",
                "title": "Reconstruction methods for networks: The case of economic and financial systems",
                "abstract": null
            },
            {
                "arxivId": "1610.03259",
                "title": "Epidemics of liquidity shortages in interbank markets",
                "abstract": null
            },
            {
                "arxivId": "1511.08068",
                "title": "The organization of the interbank network and how ECB unconventional measures affected the e-MID overnight market",
                "abstract": null
            },
            {
                "arxivId": "1511.08830",
                "title": "Disentangling bipartite and core-periphery structure in financial networks",
                "abstract": null
            },
            {
                "arxivId": "1509.00607",
                "title": "Assessing Systemic Risk Due to Fire Sales Spillover Through Maximum Entropy Network Reconstruction",
                "abstract": "Monitoring and assessing systemic risk in financial markets is of great importance but it often requires data that are unavailable or available at a very low frequency. For this reason, systemic risk assessment with partial information is potentially very useful for regulators and other stakeholders. In this paper we consider systemic risk due to fire sales spillovers and portfolio rebalancing by using the risk metrics defined by Greenwood et al. (2015). By using a method based on the constrained minimization of the Cross Entropy, we show that it is possible to assess aggregated and single bank\u2019s systemicness and vulnerability, using only the information on the size of each bank and the capitalization of each investment asset. We also compare our approach with an alternative widespread application of the Maximum Entropy principle allowing to derive graph probability distributions and generating scenarios and we use it to propose a statistical test for a change in banks\u2019 vulnerability to systemic events."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11231",
        "category": "q-fin",
        "title": "Enhancing Security in Blockchain Networks: Anomalies, Frauds, and Advanced Detection Techniques",
        "abstract": "Blockchain technology, a foundational distributed ledger system, enables secure and transparent multi-party transactions. Despite its advantages, blockchain networks are susceptible to anomalies and frauds, posing significant risks to their integrity and security. This paper offers a detailed examination of blockchain's key definitions and properties, alongside a thorough analysis of the various anomalies and frauds that undermine these networks. It describes an array of detection and prevention strategies, encompassing statistical and machine learning methods, game-theoretic solutions, digital forensics, reputation-based systems, and comprehensive risk assessment techniques. Through case studies, we explore practical applications of anomaly and fraud detection in blockchain networks, extracting valuable insights and implications for both current practice and future research. Moreover, we spotlight emerging trends and challenges within the field, proposing directions for future investigation and technological development. Aimed at both practitioners and researchers, this paper seeks to provide a technical, in-depth overview of anomaly and fraud detection within blockchain networks, marking a significant step forward in the search for enhanced network security and reliability.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11371",
        "category": "q-fin",
        "title": "Companhias a\\'ereas s\\~ao todas iguais? A converg\\^encia dos modelos de neg\\'ocios no transporte a\\'ereo",
        "abstract": "This study discusses the literature on the convergence of business models of airlines in Brazilian air transport, focusing on the formation of flight networks. Initially, it analyzes the determinants of the network formation patterns of the\"fundamental\"business models (archetypes) of airlines in the first years after the sector's deregulation. Then, it discusses how the business models of Brazilian companies resemble these patterns. The literature highlights convergences between the network formation strategies of full-service companies in relation to older low-cost companies, in addition to business model redirections after mergers and acquisitions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11373",
        "category": "q-fin",
        "title": "Determinantes concorrenciais dos atrasos dos voos no aeroporto e na rota",
        "abstract": "Flight delays are a reality in the modern air industry worldwide. However, studies in the literature have investigated the competitive determinants of delays arising from factors originating at the airport and along the route separately. This work aims to present a national study that used a unifying approach from the literature, considering the local and global effects of competition on delays. The analysis took into account a phenomenon known as the\"internalization of externalities\"of airport congestion. Furthermore, it discusses the relationship between quality and competition on the route and the impacts of the entry of a low-cost carrier (LCC) on the delays of incumbent airlines in the Brazilian air market. The literature suggests that there is evidence of congestion internalization at Brazilian airports, in parallel with competition for quality at the route level. Additionally, the potential competition caused by the presence of the LCC leads to a global effect that suggests the existence of impacts other than prices on routes where it has not entered.",
        "references": [
            {
                "arxivId": "2401.09174",
                "title": "Airline Delays, Congestion Internalization and Non-Price Spillover Effects of Low Cost Carrier Entry",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11379",
        "category": "q-fin",
        "title": "Estimating HANK with Micro Data",
        "abstract": "We propose an indirect inference strategy for estimating heterogeneous-agent business cycle models with micro data. At its heart is a first-order vector autoregression that is grounded in linear filtering theory as the cross-section grows large. The result is a fast, simple and robust algorithm for computing an approximate likelihood that can be easily paired with standard classical or Bayesian methods. Importantly, our method is compatible with the popular sequence-space solution method, unlike existing state-of-the-art approaches. We test-drive our method by estimating a canonical HANK model with shocks in both the aggregate and cross-section. Not only do simulation results demonstrate the appeal of our method, they also emphasize the important information contained in the entire micro-level distribution over and above simple moments.",
        "references": [
            {
                "arxivId": "2101.04771",
                "title": "Full-Information Estimation of Heterogeneous Agent Models Using Macro and Micro Data",
                "abstract": "We develop a generally applicable full\u2010information inference method for heterogeneous agent models, combining aggregate time series data and repeated cross\u2010sections of micro data. To handle unobserved aggregate state variables that affect cross\u2010sectional distributions, we compute a numerically unbiased estimate of the model\u2010implied likelihood function. Employing the likelihood estimate in a Markov Chain Monte Carlo algorithm, we obtain fully efficient and valid Bayesian inference. Evaluation of the micro part of the likelihood lends itself naturally to parallel computing. Numerical illustrations in models with heterogeneous households or firms demonstrate that the proposed full\u2010information method substantially sharpens inference relative to using only macro data, and for some parameters micro data is essential for identification."
            },
            {
                "arxivId": "1312.0041",
                "title": "On dynamic mode decomposition: Theory and applications",
                "abstract": "Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. \n However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken. \n We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator. \n This generalizes DMD to a larger class of datasets, including nonsequential time series. \n We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational efficiency and mitigate the effects of noise, respectively. \n We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples. \n Such computations are not considered in the existing literature but can be understood using our more general framework. \n In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory. \n It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science. \n We show that under certain conditions, DMD is equivalent to LIM."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11579",
        "category": "q-fin",
        "title": "Assessment of low-carbon tourism development from multi-aspect analysis: a case study of the Yellow River Basin, China",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11580",
        "category": "q-fin",
        "title": "Stackelberg reinsurance and premium decisions with MV criterion and irreversibility",
        "abstract": "We study a reinsurance Stackelberg game in which both the insurer and the reinsurer adopt the mean-variance (abbr. MV) criterion in their decision-making and the reinsurance is irreversible. We apply a unified singular control framework where irreversible reinsurance contracts can be signed in both discrete and continuous times. The results theoretically illustrate that, rather than continuous-time contracts or a bunch of discrete-time contracts, a single once-for-all reinsurance contract is preferred. Moreover, the Stackelberg game turns out to be centering on the signing time of the single contract. The insurer signs the contract if the premium rate is lower than a time-dependent threshold and the reinsurer designs a premium that triggers the signing of the contract at his preferred time. Further, we find that reinsurance preference, discount and reversion have a decreasing dominance in the reinsurer's decision-making, which is not seen for the insurer.",
        "references": [
            {
                "arxivId": "2309.16303",
                "title": "Irreversible reinsurance: Minimization of Capital Injections in Presence of a Fixed Cost",
                "abstract": "We propose a model in which, in exchange to the payment of a fixed transaction cost, an insurance company can choose the retention level as well as the time at which subscribing a perpetual reinsurance contract. The surplus process of the insurance company evolves according to the diffusive approximation of the Cram\\'er-Lundberg model, claims arrive at a fixed constant rate, and the distribution of their sizes is general. Furthermore, we do not specify any specific functional form of the retention level. The aim of the company is to take actions in order to minimize the sum of the expected value of the total discounted flow of capital injections needed to avoid bankruptcy and of the fixed activation cost of the reinsurance contract. We provide an explicit solution to this problem, which involves the resolution of a static nonlinear optimization problem and of an optimal stopping problem for a reflected diffusion. We then illustrate the theoretical results in the case of proportional and excess-of-loss reinsurance, by providing a numerical study of the dependency of the optimal solution with respect to the model's parameters."
            },
            {
                "arxivId": "1910.09834",
                "title": "A hybrid stochastic differential reinsurance and investment game with bounded memory",
                "abstract": null
            },
            {
                "arxivId": "1812.01270",
                "title": "An Optimal Extraction Problem with Price Impact",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11715",
        "category": "q-fin",
        "title": "The Gerber-Shiu Expected Discounted Penalty Function: An Application to Poverty Trapping",
        "abstract": "In this article, we consider a risk process with deterministic growth and prorated losses to model the capital of a household. Our work focuses on the analysis of the trapping time of such a process, where trapping occurs when a household's capital level falls into the poverty area, a region from which it is difficult to escape without external help. A function analogous to the classical Gerber-Shiu expected discounted penalty function is introduced, which incorporates information on the trapping time, the capital surplus immediately before trapping and the capital deficit at trapping. Given that the remaining proportion of capital upon experiencing a capital loss is $Beta(\\alpha,1)-$distributed, closed-form expressions are obtained for quantities typically studied in classical risk theory, including the Laplace transform of the trapping time and the distribution of the capital deficit at trapping. In particular, we derive a model belonging to the generalised beta (GB) distribution family that describes the distribution of the capital deficit at trapping given that trapping occurs. Affinities between the capital deficit at trapping and a class of poverty measures, known as the Foster-Greer-Thorbecke (FGT) index, are presented. The versatility of this model to estimate FGT indices is assessed using household microdata from Burkina Faso's Enqu\\^ete Multisectorielle Continue (EMC) 2014.",
        "references": [
            {
                "arxivId": "2310.09295",
                "title": "On the impact of insurance on households susceptible to random proportional losses: An analysis of poverty trapping",
                "abstract": "In this paper, we consider a risk process with deterministic growth and multiplicative jumps to model the capital of a low-income household. Reflecting the high-risk nature of the low-income environment, capital losses are assumed to be proportional to the level of accumulated capital at the jump time. Our aim is to derive the probability that a household falls below the poverty line, i.e. the trapping probability, where ``trapping\"occurs when the level of capital of a household holds falls below the poverty line, to an area from which it is difficult to escape without external help. Considering the remaining proportion of capital to be distributed as a special case of the beta distribution, closed-form expressions for the trapping probability are obtained via analysis of the Laplace transform of the infinitesimal generator of the process. To study the impact of insurance on this probability, introduction of an insurance product offering proportional coverage is presented. The infinitesimal generator of the insured process gives rise to non-local differential equations. To overcome this, we propose a recursive method for deriving a closed-form solution of the integro-differential equation associated with the infinitesimal generator of the insured process and provide a numerical estimation method for obtaining the trapping probability. Constraints on the rate parameters of the process that prevent certain trapping are derived in both the uninsured and insured cases using classical results from risk theory."
            },
            {
                "arxivId": "2103.17255",
                "title": "Subsidising Inclusive Insurance to Reduce Poverty",
                "abstract": "In this article, we assess the benefits of coordination and partnerships between governments and private insurers, and provide further evidence for microinsurance products as powerful and cost-effective tools for achieving poverty reduction. To explore these ideas, we model the capital of a household from a ruin-theoretic perspective to measure the impact of microinsurance on poverty dynamics and the governmental cost of social protection. We analyse the model under four frameworks: uninsured, insured (without subsidies), insured with subsidised constant premiums and insured with subsidised flexible premiums. Although insurance alone (without subsidies) may not be sufficient to reduce the likelihood of falling into the area of poverty for specific groups of households, since premium payments constrain their capital growth, our analysis suggests that subsidised schemes can provide maximum social benefits while reducing governmental costs."
            },
            {
                "arxivId": "1403.6769",
                "title": "Semi-parametric inference for the absorption features of a growth-fragmentation model",
                "abstract": "In the present paper, we focus on semi-parametric methods for estimating the absorption probability and the distribution of the absorbing time of a growth-fragmentation model observed within a long time interval. We establish that the absorption probability is the unique solution in an appropriate space of a Fredholm equation of the second kind whose parameters are unknown. We estimate this important characteristic of the underlying process by solving numerically the estimated Fredholm equation. Even if the study has been conducted for a particular model, our method is quite general."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11728",
        "category": "q-fin",
        "title": "Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis",
        "abstract": "In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism\". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.",
        "references": [
            {
                "arxivId": "2006.08097",
                "title": "FinBERT: A Pretrained Language Model for Financial Communications",
                "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at this https URL. We hope this will be useful for practitioners and researchers working on financial NLP tasks."
            },
            {
                "arxivId": "1912.01703",
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            {
                "arxivId": "1907.11692",
                "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            {
                "arxivId": "1711.10160",
                "title": "Snorkel: Rapid Training Data Creation with Weak Supervision",
                "abstract": "Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8\u00d7 faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8\u00d7 speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-20.json",
        "arxivId": "2402.11930",
        "category": "q-fin",
        "title": "Stylized Facts of High-Frequency Bitcoin Time Series",
        "abstract": "This paper analyses the high-frequency intraday Bitcoin dataset from 2019 to 2022. During this time frame, the Bitcoin market index exhibited two distinct periods characterized by abrupt changes in volatility. The Bitcoin price returns for both periods can be described by an anomalous diffusion process, transitioning from subdiffusion for short intervals to weak superdiffusion over longer time intervals. The characteristic features related to this anomalous behavior studied in the present paper include heavy tails, which can be described using a $q$-Gaussian distribution and correlations. When we sample the autocorrelation of absolute returns, we observe a power-law relationship, indicating time dependency in both periods initially. The ensemble autocorrelation of returns decays rapidly and exhibits periodicity. We fitted the autocorrelation with a power law and a cosine function to capture both the decay and the fluctuation and found that the two periods have distinctive periodicity. Further study involves the analysis of endogenous effects within the Bitcoin time series, which are examined through detrending analysis. We found that both periods are multifractal and present self-similarity in the detrended probability density function (PDF). The Hurst exponent over short time intervals shifts from less than 0.5 ($\\sim$ 0.42) in Period 1 to be closer to 0.5 in Period 2 ($\\sim$ 0.49), indicating the market is more efficient at short time scales.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2105.08804",
        "category": "q-fin",
        "title": "Efficient Approximations for Utility-Based Pricing",
        "abstract": null,
        "references": [
            {
                "arxivId": "1403.7830",
                "title": "Pseudo linear pricing rule for utility indifference valuation",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2402.12401",
        "category": "q-fin",
        "title": "An\\'alise das estrat\\'egias de planejamento de tempos de voo pelas companhias a\\'ereas",
        "abstract": "This study explores the approaches used by airlines in setting flight times. It highlights the need to balance operational and strategic factors, such as optimizing the use of resources - including aircraft, crew, and fuel - and managing the risks related to delays and congestion. The work details a national analysis focused on domestic flights, investigating the factors that influence companies to adjust scheduled flight times and the impact of this practice on punctuality. The results indicate that decisions about flight time are influenced by both operational and strategic aspects, being affected by competition and the sector's policies on punctuality and slot allocation. Furthermore, it was found that adding extra time is an effective strategy for reducing delays, although it may conceal system deficiencies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2402.12487",
        "category": "q-fin",
        "title": "Explaining the emergence of land-use frontiers",
        "abstract": "Land use expansion is linked to major sustainability concerns including climate change, food security and biodiversity loss. This expansion is largely concentrated in so-called frontiers, defined here as places experiencing marked transformations due to rapid resource exploitation. Understanding the mechanisms shaping these frontiers is crucial for sustainability. Previous work focused mainly on explaining how active frontiers advance, in particular into tropical forests. Comparatively, our understanding of how frontiers emerge in territories considered marginal in terms of agricultural productivity and global market integration remains weak. We synthesize conceptual tools explaining resource and land-use frontiers, including theories of land rent and agglomeration economies, of frontiers as successive waves, spaces of territorialization, friction, and opportunities, anticipation and expectation. We then propose a new theory of frontier emergence, which identifies exogenous pushes, legacies of past waves, and actors anticipations as key mechanisms by which frontiers emerge. Processes of abnormal rent creation and capture and the built-up of agglomeration economies then constitute key mechanisms sustaining active frontiers. Finally, we discuss five implications for the governance of frontiers for sustainability. Our theory focuses on agriculture and deforestation frontiers in the tropics, but can be inspirational for other frontier processes including for extractive resources, such as minerals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2402.12528",
        "category": "q-fin",
        "title": "Denoised Monte Carlo for option pricing and Greeks estimation",
        "abstract": "We present a novel technique of Monte Carlo error reduction that finds direct application in option pricing and Greeks estimation. The method is applicable to any LSV modelling framework and concerns a broad class of payoffs, including path-dependent and multi-asset cases. Most importantly, it allows to reduce the Monte Carlo error even by an order of magnitude, which is shown in several numerical examples.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2402.12561",
        "category": "q-fin",
        "title": "Robust Appointment Scheduling with Waiting Time Guarantees",
        "abstract": "Appointment scheduling problems under uncertainty encounter a fundamental trade-off between cost minimization and customer waiting times. Most existing studies address this trade-off using a weighted sum approach, which puts little emphasis on individual waiting times and, thus, customer satisfaction. In contrast, we study how to minimize total cost while providing waiting time guarantees to all customers. Given box uncertainty sets for service times and no-shows, we introduce the Robust Appointment Scheduling Problem with Waiting Time Guarantees. We show that the problem is NP-hard in general and introduce a mixed-integer linear program that can be solved in reasonable computation time. For special cases, we prove that polynomial-time variants of the well-known Smallest-Variance-First sequencing rule and the Bailey-Welch scheduling rule are optimal. Furthermore, a case study with data from the radiology department of a large university hospital demonstrates that the approach not only guarantees acceptable waiting times but, compared to existing robust approaches, may simultaneously reduce costs incurred by idle time and overtime. This work suggests that limiting instead of minimizing customer waiting times is a win-win solution in the trade-off between customer satisfaction and cost minimization. Additionally, it provides an easy-to-implement and customizable appointment scheduling framework with waiting time guarantees.",
        "references": [
            {
                "arxivId": "1708.06398",
                "title": "Non-indexability of the stochastic appointment scheduling problem",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2402.12848",
        "category": "q-fin",
        "title": "ATLAS: A Model of Short-term European Electricity Market Processes under Uncertainty",
        "abstract": "The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes. ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail. Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints--both regarding different production units and the different market constraints.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-21.json",
        "arxivId": "2402.13177",
        "category": "q-fin",
        "title": "The Ebb and Flow of Brand Loyalty: A 28-Year Bibliometric and Content Analysis",
        "abstract": "Business research is facing the challenge of scattered knowledge, particularly in the realm of brand loyalty (BL). Although literature reviews on BL exist, they predominantly concentrate on the pre-sent state, neglecting future trends. Therefore, a comprehensive review is imperative to ascertain emerging trends in BL This study employs a bibliometric approach, analyzing 1,468 papers from the Scopus database. Various tools including R software, VOS viewer software, and Publish or Perish are utilized. The aim is to portray the knowledge map, explore the publication years, identify the top authors and their co-occurrence, reliable documents, institutions, subjects, research hotspots, and pioneering countries and universities in the study of BL. The qualitative section of this research identifies gaps and emerging trends in BL through Word Cloud charts, word growth analysis, and a review of highly cited articles from the past four years. Results showed that highly cited articles mention topics such as brand love, consumer-brand identification, and social networks and the U.S. had the most productions in this field. Besides, most citations were related to Keller with 1,173 citations. Furthermore, in the qualitative section, social networks and brand experiences were found to be of interest to researchers in the field. Finally, by introducing the antecedents and consequences of BL, the gaps and emerging trends in BL were identified, so as to present the di-rection of future research in this area.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2303.08521",
        "category": "q-fin",
        "title": "Optimal investment in ambiguous financial markets with learning",
        "abstract": null,
        "references": [
            {
                "arxivId": "2302.08181",
                "title": "Equilibrium Portfolio Selection for Smooth Ambiguity Preferences",
                "abstract": "This paper investigates the equilibrium portfolio selection for smooth ambiguity preferences in a continuous-time market. The investor is uncertain about the risky asset\u2019s drift term and updates the subjective belief according to the Bayesian rule. A verification theorem is established, and an equilibrium strategy can be decomposed into a myopic demand and two hedging demands. When the prior is Gaussian, we provide an equilibrium solution in closed form. Moreover, a puzzle in the numerical results is interpreted via an alternative representation of the smooth ambiguity preferences. Funding: This work was supported by the National Key R&D Program of China [Grant 2020YFA0712700], the National Natural Science Foundation of China [Grants 11871036, 11901574, 12071146, 12271290, and 12371477], and the MOE Project of Key Research Institute of Humanities and Social Sciences [Grant 22JJD910003]."
            },
            {
                "arxivId": "1907.02347",
                "title": "Markov decision processes under ambiguity",
                "abstract": "We consider statistical Markov Decision Processes where the decision maker is risk averse against model ambiguity. The latter is given by an unknown parameter which influences the transition law and the cost functions. Risk aversion is either measured by the entropic risk measure or by the Average Value at Risk. We show how to solve these kind of problems using a general minimax theorem. Under some continuity and compactness assumptions we prove the existence of an optimal (deterministic) policy and discuss its computation. We illustrate our results using an example from statistical decision theory."
            },
            {
                "arxivId": "1803.03573",
                "title": "Bayesian mean\u2013variance analysis: optimal portfolio selection under parameter uncertainty",
                "abstract": "The paper solves the problem of optimal portfolio choice when the parameters of the asset returns distribution, for example the mean vector and the covariance matrix, are unknown and have to be estimated by using historical data on asset returns. Our new approach employs the Bayesian posterior predictive distribution which is the distribution of future realizations of asset returns given the observable sample. The parameters of posterior predictive distributions are functions of the observed data values and, consequently, the solution of the optimization problem is expressed in terms of data only and does not depend on unknown quantities. By contrast, the optimization problem of the traditional approach is based on unknown quantities which are estimated in the second step, and lead to a suboptimal solution. We also derive a very useful stochastic representation of the posterior predictive distribution whose application not only gives the solution of the considered optimization problem, but also provides the posterior predictive distribution of the optimal portfolio return which can be used to construct a prediction interval. A Bayesian efficient frontier, the set of optimal portfolios obtained by employing the posterior predictive distribution, is constructed as well. Theoretically and using real data we show that the Bayesian efficient frontier outperforms the sample efficient frontier, a common estimator of the set of optimal portfolios which is known to be overoptimistic."
            },
            {
                "arxivId": "1703.04423",
                "title": "Extremal behavior of long-term investors with power utility",
                "abstract": "We consider a Bayesian financial market with one bond and one stock where the aim is to maximize the expected power utility from terminal wealth. The solution of this problem is known, however there are some conjectures in the literature about the long-term behavior of the optimal strategy. In this paper we prove now that for positive coefficient in the power utility the long-term investor is very optimistic and behaves as if the best drift has been realized. In case the coefficient in the power utility is negative the long-term investor is very pessimistic and behaves as if the worst drift has been realized."
            },
            {
                "arxivId": "1502.02968",
                "title": "LEARNING AND PORTFOLIO DECISIONS FOR CRRA INVESTORS",
                "abstract": "We maximize the expected utility from terminal wealth for a Constant Relative Risk Aversion (CRRA) investor when the market price of risk is an unobservable random variable and explore the effects of learning by comparing the optimal portfolio under partial observation with the corresponding myopic policy. In particular, we show that, for a market price of risk constant in sign, the ratio between the portfolio under partial observation and its myopic counterpart increases with respect to risk tolerance. As a consequence, the absolute value of the partial observation case is larger (smaller) than the myopic one if the investor is more (less) risk tolerant than the logarithmic investor. Moreover, our explicit computations enable to study in detail the so called hedging demand induced by parameter uncertainty."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2401.14593",
        "category": "q-fin",
        "title": "Robust Estimation of the Tail Index of a Single Parameter Pareto Distribution from Grouped Data",
        "abstract": "Numerous robust estimators exist as alternatives to the maximum likelihood estimator (MLE) when a completely observed ground-up loss severity sample dataset is available. However, the options for robust alternatives to a MLE become significantly limited when dealing with grouped loss severity data, with only a handful of methods, like least squares, minimum Hellinger distance, and optimal bounded influence function, available. This paper introduces a novel robust estimation technique, the Method of Truncated Moments (MTuM), pecifically designed to estimate the tail index of a Pareto distribution from grouped data. Inferential justification of the MTuM is established by employing the central limit theorem and validating it through a comprehensive simulation study.",
        "references": [
            {
                "arxivId": "2310.11471",
                "title": "Modeling lower-truncated and right-censored insurance claims with an extension of the MBBEFD class",
                "abstract": "In general insurance, claims are often lower-truncated and right-censored because insurance contracts may involve deductibles and maximal covers. Most classical statistical models are not (directly) suited to model lower-truncated and right-censored claims. A surprisingly flexible family of distributions that can cope with lower-truncated and right-censored claims is the class of MBBEFD distributions that originally has been introduced by Bernegger (1997) for reinsurance pricing, but which has not gained much attention outside the reinsurance literature. Interestingly, in general insurance, we mainly rely on unimodal skewed densities, whereas the reinsurance literature typically proposes monotonically decreasing densities within the MBBEFD class. We show that this class contains both types of densities, and we extend it to a bigger family of distribution functions suitable for modeling lower-truncated and right-censored claims. In addition, we discuss how changes in the deductible or the maximal cover affect the chosen distributions."
            },
            {
                "arxivId": "2204.02477",
                "title": "Method of Winsorized Moments for Robust Fitting of Truncated and Censored Lognormal Distributions",
                "abstract": "When constructing parametric models to predict the cost of future claims, several important details have to be taken into account: (i) models should be designed to accommodate deductibles, policy limits, and coinsurance factors, (ii) parameters should be estimated robustly to control the influence of outliers on model predictions, and (iii) all point predictions should be augmented with estimates of their uncertainty. The methodology proposed in this paper provides a framework for addressing all these aspects simultaneously. Using payment-per-payment and payment-per-loss variables, we construct the adaptive version of method of winsorized moments (MWM) estimators for the parameters of truncated and censored lognormal distribution. Further, the asymptotic distributional properties of this approach are derived and compared with those of the maximum likelihood estimator (MLE) and method of trimmed moments (MTM) estimators. The latter being a primary competitor to MWM. Moreover, the theoretical results are validated with extensive simulation studies and risk measure sensitivity analysis. Finally, practical performance of these methods is illustrated using the well-studied data set of 1500 U.S. indemnity losses. With this real data set, it is also demonstrated that the composite models do not provide much improvement in the quality of predictive models compared to a stand-alone fitted distribution specially for truncated and censored sample data."
            },
            {
                "arxivId": "2202.13000",
                "title": "Robust Estimation of Loss Models for Truncated and Censored Severity Data",
                "abstract": "In this paper, we consider robust estimation of claim severity models in insurance, when data are affected by truncation (due to deductibles), censoring (due to policy limits), and scaling (due to coinsurance). In particular, robust estimators based on the methods of trimmed moments (T-estimators) and winsorized moments (W-estimators) are pursued and fully developed. The general definitions of such estimators are formulated and their asymptotic properties are investigated. For illustrative purposes, specific formulas for T- and W-estimators of the tail parameter of a single-parameter Pareto distribution are derived. The practical performance of these estimators is then explored using the well-known Norwegian fire claims data. Our results demonstrate that T- and W-estimators offer a robust and computationally efficient alternative to the likelihood-based inference for models that are affected by deductibles, policy limits, and coinsurance."
            },
            {
                "arxivId": "2103.02089",
                "title": "ROBUST ESTIMATION OF LOSS MODELS FOR LOGNORMAL INSURANCE PAYMENT SEVERITY DATA",
                "abstract": "Abstract The primary objective of this scholarly work is to develop two estimation procedures \u2013 maximum likelihood estimator (MLE) and method of trimmed moments (MTM) \u2013 for the mean and variance of lognormal insurance payment severity data sets affected by different loss control mechanism, for example, truncation (due to deductibles), censoring (due to policy limits), and scaling (due to coinsurance proportions), in insurance and financial industries. Maximum likelihood estimating equations for both payment-per-payment and payment-per-loss data sets are derived which can be solved readily by any existing iterative numerical methods. The asymptotic distributions of those estimators are established via Fisher information matrices. Further, with a goal of balancing efficiency and robustness and to remove point masses at certain data points, we develop a dynamic MTM estimation procedures for lognormal claim severity models for the above-mentioned transformed data scenarios. The asymptotic distributional properties and the comparison with the corresponding MLEs of those MTM estimators are established along with extensive simulation studies. Purely for illustrative purpose, numerical examples for 1500 US indemnity losses are provided which illustrate the practical performance of the established results in this paper."
            },
            {
                "arxivId": "2102.10154",
                "title": "Truncated, censored, and actuarial payment-type moments for robust fitting of a single-parameter Pareto distribution",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.04765",
        "category": "q-fin",
        "title": "Measuring the performance of investments in information security startups: An empirical analysis by cybersecurity sectors using Crunchbase data",
        "abstract": "Early-stage firms play a significant role in driving innovation and creating new products and services, especially for cybersecurity. Therefore, evaluating their performance is crucial for investors and policymakers. This work presents a financial evaluation of early-stage firms' performance in 19 cybersecurity sectors using a private-equity dataset from 2010 to 2022 retrieved from Crunchbase. We observe firms, their primary and secondary activities, funding rounds, and pre and post-money valuations. We compare cybersecurity sectors regarding the amount raised over funding rounds and post-money valuations while inferring missing observations. We observe significant investor interest variations across categories, periods, and locations. In particular, we find the average capital raised (valuations) to range from USD 7.24 mln (USD 32.39 mln) for spam filtering to USD 45.46 mln (USD 447.22 mln) for the private cloud sector. Next, we assume a log process for returns computed from post-money valuations and estimate the expected returns, systematic and specific risks, and risk-adjusted returns of investments in early-stage firms belonging to cybersecurity sectors. Again, we observe substantial performance variations with annualized expected returns ranging from 9.72\\% for privacy to 177.27\\% for the blockchain sector. Finally, we show that overall, the cybersecurity industry performance is on par with previous results found in private equity. Our results shed light on the performance of cybersecurity investments and, thus, on investors' expectations about cybersecurity.",
        "references": [
            {
                "arxivId": "2007.04074",
                "title": "Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning",
                "abstract": "Automated Machine Learning (AutoML) supports practitioners and researchers with the tedious task of designing machine learning pipelines and has recently achieved substantial success. In this paper, we introduce new AutoML approaches motivated by our winning submission to the second ChaLearn AutoML challenge. We develop PoSH Auto-sklearn, which enables AutoML systems to work well on large datasets under rigid time limits by using a new, simple and meta-feature-free meta-learning technique and by employing a successful bandit strategy for budget allocation. However, PoSH Auto-sklearn introduces even more ways of running AutoML and might make it harder for users to set it up correctly. Therefore, we also go one step further and study the design space of AutoML itself, proposing a solution towards truly hands-free AutoML. Together, these changes give rise to the next generation of our AutoML system, Auto-sklearn 2.0. We verify the improvements by these additions in an extensive experimental study on 39 AutoML benchmark datasets. We conclude the paper by comparing to other popular AutoML frameworks and Auto-sklearn 1.0, reducing the relative error by up to a factor of 4.5, and yielding a performance in 10 minutes that is substantially better than what Auto-sklearn 1.0 achieves within an hour."
            },
            {
                "arxivId": "1208.3994",
                "title": "Coordination in Network Security Games: A Monotone Comparative Statics Approach",
                "abstract": "Malicious softwares or malwares for short have become a major security threat. While originating in criminal behavior, their impact are also influenced by the decisions of legitimate end users. Getting agents in the Internet, and in networks in general, to invest in and deploy security features and protocols is a challenge, in particular because of economic reasons arising from the presence of network externalities. In this paper, we focus on the question of incentive alignment for agents of a large network towards a better security. We start with an economic model for a single agent, that determines the optimal amount to invest in protection. The model takes into account the vulnerability of the agent to a security breach and the potential loss if a security breach occurs. We derive conditions on the quality of the protection to ensure that the optimal amount spent on security is an increasing function of the agent's vulnerability and potential loss. We also show that for a large class of risks, only a small fraction of the expected loss should be invested. Building on these results, we study a network of interconnected agents subject to epidemic risks. We derive conditions to ensure that the incentives of all agents are aligned towards a better security. When agents are strategic, we show that security investments are always socially inefficient due to the network externalities. Moreover alignment of incentives typically implies a coordination problem, leading to an equilibrium with a very high price of anarchy."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.13278",
        "category": "q-fin",
        "title": "Determinantes do planejamento estrat\\'egico da rede de uma companhia a\\'erea",
        "abstract": "This work focuses on trying to understand how the construction of an airline's network is made. For this purpose, the case of Azul was studied, investigating which and how factors affect the decision of this airline to enter domestic routes, in addition to analyzing how the merger of Azul with the regional airline Trip affected the company's network planning. For this, an academic study was conducted using an econometric model to understand the airline's entry model. The results show that Azul's business model is based on connecting new destinations, not yet served by its competitors, to one of its hubs, and consistently avoiding routes or airports dominated by other airlines. Regarding the effects of the merger, the results suggest that Azul moved away from its original entry model, based on JetBlue, to a model more oriented towards regional aviation, entering shorter routes and regional airports.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.13279",
        "category": "q-fin",
        "title": "Privatiza\\c{c}\\~ao de aeroportos: motiva\\c{c}\\~oes, regula\\c{c}\\~ao e efici\\^encia operacional",
        "abstract": "In this study, we will address some topics related to the privatization of airports in the scientific literature, in an attempt to provide an answer to the following question: does the privatization of airports bring positive results? Firstly, we turn our attention to the motivations leading to privatization, considering the two main parties involved, the government and the private sector. After all, the success of such a decision will be relative to the reasons that justify it. In a second moment, we will consider the regulatory issue, whose influence on the airport's financial performance is consolidated in the literature. In the third part, we address the main documented results of privatization, with special attention to productive efficiency, whose improvement is the most popular motivation for privatization.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.13439",
        "category": "q-fin",
        "title": "Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada",
        "abstract": "This paper investigates the demand for lamb, beef, pork, and poultry in Canada, both at the national level and in disaggregated provinces, to identify meat consumption patterns in different provinces. Meat consumption plays a significant role in Canada's economy and is an important source of calories for the population. However, meat demand faces several consumption challenges due to logistic constraints, as a significant portion of the supply is imported from other countries. Therefore, there is a need for a better understanding of the causal relationships underlying lamb, beef, pork, and poultry consumption in Canada. Until recently, there have been no attempts to estimate meat consumption at the provincial level in Canada. Different Almost Ideal Demand System (AIDS) models have been applied for testing specifications to circumvent several econometric and theoretical problems. In particular, generalized AIDS and its Quadratic extension QUAIDS methods have been estimated across each province using the Iterative Linear Least Squares Estimator (ILLE) estimation Method. Weekly retail meat consumption price and quantity data from 2019 to 2022 have been used for Canada and for each province namely Quebec, Maritime provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario, total West (Yukon, Northwest Territory and Nunavut), Alberta, Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent coefficients and demand elasticities estimates reveal patterns of substitution and/or complementarity between the four categories of meat. Meat consumption patterns differ across each province. Results show that the demand for the four categories of meat is responsive to price changes. Overall, lamb expenditure was found to be elastic and thus considered a luxury good during the study period, while the other three categories are considered normal goods across Canada.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.13627",
        "category": "q-fin",
        "title": "Algorithms for Claims Trading",
        "abstract": "The recent banking crisis has again emphasized the importance of understanding and mitigating systemic risk in financial networks. In this paper, we study a market-driven approach to rescue a bank in distress based on the idea of claims trading, a notion defined in Chapter 11 of the U.S. Bankruptcy Code. We formalize the idea in the context of financial networks by Eisenberg and Noe. For two given banks v and w, we consider the operation that w takes over some claims of v and in return gives liquidity to v to ultimately rescue v. We study the structural properties and computational complexity of decision and optimization problems for several variants of claims trading. When trading incoming edges of v, we show that there is no trade in which both banks v and w strictly improve their assets. We therefore consider creditor-positive trades, in which v profits strictly and w remains indifferent. For a given set C of incoming edges of v, we provide an efficient algorithm to compute payments by w that result in maximal assets of v. When the set C must also be chosen, the problem becomes weakly NP-hard. Our main result here is a bicriteria FPTAS to compute an approximate trade. The approximate trade results in nearly the optimal amount of assets of v in any exact trade. Our results extend to the case in which banks use general monotone payment functions and the emerging clearing state can be computed efficiently. In contrast, for trading outgoing edges of v, the goal is to maximize the increase in assets for the creditors of v. Notably, for these results the characteristics of the payment functions of the banks are essential. For payments ranking creditors one by one, we show NP-hardness of approximation within a factor polynomial in the network size, when the set of claims C is part of the input or not. Instead, for proportional payments, our results indicate more favorable conditions.",
        "references": [
            {
                "arxivId": "2302.11250",
                "title": "The Complexity of Debt Swapping",
                "abstract": "A debt swap is an elementary edge swap in a directed, weighted graph, where two edges with the same weight swap their targets. Debt swaps are a natural and appealing operation in financial networks, in which nodes are banks and edges represent debt contracts. They can improve the clearing payments and the stability of these networks. However, their algorithmic properties are not well-understood. We analyze the computational complexity of debt swapping in networks with ranking-based clearing. Our main interest lies in semi-positive swaps, in which no creditor strictly suffers and at least one strictly profits. These swaps lead to a Pareto-improvement in the entire network. We consider network optimization via sequences of $v$-improving debt swaps from which a given bank $v$ strictly profits. We show that every sequence of semi-positive $v$-improving swaps has polynomial length. In contrast, for arbitrary $v$-improving swaps, the problem of reaching a network configuration that allows no further swaps is PLS-complete. We identify cases in which short sequences of semi-positive swaps exist even without the $v$-improving property. In addition, we study reachability problems, i.e., deciding if a sequence of swaps exists between given initial and final networks. We identify a polynomial-time algorithm for arbitrary swaps, show NP-hardness for semi-positive swaps and even PSPACE-completeness for $v$-improving swaps or swaps that only maintain a lower bound on the assets of a given bank $v$. A variety of our results can be extended to arbitrary monotone clearing."
            },
            {
                "arxivId": "2205.15628",
                "title": "Seniorities and Minimal Clearing in Financial Network Games",
                "abstract": "Financial network games model payment incentives in the context of networked liabilities. In this paper, we advance the understanding of incentives in financial networks in two important directions: minimal clearing (arising, e.g., as a result of sequential execution of payments) and seniorities (i.e., priorities over debt contracts). We distinguish between priorities that are chosen endogenously or exogenously. For endogenous priorities and standard (maximal) clearing, the games exhibit a coalitional form of weak acyclicity. A strong equilibrium exists and can be reached after a polynomial number of deviations. Moreover, there is a strong equilibrium that is optimal for a wide variety of social welfare functions. In contrast, for minimal clearing there are games in which no optimal strategy profile exists, even for standard utilitarian social welfare. Perhaps surprisingly, a strong equilibrium still exists and, for a wide range of strategies, can be reached after a polynomial number of deviations. In contrast, for exogenous priorities, equilibria can be absent and equilibrium existence is NP-hard to decide, for both minimal and maximal clearing."
            },
            {
                "arxivId": "2202.10986",
                "title": "Forgiving Debt in Financial Network Games",
                "abstract": "We consider financial networks, where nodes correspond to banks and directed labeled edges correspond to debt contracts between banks. Maximizing systemic liquidity, i.e., the total money flow, is a natural objective of any financial authority. In particular, the financial authority may offer bailout money to some bank(s) or forgive the debts of others in order to maximize liquidity, and we examine efficient ways to achieve this. We study the computational hardness of finding the optimal debt-removal and budget-constrained optimal bailout policy, respectively, and we investigate the approximation ratio provided by the greedy bailout policy compared to the optimal one. \n\n \n\nWe also study financial systems from a game-theoretic standpoint. We observe that the removal of some incoming debt might be in the best interest of a bank. Assuming that a bank's well-being (i.e., utility) is aligned with the incoming payments they receive from the network, we define and analyze a game among banks who want to maximize their utility by strategically giving up some incoming payments. In addition, we extend the previous game by considering bailout payments. After formally defining the above games, we prove results about the existence and quality of pure Nash equilibria, as well as the computational complexity of finding such equilibria."
            },
            {
                "arxivId": "2107.06623",
                "title": "Financial network games",
                "abstract": "We study financial systems from a game-theoretic standpoint. A financial system is represented by a network, where nodes correspond to firms, and directed labeled edges correspond to debt contracts between them. The existence of cycles in the network indicates that a payment of a firm to one of its lenders might result to some incoming payment. So, if a firm cannot fully repay its debt, then the exact (partial) payments it makes to each of its creditors can affect the cash inflow back to itself. We naturally assume that the firms are interested in their financial well-being (utility) which is aligned with the amount of incoming payments they receive from the network. This defines a game among the firms, that can be seen as utility-maximizing agents who can strategize over their payments. We are the first to study financial network games that arise under a natural set of payment strategies called priority-proportional payments. We investigate both the existence and the (in)efficiency of equilibrium strategies, under different assumptions on how the firms' utility is defined, on the types of debt contracts allowed between the firms, and on the presence of other financial features that commonly arise in practice. Surprisingly, even if all firms' strategies are fixed, the existence of a unique payment profile is not guaranteed. So, we also investigate the existence and computation of valid payment profiles for fixed payment strategies."
            },
            {
                "arxivId": "2106.07560",
                "title": "Allocating Stimulus Checks in Times of Crisis",
                "abstract": "We study the problem of financial assistance (bailouts, stimulus payments, or subsidy allocations) in a network where individuals experience income shocks. These questions are pervasive both in policy domains and in the design of new Web-enabled forms of financial interaction. We build on the financial clearing framework of Eisenberg and Noe that allows the incorporation of a bailout policy that is based on discrete bailouts motivated by stimulus programs in both off-line and on-line settings. We show that optimally allocating such bailouts on a financial network in order to maximize a variety of social welfare objectives of this form is a computationally intractable problem. We develop approximation algorithms to optimize these objectives and establish guarantees for their approximation ratios. Then, we incorporate multiple fairness constraints in the optimization problems and study their boundedness. Finally, we apply our methodology to data, both in the context of a system of large financial institutions with real-world data, as well as in a realistic societal context with financial interactions between people and businesses for which we use semi-artificial data derived from mobility patterns. Our results suggest that the algorithms we develop and study have reasonable results in practice and outperform other network-based heuristics. We argue that the presented problem through the societal-level lens could assist policymakers in making informed decisions on issuing subsidies."
            },
            {
                "arxivId": "2107.05359",
                "title": "Debt Swapping for Risk Mitigation in Financial Networks",
                "abstract": "We study financial networks where banks are connected by debt contracts. We consider the operation of debt swapping when two creditor banks decide to exchange an incoming payment obligation, thus leading to a locally different network structure. We say that a swap is positive if it is beneficial for both of the banks involved; we can interpret this notion either with respect to the amount of assets received by the banks, or their exposure to different shocks that might hit the system. We analyze various properties of these swapping operations in financial networks. We first show that there can be no positive swap for any pair of banks in a static financial system, or when a shock hits each bank in the network proportionally. We then study worst-case shock models, when a shock of given size is distributed in the worst possible way for a specific bank. If the goal of banks is to minimize their losses in such a worst-case setting, then a positive swap can indeed exist. We analyze the effects of such a positive swap on other banks of the system, the computational complexity of finding a swap, and special cases where a swap can be found efficiently. Finally, we also present some results for more complex swapping operations when the banks swap multiple contracts, or when more than two banks participate in the swap."
            },
            {
                "arxivId": "2002.07741",
                "title": "Default Ambiguity: Finding the Best Solution to the Clearing Problem",
                "abstract": null
            },
            {
                "arxivId": "2002.07566",
                "title": "Network-Aware Strategies in Financial Systems",
                "abstract": "We study the incentives of banks in a financial network, where the network consists of debt contracts and credit default swaps (CDSs) between banks. One of the most important questions in such a system is the problem of deciding which of the banks are in default, and how much of their liabilities these banks can pay. We study the payoff and preferences of the banks in the different solutions to this problem. We also introduce a more refined model which allows assigning priorities to payment obligations; this provides a more expressive and realistic model of real-life financial systems, while it always ensures the existence of a solution. \nThe main focus of the paper is an analysis of the actions that a single bank can execute in a financial system in order to influence the outcome to its advantage. We show that removing an incoming debt, or donating funds to another bank can result in a single new solution that is strictly more favorable to the acting bank. We also show that increasing the bank's external funds or modifying the priorities of outgoing payments cannot introduce a more favorable new solution into the system, but may allow the bank to remove some unfavorable solutions, or to increase its recovery rate. Finally, we show how the actions of two banks in a simple financial system can result in classical game theoretic situations like the prisoner's dilemma or the dollar auction, demonstrating the wide expressive capability of the financial system model."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.13789",
        "category": "q-fin",
        "title": "The seasonality of air ticket prices before and after the pandemic (preprint)",
        "abstract": "This study investigates price seasonality in the Brazilian air transport industry, emphasizing the impact of the COVID-19 pandemic on domestic airline pricing strategies. Given potential shifts in demand patterns following the global health crisis, this study explores possible long-term structural changes in the seasonality of Brazilian airfare. We analyze an open dataset of domestic city pairs from 2013 to 2023, employing an econometric model developed using Stata software. Our findings indicate alterations in seasonal patterns and long-term trends in the post-pandemic era. These changes underscore potential shifts in the composition of leisure and business travelers, along with the cost pressures faced by airlines.",
        "references": [
            {
                "arxivId": "2402.07124",
                "title": "Econometric analysis to estimate the impact of holidays on airfares",
                "abstract": "The number of air transportation passengers during the holidays in Brazil has grown notably since the late nineties. One of the reasons is greater competition in airfares made possible by economic liberalization. This paper presents an econometric model of airline pricing aiming at estimating the impacts of holiday periods on fares, with special emphasis on three-day holiday events. It makes use of a database with daily collected data from the internet between 2008 and 2010 for the major Brazilian city, Sao Paulo. The econometric panel data model employs a two-way error components \u201cwithin\u201d estimator, controlling for airline/airport-pair fixed effect along with quotation and departure months effects. The decomposition of time effects between quotation and departure month effects is the main methodological contribution of the paper. Results allow for a comparative analysis of the performance of Sao Paulo\u2019s downtown and international airports - respectively, Congonhas (CGH), and Guarulhos (GRU) airports. As a result, the price of tickets bought 60 days in advance for flights with two stops leaving from the downtown airport fell by most."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-22.json",
        "arxivId": "2402.13807",
        "category": "q-fin",
        "title": "Offshoring emissions through used vehicle exports",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2109.00643",
        "category": "q-fin",
        "title": "Using temperature sensitivity to estimate shiftable electricity demand",
        "abstract": null,
        "references": [
            {
                "arxivId": "1804.05481",
                "title": "Switch 2.0: A modern platform for planning high-renewable power systems",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2211.16159",
        "category": "q-fin",
        "title": "Estimation of Systemic Shortfall Risk Measure using Stochastic Algorithms",
        "abstract": "Systemic risk measures were introduced to capture the global risk and the corresponding contagion effects that is generated by an interconnected system of financial institutions. To this purpose, two approaches were suggested. In the first one, systemic risk measures can be interpreted as the minimal amount of cash needed to secure a system after aggregating individual risks. In the second approach, systemic risk measures can be interpreted as the minimal amount of cash that secures a system by allocating capital to each single institution before aggregating individual risks. Although the theory behind these risk measures has been well investigated by several authors, the numerical part has been neglected so far. In this paper, we use stochastic algorithms schemes in estimating MSRM and prove that the resulting estimators are consistent and asymptotically normal. We also test numerically the performance of these algorithms on several examples.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2303.04204",
        "category": "q-fin",
        "title": "Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?",
        "abstract": null,
        "references": [
            {
                "arxivId": "2208.05908",
                "title": "Uncertainty Quantification of Sparse Travel Demand Prediction with Spatial-Temporal Graph Neural Networks",
                "abstract": "Origin-Destination (O-D) travel demand prediction is a fundamental challenge in transportation. Recently, spatial-temporal deep learning models demonstrate the tremendous potential to enhance prediction accuracy. However, few studies tackled the uncertainty and sparsity issues in fine-grained O-D matrices. This presents a serious problem, because a vast number of zeros deviate from the Gaussian assumption underlying the deterministic deep learning models. To address this issue, we design a Spatial-Temporal Zero-Inflated Negative Binomial Graph Neural Network (STZINB-GNN) to quantify the uncertainty of the sparse travel demand. It analyzes spatial and temporal correlations using diffusion and temporal convolution networks, which are then fused to parameterize the probabilistic distributions of travel demand. The STZINB-GNN is examined using two real-world datasets with various spatial and temporal resolutions. The results demonstrate the superiority of STZINB-GNN over benchmark models, especially under high spatial-temporal resolutions, because of its high accuracy, tight confidence intervals, and interpretable parameters. The sparsity parameter of the STZINB-GNN has physical interpretation for various transportation applications."
            },
            {
                "arxivId": "2109.12422",
                "title": "Equality of opportunity in travel behavior prediction with deep neural networks and discrete choice models",
                "abstract": null
            },
            {
                "arxivId": "2109.12042",
                "title": "Combining Discrete Choice Models and Neural Networks through Embeddings: Formulation, Interpretability and Performance",
                "abstract": null
            },
            {
                "arxivId": "2109.12144",
                "title": "Spatial Aggregation and Temporal Convolution Networks for Real-time Kriging",
                "abstract": "Spatiotemporal kriging is an important application in spatiotemporal data analysis, aiming to recover/interpolate signals for unsampled/unobserved locations based on observed signals. The principle challenge for spatiotemporal kriging is how to effectively model and leverage the spatiotemporal dependencies within the data. Recently, graph neural networks (GNNs) have shown great promise for spatiotemporal kriging tasks. However, standard GNNs often require a carefully designed adjacency matrix and specific aggregation functions, which are inflexible for general applications/problems. To address this issue, we present SATCN -- Spatial Aggregation and Temporal Convolution Networks -- a universal and flexible framework to perform spatiotemporal kriging for various spatiotemporal datasets without the need for model specification. Specifically, we propose a novel spatial aggregation network (SAN) inspired by Principal Neighborhood Aggregation, which uses multiple aggregation functions to help one node gather diverse information from its neighbors. To exclude information from unsampled nodes, a masking strategy that prevents the unsampled sensors from sending messages to their neighborhood is introduced to SAN. We capture temporal dependencies by the temporal convolutional networks, which allows our model to cope with data of diverse sizes. To make SATCN generalizable to unseen nodes and even unseen graph structures, we employ an inductive strategy to train SATCN. We conduct extensive experiments on three real-world spatiotemporal datasets, including traffic speed and climate recordings. Our results demonstrate the superiority of SATCN over traditional and GNN-based kriging models."
            },
            {
                "arxivId": "2106.13319",
                "title": "A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling",
                "abstract": null
            },
            {
                "arxivId": "2106.05876",
                "title": "Data Fusion for Deep Learning on Transport Mode Detection: A Case Study",
                "abstract": null
            },
            {
                "arxivId": "2012.09841",
                "title": "Taming Transformers for High-Resolution Image Synthesis",
                "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY."
            },
            {
                "arxivId": "2007.03639",
                "title": "Human Trajectory Forecasting in Crowds: A Deep Learning Perspective",
                "abstract": "Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, deployment of intelligent transport systems, traffic operations, to name a few. In this work, we cast the problem of human trajectory forecasting as learning a representation of human social interactions. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learn about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning-based methods for modelling social interactions. We propose two domain-knowledge inspired data-driven methods to effectively capture these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets."
            },
            {
                "arxivId": "2002.01612",
                "title": "Generating Interpretable Poverty Maps using Object Detection in Satellite Images",
                "abstract": "Accurate local-level poverty measurement is an essential task for governments and humanitarian organizations to track the progress towards improving livelihoods and distribute scarce resources. Recent computer vision advances in using satellite imagery to predict poverty have shown increasing accuracy, but they do not generate features that are interpretable to policymakers, inhibiting adoption by practitioners. Here we demonstrate an interpretable computational framework to accurately predict poverty at a local level by applying object detectors to high resolution (30cm) satellite images. Using the weighted counts of objects as features, we achieve 0.539 Pearson's r^2 in predicting village-level poverty in Uganda, a 31% improvement over existing (and less interpretable) benchmarks. Feature importance and ablation analysis reveal intuitive relationships between object counts and poverty predictions. Our results suggest that interpretability does not have to come at the cost of performance, at least in this important domain."
            },
            {
                "arxivId": "1912.11114",
                "title": "Multi-level Convolutional Autoencoder Networks for Parametric Prediction of Spatio-temporal Dynamics",
                "abstract": null
            },
            {
                "arxivId": "1904.08933",
                "title": "Ensemble Convolutional Neural Networks for Mode Inference in Smartphone Travel Survey",
                "abstract": "We develop ensemble convolutional neural networks (CNNs) to classify the transportation mode of trip data collected as part of a large-scale smartphone travel survey in Montreal, Canada. Our proposed ensemble library is composed of a series of CNN models with different hyper-parameter values and CNN architectures. In our final model, we combine the output of CNN models using \u201caverage voting,\u201d \u201cmajority voting,\u201d and \u201coptimal weights\u201d methods. Furthermore, we exploit the ensemble library by deploying a random forest model as a meta-learner. The ensemble method with random forest as meta-learner shows an accuracy of 91.8% which surpasses the other three ensemble combination methods, and other comparable models reported in the literature. The \u201cmajority voting\u201d and \u201coptimal weights\u201d combination methods result in prediction accuracy rates around 89%, while \u201caverage voting\u201d is able to achieve an accuracy of only 85%."
            },
            {
                "arxivId": "1902.10768",
                "title": "Semi-supervised GANs to Infer Travel Modes in GPS Trajectories",
                "abstract": null
            },
            {
                "arxivId": "1902.05581",
                "title": "Adversarially Approximated Autoencoder for Image Generation and Manipulation",
                "abstract": "Regularized autoencoders learn the latent codes, a structure with the regularization under the distribution, which enables them the capability to infer the latent codes given observations and generate new samples given the codes. However, they are sometimes ambiguous as they tend to produce reconstructions that are not necessarily a faithful reproduction of the inputs. The main reason is to enforce the learned latent code distribution to match a prior distribution while the true distribution remains unknown. To improve the reconstruction quality and learn the latent space a manifold structure, this paper presents a novel approach using the adversarially approximated autoencoder (AAAE) to investigate the latent codes with adversarial approximation. Instead of regularizing the latent codes by penalizing on the distance between the distributions of the model and the target, AAAE learns the autoencoder flexibly and approximates the latent space with a simpler generator. The ratio is estimated using a generative adversarial network to enforce the similarity of the distributions. In addition, the image space is regularized with an additional adversarial regularizer. The proposed approach unifies two deep generative models for both latent space inference and diverse generation. The learning scheme is realized without regularization on the latent codes, which also encourages faithful reconstruction. Extensive validation experiments on four real-world datasets demonstrate the superior performance of AAAE. In comparison to the state-of-the-art approaches, AAAE generates samples with better quality and shares the properties of a regularized autoencoder with a nice latent manifold structure."
            },
            {
                "arxivId": "1809.05781",
                "title": "Modelling Latent Travel Behaviour Characteristics with Generative Machine Learning",
                "abstract": "In this paper, we implement an information-theoretic approach to travel behaviour analysis by introducing a generative modelling framework to identify informative latent characteristics in travel decision making. It involves developing a joint tri-partite Bayesian graphical network model using a Restricted Boltzmann Machine (RBM) generative modelling framework. We apply this framework on a mode choice survey data to identify abstract latent variables and compare the performance with a traditional latent variable model with specific latent preferences \u2013 safety, comfort, and environmental. Data collected from a joint stated and revealed preference mode choice survey in Quebec, Canada were used to calibrate the RBM model. Results show that a significant impact on model likelihood statistics and suggests that machine learning tools are highly suitable for modelling complex networks of conditional independent behaviour interactions."
            },
            {
                "arxivId": "1807.07543",
                "title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer",
                "abstract": "Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can \"interpolate\": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations."
            },
            {
                "arxivId": "1801.03924",
                "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations."
            },
            {
                "arxivId": "1704.03477",
                "title": "A Neural Representation of Sketch Drawings",
                "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format."
            },
            {
                "arxivId": "1704.02965",
                "title": "Using Convolutional Networks and Satellite Imagery to Identify Patterns in Urban Environments at a Large Scale",
                "abstract": "Urban planning applications (energy audits, investment, etc.) require an understanding of built infrastructure and its environment, i.e., both low-level, physical features (amount of vegetation, building area and geometry etc.), as well as higher-level concepts such as land use classes (which encode expert understanding of socio-economic end uses). This kind of data is expensive and labor-intensive to obtain, which limits its availability (particularly in developing countries). We analyze patterns in land use in urban neighborhoods using large-scale satellite imagery data (which is available worldwide from third-party providers) and state-of-the-art computer vision techniques based on deep convolutional neural networks. For supervision, given the limited availability of standard benchmarks for remote-sensing data, we obtain ground truth land use class labels carefully sampled from open-source surveys, in particular the Urban Atlas land classification dataset of $20$ land use classes across $~300$ European cities. We use this data to train and compare deep architectures which have recently shown good performance on standard computer vision tasks (image classification and segmentation), including on geospatial data. Furthermore, we show that the deep representations extracted from satellite imagery of urban environments can be used to compare neighborhoods across several cities. We make our dataset available for other machine learning researchers to use for remote-sensing applications."
            },
            {
                "arxivId": "1702.06683",
                "title": "Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States",
                "abstract": "Significance We show that socioeconomic attributes such as income, race, education, and voting patterns can be inferred from cars detected in Google Street View images using deep learning. Our model works by discovering associations between cars and people. For example, if the number of sedans in a city is higher than the number of pickup trucks, that city is likely to vote for a Democrat in the next presidential election (88% chance); if not, then the city is likely to vote for a Republican (82% chance). The United States spends more than $250 million each year on the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed several years. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may become an increasingly practical supplement to the ACS. Here, we present a method that estimates socioeconomic characteristics of regions spanning 200 US cities by using 50 million images of street scenes gathered with Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22 million automobiles in total (8% of all automobiles in the United States), were used to accurately estimate income, race, education, and voting patterns at the zip code and precinct level. (The average US precinct contains \u223c1,000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next presidential election (88% chance); otherwise, it is likely to vote Republican (82%). Our results suggest that automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time."
            },
            {
                "arxivId": "1701.01272",
                "title": "Autoencoder Regularized Network For Driving Style Representation Learning",
                "abstract": "In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the driver identification problem show that ARNet can learn a good generalized driving style representation: It significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods."
            },
            {
                "arxivId": "1611.05431",
                "title": "Aggregated Residual Transformations for Deep Neural Networks",
                "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
            },
            {
                "arxivId": "1610.02391",
                "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization",
                "abstract": null
            },
            {
                "arxivId": "1608.01769",
                "title": "Deep Learning the City: Quantifying Urban Perception at a Global Scale",
                "abstract": null
            },
            {
                "arxivId": "1606.00704",
                "title": "Adversarially Learned Inference",
                "abstract": "We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks."
            },
            {
                "arxivId": "1605.09782",
                "title": "Adversarial Feature Learning",
                "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning."
            },
            {
                "arxivId": "1512.09300",
                "title": "Autoencoding beyond pixels using a learned similarity metric",
                "abstract": "We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic."
            },
            {
                "arxivId": "1512.03385",
                "title": "Deep Residual Learning for Image Recognition",
                "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            {
                "arxivId": "1505.06279",
                "title": "The Benefit of Multitask Representation Learning",
                "abstract": "We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks."
            },
            {
                "arxivId": "1412.6806",
                "title": "Striving for Simplicity: The All Convolutional Net",
                "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2305.00545",
        "category": "q-fin",
        "title": "Optimal multi-action treatment allocation: A two-phase field experiment to boost immigrant naturalization",
        "abstract": "Research underscores the role of naturalization in enhancing immigrants' socio-economic integration, yet application rates remain low. We estimate a policy rule for a letter-based information campaign encouraging newly eligible immigrants in Zurich, Switzerland, to naturalize. The policy rule assigns one out of three treatment letters to each individual, based on their observed characteristics. We field the policy rule to one-half of 1,717 immigrants, while sending random treatment letters to the other half. Despite only moderate treatment effect heterogeneity, the policy tree yields a larger, albeit insignificant, increase in application rates compared to assigning the same letter to everyone.",
        "references": [
            {
                "arxivId": "2012.04055",
                "title": "Who should get vaccinated? Individualized allocation of vaccines over SIR network",
                "abstract": null
            },
            {
                "arxivId": "1912.08726",
                "title": "Econometrics for Decision Making: Building Foundations Sketched by Haavelmo and Wald",
                "abstract": "Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes use of statistical decision theory to evaluate the performance of models in decision making. I consider the common practice of \n as\u2010if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as\u2010if optimization or any other model\u2010based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancing computation is the primary task to complete the foundations sketched by Haavelmo and Wald.\n"
            },
            {
                "arxivId": "1810.13237",
                "title": "Machine Learning Estimation of Heterogeneous Causal Effects: Empirical Monte Carlo Evidence",
                "abstract": "\n We investigate the finite-sample performance of causal machine learning estimators for heterogeneous causal effects at different aggregation levels. We employ an empirical Monte Carlo study that relies on arguably realistic data generation processes (DGPs) based on actual data in an observational setting. We consider 24 DGPs, eleven causal machine learning estimators, and three aggregation levels of the estimated effects. Four of the considered estimators perform consistently well across all DGPs and aggregation levels. These estimators have multiple steps to account for the selection into the treatment and the outcome process."
            },
            {
                "arxivId": "1810.04778",
                "title": "Offline Multi-Action Policy Learning: Generalization and Optimization",
                "abstract": "As a result of digitization of the economy, more and more decision makers from a wide range of domains have gained the ability to target products, services, and information provision based on individual characteristics. Examples include selecting offers, prices, advertisements, or emails to send to consumers, choosing a bid to submit in a contextual first-price auctions, and determining which medication to prescribe to a patient. The key to enabling this is to learn a treatment policy from historical observational data in a sample-efficient way, hence uncovering the best personalized treatment choice recommendation. In \u201cOffline Policy Learning: Generalization and Optimization,\u201d Z. Zhou, S. Athey, and S. Wager provide a sample-optimal policy learning algorithm that is computationally efficient and that learns a tree-based treatment policy from observational data. In our quest toward fully automated personalization, the work provides a theoretically sound and practically implementable approach."
            },
            {
                "arxivId": "1709.10279",
                "title": "Heterogeneous Employment Effects of Job Search Programs",
                "abstract": "We systematically investigate the effect heterogeneity of job search programs for unemployed workers. To investigate possibly heterogeneous employment effects, we combine nonexperimental causal empirical models with Lassotype estimators. The empirical analyses are based on rich administrative data from Swiss social security records. We find considerable heterogeneities during the first six months after the start of training. Consistent with previous results in the literature, unemployed persons with fewer employment opportunities profit more from participating in these programs. Finally, we show the potential of easy-to-implement program participation rules for improving average employment effects of these active labor market programs."
            },
            {
                "arxivId": "1706.03461",
                "title": "Metalearners for estimating heterogeneous treatment effects using machine learning",
                "abstract": "Significance Estimating and analyzing heterogeneous treatment effects is timely, yet challenging. We introduce a unifying framework for many conditional average treatment effect estimators, and we propose a metalearner, the X-learner, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect. We present its favorable properties, using theory and simulations. We apply it, using random forests, to two field experiments in political science, where it is shown to be easy to use and to produce results that are interpretable. There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms\u2014such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks\u2014to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods."
            },
            {
                "arxivId": "1610.01271",
                "title": "Generalized random forests",
                "abstract": "We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN."
            },
            {
                "arxivId": "1103.4601",
                "title": "Doubly Robust Policy Evaluation and Learning",
                "abstract": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. \nIn this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2312.09843",
        "category": "q-fin",
        "title": "Drivers and Barriers of AI Adoption and Use in Scientific Research",
        "abstract": "New technologies have the power to revolutionize science. It has happened in the past and is happening again with the emergence of new computational tools, such as artificial intelligence and machine learning. Despite the documented impact of these technologies, there remains a significant gap in understanding the process of their adoption within the scientific community. In this paper, we draw on theories of scientific and technical human capital to study the integration of AI in scientific research, focusing on the human capital of scientists and the external resources available within their network of collaborators and institutions. We validate our hypotheses on a large sample of publications from OpenAlex, covering all sciences from 1980 to 2020, and identify a set key drivers and inhibitors of AI adoption and use in science. Our results suggest that AI is pioneered by domain scientists with a `taste for exploration' and who are embedded in a network rich of computer scientists, experienced AI scientists and early-career researchers; they come from institutions with high citation impact and a relatively strong publication history on AI. The access to computing resources only matters for a few scientific disciplines, such as chemistry and medical sciences. Once AI is integrated into research, most adoption factors continue to influence its subsequent reuse. Implications for the organization and management of science in the evolving era of AI-driven discovery are discussed.",
        "references": [
            {
                "arxivId": "2206.14007",
                "title": "The Importance of (Exponentially More) Computing Power",
                "abstract": "Denizens of Silicon Valley have called Moore's Law\"the most important graph in human history,\"and economists have found that Moore's Law-powered I.T. revolution has been one of the most important sources of national productivity growth. But data substantiating these claims tend to either be abstracted - for example by examining spending on I.T., rather than I.T. itself - or anecdotal. In this paper, we assemble direct quantitative evidence of the impact that computing power has had on five domains: two computing bellwethers (Chess and Go), and three economically important applications (weather prediction, protein folding, and oil exploration). Computing power explains 49%-94% of the performance improvements in these domains. But whereas economic theory typically assumes a power-law relationship between inputs and outputs, we find that an exponential increase in computing power is needed to get linear improvements in these outcomes. This helps clarify why the exponential growth of computing power from Moore's Law has been so important for progress, and why performance improvements across many domains are becoming economically tenuous as Moore's Law breaks down."
            },
            {
                "arxivId": "2202.05924",
                "title": "Compute Trends Across Three Eras of Machine Learning",
                "abstract": "Compute, data, and algorithmic advances are the three fundamental factors that drive progress in modern Machine Learning (ML). In this paper we study trends in the most readily quantified factor - compute. We make three novel contributions: (1) we curate a dataset with the training compute of 123 milestone ML systems, 3\u00d7 larger than previous such datasets. (2) We frame the trends in compute in in three eras - the Pre Deep Learning Era, the Deep Learning Era, and the Large-Scale Era, based on our identification of a novel trend emerging around 2015. (3) We find a Deep Learning Era compute doubling time of around 6 months, significantly longer than previous findings. Overall, our work highlights the fast-growing compute requirements for training advanced ML systems."
            },
            {
                "arxivId": "2010.15581",
                "title": "The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research",
                "abstract": "Increasingly, modern Artificial Intelligence (AI) research has become more computationally intensive. However, a growing concern is that due to unequal access to computing power, only certain firms and elite universities have advantages in modern AI research. Using a novel dataset of 171394 papers from 57 prestigious computer science conferences, we document that firms, in particular, large technology firms and elite universities have increased participation in major AI conferences since deep learning's unanticipated rise in 2012. The effect is concentrated among elite universities, which are ranked 1-50 in the QS World University Rankings. Further, we find two strategies through which firms increased their presence in AI research: first, they have increased firm-only publications; and second, firms are collaborating primarily with elite universities. Consequently, this increased presence of firms and elite universities in AI research has crowded out mid-tier (QS ranked 201-300) and lower-tier (QS ranked 301-500) universities. To provide causal evidence that deep learning's unanticipated rise resulted in this divergence, we leverage the generalized synthetic control method, a data-driven counterfactual estimator. Using machine learning based text analysis methods, we provide additional evidence that the divergence between these two groups - large firms and non-elite universities - is driven by access to computing power or compute, which we term as the \"compute divide\". This compute divide between large firms and non-elite universities increases concerns around bias and fairness within AI technology, and presents an obstacle towards \"democratizing\" AI. These results suggest that a lack of access to specialized equipment such as compute can de-democratize knowledge production."
            },
            {
                "arxivId": "1811.06533",
                "title": "Learning to predict the cosmological structure formation",
                "abstract": "Significance To understand the evolution of the Universe requires a concerted effort of accurate observation of the sky and fast prediction of structures in the Universe. N-body simulation is an effective approach to predicting structure formation of the Universe, though computationally expensive. Here, we build a deep neural network to predict structure formation of the Universe. It outperforms the traditional fast-analytical approximation and accurately extrapolates far beyond its training data. Our study proves that deep learning is an accurate alternative to the traditional way of generating approximate cosmological simulations. Our study shows that one can use deep learning to generate complex 3D simulations in cosmology. This suggests that deep learning can provide a powerful alternative to traditional numerical simulations in cosmology. Matter evolved under the influence of gravity from minuscule density fluctuations. Nonperturbative structure formed hierarchically over all scales and developed non-Gaussian features in the Universe, known as the cosmic web. To fully understand the structure formation of the Universe is one of the holy grails of modern astrophysics. Astrophysicists survey large volumes of the Universe and use a large ensemble of computer simulations to compare with the observed data to extract the full information of our own Universe. However, to evolve billions of particles over billions of years, even with the simplest physics, is a daunting task. We build a deep neural network, the Deep Density Displacement Model (D3M), which learns from a set of prerun numerical simulations, to predict the nonlinear large-scale structure of the Universe with the Zel\u2019dovich Approximation (ZA), an analytical approximation based on perturbation theory, as the input. Our extensive analysis demonstrates that D3M outperforms the second-order perturbation theory (2LPT), the commonly used fast-approximate simulation method, in predicting cosmic structure in the nonlinear regime. We also show that D3M is able to accurately extrapolate far beyond its training data and predict structure formation for significantly different cosmological parameters. Our study proves that deep learning is a practical and accurate alternative to approximate 3D simulations of the gravitational structure formation of the Universe."
            },
            {
                "arxivId": "1803.08971",
                "title": "Computational Power and the Social Impact of Artificial Intelligence",
                "abstract": "Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. \n \nDespite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. \n \nThis paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI. On one hand, this examination shines a spotlight on how hardware works to exacerbate a range of concerns around ubiquitous surveillance, technological unemployment, and geopolitical conflict. On the other, it also highlights the potentially significant role that shaping the development of computing power might play in addressing these concerns."
            },
            {
                "arxivId": "1302.6906",
                "title": "Tradition and Innovation in Scientists\u2019 Research Strategies",
                "abstract": "What factors affect a scientist\u2019s choice of research problem? Qualitative research in the history and sociology of science suggests that this choice is patterned by an \u201cessential tension\u201d between productive tradition and risky innovation. We examine this tension through Bourdieu\u2019s field theory of science, and we explore it empirically by analyzing millions of biomedical abstracts from MEDLINE. We represent the evolving state of chemical knowledge with networks extracted from these abstracts. We then develop a typology of research strategies on these networks. Scientists can introduce novel chemicals and chemical relationships (innovation) or delve deeper into known ones (tradition). They can consolidate knowledge clusters or bridge them. The aggregate distribution of published strategies remains remarkably stable. High-risk innovation strategies are rare and reflect a growing focus on established knowledge. An innovative publication is more likely to achieve high impact than a conservative one, but the additional reward does not compensate for the risk of failing to publish. By studying prizewinners in biomedicine and chemistry, we show that occasional gambles for extraordinary impact are a compelling explanation for observed levels of risky innovation. Our analysis of the essential tension identifies institutional forces that sustain tradition and suggests policy interventions to foster innovation."
            },
            {
                "arxivId": "1301.3781",
                "title": "Efficient Estimation of Word Representations in Vector Space",
                "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            {
                "arxivId": "cond-mat/0007214",
                "title": "The structure of scientific collaboration networks.",
                "abstract": "The structure of scientific collaboration networks is investigated. Two scientists are considered connected if they have authored a paper together and explicit networks of such connections are constructed by using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). I show that these collaboration networks form \"small worlds,\" in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances. I further give results for mean and distribution of numbers of collaborators of authors, demonstrate the presence of clustering in the networks, and highlight a number of apparent differences in the patterns of collaboration between the fields studied."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2312.15198",
        "category": "q-fin",
        "title": "Do LLM Agents Exhibit Social Behavior?",
        "abstract": "The advances of Large Language Models (LLMs) are expanding their utility in both academic research and practical applications. Recent social science research has explored the use of these ``black-box'' LLM agents for simulating complex social systems and potentially substituting human subjects in experiments. Our study delves into this emerging domain, investigating the extent to which LLMs exhibit key social interaction principles, such as social learning, social preference, and cooperative behavior (indirect reciprocity), in their interactions with humans and other agents. We develop a framework for our study, wherein classical laboratory experiments involving human subjects are adapted to use LLM agents. This approach involves step-by-step reasoning that mirrors human cognitive processes and zero-shot learning to assess the innate preferences of LLMs. Our analysis of LLM agents' behavior includes both the primary effects and an in-depth examination of the underlying mechanisms. Focusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a range of human-like social behaviors such as distributional and reciprocity preferences, responsiveness to group identity cues, engagement in indirect reciprocity, and social learning capabilities. However, our analysis also reveals notable differences: LLMs demonstrate a pronounced fairness preference, weaker positive reciprocity, and a more calculating approach in social learning compared to humans. These insights indicate that while LLMs hold great promise for applications in social science research, such as in laboratory experiments and agent-based modeling, the subtle behavioral differences between LLM agents and humans warrant further investigation. Careful examination and development of protocols in evaluating the social behaviors of LLMs are necessary before directly applying these models to emulate human behavior.",
        "references": [
            {
                "arxivId": "2312.15524",
                "title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective",
                "abstract": "Large Language Models (LLMs) have demonstrated impressive potential to simulate human behavior. Using a causal inference framework, we empirically and theoretically analyze the challenges of conducting LLM-simulated experiments, and explore potential solutions. In the context of demand estimation, we show that variations in the treatment included in the prompt (e.g., price of focal product) can cause variations in unspecified confounding factors (e.g., price of competitors, historical prices, outside temperature), introducing endogeneity and yielding implausibly flat demand curves. We propose a theoretical framework suggesting this endogeneity issue generalizes to other contexts and won't be fully resolved by merely improving the training data. Unlike real experiments where researchers assign pre-existing units across conditions, LLMs simulate units based on the entire prompt, which includes the description of the treatment. Therefore, due to associations in the training data, the characteristics of individuals and environments simulated by the LLM can be affected by the treatment assignment. We explore two potential solutions. The first specifies all contextual variables that affect both treatment and outcome, which we demonstrate to be challenging for a general-purpose LLM. The second explicitly specifies the source of treatment variation in the prompt given to the LLM (e.g., by informing the LLM that the store is running an experiment). While this approach only allows the estimation of a conditional average treatment effect that depends on the specific experimental design, it provides valuable directional results for exploratory analysis."
            },
            {
                "arxivId": "2311.04076",
                "title": "Do LLMs exhibit human-like response biases? A case study in survey design",
                "abstract": "As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of\"prompts\"have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey"
            },
            {
                "arxivId": "2310.15819",
                "title": "Generative Language Models Exhibit Social Identity Biases",
                "abstract": "The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g.,\"We are...\"). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase in outgroup hostility. Furthermore, removing either ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning data leads to a significant reduction in both ingroup solidarity and outgroup hostility, suggesting that biases can be reduced by removing biased training data. Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data. Our results have practical implications for creating less biased large-language models and further underscore the need for more research into user interactions with LLMs to prevent potential bias reinforcement in humans."
            },
            {
                "arxivId": "2310.06500",
                "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
                "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations."
            },
            {
                "arxivId": "2307.14984",
                "title": "S3: Social-network Simulation System with Large Language Model-Empowered Agents",
                "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science."
            },
            {
                "arxivId": "2307.00184",
                "title": "Personality Traits in Large Language Models",
                "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI."
            },
            {
                "arxivId": "2305.16867",
                "title": "Playing repeated games with Large Language Models",
                "abstract": "Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM's cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4's behavior can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice. These results enrich our understanding of LLM's social behavior and pave the way for a behavioral game theory for machines."
            },
            {
                "arxivId": "2305.14930",
                "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
                "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases."
            },
            {
                "arxivId": "2305.12763",
                "title": "The emergence of economic rationality of GPT.",
                "abstract": "As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory. We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms."
            },
            {
                "arxivId": "2305.00050",
                "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
                "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2303.11504",
                "title": "Language Model Behavior: A Comprehensive Survey",
                "abstract": "Abstract Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models."
            },
            {
                "arxivId": "2303.06074",
                "title": "Susceptibility to Influence of Large Language Models",
                "abstract": "Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence."
            },
            {
                "arxivId": "2210.13966",
                "title": "The debate over understanding in AI\u2019s large language models",
                "abstract": "We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language\u2014and the physical and social situations language encodes\u2014in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition."
            },
            {
                "arxivId": "2209.14338",
                "title": "Who is GPT-3? An exploration of personality, values and demographics",
                "abstract": "Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa."
            },
            {
                "arxivId": "2209.06899",
                "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
                "abstract": "Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one such tool\u2014the GPT-3 language model\u2014is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create \u201csilicon samples\u201d by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."
            },
            {
                "arxivId": "2208.10264",
                "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."
            },
            {
                "arxivId": "2206.14576",
                "title": "Using cognitive psychology to understand GPT-3",
                "abstract": "Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations."
            },
            {
                "arxivId": "2206.02336",
                "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
                "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%)."
            },
            {
                "arxivId": "2006.01028",
                "title": "Interpretable Stochastic Block Influence Model: Measuring Social Influence Among Homophilous Communities",
                "abstract": "Decision-making on networks can be explained by both homophily and social influences. While homophily drives the formation of communities with similar characteristics, social influences occur both within and between communities. Social influences can be reasoned through role theory, which indicates that the influences among individuals depending on their roles and the behavior of interest. To operationalize these social science theories, we empirically identify the homophilous communities and use the community structures to capture such \u201croles\u201d, affecting particular decision-making processes. We propose a generative model named the Stochastic Block Influence Model and jointly analyze both network formation and behavioral influences within and between different empirically-identified communities. To evaluate the performance and demonstrate the interpretability of our method, we study the adoption decisions for a microfinance product in Indian villages. We show that although individuals tend to form links within communities, there are strongly positive and negative social influences between communities, supporting the weak ties theory. Moreover, communities with shared characteristics are associated with positive influences. In contrast, communities that do not overlap are associated with negative influences. Our framework facilitates the quantification of the influences underlying decision communities and is thus a helpful tool for driving information diffusion, viral marketing, and technology adoption."
            },
            {
                "arxivId": "1906.09698",
                "title": "Gift Contagion in Online Groups: Evidence from Virtual Red Packets",
                "abstract": "Gifts are important instruments for forming bonds in interpersonal relationships. Our study analyzes the phenomenon of gift contagion in online groups. Gift contagion encourages social bonds by prompting further gifts; it may also promote group interaction and solidarity. Using data on 36 million online red packet gifts on a large social site in East Asia, we leverage a natural experimental design to identify the social contagion of gift giving in online groups. Our natural experiment is enabled by the randomization of the gift amount allocation algorithm on the platform, which addresses the common challenge of causal identification in observational data. Our study provides evidence of gift contagion: On average, receiving one additional dollar causes a recipient to send 18 cents back to the group within the subsequent 24\u2009hours. Decomposing this effect, we find that it is mainly driven by the extensive margin: more recipients are triggered to send red packets. Moreover, we find that this effect is stronger for \u201cluckiest draw\u201d recipients, suggesting the presence of a group norm regarding the next red packet sender. Finally, we investigate the moderating effects of group- and individual-level social network characteristics on gift contagion as well as the causal impact of receiving gifts on group network structure. Our study has implications for promoting group dynamics and designing marketing strategies for product adoption. This paper was accepted by Axel Ockenfels, behavioral economics and decision analysis. Funding: T. Liu was supported by Natural Science Foundation of China [Grant 72222005] and Tsinghua University [Grant 2022Z04W01032]. J. Tang was supported by Natural Science Foundation of China for Distinguished Young Scholar [Grant 61825602]. Supplemental Material: The data files and online appendices are available at https://doi.org/10.1287/mnsc.2023.4906 ."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.11372",
        "category": "q-fin",
        "title": "Low costs na aviacao: importancia e desdobramentos",
        "abstract": "This study aims to discuss the impacts of a low-cost airline on the air transport market and, especially, to present the most recent findings from specialized literature in the field. To this end, various works on this topic, published since 2015, were selected and analyzed. From this analysis, it was possible to categorize the main topics discussed in the papers into five groups: (i) the impacts of a low-cost airline on competing airlines; (ii) impacts on airports; (iii) general impacts on the demand for air transport; (iv) effects on passengers' choice process; and (v) general effects on a geographical region.",
        "references": [
            {
                "arxivId": "2312.05630",
                "title": "An empirical analysis of the determinants of network construction for Azul Airlines",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14090",
        "category": "q-fin",
        "title": "Social Environment Design",
        "abstract": "Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.",
        "references": [
            {
                "arxivId": "2205.14953",
                "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem",
                "abstract": "Large sequence model (SM) such as GPT series and BERT has displayed outstanding performance and generalization capabilities on vision, language, and recently reinforcement learning tasks. A natural follow-up question is how to abstract multi-agent decision making into an SM problem and benefit from the prosperous development of SMs. In this paper, we introduce a novel architecture named Multi-Agent Transformer (MAT) that effectively casts cooperative multi-agent reinforcement learning (MARL) into SM problems wherein the task is to map agents' observation sequence to agents' optimal action sequence. Our goal is to build the bridge between MARL and SMs so that the modeling power of modern sequence models can be unleashed for MARL. Central to our MAT is an encoder-decoder architecture which leverages the multi-agent advantage decomposition theorem to transform the joint policy search problem into a sequential decision making process; this renders only linear time complexity for multi-agent problems and, most importantly, endows MAT with monotonic performance improvement guarantee. Unlike prior arts such as Decision Transformer fit only pre-collected offline data, MAT is trained by online trials and errors from the environment in an on-policy fashion. To validate MAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo, Dexterous Hands Manipulation, and Google Research Football benchmarks. Results demonstrate that MAT achieves superior performance and data efficiency compared to strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that MAT is an excellent few-short learner on unseen tasks regardless of changes in the number of agents. See our project page at https://sites.google.com/view/multi-agent-transformer."
            },
            {
                "arxivId": "2202.13110",
                "title": "Optimal-er Auctions through Attention",
                "abstract": "RegretNet is a recent breakthrough in the automated design of revenue-maximizing auctions. It combines the flexibility of deep learning with the regret-based approach to relax the Incentive Compatibility (IC) constraint (that participants prefer to bid truthfully) in order to approximate optimal auctions. We propose two independent improvements of RegretNet. The first is a neural architecture denoted as RegretFormer that is based on attention layers. The second is a loss function that requires explicit specification of an acceptable IC violation denoted as regret budget. We investigate both modifications in an extensive experimental study that includes settings with constant and inconstant number of items and participants, as well as novel validation procedures tailored to regret-based approaches. We find that RegretFormer consistently outperforms RegretNet in revenue (i.e. is optimal-er) and that our loss function both simplifies hyperparameter tuning and allows to unambiguously control the revenue-regret trade-off by selecting the regret budget."
            },
            {
                "arxivId": "2112.10859",
                "title": "Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning",
                "abstract": "Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents\u2019 behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents\u2019 learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis."
            },
            {
                "arxivId": "2007.12322",
                "title": "Off-Policy Multi-Agent Decomposed Policy Gradients",
                "abstract": "Multi-agent policy gradient (MAPG) methods recently witness vigorous progress. However, there is a significant performance discrepancy between MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate causes that hinder the performance of MAPG algorithms and present a multi-agent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that DOP significantly outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at this https URL."
            },
            {
                "arxivId": "2003.08039",
                "title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "abstract": "The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at this https URL."
            },
            {
                "arxivId": "1910.05366",
                "title": "Learning Nearly Decomposable Value Functions Via Communication Minimization",
                "abstract": "Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between agents' action selection and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows us to cut off more than $80\\%$ of communication without sacrificing the performance. The videos of our experiments are available at this https URL."
            },
            {
                "arxivId": "1812.09755",
                "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks",
                "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability."
            },
            {
                "arxivId": "1707.06347",
                "title": "Proximal Policy Optimization Algorithms",
                "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
            },
            {
                "arxivId": "1706.03459",
                "title": "Optimal Auctions through Deep Learning: Advances in Differentiable Economics",
                "abstract": "Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981, but more than 40 years later, a full analytical understanding of the optimal design still remains elusive for settings with two or more items. In this work, we initiate the exploration of the use of tools from deep learning for the automated design of optimal auctions. We model an auction as a multi-layer neural network, frame optimal auction design as a constrained learning problem, and show how it can be solved using standard machine learning pipelines. In addition to providing generalization bounds, we present extensive experimental results, recovering essentially all known solutions that come from the theoretical analysis of optimal auction design problems and obtaining novel mechanisms for settings in which the optimal mechanism is unknown."
            },
            {
                "arxivId": "1706.02275",
                "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
                "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14100",
        "category": "q-fin",
        "title": "A Note on Optimal Liquidation with Linear Price Impact",
        "abstract": "In this note we consider the maximization of the expected terminal wealth for the setup of quadratic transaction costs. First, we provide a very simple probabilistic solution to the problem. Although the problem was largely studied, as far as we know up to date this simple and probabilistic form of the solution has not appeared in the literature. Next, we apply the general result for the study of the case where the risky asset is given by a fractional Brownian Motion and the information flow of the investor can be diversified.",
        "references": [
            {
                "arxivId": "1804.07392",
                "title": "Optimal Investment with Transient Price Impact",
                "abstract": "We introduce a price impact model which accounts for finite market depth, tightness and resilience. Its coupled bid- and ask-price dynamics induce convex liquidity costs. We provide existence of an optimal solution to the classical problem of maximizing expected utility from terminal liquidation wealth at a finite planning horizon. In the specific case when market uncertainty is generated by an arithmetic Brownian motion with drift and the investor exhibits constant absolute risk aversion, we show that the resulting singular optimal stochastic control problem readily reduces to a deterministic optimal tracking problem of the optimal frictionless constant Merton portfolio in the presence of convex costs. Rather than studying the associated Hamilton-Jacobi-Bellmann PDE, we exploit convex analytic and calculus of variations techniques allowing us to construct the solution explicitly and to describe the free boundaries of the action- and non-action regions in the underlying state space. As expected, it is optimal to trade towards the frictionless Merton position, taking into account the initial bid-ask spread as well as the optimal liquidation of the accrued position when approaching terminal time. It turns out that this leads to a surprisingly rich phenomenology of possible trajectories for the optimal share holdings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14161",
        "category": "q-fin",
        "title": "Short-Maturity Asymptotics for Option Prices with Interest Rates Effects",
        "abstract": "We derive the short-maturity asymptotics for option prices in the local volatility model in a new short-maturity limit $T\\to 0$ at fixed $\\rho = (r-q) T$, where $r$ is the interest rate and $q$ is the dividend yield. In cases of practical relevance $\\rho$ is small, however our result holds for any fixed $\\rho$. The result is a generalization of the Berestycki-Busca-Florent formula for the short-maturity asymptotics of the implied volatility which includes interest rates and dividend yield effects of $O(((r-q) T)^n)$ to all orders in $n$. We obtain analytical results for the ATM volatility and skew in this asymptotic limit. Explicit results are derived for the CEV model. The asymptotic result is tested numerically against exact evaluation in the square-root model model $\\sigma(S)=\\sigma/\\sqrt{S}$, which demonstrates that the new asymptotic result is in very good agreement with exact evaluation in a wide range of model parameters relevant for practical applications.",
        "references": [
            {
                "arxivId": "2306.09084",
                "title": "Asymptotics for the Laplace transform of the time integral of the geometric Brownian motion",
                "abstract": null
            },
            {
                "arxivId": "1706.09659",
                "title": "Asymptotics for the discrete-time average of the geometric Brownian motion and Asian options",
                "abstract": "Abstract The time average of geometric Brownian motion plays a crucial role in the pricing of Asian options in mathematical finance. In this paper we consider the asymptotics of the discrete-time average of a geometric Brownian motion sampled on uniformly spaced times in the limit of a very large number of averaging time steps. We derive almost sure limit, fluctuations, large deviations, and also the asymptotics of the moment generating function of the average. Based on these results, we derive the asymptotics for the price of Asian options with discrete-time averaging in the Black\u2013Scholes model, with both fixed and floating strike."
            },
            {
                "arxivId": "1702.08081",
                "title": "Probability Density of Lognormal Fractional SABR Model",
                "abstract": "Instantaneous volatility of logarithmic return in the lognormal fractional SABR model is driven by the exponentiation of a correlated fractional Brownian motion. Due to the mixed nature of driving Brownian and fractional Brownian motions, probability density for such a model is less studied in the literature. We show in this paper a bridge representation for the joint density of the lognormal fractional SABR model in a Fourier space. Evaluating the bridge representation along a properly chosen deterministic path yields a small time asymptotic expansion to the leading order for the probability density of the fractional SABR model. A direct generalization of the representation of joint density often leads to a heuristic derivation of the large deviations principle for joint density in a small time. Approximation of implied volatility is readily obtained by applying the Laplace asymptotic formula to the call or put prices and comparing coefficients."
            },
            {
                "arxivId": "1510.06084",
                "title": "The exact Taylor formula of the implied volatility",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14189",
        "category": "q-fin",
        "title": "Optimal transmission expansion minimally reduces decarbonization costs of U.S. electricity",
        "abstract": "Solar and wind power are cost-competitive with fossil fuels, yet their intermittent nature presents challenges. Significant temporal and geographic differences in land, wind, and solar resources suggest that long-distance transmission could be particularly beneficial. Using a detailed, open-source model, we analyze optimal transmission expansion jointly with storage, generation, and hourly operations across the three primary interconnects in the United States. Transmission expansion offers far more benefits in a high-renewable system than in a system with mostly conventional generation. Yet while an optimal nationwide plan would have more than triple current interregional transmission, transmission decreases the cost of a 100% clean system by only 4% compared to a plan that relies solely on current transmission. Expanding capacity only within existing interconnects can achieve most of these savings. Adjustments to energy storage and generation mix can leverage the current interregional transmission infrastructure to build a clean power system at a reasonable cost.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14206",
        "category": "q-fin",
        "title": "The impact of Facebook-Cambridge Analytica data scandal on the USA tech stock market: An event study based on clustering method",
        "abstract": "This study delves into the intra-industry effects following a firm-specific scandal, with a particular focus on the Facebook data leakage scandal and its associated events within the U.S. tech industry and two additional relevant groups. We employ various metrics including daily spread, volatility, volume-weighted return, and CAPM-beta for the pre-analysis clustering, and subsequently utilize CAR (Cumulative Abnormal Return) to evaluate the impact on firms grouped within these clusters. From a broader industry viewpoint, significant positive CAARs are observed across U.S. sample firms over the three days post-scandal announcement, indicating no adverse impact on the tech sector overall. Conversely, after Facebook's initial quarterly earnings report, it showed a notable negative effect despite reported positive performance. The clustering principle should aid in identifying directly related companies and thus reducing the influence of randomness. This was indeed achieved for the effect of the key event, namely\"The Effect of Congressional Hearing on Certain Clusters across U.S. Tech Stock Market,\"which was identified as delayed and significantly negative. Therefore, we recommend applying the clustering method when conducting such or similar event studies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14269",
        "category": "q-fin",
        "title": "Optimal Mechanism in a Dynamic Stochastic Knapsack Environment",
        "abstract": "This study introduces an optimal mechanism in a dynamic stochastic knapsack environment. The model features a single seller who has a fixed quantity of a perfectly divisible item. Impatient buyers with a piece-wise linear utility function arrive randomly and they report the two-dimensional private information: marginal value and demanded quantity. We derive a revenue-maximizing dynamic mechanism in a finite discrete time framework that satisfies incentive compatibility, individual rationality, and feasibility conditions. This is achieved by characterizing buyers' utility and utilizing the Bellman equation. Moreover, we establish the essential penalty scheme for incentive compatibility, as well as the allocation and payment policies. Lastly, we propose algorithms to approximate the optimal policy, based on the Monte Carlo simulation-based regression method and reinforcement learning.",
        "references": [
            {
                "arxivId": "1502.06934",
                "title": "An optimal bidimensional multi-armed bandit auction for multi-unit procurement",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14322",
        "category": "q-fin",
        "title": "Estimation of Spectral Risk Measure for Left Truncated and Right Censored Data",
        "abstract": "Left truncated and right censored data are encountered frequently in insurance loss data due to deductibles and policy limits. Risk estimation is an important task in insurance as it is a necessary step for determining premiums under various policy terms. Spectral risk measures are inherently coherent and have the benefit of connecting the risk measure to the user's risk aversion. In this paper we study the estimation of spectral risk measure based on left truncated and right censored data. We propose a non parametric estimator of spectral risk measure using the product limit estimator and establish the asymptotic normality for our proposed estimator. We also develop an Edgeworth expansion of our proposed estimator. The bootstrap is employed to approximate the distribution of our proposed estimator and shown to be second order ``accurate''. Monte Carlo studies are conducted to compare the proposed spectral risk measure estimator with the existing parametric and non parametric estimators for left truncated and right censored data. Based on our simulation study we estimate the exponential spectral risk measure for three data sets viz; Norwegian fire claims data set, Spain automobile insurance claims and French marine losses.",
        "references": [
            {
                "arxivId": "1103.5674",
                "title": "Spectral Risk Measures: Properties and Limitations",
                "abstract": "Spectral risk measures (SRMs) are risk measures that take account of user risk-aversion, but to date there has been little guidance on the choice of utility function underlying them. This paper addresses this issue by examining alternative approaches based on exponential and power utility functions. A number of problems are identified with both types of spectral risk measure. The general lesson is that users of spectral risk measures must be careful to select utility functions that fit the features of the particular problems they are dealing with, and should be especially careful when using power SRMs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14389",
        "category": "q-fin",
        "title": "Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search",
        "abstract": "Financial institutions and businesses face an ongoing challenge from fraudulent transactions, prompting the need for effective detection methods. Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.Timely detection of fraud enables investigators to take swift actions to mitigate further losses. However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day. Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases. In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To address the data imbalance issue, we employ the Instant Hardness Threshold (IHT) technique in conjunction with Logistic Regression (LR), surpassing conventional approaches. Our experiments are conducted on a publicly available credit card dataset comprising 284,807 transactions. The proposed model achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid ensemble model outperforms existing works, establishing a new benchmark for detecting fraudulent transactions in high-frequency scenarios. The results highlight the effectiveness and reliability of our approach, demonstrating superior performance metrics and showcasing its exceptional potential for real-world fraud detection applications.",
        "references": [
            {
                "arxivId": "2402.13277",
                "title": "MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs",
                "abstract": "In the domain of cyber-physical systems, wireless sensor networks (WSNs) play a pivotal role as infrastructures, encompassing both stationary and mobile sensors. These sensors self-organize and establish multi-hop connections for communication, collectively sensing, gathering, processing, and transmitting data about their surroundings. Despite their significance, WSNs face rapid and detrimental attacks that can disrupt functionality. Existing intrusion detection methods for WSNs encounter challenges such as low detection rates, computational overhead, and false alarms. These issues stem from sensor node resource constraints, data redundancy, and high correlation within the network. To address these challenges, we propose an innovative intrusion detection approach that integrates machine learning (ML) techniques with the Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend synthesizes minority instances and eliminates Tomek links, resulting in a balanced dataset that significantly enhances detection accuracy in WSNs. Additionally, we incorporate feature scaling through standardization to render input features consistent and scalable, facilitating more precise training and detection. To counteract imbalanced WSN datasets, we employ the SMOTE-Tomek resampling technique, mitigating overfitting and underfitting issues. Our comprehensive evaluation, using the wireless sensor network dataset (WSN-DS) containing 374,661 records, identifies the optimal model for intrusion detection in WSNs. The standout outcome of our research is the remarkable performance of our model. In binary classification scenarios, it achieves an accuracy rate of 99.78%, and in multiclass classification scenarios, it attains an exceptional accuracy rate of 99.92%. These findings underscore the efficiency and superiority of our proposal in the context of WSN intrusion detection, showcasing its effectiveness in detecting and mitigating intrusions in WSNs."
            },
            {
                "arxivId": "2401.12262",
                "title": "Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction",
                "abstract": "Cybersecurity has emerged as a critical global concern. Intrusion Detection Systems (IDS) play a critical role in protecting interconnected networks by detecting malicious actors and activities. Machine Learning (ML)-based behavior analysis within the IDS has considerable potential for detecting dynamic cyber threats, identifying abnormalities, and identifying malicious conduct within the network. However, as the number of data grows, dimension reduction becomes an increasingly difficult task when training ML models. Addressing this, our paper introduces a novel ML-based network intrusion detection model that uses Random Oversampling (RO) to address data imbalance and Stacking Feature Embedding based on clustering results, as well as Principal Component Analysis (PCA) for dimension reduction and is specifically designed for large and imbalanced datasets. This model\u2019s performance is carefully evaluated using three cutting-edge benchmark datasets: UNSW-NB15, CIC-IDS-2017, and CIC-IDS-2018. On the UNSW-NB15 dataset, our trials show that the RF and ET models achieve accuracy rates of 99.59% and 99.95%, respectively. Furthermore, using the CIC-IDS2017 dataset, DT, RF, and ET models reach 99.99% accuracy, while DT and RF models obtain 99.94% accuracy on CIC-IDS2018. These performance results continuously outperform the state-of-art, indicating significant progress in the field of network intrusion detection. This achievement demonstrates the efficacy of the suggested methodology, which can be used practically to accurately monitor and identify network traffic intrusions, thereby blocking possible threats."
            },
            {
                "arxivId": "2212.04546",
                "title": "A Dependable Hybrid Machine Learning Model for Network Intrusion Detection",
                "abstract": null
            },
            {
                "arxivId": "2206.01088",
                "title": "Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14476",
        "category": "q-fin",
        "title": "Quantifying neural network uncertainty under volatility clustering",
        "abstract": "Time-series with time-varying variance pose a unique challenge to uncertainty quantification (UQ) methods. Time-varying variance, such as volatility clustering as seen in financial time-series, can lead to large mismatch between predicted uncertainty and forecast error. Building on recent advances in neural network UQ literature, we extend and simplify Deep Evidential Regression and Deep Ensembles into a unified framework to deal with UQ under the presence of volatility clustering. We show that a Scale Mixture Distribution is a simpler alternative to the Normal-Inverse-Gamma prior that provides favorable complexity-accuracy trade-off. To illustrate the performance of our proposed approach, we apply it to two sets of financial time-series exhibiting volatility clustering: cryptocurrencies and U.S. equities.",
        "references": [
            {
                "arxivId": "2306.02738",
                "title": "A Large-Scale Study of Probabilistic Calibration in Neural Network Regression",
                "abstract": "Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction. Our study is fully reproducible and implemented in a common code base for fair comparisons."
            },
            {
                "arxivId": "2305.16703",
                "title": "Sources of Uncertainty in Machine Learning - A Statisticians' View",
                "abstract": "Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the role of data and their influence on uncertainty."
            },
            {
                "arxivId": "2301.12736",
                "title": "On Second-Order Scoring Rules for Epistemic Uncertainty Quantification",
                "abstract": "It is well known that accurate probabilistic predictors can be trained through empirical risk minimisation with proper scoring rules as loss functions. While such learners capture so-called aleatoric uncertainty of predictions, various machine learning methods have recently been developed with the goal to let the learner also represent its epistemic uncertainty, i.e., the uncertainty caused by a lack of knowledge and data. An emerging branch of the literature proposes the use of a second-order learner that provides predictions in terms of distributions on probability distributions. However, recent work has revealed serious theoretical shortcomings for second-order predictors based on loss minimisation. In this paper, we generalise these findings and prove a more fundamental result: There seems to be no loss function that provides an incentive for a second-order learner to faithfully represent its epistemic uncertainty in the same manner as proper scoring rules do for standard (first-order) learners. As a main mathematical tool to prove this result, we introduce the generalised notion of second-order scoring rules."
            },
            {
                "arxivId": "2112.09368",
                "title": "Improving evidential deep learning via multi-task learning",
                "abstract": "The Evidential regression network (ENet) estimates a continuous target and its predictive uncertainty without costly Bayesian model averaging. However, it is possible that the target is inaccurately predicted due to the gradient shrinkage problem of the original loss function of the ENet, the negative log marginal likelihood (NLL) loss. In this paper, the objective is to improve the prediction accuracy of the ENet while maintaining its efficient uncertainty estimation by resolving the gradient shrinkage problem. A multi-task learning (MTL) framework, referred to as MT-ENet, is proposed to accomplish this aim. In the MTL, we define the Lipschitz modified mean squared error (MSE) loss function as another loss and add it to the existing NLL loss. The Lipschitz modified MSE loss is designed to mitigate the gradient conflict with the NLL loss by dynamically adjusting its Lipschitz constant. By doing so, the Lipschitz MSE loss does not disturb the uncertainty estimation of the NLL loss. The MT-ENet enhances the predictive accuracy of the ENet without losing uncertainty estimation capability on the synthetic dataset and real-world benchmarks, including drug-target affinity (DTA) regression. Furthermore, the MT-ENet shows remarkable calibration and out-of-distribution detection capability on the DTA benchmarks."
            },
            {
                "arxivId": "2111.14259",
                "title": "3D High-Quality Magnetic Resonance Image Restoration in Clinics Using Deep Learning",
                "abstract": "Shortening acquisition time and reducing the motion artifacts are two of the most essential concerns in magnetic resonance imaging. As a promising solution, deep learning-based high-quality MR image restoration has been investigated to generate highly-resolved and motion artifact-free MR images from lower resolution images acquired with shortened acquisition time or motion artifact-corrupted images. However, numerous problems still exist to prevent deep learning approaches from becoming practical in the clinic environment. Specifically, most of the prior works focus solely on the network but ignore the impact of various down-sampling strategies on the acquisition time. Besides, the long inference time and high GPU consumption are also the bottlenecks to deploy most of the prior works in clinics. Furthermore, prior studies employ random movement in retrospective motion artifact generation, resulting in uncontrollable severity of motion artifact. More importantly, doctors are unsure whether the generated MR images are trustworthy, making diagnosis difficult. To overcome all these problems, we adopted a unified framework of 2D deep learning neural network for both 3D MRI super-resolution and motion artifact reduction, demonstrating such a framework can achieve better performance in 3D MRI restoration tasks compared to other state-of-the-art methods and remain the GPU consumption and inference time significantly low, thus easier to deploy. We also analyzed several down-sampling strategies based on the acceleration factor, including multiple combinations of in-plane and through-plane down-sampling, and developed a controllable and quantifiable motion artifact generation method. At last, the pixel-wise uncertainty was calculated and used to estimate the accuracy of the generated image, providing additional information for a reliable diagnosis."
            },
            {
                "arxivId": "2107.08325",
                "title": "Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning",
                "abstract": "Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/."
            },
            {
                "arxivId": "2107.03342",
                "title": "A survey of uncertainty in deep neural networks",
                "abstract": null
            },
            {
                "arxivId": "2105.09932",
                "title": "Efficient and Robust LiDAR-Based End-to-End Navigation",
                "abstract": "Deep learning has been used to demonstrate end-to-end neural network learning for autonomous vehicle control from raw sensory input. While LiDAR sensors provide reliably accurate information, existing end-to-end driving solutions are mainly based on cameras since processing 3D data requires a large memory footprint and computation cost. On the other hand, increasing the robustness of these systems is also critical; however, even estimating the model\u2019s uncertainty is very challenging due to the cost of sampling-based methods. In this paper, we present an efficient and robust LiDAR-based end-to-end navigation framework. We first introduce Fast-LiDARNet that is based on sparse convolution kernel optimization and hardware-aware model design. We then propose Hybrid Evidential Fusion that directly estimates the uncertainty of the prediction from only a single forward pass and then fuses the control predictions intelligently. We evaluate our system on a full-scale vehicle and demonstrate lane-stable as well as navigation capabilities. In the presence of out-of-distribution events (e.g., sensor failures), our system significantly improves robustness and reduces the number of takeovers in the real world."
            },
            {
                "arxivId": "2009.04461",
                "title": "Investing with cryptocurrencies \u2013 evaluating their potential for portfolio allocation strategies",
                "abstract": "Cryptocurrencies (CCs) have risen rapidly in market capitalization over the past years. Despite striking volatility, their high average returns and low correlations have established CCs as alternative investment assets for portfolio and risk management. We investigate the benefits of adding CCs to well-diversified portfolios of conventional financial assets for different types of investors, including risk-averse, return-maximizing and diversification-seeking investors who may trade at different frequencies, namely, daily, weekly or monthly. We calculate out-of-sample performance and diversification benefits for the most popular portfolio-construction rules, including mean-variance optimization, risk-parity, and maximum-diversification strategies, as well as combined strategies. Our results demonstrate that CCs can improve the risk-return profile of portfolios, but their benefit depends on investor objectives. In particular, diversification strategies (maximizing the portfolio diversification index or equating risk contributions) draw appreciably on CCs and show, in line with spanning tests, CCs to be non-redundant extensions of the investment universe. However, when we introduce liquidity constraints via the LIBRO method to account for illiquidity of many CCs, out-of-sample performance drops considerably, while the diversification benefits persist. We conclude that the utility of CC investments strongly depends on investor characteristics."
            },
            {
                "arxivId": "2007.06823",
                "title": "Hands-On Bayesian Neural Networks\u2014A Tutorial for Deep Learning Users",
                "abstract": "Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides deep learning practitioners with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks, i.e., stochastic artificial neural networks trained using Bayesian methods."
            },
            {
                "arxivId": "2006.11590",
                "title": "Regression Prior Networks",
                "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via Ensemble Distribution Distillation (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches."
            },
            {
                "arxivId": "2002.10118",
                "title": "Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks",
                "abstract": "The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is \"to be a bit Bayesian\". These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations."
            },
            {
                "arxivId": "1912.01530",
                "title": "On the Validity of Bayesian Neural Networks for Uncertainty Estimation",
                "abstract": "Deep neural networks (DNN) are versatile parametric models utilised successfully in a diverse number of tasks and domains. However, they have limitations---particularly from their lack of robustness and over-sensitivity to out of distribution samples. Bayesian Neural Networks, due to their formulation under the Bayesian framework, provide a principled approach to building neural networks that address these limitations. This paper describes a study that empirically evaluates and compares Bayesian Neural Networks to their equivalent point estimate Deep Neural Networks to quantify the predictive uncertainty induced by their parameters, as well as their performance in view of this uncertainty. In this study, we evaluated and compared three point estimate deep neural networks against comparable Bayesian neural network alternatives using two well-known benchmark image classification datasets (CIFAR-10 and SVHN)."
            },
            {
                "arxivId": "1910.02600",
                "title": "Deep Evidential Regression",
                "abstract": "Deterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust and efficient measures of uncertainty are crucial. While it is possible to train regression networks to output the parameters of a probability distribution by maximizing a Gaussian likelihood function, the resulting model remains oblivious to the underlying confidence of its predictions. In this paper, we propose a novel method for training deterministic NNs to not only estimate the desired target but also the associated evidence in support of that target. We accomplish this by placing evidential priors over our original Gaussian likelihood function and training our NN to infer the hyperparameters of our evidential distribution. We impose priors during training such that the model is penalized when its predicted evidence is not aligned with the correct output. Thus the model estimates not only the probabilistic mean and variance of our target but also the underlying uncertainty associated with each of those parameters. We observe that our evidential regression method learns well-calibrated measures of uncertainty on various benchmarks, scales to complex computer vision tasks, and is robust to adversarial input perturbations."
            },
            {
                "arxivId": "1906.02530",
                "title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift",
                "abstract": "Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks."
            },
            {
                "arxivId": "1806.05293",
                "title": "GENERALIZED FRAMEWORK FOR APPLYING THE KELLY CRITERION TO STOCK MARKETS",
                "abstract": "We develop a general framework for applying the Kelly criterion to the stock market. By supplying an arbitrary probability distribution modeling the future price movement of a set of stocks, the Kelly fraction for investing each stock can be calculated by inverting a matrix involving only first and second moments. The framework works for one or a portfolio of stocks and the Kelly fractions can be efficiently calculated. For a simple model of geometric Brownian motion of a single stock we show that our calculated Kelly fraction agrees with existing results. We demonstrate that the Kelly fractions can be calculated easily for other types of probabilities such as the Gaussian distribution and correlated multivariate assets."
            },
            {
                "arxivId": "1612.01474",
                "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
                "abstract": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."
            },
            {
                "arxivId": "1506.02142",
                "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
                "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."
            },
            {
                "arxivId": "1502.05336",
                "title": "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks",
                "abstract": "Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights."
            },
            {
                "arxivId": "1404.4178",
                "title": "Speeding Up MCMC by Efficient Data Subsampling",
                "abstract": "ABSTRACT We propose subsampling Markov chain Monte Carlo (MCMC), an MCMC framework where the likelihood function for n observations is estimated from a random subset of m observations. We introduce a highly efficient unbiased estimator of the log-likelihood based on control variates, such that the computing cost is much smaller than that of the full log-likelihood in standard MCMC. The likelihood estimate is bias-corrected and used in two dependent pseudo-marginal algorithms to sample from a perturbed posterior, for which we derive the asymptotic error with respect to n and m, respectively. We propose a practical estimator of the error and show that the error is negligible even for a very small m in our applications. We demonstrate that subsampling MCMC is substantially more efficient than standard MCMC in terms of sampling efficiency for a given computational budget, and that it outperforms other subsampling methods for MCMC proposed in the literature. Supplementary materials for this article are available online."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14555",
        "category": "q-fin",
        "title": "The Riccati Tontine: How to Satisfy Regulators on Average",
        "abstract": "This paper presents a new type of modern accumulation-based tontine, called the Riccati tontine, named after two Italians: mathematician Jacobo Riccati (b. 1676, d. 1754) and financier Lorenzo di Tonti (b. 1602, d. 1684). The Riccati tontine is yet another way of pooling and sharing longevity risk, but is different from competing designs in two key ways. The first is that in the Riccati tontine, the representative investor is expected -- although not guaranteed -- to receive their money back if they die, or when the tontine lapses. The second is that the underlying funds within the tontine are deliberately {\\em not} indexed to the stock market. Instead, the risky assets or underlying investments are selected so that return shocks are negatively correlated with stochastic mortality, which will maximize the expected payout to survivors. This means that during a pandemic, for example, the Riccati tontine fund's performance will be impaired relative to the market index, but will not be expected to lose money for participants. In addition to describing and explaining the rationale for this non-traditional asset allocation, the paper provides a mathematical proof that the recovery schedule that generates this financial outcome satisfies a first-order ODE that is quadratic in the unknown function, which (yes) is known as a Riccati equation.",
        "references": [
            {
                "arxivId": "2402.00855",
                "title": "'Egalitarian pooling and sharing of longevity risk', a.k.a. 'The many ways to skin a tontine cat'",
                "abstract": "There is little disagreement among insurance actuaries and financial economists about the societal benefits of longevity-risk pooling in the form of life annuities, defined benefit pensions, self-annuitization funds, and even tontine schemes. Indeed, the discounted value or cost of providing an income for life is lower -- in other words, the amount of upfront capital required to generate a similar income stream with the same level of statistical safety is lower -- when participants pool their financial resources versus going it alone. Moreover, when participants' financial circumstances and lifespans are homogenous, there is consensus on how to share the\"winnings\"among survivors, namely by distributing them equally among survivors, a.k.a. a uniform rule. Alas, what is lesser-known and much more problematic is allocating the winnings in such a pool when participants differ in wealth (contributions) and health (longevity), especially when the pools are relatively small in size. The same problems arise when viewed from the dual perspective of decentralized risk sharing (DRS). The positive correlation between health and income and the fact that wealthier participants are likely to live longer is a growing concern among pension and retirement policymakers. With that motivation in mind, this paper offers a modelling framework for distributing longevity-risk pools' income and benefits (or tontine winnings) when participants are heterogeneous. Similar to the nascent literature on decentralized risk sharing, there are several equally plausible arrangements for sharing benefits (a.k.a.\"skinning the cat\") among survivors. Moreover, the selected rule depends on the extent of social cohesion within the longevity risk pool, ranging from solidarity and altruism to pure individualism. In sum, actuarial science cannot really offer or guarantee uniqueness, only a methodology."
            },
            {
                "arxivId": "2211.10509",
                "title": "Optimal Performance of a Tontine Overlay Subject to Withdrawal Constraints",
                "abstract": "\n We consider the holder of an individual tontine retirement account, with maximum and minimum withdrawal amounts (per year) specified. The tontine account holder initiates the account at age 65 and earns mortality credits while alive, but forfeits all wealth in the account upon death. The holder wants to maximize total withdrawals and minimize expected shortfall at the end of the retirement horizon of 30 years (i.e., it is assumed that the holder survives to age 95). The holder controls the amount withdrawn each year and the fraction of the retirement portfolio invested in stocks and bonds. The optimal controls are determined based on a parametric model fitted to almost a century of market data. The optimal control algorithm is based on dynamic programming and the solution of a partial integro differential equation (PIDE) using Fourier methods. The optimal strategy (based on the parametric model) is tested out of sample using stationary block bootstrap resampling of the historical data. In terms of an expected total withdrawal, expected shortfall (EW-ES) efficient frontier, the tontine overlay dramatically outperforms an optimal strategy (without the tontine overlay), which in turn outperforms a constant weight strategy with withdrawals based on the ubiquitous four per cent rule."
            },
            {
                "arxivId": "2111.01239",
                "title": "Refundable Income Annuities: Feasibility of Money-Back Guarantees",
                "abstract": "[Refundable income annuities (IA), such as cash-refund and instalment-refund, differ in material ways from the life-only version beloved by pension and financial economists. In addition to lifetime income they also guarantee the annuitant or beneficiary will receive their money back albeit slowly over time. We document that refundable IAs now represent the majority of sales in the U.S., yet they are mostly ignored by the literature. And, although their pricing, duration, and money's-worth-ratio is complicated by internal recursivity -- which is carefully explained in the paper -- we offer a path forward to make refundable IAs tractable. A key -- and perhaps even the primary and quotable -- result concerns the market price of cash-refund IAs, when the actuarial present value is grossed-up by an insurance loading. We prove that price is counterintuitively no longer a declining function of age and older buyers might pay more than younger ones for this type of pension annuity. Moreover, there exists a threshold valuation rate below which no market price is viable. The product can't exist. This may also explain why inflation-adjusted IAs have all but disappeared."
            },
            {
                "arxivId": "2010.16009",
                "title": "QUANTIFYING THE TRADE-OFF BETWEEN INCOME STABILITY AND THE NUMBER OF MEMBERS IN A POOLED ANNUITY FUND",
                "abstract": "Abstract The number of people who receive a stable income for life from a closed pooled annuity fund is studied. Income stability is defined as keeping the income within a specified tolerance of the initial income in a fixed proportion of future scenarios. The focus is on quantifying the effect of the number of members, which drives the level of idiosyncratic longevity risk in the fund, on the income stability. To do this, investment returns are held constant, and systematic longevity risk is omitted. An analytical expression that closely approximates the number of fund members who receive a stable income is derived and is seen to be independent of the mortality model. An application of the result is to calculate the length of time for which the pooled annuity fund can provide the desired level of income stability."
            },
            {
                "arxivId": "2005.00715",
                "title": "Closed-form Solutions for an Explicit Modern Ideal Tontine with Bequest Motive",
                "abstract": null
            },
            {
                "arxivId": "1903.05990",
                "title": "Modern tontine with bequest: Innovation in pooled annuity products",
                "abstract": null
            },
            {
                "arxivId": "1811.09921",
                "title": "Retirement Spending and Biological Age",
                "abstract": "We solve a lifecycle model in which the consumer\u2019s chronological age does not move in lockstep with calendar time. Instead, biological age increases at a stochastic non-linear rate in time like a broken clock that might occasionally move backwards. In other words, biological age could actually decline. Our paper is inspired by the growing body of medical literature that has identified biomarkers which indicate how people age at different rates. This offers better estimates of expected remaining lifetime and future mortality rates. It isn\u2019t farfetched to argue that in the not-too-distant future personal age will be more closely associated with biological vs. calendar age. Thus, after introducing our stochastic mortality model we derive optimal consumption rates in a classic (Yaari, 1965) framework adjusted to our proper clock time. In addition to the normative implications of having access to biological age, our positive objective is to partially explain the cross-sectional heterogeneity in retirement spending rates at any given chronological age. In sum, we argue that neither biological nor chronological age alone is a sufficient statistic for making economic decisions. Rather, both ages are required to behave rationally."
            },
            {
                "arxivId": "1610.10078",
                "title": "Optimal retirement income tontines",
                "abstract": null
            },
            {
                "arxivId": "1311.5120",
                "title": "ACTUARIAL FAIRNESS AND SOLIDARITY IN POOLED ANNUITY FUNDS",
                "abstract": "Abstract Various types of structures that enable a group of individuals to pool their mortality risk have been proposed in the literature. Collectively, the structures are called pooled annuity funds. Since the pooled annuity funds propose different methods of pooling mortality risk, we investigate the connections between them and find that they are genuinely different for a finite heterogeneous membership profile. We discuss the importance of actuarial fairness, defined as the expected benefits equalling the contributions for each member, in the context of pooling mortality risk and comment on whether actuarial unfairness can be seen as solidarity between members. We show that, with a finite number of members in the fund, the group self-annuitization scheme is not actuarially fair: some members subsidize the other members. The implication is that the members who are subsidizing the others may obtain a higher expected benefit by joining a fund with a more favorable membership profile. However, we find that the subsidies are financially significant only for very small or highly heterogeneous membership profiles."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-23.json",
        "arxivId": "2402.14708",
        "category": "q-fin",
        "title": "CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks",
        "abstract": "Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.",
        "references": [
            {
                "arxivId": "2402.03781",
                "title": "MolTC: Towards Molecular Relational Modeling In Language Models",
                "abstract": "Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the issue of information underutilization, as it hinders the sharing of interaction mechanism learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which effectively integrate graphical information of two molecules in pair. For achieving a unified MRL, MolTC innovatively develops a dynamic parameter-sharing strategy for cross-dataset information sharing. Moreover, to train MolTC efficiently, we introduce a Multi-hierarchical CoT concept to refine its training paradigm, and conduct a comprehensive Molecular Interactive Instructions dataset for the development of biochemical LLMs involving MRL. Our experiments, conducted across various datasets involving over 4,000,000 molecular pairs, exhibit the superiority of our method over current GNN and LLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC."
            },
            {
                "arxivId": "2402.05970",
                "title": "Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts",
                "abstract": "In this paper, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical simulation systems depend largely on the initial settings and correctness of the constructed partial differential equations (PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this paper propose the universal expert module -- that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic properties of the whole system. Further, we harness currently popular neural discrete learning to unveil the underlying important features in its latent space, this process better injects interpretability, which can help us obtain a powerful prior over these discrete random variables. We conduct extensive experiments and ablations to demonstrate that the proposed framework achieves large performance margins, compared with the existing SOTA baselines."
            },
            {
                "arxivId": "2402.01242",
                "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness",
                "abstract": "Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology&semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to guide this process, effectively balancing the preservation of both topological and semantic information. Ultimately, GST produces a sparse graph with maximum topological integrity and no performance degradation. Extensive experiments on 6 datasets and 5 backbones showcase that GST (I) identifies subgraphs at higher graph sparsity levels (1.67%~15.85% $\\uparrow$) than state-of-the-art sparsification methods, (II) preserves more key spectral properties, (III) achieves 1.27-3.42$\\times$ speedup in GNN inference and (IV) successfully helps graph adversarial defense and graph lottery tickets."
            },
            {
                "arxivId": "2312.08403",
                "title": "Earthfarseer: Versatile Spatio-Temporal Dynamical Systems Modeling in One Model",
                "abstract": "Efficiently modeling spatio-temporal (ST) physical processes and observations presents a challenging problem for the deep learning community. Many recent studies have concentrated on meticulously reconciling various advantages, leading to designed models that are neither simple nor practical. To address this issue, this paper presents a systematic study on existing shortcomings faced by off-the-shelf models, including lack of local fidelity, poor prediction performance over long time-steps,low scalability, and inefficiency. To systematically address the aforementioned problems, we propose an EarthFarseer, a concise framework that combines parallel local convolutions and global Fourier-based transformer architectures, enabling dynamically capture the local-global spatial interactions and dependencies. EarthFarseer also incorporates a multi-scale fully convolutional and Fourier architectures to efficiently and effectively capture the temporal evolution. Our proposal demonstrates strong adaptability across various tasks and datasets, with fast convergence and better local fidelity in long time-steps predictions. Extensive experiments and visualizations over eight human society physical and natural physical datasets demonstrates the state-of-the-art performance of EarthFarseer. We release our code at https://github.com/easylearningscores/EarthFarseer."
            },
            {
                "arxivId": "2311.15772",
                "title": "Attend Who is Weak: Enhancing Graph Condensation via Cross-Free Adversarial Training",
                "abstract": "In this paper, we study the \\textit{graph condensation} problem by compressing the large, complex graph into a concise, synthetic representation that preserves the most essential and discriminative information of structure and features. We seminally propose the concept of Shock Absorber (a type of perturbation) that enhances the robustness and stability of the original graphs against changes in an adversarial training fashion. Concretely, (I) we forcibly match the gradients between pre-selected graph neural networks (GNNs) trained on a synthetic, simplified graph and the original training graph at regularly spaced intervals. (II) Before each update synthetic graph point, a Shock Absorber serves as a gradient attacker to maximize the distance between the synthetic dataset and the original graph by selectively perturbing the parts that are underrepresented or insufficiently informative. We iteratively repeat the above two processes (I and II) in an adversarial training fashion to maintain the highly-informative context without losing correlation with the original dataset. More importantly, our shock absorber and the synthesized graph parallelly share the backward process in a free training manner. Compared to the original adversarial training, it introduces almost no additional time overhead. We validate our framework across 8 datasets (3 graph and 5 node classification datasets) and achieve prominent results: for example, on Cora, Citeseer and Ogbn-Arxiv, we can gain nearly 1.13% to 5.03% improvements compare with SOTA models. Moreover, our algorithm adds only about 0.2% to 2.2% additional time overhead over Flicker, Citeseer and Ogbn-Arxiv. Compared to the general adversarial training, our approach improves time efficiency by nearly 4-fold."
            },
            {
                "arxivId": "2309.13378",
                "title": "Deciphering Spatio-Temporal Graph Forecasting: A Causal Lens and Treatment",
                "abstract": "Spatio-Temporal Graph (STG) forecasting is a fundamental task in many real-world applications. Spatio-Temporal Graph Neural Networks have emerged as the most popular method for STG forecasting, but they often struggle with temporal out-of-distribution (OoD) issues and dynamic spatial causation. In this paper, we propose a novel framework called CaST to tackle these two challenges via causal treatments. Concretely, leveraging a causal lens, we first build a structural causal model to decipher the data generation process of STGs. To handle the temporal OoD issue, we employ the back-door adjustment by a novel disentanglement block to separate invariant parts and temporal environments from input data. Moreover, we utilize the front-door adjustment and adopt the Hodge-Laplacian operator for edge-level convolution to model the ripple effect of causation. Experiments results on three real-world datasets demonstrate the effectiveness and practicality of CaST, which consistently outperforms existing methods with good interpretability."
            },
            {
                "arxivId": "2307.04725",
                "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
                "abstract": "With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff."
            },
            {
                "arxivId": "1905.07953",
                "title": "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks",
                "abstract": "Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by~\\citezhang2018gaan."
            },
            {
                "arxivId": "1801.07455",
                "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
                "abstract": "\n \n Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.\n \n"
            },
            {
                "arxivId": "cond-mat/0109119",
                "title": "A simple model of bank bankruptcies",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-26.json",
        "arxivId": "2311.06519",
        "category": "q-fin",
        "title": "Portfolio diversification with varying investor abilities",
        "abstract": "\n We introduce new mathematical methods to study the optimal portfolio size of investment portfolios over time, considering investors with varying skill levels. First, we explore the benefit of portfolio diversification on an annual basis for poor, average and strong investors defined by the 10th, 50th and 90th percentiles of risk-adjusted returns, respectively. Second, we conduct a thorough regression experiment examining quantiles of risk-adjusted return as a function of portfolio size across investor ability, testing for trends and curvature within these functions. Finally, we study the optimal portfolio size for poor, average and strong investors in a continuously temporal manner using more than 20 years of data. We show that strong investors should hold concentrated portfolios, poor investors should hold diversified portfolios; average investors have a less obvious distribution with the optimal number varying materially over time.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-26.json",
        "arxivId": "2402.13326",
        "category": "q-fin",
        "title": "Deep Hedging with Market Impact",
        "abstract": "Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging. Results show our DRL model behaves better in contexts of low liquidity by, among others: 1) learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs, 2) factoring in the impact of features not considered by conventional approaches, such as previous hedging errors through the portfolio value, and the underlying asset's drift (i.e. the magnitude of its expected return).",
        "references": [
            {
                "arxivId": "2303.15216",
                "title": "Robust Risk-Aware Option Hedging",
                "abstract": "The objectives of option hedging/trading extend beyond mere protection against downside risks, with a desire to seek gains also driving agent's strategies. In this study, we showcase the potential of robust risk-aware reinforcement learning (RL) in mitigating the risks associated with path-dependent financial derivatives. We accomplish this by leveraging a policy gradient approach that optimises robust risk-aware performance criteria. We specifically apply this methodology to the hedging of barrier options, and highlight how the optimal hedging strategy undergoes distortions as the agent moves from being risk-averse to risk-seeking. As well as how the agent robustifies their strategy. We further investigate the performance of the hedge when the data generating process (DGP) varies from the training DGP, and demonstrate that the robust strategies outperform the non-robust ones."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "1311.4342",
                "title": "OPTION PRICING AND HEDGING WITH EXECUTION COSTS AND MARKET IMPACT",
                "abstract": "This paper considers the pricing and hedging of a call option when liquidity matters, that is, either for a large nominal or for an illiquid underlying asset. In practice, as opposed to the classical assumptions of a price\u2010taking agent in a frictionless market, traders cannot be perfectly hedged because of execution costs and market impact. They indeed face a trade\u2010off between hedging errors and costs that can be solved by using stochastic optimal control. Our modeling framework, which is inspired by the recent literature on optimal execution, makes it possible to account for both execution costs and the lasting market impact of trades. Prices are obtained through the indifference pricing approach. Numerical examples are provided, along with comparisons to standard methods."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-26.json",
        "arxivId": "2402.15072",
        "category": "q-fin",
        "title": "Impacts of Extreme Heat on Labor Force Dynamics",
        "abstract": "We use daily longitudinal data and a within-worker identification approach to examine the impacts of heat on labor force dynamics in Australia. High temperatures during 2001-2019 significantly reduced work attendance and hours worked, which were not compensated for in subsequent days and weeks. The largest reductions occurred in cooler regions and recent years, and were not solely concentrated amongst outdoor-based workers. Financial and Insurance Services was the most strongly affected industry, with temperatures above 38{\\deg}C (100{\\deg}F) increasing absenteeism by 15 percent. Adverse heat effects during the work commute and during outdoor work hours are shown to be key mechanisms.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-26.json",
        "arxivId": "2402.15387",
        "category": "q-fin",
        "title": "Higher order measures of risk and stochastic dominance",
        "abstract": "Higher order risk measures are stochastic optimization problems by design, and for this reason they enjoy valuable properties in optimization under uncertainties. They nicely integrate with stochastic optimization problems, as has been observed by the intriguing concept of the risk quadrangles, for example. Stochastic dominance is a binary relation for random variables to compare random outcomes. It is demonstrated that the concepts of higher order risk measures and stochastic dominance are equivalent, they can be employed to characterize the other. The paper explores these relations and connects stochastic orders, higher order risk measures and the risk quadrangle. Expectiles are employed to exemplify the relations obtained.",
        "references": [
            {
                "arxivId": "2303.03522",
                "title": "Expectiles In Risk Averse Stochastic Programming and Dynamic Optimization",
                "abstract": "This paper features expectiles in dynamic and stochastic optimization. Expectiles are a family of risk functionals characterized as minimizers of optimization problems. For this reason, they enjoy various unique stability properties, which can be exploited in risk averse management, in stochastic optimization and in optimal control. The paper provides tight relates of expectiles to other risk functionals and addresses their properties in regression. Further, we extend expectiles to a dynamic framework. As such, they allow incorporating a risk averse aspect in continuous-time dynamic optimization and a risk averse variant of the Hamilton-Jacobi-Bellman equations."
            },
            {
                "arxivId": "1303.6675",
                "title": "The natural Banach space for version independent risk measures",
                "abstract": null
            },
            {
                "arxivId": "1303.1690",
                "title": "COHERENCE AND ELICITABILITY",
                "abstract": "The risk of a financial position is usually summarized by a risk measure. As this risk measure has to be estimated from historical data, it is important to be able to verify and compare competing estimation procedures. In statistical decision theory, risk measures for which such verification and comparison is possible, are called elicitable. It is known that quantile\u2010based risk measures such as value at risk are elicitable. In this paper, the existing result of the nonelicitability of expected shortfall is extended to all law\u2010invariant spectral risk measures unless they reduce to minus the expected value. Hence, it is unclear how to perform forecast verification or comparison. However, the class of elicitable law\u2010invariant coherent risk measures does not reduce to minus the expected value. We show that it consists of certain expectiles."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "1912.12983",
        "category": "q-fin",
        "title": "A consistently oriented basis for eigenanalysis",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2202.07610",
        "category": "q-fin",
        "title": "$\\rho$-arbitrage and $\\rho$-consistent pricing for star-shaped risk measures",
        "abstract": "This paper revisits mean-risk portfolio selection in a one-period financial market, where risk is quantified by a star-shaped risk measure $\\rho$. We make three contributions. First, we introduce the new axiom of weak sensitivity to large losses and show that it is key to ensure the existence of optimal portfolios. Second, we give primal and dual characterisations of (strong) $\\rho$-arbitrage. Finally, we use our conditions for the absence of (strong) $\\rho$-arbitrage to explicitly derive the (strong) $\\rho$-consistent price interval for an external financial contract.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2202.09323",
        "category": "q-fin",
        "title": "Market-Based Price Autocorrelation",
        "abstract": "This paper assumes that the randomness of market trade values and volumes determines the properties of stochastic market prices. We derive the direct dependence of the first two price statistical moments and price volatility on statistical moments, volatilities, and correlations of market trade values and volumes. That helps describe the dependence of market-based price autocorrelation between times t and t-{\\tau} on statistical moments and correlations between trade values and volumes. That highlights the impact of the randomness of the size of market deals on price statistical moments and autocorrelation. Statistical moments and correlations of market trade values and volumes are assessed by conventional frequency-based probabilities. The distinctions between market-based price autocorrelation and autocorrelation that are assessed by the frequency-based probability analysis of price time series reveal the different approaches to the definitions of price probabilities. To forecast market-based price autocorrelation, one should predict the statistical moments and correlations of trade values and volumes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2203.03179",
        "category": "q-fin",
        "title": "Detecting data-driven robust statistical arbitrage strategies with deep neural networks",
        "abstract": "We present an approach, based on deep neural networks, that allows identifying robust statistical arbitrage strategies in financial markets. Robust statistical arbitrage strategies refer to trading strategies that enable profitable trading under model ambiguity. The presented novel methodology allows to consider a large amount of underlying securities simultaneously and does not depend on the identification of cointegrated pairs of assets, hence it is applicable on high-dimensional financial markets or in markets where classical pairs trading approaches fail. Moreover, we provide a method to build an ambiguity set of admissible probability measures that can be derived from observed market data. Thus, the approach can be considered as being model-free and entirely data-driven. We showcase the applicability of our method by providing empirical investigations with highly profitable trading performances even in 50 dimensions, during financial crises, and when the cointegration relationship between asset pairs stops to persist.",
        "references": [
            {
                "arxivId": "2106.10024",
                "title": "Robust deep hedging",
                "abstract": "We study pricing and hedging under parameter uncertainty for a class of Markov processes which we call generalized affine processes and which includes the Black\u2013Scholes model as well as the constant elasticity of variance (CEV) model as special cases. Based on a general dynamic programming principle, we are able to link the associated nonlinear expectation to a variational form of the Kolmogorov equation which opens the door for fast numerical pricing in the robust framework. The main novelty of the paper is that we propose a deep hedging approach which efficiently solves the hedging problem under parameter uncertainty. We numerically evaluate this method on simulated and real data and show that the robust deep hedging outperforms existing hedging approaches in highly volatile periods."
            },
            {
                "arxivId": "2103.11435",
                "title": "A Deep Learning Approach to Data-Driven Model-Free Pricing and to Martingale Optimal Transport",
                "abstract": "We introduce a novel and highly tractable supervised learning approach based on neural networks that can be applied for the computation of model-free price bounds of, potentially high-dimensional, financial derivatives and for the determination of optimal hedging strategies attaining these bounds. In particular, our methodology allows to train a single neural network offline and then to use it online for the fast determination of model-free price bounds of a whole class of financial derivatives with current market data. We show the applicability of this approach and highlight its accuracy in several examples involving real market data. Further, we show how a neural network can be trained to solve martingale optimal transport problems involving fixed marginal distributions instead of financial market data."
            },
            {
                "arxivId": "2009.13881",
                "title": "Lipschitz neural networks are dense in the set of all Lipschitz functions",
                "abstract": "This note shows that, for a fixed Lipschitz constant $L > 0$, one layer neural networks that are $L$-Lipschitz are dense in the set of all $L$-Lipschitz functions with respect to the uniform norm on bounded sets."
            },
            {
                "arxivId": "2008.09454",
                "title": "Detecting and Repairing Arbitrage in Traded Option Prices",
                "abstract": "ABSTRACT Option price data are used as inputs for model calibration, risk-neutral density estimation and many other financial applications. The presence of arbitrage in option price data can lead to poor performance or even failure of these tasks, making pre-processing of the data to eliminate arbitrage necessary. Most attention in the relevant literature has been devoted to arbitrage-free smoothing and filtering (i.e., removing) of data. In contrast to smoothing, which typically changes nearly all data, or filtering, which truncates data, we propose to repair data by only necessary and minimal changes. We formulate the data repair as a linear programming (LP) problem, where the no-arbitrage relations are constraints, and the objective is to minimize prices\u2019 changes within their bid and ask price bounds. Through empirical studies, we show that the proposed arbitrage repair method gives sparse perturbations on data, and is fast when applied to real-world large-scale problems due to the LP formulation. In addition, we show that removing arbitrage from prices data by our repair method can improve model calibration with enhanced robustness and reduced calibration error."
            },
            {
                "arxivId": "2006.14288",
                "title": "Model-Free Bounds for Multi-Asset Options Using Option-Implied Information and Their Exact Computation",
                "abstract": "We consider derivatives written on multiple underlyings in a one-period financial market, and we are interested in the computation of model-free upper and lower bounds for their arbitrage-free prices. We work in a completely realistic setting, in that we only assume the knowledge of traded prices for other single- and multi-asset derivatives and even allow for the presence of bid\u2013ask spread in these prices. We provide a fundamental theorem of asset pricing for this market model, as well as a superhedging duality result, that allows to transform the abstract maximization problem over probability measures into a more tractable minimization problem over vectors, subject to certain constraints. Then, we recast this problem into a linear semi-infinite optimization problem and provide two algorithms for its solution. These algorithms provide upper and lower bounds for the prices that are \u03b5-optimal, as well as a characterization of the optimal pricing measures. These algorithms are efficient and allow the computation of bounds in high-dimensional scenarios (e.g., when d = 60). Moreover, these algorithms can be used to detect arbitrage opportunities and identify the corresponding arbitrage strategies. Numerical experiments using both synthetic and real market data showcase the efficiency of these algorithms, and they also allow understanding of the reduction of model risk by including additional information in the form of known derivative prices. This paper was accepted by Chung Piaw Teo, optimization."
            },
            {
                "arxivId": "2004.10178",
                "title": "Forecasting directional movements of stock prices for intraday trading using LSTM and random forests",
                "abstract": null
            },
            {
                "arxivId": "2004.08891",
                "title": "Hedging With Linear Regressions and Neural Networks",
                "abstract": "Abstract We study neural networks as nonparametric estimation tools for the hedging of options. To this end, we design a network, named HedgeNet, that directly outputs a hedging strategy. This network is trained to minimize the hedging error instead of the pricing error. Applied to end-of-day and tick prices of S&P 500 and Euro Stoxx 50 options, the network is able to reduce the mean squared hedging error of the Black-Scholes benchmark significantly. However, a similar benefit arises by simple linear regressions that incorporate the leverage effect."
            },
            {
                "arxivId": "1912.01703",
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            {
                "arxivId": "1911.05523",
                "title": "BOUNDS ON MULTI-ASSET DERIVATIVES VIA NEURAL NETWORKS",
                "abstract": "Using neural networks, we compute bounds on the prices of multi-asset derivatives given information on prices of related payoffs. As a main example, we focus on European basket options and include information on the prices of other similar options, such as spread options and/or basket options on subindices. We show that, in most cases, adding further constraints gives rise to bounds that are considerably tighter. Our approach follows the literature on constrained optimal transport and, in particular, builds on the work of Eckstein & Kupper (2018) [Computation of optimal transport and related hedging problems via penalization and neural networks, Appl. Math. Optimiz. 1\u201329]."
            },
            {
                "arxivId": "1909.03870",
                "title": "Robust Pricing and Hedging of Options on Multiple Assets and Its Numerics",
                "abstract": "We consider robust pricing and hedging for options written on multiple assets given market option prices for the individual assets. The resulting problem is called the multi-marginal martingale optimal transport problem. We propose two numerical methods to solve such problems: using discretisation and linear programming applied to the primal side and using penalisation and deep neural networks optimisation applied to the dual side. We prove convergence for our methods and compare their numerical performance. We show how adding further information about call option prices at additional maturities can be incorporated and narrows down the no-arbitrage pricing bounds. Finally, we obtain structural results for the case of the payoff given by a weighted sum of covariances between the assets."
            },
            {
                "arxivId": "1908.00461",
                "title": "Low-Rank Plus Sparse Decomposition of Covariance Matrices Using Neural Network Parametrization",
                "abstract": "This article revisits the problem of decomposing a positive semidefinite matrix as a sum of a matrix with a given rank plus a sparse matrix. An immediate application can be found in portfolio optimization, when the matrix to be decomposed is the covariance between the different assets in the portfolio. Our approach consists in representing the low-rank part of the solution as the product <inline-formula> <tex-math notation=\"LaTeX\">$MM^{T}$ </tex-math></inline-formula>, where <inline-formula> <tex-math notation=\"LaTeX\">$M$ </tex-math></inline-formula> is a rectangular matrix of appropriate size, parametrized by the coefficients of a deep neural network. We then use a gradient descent algorithm to minimize an appropriate loss function over the parameters of the network. We deduce its convergence rate to a local optimum from the Lipschitz smoothness of our loss function. We show that the rate of convergence grows polynomially in the dimensions of the input\u2013output, and the size of each of the hidden layers."
            },
            {
                "arxivId": "1907.09218",
                "title": "Generalized statistical arbitrage concepts and related gain strategies",
                "abstract": "The notion of statistical arbitrage introduced in Bondarenko (2003) is generalized to statistical G \u2010arbitrage corresponding to trading strategies which yield positive gains on average in a class of scenarios described by a \u03c3 \u2010algebra G . This notion contains classical arbitrage as a special case. Admitting general static payoffs as generalized strategies, as done in Kassberger and Liebmann (2017) in the case of one pricing measure, leads to the notion of generalized statistical G \u2010arbitrage. We show that even under standard no\u2010arbitrage there may exist generalized gain strategies yielding positive gains on average under the specified scenarios. In the first part of the paper we prove that the characterization in Bondarenko (2003), no statistical arbitrage being equivalent to the existence of an equivalent local martingale measure with a path\u2010independent density, is not correct in general. We establish that this equivalence holds true in complete markets and we derive a general sufficient condition for statistical G \u2010arbitrages. As a main result we derive the equivalence of no statistical G \u2010arbitrage to no generalized statistical G \u2010arbitrage. In the second part of the paper we construct several classes of profitable generalized strategies with respect to various choices of the \u03c3 \u2010algebra G . In particular, we consider several forms of embedded binomial strategies and follow\u2010the\u2010trend strategies as well as partition\u2010type strategies. We study and compare their behavior on simulated data and also evaluate their performance on market data."
            },
            {
                "arxivId": "1905.08539",
                "title": "Universal Approximation with Deep Narrow Networks",
                "abstract": "The classical Universal Approximation Theorem holds for neural networks of arbitrary width and bounded depth. Here we consider the natural `dual' scenario for networks of bounded width and arbitrary depth. Precisely, let $n$ be the number of inputs neurons, $m$ be the number of output neurons, and let $\\rho$ be any nonaffine continuous function, with a continuous nonzero derivative at some point. Then we show that the class of neural networks of arbitrary depth, width $n + m + 2$, and activation function $\\rho$, is dense in $C(K; \\mathbb{R}^m)$ for $K \\subseteq \\mathbb{R}^n$ with $K$ compact. This covers every activation function possible to use in practice, and also includes polynomial activation functions, which is unlike the classical version of the theorem, and provides a qualitative difference between deep narrow networks and shallow wide networks. We then consider several extensions of this result. In particular we consider nowhere differentiable activation functions, density in noncompact domains with respect to the $L^p$-norm, and how the width may be reduced to just $n + m + 1$ for `most' activation functions."
            },
            {
                "arxivId": "1904.04546",
                "title": "(Martingale) Optimal Transport and Anomaly Detection with Neural Networks: A Primal-Dual Algorithm",
                "abstract": "In this paper, we introduce a primal-dual algorithm for solving (martingale) optimal transportation problems, with cost functions satisfying the twist condition, close to the one that has been used recently for training generative adversarial networks. As some additional applications, we consider anomaly detection and automatic generation of financial data."
            },
            {
                "arxivId": "1811.00304",
                "title": "Robust risk aggregation with neural networks",
                "abstract": "We consider settings in which the distribution of a multivariate random variable is partly ambiguous. We assume the ambiguity lies on the level of the dependence structure, and that the marginal distributions are known. Furthermore, a current best guess for the distribution, called reference measure, is available. We work with the set of distributions that are both close to the given reference measure in a transportation distance (e.g., the Wasserstein distance), and additionally have the correct marginal structure. The goal is to find upper and lower bounds for integrals of interest with respect to distributions in this set. The described problem appears naturally in the context of risk aggregation. When aggregating different risks, the marginal distributions of these risks are known and the task is to quantify their joint effect on a given system. This is typically done by applying a meaningful risk measure to the sum of the individual risks. For this purpose, the stochastic interdependencies between the risks need to be specified. In practice, the models of this dependence structure are however subject to relatively high model ambiguity. The contribution of this paper is twofold: First, we derive a dual representation of the considered problem and prove that strong duality holds. Second, we propose a generally applicable and computationally feasible method, which relies on neural networks, in order to numerically solve the derived dual problem. The latter method is tested on a number of toy examples, before it is finally applied to perform robust risk aggregation in a real\u2010world instance."
            },
            {
                "arxivId": "1811.05381",
                "title": "Sorting out Lipschitz function approximation",
                "abstract": "Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation is 1-Lipschitz. The challenge is to do this while maintaining the expressive power. We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy."
            },
            {
                "arxivId": "1804.04368",
                "title": "Regularisation of neural networks by enforcing Lipschitz continuity",
                "abstract": null
            },
            {
                "arxivId": "1802.08539",
                "title": "Computation of Optimal Transport and Related Hedging Problems via Penalization and Neural Networks",
                "abstract": null
            },
            {
                "arxivId": "1802.03042",
                "title": "Deep hedging",
                "abstract": "We present a framework for hedging a portfolio of derivatives in the presence of market frictions such as transaction costs, liquidity constraints or risk limits using modern deep reinforcement machine learning methods. We discuss how standard reinforcement learning methods can be applied to non-linear reward structures, i.e. in our case convex risk measures. As a general contribution to the use of deep learning for stochastic processes, we also show in Section 4 that the set of constrained trading strategies used by our algorithm is large enough to \u03b5-approximate any optimal solution. Our algorithm can be implemented efficiently even in high-dimensional situations using modern machine learning tools. Its structure does not depend on specific market dynamics, and generalizes across hedging instruments including the use of liquid derivatives. Its computational performance is largely invariant in the size of the portfolio as it depends mainly on the number of hedging instruments available. We illustrate our approach by an experiment on the S&P500 index and by showing the effect on hedging under transaction costs in a synthetic market driven by the Heston model, where we outperform the standard \u2018complete-market\u2019 solution."
            },
            {
                "arxivId": "1709.08894",
                "title": "On the regularization of Wasserstein GANs",
                "abstract": "Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. It was proposed that training can be improved by instead augmenting the loss by a regularization term that penalizes the deviation of the gradient of the critic (as a function of the network's input) from one. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on toy data sets."
            },
            {
                "arxivId": "1707.03335",
                "title": "Viability and Arbitrage Under Knightian Uncertainty",
                "abstract": "We reconsider the microeconomic foundations of financial economics. Motivated by the importance of Knightian uncertainty in markets, we present a model that does not carry any probabilistic structure ex ante, yet is based on a common order. We derive the fundamental equivalence of economic viability of asset prices and absence of arbitrage. We also obtain a modified version of the fundamental theorem of asset pricing using the notion of \n sublinear pricing measures. Different versions of the efficient market hypothesis are related to the assumptions one is willing to impose on the common order.\n"
            },
            {
                "arxivId": "1607.06450",
                "title": "Layer Normalization",
                "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques."
            },
            {
                "arxivId": "1412.5154",
                "title": "Iterative Bregman Projections for Regularized Transportation Problems",
                "abstract": "This article details a general numerical framework to approximate so-lutions to linear programs related to optimal transport. The general idea is to introduce an entropic regularization of the initial linear program. This regularized problem corresponds to a Kullback-Leibler Bregman di-vergence projection of a vector (representing some initial joint distribu-tion) on the polytope of constraints. We show that for many problems related to optimal transport, the set of linear constraints can be split in an intersection of a few simple constraints, for which the projections can be computed in closed form. This allows us to make use of iterative Bregman projections (when there are only equality constraints) or more generally Bregman-Dykstra iterations (when inequality constraints are in-volved). We illustrate the usefulness of this approach to several variational problems related to optimal transport: barycenters for the optimal trans-port metric, tomographic reconstruction, multi-marginal optimal trans-port and in particular its application to Brenier's relaxed solutions of in-compressible Euler equations, partial un-balanced optimal transport and optimal transport with capacity constraints."
            },
            {
                "arxivId": "1305.6008",
                "title": "Arbitrage and duality in nondominated discrete-time models",
                "abstract": "We consider a nondominated model of a discrete-time financial market where stocks are traded dynamically and options are available for static hedging. In a general measure-theoretic setting, we show that absence of arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of martingale measures. In the arbitrage-free case, we show that optimal superhedging strategies exist for general contingent claims, and that the minimal superhedging price is given by the supremum over the martingale measures. Moreover, we obtain a nondominated version of the Optional Decomposition Theorem."
            },
            {
                "arxivId": "1301.5568",
                "title": "A MODEL\u2010FREE VERSION OF THE FUNDAMENTAL THEOREM OF ASSET PRICING AND THE SUPER\u2010REPLICATION THEOREM",
                "abstract": "We propose a Fundamental Theorem of Asset Pricing and a Super\u2010Replication Theorem in a model\u2010independent framework. We prove these theorems in the setting of finite, discrete time and a market consisting of a risky asset S as well as options written on this risky asset. As a technical condition, we assume the existence of a traded option with a superlinearly growing payoff\u2010function, e.g., a power option. This condition is not needed when sufficiently many vanilla options maturing at the horizon T are traded in the market."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2211.08405",
        "category": "q-fin",
        "title": "Multimodal Generative Models for Bankruptcy Prediction Using Textual Data",
        "abstract": "Textual data from financial filings, e.g., the Management's Discussion&Analysis (MDA) section in Form 10-K, has been used to improve the prediction accuracy of bankruptcy models. In practice, however, we cannot obtain the MDA section for all public companies, which limits the use of MDA data in traditional bankruptcy models, as they need complete data to make predictions. The two main reasons for the lack of MDA are: (i) not all companies are obliged to submit the MDA and (ii) technical problems arise when crawling and scrapping the MDA section. To solve this limitation, this research introduces the Conditional Multimodal Discriminative (CMMD) model that learns multimodal representations that embed information from accounting, market, and textual data modalities. The CMMD model needs a sample with all data modalities for model training. At test time, the CMMD model only needs access to accounting and market modalities to generate multimodal representations, which are further used to make bankruptcy predictions and to generate words from the missing MDA modality. With this novel methodology, it is realistic to use textual data in bankruptcy prediction models, since accounting and market data are available for all companies, unlike textual data. The empirical results of this research show that if financial regulators, or investors, were to use traditional models using MDA data, they would only be able to make predictions for 60% of the companies. Furthermore, the classification performance of our proposed methodology is superior to that of a large number of traditional classifier models, taking into account all the companies in our sample.",
        "references": [
            {
                "arxivId": "2110.04616",
                "title": "Discriminative Multimodal Learning via Conditional Priors in Generative Models",
                "abstract": null
            },
            {
                "arxivId": "2105.02470",
                "title": "Generalized Multimodal ELBO",
                "abstract": "Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks."
            },
            {
                "arxivId": "1911.03393",
                "title": "Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models",
                "abstract": "Learning generative models that span multiple data modalities, such as vision and language, is often motivated by the desire to learn more useful, generalisable representations that faithfully capture common underlying factors between the modalities. In this work, we characterise successful learning of such models as the fulfilment of four criteria: i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration. Here, we propose a mixture-of-experts multi-modal variational autoencoder (MMVAE) for learning of generative models on different sets of modalities, including a challenging image language dataset, and demonstrate its ability to satisfy all four criteria, both qualitatively and quantitatively."
            },
            {
                "arxivId": "1705.09406",
                "title": "Multimodal Machine Learning: A Survey and Taxonomy",
                "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research."
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1611.01891",
                "title": "Joint Multimodal Learning with Deep Generative Models",
                "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally."
            },
            {
                "arxivId": "1610.03454",
                "title": "Deep Variational Canonical Correlation Analysis",
                "abstract": "We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks. We derive variational lower bounds of the data likelihood by parameterizing the posterior probability of the latent variables from the view that is available at test time. We also propose a variant of VCCA called VCCA-private that can, in addition to the \"common variables\" underlying both views, extract the \"private variables\" within each view, and disentangles the shared and private information for multi-view data without hard supervision. Experimental results on real-world datasets show that our methods are competitive across domains."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2311.14320",
        "category": "q-fin",
        "title": "Consumption Smoothing in Metropolis: Evidence from the Working-class Households in Prewar Tokyo",
        "abstract": "I analyze the risk-coping behaviors among factory worker households in early 20th-century Tokyo. I digitize a unique daily longitudinal survey dataset on household budgets to examine the extent to which consumption is affected by idiosyncratic shocks. I find that while the households were so vulnerable that the shocks impacted their consumption levels, the income elasticity for food consumption is relatively low in the short-run perspective. The result from mechanism analysis suggests that credit purchases with local retailers played a role in smoothing short-run food consumption. The event-study analysis using the adverse health shock as the idiosyncratic income shock confirms the robustness of the results. I also find evidence that the misassignment of payday in data aggregation results in a systematic attenuation bias due to measurement error in the standard consumption smoothing regressions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.15588",
        "category": "q-fin",
        "title": "Sizing the bets in a focused portfolio",
        "abstract": "The paper provides a mathematical model and a tool for the focused investing strategy as advocated by Buffett, Munger, and others from this investment community. The approach presented here assumes that the investor's role is to think about probabilities of different outcomes for a set of businesses. Based on these assumptions, the tool calculates the optimal allocation of capital for each of the investment candidates. The model is based on a generalized Kelly Criterion with options to provide constraints that ensure: no shorting, limited use of leverage, providing a maximum limit to the risk of permanent loss of capital, and maximum individual allocation. The software is applied to an example portfolio from which certain observations about excessive diversification are obtained. In addition, the software is made available for public use.",
        "references": [
            {
                "arxivId": "2311.06519",
                "title": "Portfolio diversification with varying investor abilities",
                "abstract": "\n We introduce new mathematical methods to study the optimal portfolio size of investment portfolios over time, considering investors with varying skill levels. First, we explore the benefit of portfolio diversification on an annual basis for poor, average and strong investors defined by the 10th, 50th and 90th percentiles of risk-adjusted returns, respectively. Second, we conduct a thorough regression experiment examining quantiles of risk-adjusted return as a function of portfolio size across investor ability, testing for trends and curvature within these functions. Finally, we study the optimal portfolio size for poor, average and strong investors in a continuously temporal manner using more than 20 years of data. We show that strong investors should hold concentrated portfolios, poor investors should hold diversified portfolios; average investors have a less obvious distribution with the optimal number varying materially over time."
            },
            {
                "arxivId": "2311.01985",
                "title": "Maximizing Portfolio Predictability with Machine Learning",
                "abstract": "We construct the maximally predictable portfolio (MPP) of stocks using machine learning. Solving for the optimal constrained weights in the multi-asset MPP gives portfolios with a high monthly coefficient of determination, given the sample covariance matrix of predicted return errors from a machine learning model. Various models for the covariance matrix are tested. The MPPs of S&P 500 index constituents with estimated returns from Elastic Net, Random Forest, and Support Vector Regression models can outperform or underperform the index depending on the time period. Portfolios that take advantage of the high predictability of the MPP's returns and employ a Kelly criterion style strategy consistently outperform the benchmark."
            },
            {
                "arxivId": "2308.01305",
                "title": "A quantum double-or-nothing game: The Kelly Criterion for Spins",
                "abstract": "A sequence of spin-1/2 particles polarised in one of two possible directions is presented to an experimenter, who can wager in a double-or-nothing game on the outcomes of measurements in freely chosen polarisation directions. Wealth is accrued through astute betting. As information is gained from the stream of particles, the measurement directions are progressively adjusted, and the portfolio growth rate is raised. The optimal quantum strategy is determined numerically and shown to differ from the classical strategy, which is associated with the Kelly criterion. The paper contributes to the development of quantum finance, as aspects of portfolio optimisation are extended to the quantum realm."
            },
            {
                "arxivId": "2303.10417",
                "title": "On the Benefit of Nonlinear Control for Robust Logarithmic Growth: Coin Flipping Games as a Demonstration Case",
                "abstract": "The takeoff point for this letter is the voluminous body of literature addressing recursive betting games with expected logarithmic growth of wealth being the performance criterion. Whereas almost all existing papers involve use of linear feedback, the use of nonlinear control is conspicuously absent. This is epitomized by the large subset of this literature dealing with Kelly Betting. With this as the high-level motivation, we study the potential for use of nonlinear control in this framework. To this end, we consider a \u201cdemonstration case\u201d which is one of the simplest scenarios encountered in this line of research: repeated flips of a biased coin with probability of heads <inline-formula> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula>, and even-money payoff on each flip. First, we formulate a new robust nonlinear control problem which we believe is both simple to understand and apropos for dealing with concerns about distributional robustness; i.e., instead of assuming that p is perfectly known as in the case of the classical Kelly formulation, we begin with a bounding set <inline-formula> <tex-math notation=\"LaTeX\">${\\mathcal{ P}} \\subseteq [0{,}1]$ </tex-math></inline-formula> for this probability. Then, we provide a theorem, our main result, which gives a closed-form description of the optimal robust nonlinear controller and a corollary which establishes that it robustly outperforms linear controllers such as those found in the literature. A second, less significant, contribution of this letter bears upon the computability of our solution. For an n-flip game, whereas an admissible controller has <inline-formula> <tex-math notation=\"LaTeX\">$2^{\\mathrm{ n}}-1$ </tex-math></inline-formula> parameters, at the optimum only O(<inline-formula> <tex-math notation=\"LaTeX\">$\\rm n^{2}$ </tex-math></inline-formula>) of them turn out to be distinct. Finally, it is noted that the initial assumptions on payoffs and the use of the uniform distribution on <inline-formula> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula> are made mainly for simplicity of the exposition and compliance with length requirements for a Letter. Accordingly, this letter also includes a new Section with a discussion indicating how these assumptions can be relaxed."
            },
            {
                "arxivId": "2302.13994",
                "title": "Gambling the World Away: Myopic Investors",
                "abstract": "Myopic investors are locally rational decision-makers but globally irrational. Their suboptimal portfolios lag the market. As a consequence, other market participants are provided with profit opportunities. Not subterfuge but constrained optimisation leads to disparities. Four overlapping examples are given. The first case centres on the difference between local and global optimisers and their respective Kelly fractions, the second on isolated versus combined optimisation, the third on the distinction between qualitative and quantitative investor, the fourth on the non-commutative nature of information and the resulting asymmetries."
            },
            {
                "arxivId": "2002.03448",
                "title": "Kelly Criterion: From a Simple Random Walk to L\u00e9vy Processes",
                "abstract": "The original Kelly criterion provides a strategy to maximize the long-term growth of winnings in a sequence of simple Bernoulli bets with an edge, that is, when the expected return on each bet is positive. The objective of this work is to consider more general models of returns and the continuous time, or high frequency, limits of those models."
            },
            {
                "arxivId": "1806.05293",
                "title": "GENERALIZED FRAMEWORK FOR APPLYING THE KELLY CRITERION TO STOCK MARKETS",
                "abstract": "We develop a general framework for applying the Kelly criterion to the stock market. By supplying an arbitrary probability distribution modeling the future price movement of a set of stocks, the Kelly fraction for investing each stock can be calculated by inverting a matrix involving only first and second moments. The framework works for one or a portfolio of stocks and the Kelly fractions can be efficiently calculated. For a simple model of geometric Brownian motion of a single stock we show that our calculated Kelly fraction agrees with existing results. We demonstrate that the Kelly fractions can be calculated easily for other types of probabilities such as the Gaussian distribution and correlated multivariate assets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.15620",
        "category": "q-fin",
        "title": "Comparison of sectoral structures between China and Japan: A network perspective",
        "abstract": "Economic structure comparisons between China and Japan have long captivated development economists. To delve deeper into their sectoral differences from 1995 to 2018, we used the annual input-output tables (IOTs) of both nations to construct weighted and directed input-output networks (IONs). This facilitated deeper network analyses. Strength distributions underscored variations in inter-sector economic interactions. Weighted, directed assortativity coefficients encapsulated the homophily among connecting sectors' features. By adjusting emphasis in PageRank centrality, key sectors were identified. Community detection revealed their clustering tendencies among the sectors. As anticipated, the analysis pinpointed manufacturing as China's central sector, while Japan favored services. Yet, at a finer level of the specific sectors, both nations exhibited varied structural evolutions. Contrastingly, sectoral communities in both China and Japan demonstrated commendable stability over the examined duration.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.15936",
        "category": "q-fin",
        "title": "Optimizing Neural Networks for Bermudan Option Pricing: Convergence Acceleration, Future Exposure Evaluation and Interpolation in Counterparty Credit Risk",
        "abstract": "This paper presents a Monte-Carlo-based artificial neural network framework for pricing Bermudan options, offering several notable advantages. These advantages encompass the efficient static hedging of the target Bermudan option and the effective generation of exposure profiles for risk management. We also introduce a novel optimisation algorithm designed to expedite the convergence of the neural network framework proposed by Lokeshwar et al. (2022) supported by a comprehensive error convergence analysis. We conduct an extensive comparative analysis of the Present Value (PV) distribution under Markovian and no-arbitrage assumptions. We compare the proposed neural network model in conjunction with the approach initially introduced by Longstaff and Schwartz (2001) and benchmark it against the COS model, the pricing model pioneered by Fang and Oosterlee (2009), across all Bermudan exercise time points. Additionally, we evaluate exposure profiles, including Expected Exposure and Potential Future Exposure, generated by our proposed model and the Longstaff-Schwartz model, comparing them against the COS model. We also derive exposure profiles at finer non-standard grid points or risk horizons using the proposed approach, juxtaposed with the Longstaff Schwartz method with linear interpolation and benchmark against the COS method. In addition, we explore the effectiveness of various interpolation schemes within the context of the Longstaff-Schwartz method for generating exposures at finer grid horizons.",
        "references": [
            {
                "arxivId": "2005.02633",
                "title": "Deep xVA Solver \u2013 A Neural Network Based Counterparty Credit Risk Management Framework",
                "abstract": "In this paper, we present a novel computational framework for portfolio-wide risk management problems, where the presence of a potentially large number of risk factors makes traditional numerical techniques ineffective. The new method utilises a coupled system of BSDEs for the valuation adjustments (xVA) and solves these by a recursive application of a neural network based BSDE solver. This not only makes the computation of xVA for high-dimensional problems feasible, but also produces hedge ratios and dynamic risk measures for xVA, and allows simulations of the collateral account."
            },
            {
                "arxivId": "1911.11362",
                "title": "Neural Network for Pricing and Universal Static Hedging of Contingent Claims",
                "abstract": "We present here a regress later based Monte Carlo approach that uses neural networks for pricing high-dimensional contingent claims. The choice of specific architecture of the neural networks used in the proposed algorithm provides for interpretability of the model, a feature that is often desirable in the financial context. Specifically, the interpretation leads us to demonstrate that any contingent claim -- possibly high dimensional and path-dependent -- under the Markovian and the no-arbitrage assumptions, can be semi-statically hedged using a portfolio of short maturity options. We show how the method can be used to obtain an upper and lower bound to the true price, where the lower bound is obtained by following a sub-optimal policy, while the upper bound by exploiting the dual formulation. Unlike other duality based upper bounds where one typically has to resort to nested simulation for constructing super-martingales, the martingales in the current approach come at no extra cost, without the need for any sub-simulations. We demonstrate through numerical examples the simplicity and efficiency of the method for both pricing and semi-static hedging of path-dependent options"
            },
            {
                "arxivId": "1908.01602",
                "title": "Solving high-dimensional optimal stopping problems using deep learning",
                "abstract": "Nowadays many financial derivatives, such as American or Bermudan options, are of early exercise type. Often the pricing of early exercise options gives rise to high-dimensional optimal stopping problems, since the dimension corresponds to the number of underlying assets. High-dimensional optimal stopping problems are, however, notoriously difficult to solve due to the well-known curse of dimensionality. In this work, we propose an algorithm for solving such problems, which is based on deep learning and computes, in the context of early exercise option pricing, both approximations of an optimal exercise strategy and the price of the considered option. The proposed algorithm can also be applied to optimal stopping problems that arise in other areas where the underlying stochastic process can be efficiently simulated. We present numerical results for a large number of example problems, which include the pricing of many high-dimensional American and Bermudan options, such as Bermudan max-call options in up to 5000 dimensions. Most of the obtained results are compared to reference values computed by exploiting the specific problem design or, where available, to reference values from the literature. These numerical results suggest that the proposed algorithm is highly effective in the case of many underlyings, in terms of both accuracy and speed."
            },
            {
                "arxivId": "1907.06474",
                "title": "Neural network regression for Bermudan option pricing",
                "abstract": "Abstract The pricing of Bermudan options amounts to solving a dynamic programming principle, in which the main difficulty, especially in high dimension, comes from the conditional expectation involved in the computation of the continuation value. These conditional expectations are classically computed by regression techniques on a finite-dimensional vector space. In this work, we study neural networks approximations of conditional expectations. We prove the convergence of the well-known Longstaff and Schwartz algorithm when the standard least-square regression is replaced by a neural network approximation, assuming an efficient algorithm to compute this approximation. We illustrate the numerical efficiency of neural networks as an alternative to standard regression methods for approximating conditional expectations on several numerical examples."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.15965",
        "category": "q-fin",
        "title": "Evolving E-commerce Logistics Planning- Integrating Embedded Technology and Ant Colony Algorithm for Enhanced Efficiency",
        "abstract": "Amidst the era of networking, the e-commerce sector has undergone notable expansion, notably with the advent of Cross-border E-commerce (CBEC) in recent times. This growth trend persists, necessitating robust logistical frameworks to sustainably support operations. However, the current e-commerce logistics paradigm faces challenges in meeting evolving user demands, prompting a quest for innovative solutions. This research endeavors to address these complexities by undertaking a comprehensive analysis of CBEC logistics models and integrating embedded technology into logistical frameworks, resulting in the development of an advanced logistics tracking system. Moreover, employing the ant colony algorithm, the study conducts experimental investigations into optimizing logistics package distribution route planning. Noteworthy enhancements are observed in key metrics such as average delivery time, signaling the efficacy of this approach. In essence, this research offers a promising pathway towards optimizing logistics package distribution routes and bolstering package transportation efficiency within the CBEC domain.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.16345",
        "category": "q-fin",
        "title": "On convergence of forecasts in prediction markets",
        "abstract": "We propose a dynamic model of a prediction market in which agents predict the values of a sequence of random vectors. The main result shows that if there are agents who make correct (or asymptotically correct) next-period forecasts, then the aggregated market forecasts converge to the next-period conditional expectations of the random vectors.",
        "references": [
            {
                "arxivId": "2101.09777",
                "title": "Capital Growth and Survival Strategies in a Market with Endogenous Prices",
                "abstract": "We call an investment strategy survival, if an agent who uses it maintains a non-vanishing share of market wealth over the infinite time horizon. In a discrete-time multi-agent model with endogenous asset prices determined through a short-run equilibrium of supply and demand, we show that a survival strategy can be constructed as follows: an agent should assume that only their actions determine the prices and use a growth optimal (log-optimal) strategy with respect to these prices, disregarding the actual prices. Then any survival strategy turns out to be close to this strategy asymptotically. The main results are obtained under the assumption that the assets are short-lived."
            },
            {
                "arxivId": "2008.13230",
                "title": "A continuous-time asset market game with short-lived assets",
                "abstract": null
            },
            {
                "arxivId": "1201.6655",
                "title": "Learning performance of prediction markets with Kelly bettors",
                "abstract": "Consider a prediction market, in which participants can trade shares (binary options) at the current market price pm. Each share is worth $1 if the event occurs, and nothing otherwise. What fraction of your wealth w should you risk if you believe the probability of the event is p? Buying is favorable if p > pm, in which case risking your entire wealth will maximize your expected profit with respect to your belief. However, that's extraordinarily risky: A single stroke of bad luck loses everything. On the other hand, risking a small fixed amount cannot take advantage of compounding growth."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.16401",
        "category": "q-fin",
        "title": "A Stationary Equilibrium Model of Green Technology Adoption with Endogenous Carbon Price",
        "abstract": "This paper proposes and analyzes a stationary equilibrium model for a competitive industry which endogenously determines the carbon price necessary to achieve a given emission target. In the model, firms are identified by their level of technology and make production, entry, and abatement decisions. Polluting firms are subject to a carbon price and abatement is formulated as an irreversible investment, which entails a sunk cost and results in the firms switching to a carbon neutral technology. In equilibrium, we identify a carbon price and a stationary distribution of incumbent, polluting firms, that guarantee the compliance with a certain emission target. Our general theoretical framework is complemented with a case study with Brownian technology shocks, in which we discuss some implications of our model. We observe that a carbon pricing system alongside installation subsidies and tax benefits for green firms trigger earlier investment, while higher income taxes for polluting firms may be distorting. Moreover, we discuss the role of a welfare maximizing regulator, who, by optimally setting the emission target, may mitigate or revert some parameters' effects observed in the model with fixed limit.",
        "references": [
            {
                "arxivId": "2310.03650",
                "title": "General Equilibrium Theory for Climate Change",
                "abstract": "We propose two general equilibrium models, quota equilibrium and emission tax equilibrium. The government specifies quotas or taxes on emissions, then refrains from further action. Quota equilibrium exists; the allocation of emission property rights strongly impacts the distribution of welfare. If the only externality arises from total net emissions, quota equilibrium is constrained Pareto Optimal. Every quota equilibrium can be realized as an emission tax equilibrium and vice versa. However, for certain tax rates, emission tax equilibrium may not exist, or may exhibit high multiplicity. Full Pareto Optimality of quota equilibrium can often be achieved by setting the right quota."
            },
            {
                "arxivId": "2304.10344",
                "title": "Uncertainty over Uncertainty in Environmental Policy Adoption: Bayesian Learning of Unpredictable Socioeconomic Costs",
                "abstract": null
            },
            {
                "arxivId": "2012.09493",
                "title": "Optimal switch from a fossil-fueled to an electric vehicle",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.16428",
        "category": "q-fin",
        "title": "Closed form solution to zero coupon bond using a linear stochastic delay differential equation",
        "abstract": "We present a short rate model that satisfies a stochastic delay differential equation. The model can be considered a delayed version of the Merton model (Merton 1970, 1973) or the Vasi\\v{c}ek model (Vasi\\v{c}ek 1977). Using the same technique as the one used by Flore and Nappo (2019), we show that the bond price is an affine function of the short rate, whose coefficients satisfy a system of delay differential equations. We give an analytical solution to this system of delay differential equations, obtaining a closed formula for the zero coupon bond price. Under this model, we can show that the distribution of the short rate is a normal distribution whose mean depends on past values of the short rate. Based on the results of K\\\"uchler and Mensch (1992), we prove the existence of stationary and limiting distributions.",
        "references": [
            {
                "arxivId": "1806.00997",
                "title": "A Feynman-Kac type formula for a fixed delay CIR model",
                "abstract": "Abstract Stochastic delay differential equations (SDDE\u2019s) have been used for financial modeling. In this article, we study a SDDE obtained by the equation of a CIR process, with an additional fixed delay term in drift; in particular, we prove that there exists a unique strong solution (positive and integrable) which we call fixed delay CIR process. Moreover, for the fixed delay CIR process, we derive a Feynman-Kac type formula, leading to a generalized exponential-affine formula, which is used to determine a bond pricing formula when the interest rate follows the delay\u2019s equation. It turns out that, for each maturity time T, the instantaneous forward rate is an affine function (with time dependent coefficients) of the rate process and of an auxiliary process (also depending on T). The coefficients satisfy a system of deterministic differential equations."
            },
            {
                "arxivId": "1204.3679",
                "title": "TIME\u2010CHANGED ORNSTEIN\u2013UHLENBECK PROCESSES AND THEIR APPLICATIONS IN COMMODITY DERIVATIVE MODELS",
                "abstract": "This paper studies subordinate Ornstein\u2013Uhlenbeck (OU) processes, i.e., OU diffusions time changed by L\u00e9vy subordinators. We construct their sample path decomposition, show that they possess mean\u2010reverting jumps, study their equivalent measure transformations, and the spectral representation of their transition semigroups in terms of Hermite expansions. As an application, we propose a new class of commodity models with mean\u2010reverting jumps based on subordinate OU processes. Further time changing by the integral of a Cox\u2013Ingersoll\u2013Ross process plus a deterministic function of time, we induce stochastic volatility and time inhomogeneity, such as seasonality, in the models. We obtain analytical solutions for commodity futures options in terms of Hermite expansions. The models are consistent with the initial futures curve, exhibit Samuelson's maturity effect, and are flexible enough to capture a variety of implied volatility smile patterns observed in commodities futures options."
            },
            {
                "arxivId": "math/0604640",
                "title": "A Delayed Black and Scholes Formula",
                "abstract": "Abstract In this article we develop an explicit formula for pricing European options when the underlying stock price follows nonlinear stochastic functional differential equations with fixed and variable delays. We believe that the proposed models are sufficiently flexible to fit real market data, and yet simple enough to allow for a closed-form representation of the option price. Furthermore, the models maintain the no-arbitrage property and the completeness of the market. The derivation of the option-pricing formula is based on an equivalent local martingale measure."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.16538",
        "category": "q-fin",
        "title": "Learning to Maximize (Expected) Utility",
        "abstract": "We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind. We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles. In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems. Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences. A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory. Finally, in addition to finding that choice consistency is positively correlated with cognitive ability, we document that subjects who learned to maximize utility were more cognitively able than those who did not. We discuss potential implications of our analysis.",
        "references": [
            {
                "arxivId": "2307.09411",
                "title": "Risk Preference Types, Limited Consideration, and Welfare",
                "abstract": "Abstract We provide sufficient conditions for semi-nonparametric point identification of a mixture model of decision making under risk, when agents make choices in multiple lines of insurance coverage (contexts) by purchasing a bundle. As a first departure from the related literature, the model allows for two preference types. In the first one, agents behave according to standard expected utility theory with CARA Bernoulli utility function, with an agent-specific coefficient of absolute risk aversion whose distribution is left completely unspecified. In the other, agents behave according to the dual theory of choice under risk combined with a one-parameter family distortion function, where the parameter is agent-specific and is drawn from a distribution that is left completely unspecified. Within each preference type, the model allows for unobserved heterogeneity in consideration sets, where the latter form at the bundle level\u2014a second departure from the related literature. Our point identification result rests on observing sufficient variation in covariates across contexts, without requiring any independent variation across alternatives within a single context. We estimate the model on data on households\u2019 deductible choices in two lines of property insurance, and use the results to assess the welfare implications of a hypothetical market intervention where the two lines of insurance are combined into a single one. We study the role of limited consideration in mediating the welfare effects of such intervention."
            },
            {
                "arxivId": "2212.03931",
                "title": "A Better Test of Choice Overload",
                "abstract": "Choice overload - by which larger choice sets are detrimental to a chooser's wellbeing - is potentially of great importance to the design of economic policy. Yet the current evidence on its prevalence is inconclusive. We argue that existing tests are likely to be underpowered and hence that choice overload may occur more often than the literature suggests. We propose more powerful tests based on richer data and characterization theorems for the Random Utility Model. These new approaches come with significant econometric challenges, which we show how to address. We apply our tests to new experimental data and find strong evidence of choice overload that would likely be missed using current approaches."
            },
            {
                "arxivId": "1902.06629",
                "title": "Discrete Choice under Risk with Limited Consideration",
                "abstract": "This paper is concerned with learning decision-makers\u2019 preferences using data on observed choices from a finite set of risky alternatives. We propose a discrete choice model with unobserved heterogeneity in consideration sets and in standard risk aversion. We obtain sufficient conditions for the model\u2019s semi-nonparametric point identification, including in cases where consideration depends on preferences and on some of the exogenous variables. Our method yields an estimator that is easy to compute and is applicable in markets with large choice sets. We illustrate its properties using a dataset on property insurance purchases. (JEL D81, D83, D91, G22, G52)"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.16609",
        "category": "q-fin",
        "title": "Combining Transformer based Deep Reinforcement Learning with Black-Litterman Model for Portfolio Optimization",
        "abstract": "As a model-free algorithm, deep reinforcement learning (DRL) agent learns and makes decisions by interacting with the environment in an unsupervised way. In recent years, DRL algorithms have been widely applied by scholars for portfolio optimization in consecutive trading periods, since the DRL agent can dynamically adapt to market changes and does not rely on the specification of the joint dynamics across the assets. However, typical DRL agents for portfolio optimization cannot learn a policy that is aware of the dynamic correlation between portfolio asset returns. Since the dynamic correlations among portfolio assets are crucial in optimizing the portfolio, the lack of such knowledge makes it difficult for the DRL agent to maximize the return per unit of risk, especially when the target market permits short selling (i.e., the US stock market). In this research, we propose a hybrid portfolio optimization model combining the DRL agent and the Black-Litterman (BL) model to enable the DRL agent to learn the dynamic correlation between the portfolio asset returns and implement an efficacious long/short strategy based on the correlation. Essentially, the DRL agent is trained to learn the policy to apply the BL model to determine the target portfolio weights. To test our DRL agent, we construct the portfolio based on all the Dow Jones Industrial Average constitute stocks. Empirical results of the experiments conducted on real-world United States stock market data demonstrate that our DRL agent significantly outperforms various comparison portfolio choice strategies and alternative DRL frameworks by at least 42% in terms of accumulated return. In terms of the return per unit of risk, our DRL agent significantly outperforms various comparative portfolio choice strategies and alternative strategies based on other machine learning frameworks.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-27.json",
        "arxivId": "2402.16724",
        "category": "q-fin",
        "title": "Alternative Models for FX: Pricing Double Barrier Options in Regime-switching L\u00e9vy Models With Memory",
        "abstract": "This paper is a supplement to our recent paper ``Alternative models for FX, arbitrage opportunities and efficient pricing of double barrier options in L\\'evy models\". We introduce the class of regime-switching L\\'evy models with memory, which take into account the evolution of the stochastic parameters in the past. This generalization of the class of L\\'evy models modulated by Markov chains is similar in spirit to rough volatility models. It is flexible and suitable for application of the machine-learning tools. We formulate the modification of the numerical method in ``Alternative models for FX, arbitrage opportunities and efficient pricing of double barrier options in L\\'evy models\", which has the same number of the main time-consuming blocks as the method for Markovian regime-switching models.",
        "references": [
            {
                "arxivId": "2312.03915",
                "title": "Alternative models for FX, arbitrage opportunities and efficient pricing of double barrier options in L\\'evy models",
                "abstract": "We analyze the qualitative differences between prices of double barrier no-touch options in the Heston model and pure jump KoBoL model calibrated to the same set of the empirical data, and discuss the potential for arbitrage opportunities if the correct model is a pure jump model. We explain and demonstrate with numerical examples that accurate and fast calculations of prices of double barrier options in jump models are extremely difficult using the numerical methods available in the literature. We develop a new efficient method (GWR-SINH method) based of the Gaver-Wynn-Rho acceleration applied to the Bromwich integral; the SINH-acceleration and simplified trapezoid rule are used to evaluate perpetual double barrier options for each value of the spectral parameter in GWR-algorithm. The program in Matlab running on a Mac with moderate characteristics achieves the precision of the order of E-5 and better in several several dozen of milliseconds; the precision E-07 is achievable in about 0.1 sec. We outline the extension of GWR-SINH method to regime-switching models and models with stochastic parameters and stochastic interest rates."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2304.12501",
        "category": "q-fin",
        "title": "The cross-sectional stock return predictions via quantum neural network and tensor network",
        "abstract": null,
        "references": [
            {
                "arxivId": "2105.10019",
                "title": "Enhancing Cross-Sectional Currency Strategies by Context-Aware Learning to Rank with Self-Attention",
                "abstract": "The performance of a cross-sectional currency strategy depends crucially on accurately ranking instruments prior to portfolio construction. Although this ranking step is traditionally performed using heuristics or by sorting the outputs produced by pointwise regression or classification techniques, strategies using learning-to-rank algorithms have recently presented themselves as competitive and viable alternatives. Although the rankers at the core of these strategies are learned globally and improve ranking accuracy on average, they ignore the differences between the distributions of asset features over the times when the portfolio is rebalanced. This flaw renders them susceptible to producing suboptimal rankings, possibly at important periods when accuracy is actually needed the most. For example, this might happen during critical risk-off episodes, which consequently exposes the portfolio to substantial, unwanted drawdowns. The authors tackle this shortcoming with an analogous idea from information retrieval: that a query\u2019s top retrieved documents or the local ranking context provide vital information about the query\u2019s own characteristics, which can then be used to refine the initial ranked list. In this work, the authors use a context-aware learning-to-rank model that is based on the transformer architecture to encode top/bottom-ranked assets, learn the context and exploit this information to rerank the initial results. Back testing on a slate of 31 currencies, the authors\u2019 proposed methodology increases the Sharpe ratio by around 30% and significantly enhances various performance metrics. Additionally, this approach also improves the Sharpe ratio when separately conditioning on normal and risk-off market states."
            },
            {
                "arxivId": "2012.11242",
                "title": "Learning temporal data with a variational quantum recurrent neural network",
                "abstract": "We propose a method for learning temporal data using a parametrized quantum circuit. We use the circuit that has a similar structure as the recurrent neural network which is one of the standard approaches employed for this type of machine learning task. Some of the qubits in the circuit are utilized for memorizing past data, while others are measured and initialized at each time step for obtaining predictions and encoding a new input datum. The proposed approach utilizes the tensor product structure to get nonlinearity with respect to the inputs. Fully controllable, ensemble quantum systems such as an NMR quantum computer is a suitable choice of an experimental platform for this proposal. We demonstrate its capability with Simple numerical simulations, in which we test the proposed method for the task of predicting cosine and triangular waves and quantum spin dynamics. Finally, we analyze the dependency of its performance on the interaction strength among the qubits in numerical simulation and find that there is an appropriate range of the strength. This work provides a way to exploit complex quantum dynamics for learning temporal data."
            },
            {
                "arxivId": "2012.09265",
                "title": "Variational quantum algorithms",
                "abstract": null
            },
            {
                "arxivId": "2011.13524",
                "title": "Qulacs: a fast and versatile quantum circuit simulator for research purpose",
                "abstract": "To explore the possibilities of a near-term intermediate-scale quantum algorithm and long-term fault-tolerant quantum computing, a fast and versatile quantum circuit simulator is needed. Here, we introduce Qulacs, a fast simulator for quantum circuits intended for research purpose. We show the main concepts of Qulacs, explain how to use its features via examples, describe numerical techniques to speed-up simulation, and demonstrate its performance with numerical benchmarks."
            },
            {
                "arxivId": "2006.14619",
                "title": "Recurrent Quantum Neural Networks",
                "abstract": "Recurrent neural networks are the foundation of many sequence-to-sequence models in machine learning, such as machine translation and speech synthesis. In contrast, applied quantum computing is in its infancy. Nevertheless there already exist quantum machine learning models such as variational quantum eigensolvers which have been used successfully e.g. in the context of energy minimization tasks. In this work we construct a quantum recurrent neural network (QRNN) with demonstrable performance on non-trivial tasks such as sequence learning and integer digit classification. The QRNN cell is built from parametrized quantum neurons, which, in conjunction with amplitude amplification, create a nonlinear activation of polynomials of its inputs and cell state, and allow the extraction of a probability distribution over predicted classes at each step. To study the model's performance, we provide an implementation in pytorch, which allows the relatively efficient optimization of parametrized quantum circuits with thousands of parameters. We establish a QRNN training setup by benchmarking optimization hyperparameters, and analyse suitable network topologies for simple memorisation and sequence prediction tasks from Elman's seminal paper (1990) on temporal structure learning. We then proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN each image pixel-by-pixel; and by utilising modern data augmentation as preprocessing step. Finally, we analyse to what extent the unitary nature of the network counteracts the vanishing gradient problem that plagues many existing quantum classifiers and classical RNNs."
            },
            {
                "arxivId": "1910.11333",
                "title": "Quantum supremacy using a programmable superconducting processor",
                "abstract": null
            },
            {
                "arxivId": "1910.01491",
                "title": "RIC-NN: A Robust Transferable Deep Learning Framework for Cross-sectional Investment Strategy",
                "abstract": "Stock return predictability is an important research theme as it reflects our economic and social organization, and significant efforts are made to explain the dynamism therein. Statistics of strong explanative power, called \"factor\", have been proposed to summarize the essence of predictive stock returns. The challenge here is to make a multi-factor investment strategy that is consistent over a reasonably long period based on supervised machine learning. Although machine learning methods are increasingly popular in stock return prediction, an inference of the stock return is highly elusive, and naive use of complex machine learning methods easily overfits the current data and results in poor performance on future data. We propose a principled stock return prediction framework that we call Ranked Information Coefficient Neural Network (RIC-NN) that alleviates the overfitting. RIC-NN addresses the difficulty that arises in nonconvex machine learning: Namely, initialization and the stopping of the training model and the transfer among several different tasks (markets). RIC-NN is a deep learning approach and includes the following three novel ideas: (1) nonlinear multi-factor approach, (2) stopping criteria with ranked information coefficient (rank IC), and (3) deep transfer learning among multiple regions. Experimental comparison with the stocks in the Morgan Stanley Capital International indices shows that RIC-NN outperforms not only off-the-shelf machine learning methods but also the average return of major equity investment funds in the last fourteen years."
            },
            {
                "arxivId": "1906.06329",
                "title": "TensorNetwork for Machine Learning",
                "abstract": "We demonstrate the use of tensor networks for image classification with the TensorNetwork open source library. We explain in detail the encoding of image data into a matrix product state form, and describe how to contract the network in a way that is parallelizable and well-suited to automatic gradients for optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we find out-of-the-box performance of 98% and 88% accuracy, respectively, using the same tensor network architecture. The TensorNetwork library allows us to seamlessly move from CPU to GPU hardware, and we see a factor of more than 10 improvement in computational speed using a GPU."
            },
            {
                "arxivId": "1905.01330",
                "title": "TensorNetwork: A Library for Physics and Machine Learning",
                "abstract": "TensorNetwork is an open source library for implementing tensor network algorithms. Tensor networks are sparse data structures originally designed for simulating quantum many-body physics, but are currently also applied in a number of other research areas, including machine learning. We demonstrate the use of the API with applications both physics and machine learning, with details appearing in companion papers."
            },
            {
                "arxivId": "1904.04912",
                "title": "Enhancing Time-Series Momentum Strategies Using Deep Neural Networks",
                "abstract": "Although time-series momentum is a well-studied phenomenon in finance, common strategies require the explicit definition of both a trend estimator and a position sizing rule. In this article, the authors introduce deep momentum networks\u2014a hybrid approach that injects deep learning\u2013based trading rules into the volatility scaling framework of time-series momentum. The model also simultaneously learns both trend estimation and position sizing in a data-driven manner, with networks directly trained by optimizing the Sharpe ratio of the signal. Backtesting on a portfolio of 88 continuous futures contracts, the authors demonstrate that the Sharpe-optimized long short-term memory improved traditional methods by more than two times in the absence of transactions costs and continued outperforming when considering transaction costs up to 2\u20133 bps. To account for more illiquid assets, the authors also propose a turnover regularization term that trains the network to factor in costs at run-time. TOPICS: Statistical methods, simulations, big data/machine learning Key Findings \u2022 While time-series momentum strategies have been extensively studied in f inance, common strategies require the explicit specification of a trend estimator and position sizing rule. \u2022 In this article, the authors introduce deep momentum networks \u2014a hybrid approach that injects deep learning\u2013based trading rules into the volatility scaling framework of timeseries momentum. \u2022 Backtesting on a portfolio of continuous futures contracts, Deep Momentum Networks were shown to outperform traditional methods for transaction costs of up to 2\u20133 bps, with a turnover regularisation term proposed for more illiquid assets."
            },
            {
                "arxivId": "1903.07677",
                "title": "Deep Fundamental Factor Models",
                "abstract": "Deep fundamental factor models are developed to automatically capture non-linearity and interaction effects in factor modeling. Uncertainty quantification provides interpretability with interval estimation, ranking of factor importances and estimation of interaction effects. With no hidden layers we recover a linear factor model and for one or more hidden layers, uncertainty bands for the sensitivity to each input naturally arise from the network weights. Using 3290 assets in the Russell 1000 index over a period of December 1989 to January 2018, we assess a 49 factor model and generate information ratios that are approximately 1.5x greater than the OLS factor model. Furthermore, we compare our deep fundamental factor model with a quadratic LASSO model and demonstrate the superior performance and robustness to outliers. The Python source code and the data used for this study are provided."
            },
            {
                "arxivId": "1811.11184",
                "title": "Evaluating analytic gradients on quantum hardware",
                "abstract": "An important application for near-term quantum computing lies in optimization tasks, with applications ranging from quantum chemistry and drug discovery to machine learning. In many settings --- most prominently in so-called parametrized or variational algorithms --- the objective function is a result of hybrid quantum-classical processing. To optimize the objective, it is useful to have access to exact gradients of quantum circuits with respect to gate parameters. This paper shows how gradients of expectation values of quantum measurements can be estimated using the same, or almost the same, architecture that executes the original circuit. It generalizes previous results for qubit-based platforms, and proposes recipes for the computation of gradients of continuous-variable circuits. Interestingly, in many important instances it is sufficient to run the original quantum circuit twice while shifting a single gate parameter to obtain the corresponding component of the gradient. More general cases can be solved by conditioning a single gate on an ancilla."
            },
            {
                "arxivId": "1803.11537",
                "title": "Towards quantum machine learning with tensor networks",
                "abstract": "Machine learning is a promising application of quantum computing, but challenges remain for implementation today because near-term devices have a limited number of physical qubits and high error rates. Motivated by the usefulness of tensor networks for machine learning in the classical context, we propose quantum computing approaches to both discriminative and generative learning, with circuits based on tree and matrix product state tensor networks, that could already have benefits with such near-term devices. The result is a unified framework in which classical and quantum computing can benefit from the same theoretical and algorithmic developments, and the same model can be trained classically then transferred to the quantum setting for additional optimization. Tensor network circuits can also provide qubit-efficient schemes in which, depending on the architecture, the number of physical qubits required scales only logarithmically with, or independently of the input or output data sizes. We demonstrate our proposals with numerical experiments, training a discriminative model to perform handwriting recognition using a hybrid quantum-classical optimization procedure that could be carried out on quantum hardware today, and testing the noise resilience of the trained model."
            },
            {
                "arxivId": "1803.00745",
                "title": "Quantum circuit learning",
                "abstract": "We propose a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, which we call quantum circuit learning. A quantum circuit driven by our framework learns a given task by tuning parameters implemented on it. The iterative optimization of the parameters allows us to circumvent the high-depth circuit. Theoretical investigation shows that a quantum circuit can approximate nonlinear functions, which is further confirmed by numerical simulations. Hybridizing a low-depth quantum circuit and a classical computer for machine learning, the proposed framework paves the way toward applications of near-term quantum devices for quantum machine learning."
            },
            {
                "arxivId": "1801.01777",
                "title": "Deep Learning for Forecasting Stock Returns in the Cross-Section",
                "abstract": null
            },
            {
                "arxivId": "1801.00862",
                "title": "Quantum Computing in the NISQ era and beyond",
                "abstract": "Noisy Intermediate-Scale Quantum (NISQ) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. NISQ devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away - we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing."
            },
            {
                "arxivId": "1801.00315",
                "title": "Learning relevant features of data with multi-scale tensor networks",
                "abstract": "Inspired by coarse-graining approaches used in physics, we show how similar algorithms can be adapted for data. The resulting algorithms are based on layered tree tensor networks and scale linearly with both the dimension of the input and the training set size. Computing most of the layers with an unsupervised algorithm, then optimizing just the top layer for supervised classification of the MNIST and fashion MNIST data sets gives very good results. We also discuss mixing a prior guess for supervised weights together with an unsupervised representation of the data, yielding a smaller number of features nevertheless able to give good performance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2311.04727",
        "category": "q-fin",
        "title": "Forecasting volatility with machine learning and rough volatility: example from the crypto-winter",
        "abstract": "We extend the application and test the performance of a recently introduced volatility prediction framework encompassing LSTM and rough volatility. Our asset class of interest is cryptocurrencies, at the beginning of the \u201ccrypto-winter\u201d in 2022. We first show that to forecast volatility, a universal LSTM approach trained on a pool of assets outperforms traditional models. We then consider a parsimonious parametric model based on rough volatility and Zumbach effect. We obtain similar prediction performances with only five parameters whose values are non-asset-dependent. Our findings provide further evidence on the universality of the mechanisms underlying the volatility formation process.",
        "references": [
            {
                "arxivId": "2206.14114",
                "title": "On the universality of the volatility formation process: when machine learning and rough volatility agree",
                "abstract": "We train an LSTM network based on a pooled dataset made of hundreds of liquid stocks aiming to forecast the next daily realized volatility for all stocks. Showing the consistent outperformance of this universal LSTM relative to other asset-specific parametric models, we uncover nonparametric evidences of a universal volatility formation mechanism across assets relating past market realizations, including daily returns and volatilities, to current volatilities. A parsimonious parametric forecasting device combining the rough fractional stochastic volatility and quadratic rough Heston models with fixed parameters results in the same level of performance as the universal LSTM, which confirms the universality of the volatility formation process from a parametric perspective."
            },
            {
                "arxivId": "2201.09516",
                "title": "From rough to multifractal volatility: The log S-fBM model",
                "abstract": null
            },
            {
                "arxivId": "2107.01611",
                "title": "Deep calibration of the quadratic rough Heston model",
                "abstract": "The quadratic rough Heston model provides a natural way to encode Zumbach e\ufb00ect in the rough volatility paradigm. We apply multi-factor approximation and use deep learning methods to build an e\ufb03cient calibration procedure for this model. We show that the model is able to reproduce very well both SPX and VIX implied volatilities. We typically obtain VIX option prices within the bid-ask spread and an excellent \ufb01t of the SPX at-the-money skew. Moreover, we also explain how to use the trained neural networks for hedging with instantaneous computation of hedging quantities."
            },
            {
                "arxivId": "2003.11352",
                "title": "Cryptocurrency trading: a comprehensive survey",
                "abstract": null
            },
            {
                "arxivId": "2001.01789",
                "title": "The Quadratic Rough Heston Model and the Joint S&P 500/VIX Smile Calibration Problem",
                "abstract": "Fitting simultaneously SPX and VIX smiles is known to be one of the most challenging problems in volatility modeling. A long-standing conjecture due to Julien Guyon is that it may not be possible to calibrate jointly these two quantities with a model with continuous sample-paths. We present the quadratic rough Heston model as a counterexample to this conjecture. The key idea is the combination of rough volatility together with a price-feedback (Zumbach) effect."
            },
            {
                "arxivId": "1906.06037",
                "title": "What is Stablecoin?: A Survey on Price Stabilization Mechanisms for Decentralized Payment Systems",
                "abstract": "Since the first theoretical concept of blockchains was proposed, over 100 digital currencies have been issued by online platformers as cryptocurrencies and traded by online consumers mainly in emerging countries. From the perspective of online payment systems, several studies have regarded blockchains as decentralized payment systems (DPSs), enabling international payment with lower cost and higher traceability with sophisticated peer-to-peer protocols in contrast to other centralized systems. Despite the advantages, DPSs are not chosen by the owners of online shops due to the high volatility of cryptocurrency prices. Stablecoins are cryptocurrencies with price stabilization mechanisms to match the price of another currency with lower volatility. Our motivation is to gather various price stabilization mechanisms for the purpose of comparing them from the perspective of implementation and enterprise usage. After dividing the methods into four collateral types (fiat, crypto, commodity, and non-collateralized) and two layers (protocol and application), we show that non-collateralized stablecoin on the application layer is the simplest approach for implementation. Moreover, we discuss their connection with traditional economic studies on Hayek money, Seigniorage Share, and Tobin tax. Some current stablecoin projects are also discussed and compared. This is the first survey of stablecoins to the best of our knowledge."
            },
            {
                "arxivId": "1904.12346",
                "title": "Rough volatility of Bitcoin",
                "abstract": null
            },
            {
                "arxivId": "1811.08572",
                "title": "Bitcoin: A Natural Oligopoly",
                "abstract": "We argue that the concentrated production and ownership of Bitcoin mining hardware arise naturally from the economic incentives of Bitcoin mining. We model Bitcoin mining as a two-stage competition; miners compete in prices to sell hardware while competing in quantities for mining rewards. We characterize equilibria in our model and show that small asymmetries in operational costs result in highly concentrated ownership of mining equipment. We further show that production of mining equipment will be dominated by the miner with the most efficient hardware, who will sell hardware to competitors while possibly also using it to mine. This paper was accepted by Kay Giesecke, finance."
            },
            {
                "arxivId": "1703.05049",
                "title": "Perfect hedging in rough Heston models",
                "abstract": "Rough volatility models are known to reproduce the behavior of historical volatility data while at the same time fitting the volatility surface remarkably well, with very few parameters. However, managing the risks of derivatives under rough volatility can be intricate since the dynamics involve fractional Brownian motion. We show in this paper that surprisingly enough, explicit hedging strategies can be obtained in the case of rough Heston models. The replicating portfolios contain the underlying asset and the forward variance curve, and lead to perfect hedging (at least theoretically). From a probabilistic point of view, our study enables us to disentangle the infinite-dimensional Markovian structure associated to rough volatility models."
            },
            {
                "arxivId": "1610.00332",
                "title": "Decoupling the Short- and Long-Term Behavior of Stochastic Volatility",
                "abstract": "\n We introduce a new class of continuous-time models of the stochastic volatility of asset prices. The models can simultaneously incorporate roughness and slowly decaying autocorrelations, including proper long memory, which are two stylized facts often found in volatility data. Our prime model is based on the so-called Brownian semistationary process and we derive a number of theoretical properties of this process, relevant to volatility modeling. Applying the models to realized volatility measures covering a vast panel of assets, we find evidence consistent with the hypothesis that time series of realized measures of volatility are both rough and very persistent. Lastly, we illustrate the utility of the models in an extensive forecasting study; we find that the models proposed in this article outperform a wide array of benchmarks considerably, indicating that it pays off to exploit both roughness and persistence in volatility forecasting."
            },
            {
                "arxivId": "1609.02108",
                "title": "The characteristic function of rough Heston models",
                "abstract": "It has been recently shown that rough volatility models, where the volatility is driven by a fractional Brownian motion with small Hurst parameter, provide very relevant dynamics in order to reproduce the behavior of both historical and implied volatilities. However, due to the non\u2010Markovian nature of the fractional Brownian motion, they raise new issues when it comes to derivatives pricing. Using an original link between nearly unstable Hawkes processes and fractional volatility models, we compute the characteristic function of the log\u2010price in rough Heston models. In the classical Heston model, the characteristic function is expressed in terms of the solution of a Riccati equation. Here, we show that rough Heston models exhibit quite a similar structure, the Riccati equation being replaced by a fractional Riccati equation."
            },
            {
                "arxivId": "1606.08415",
                "title": "Gaussian Error Linear Units (GELUs)",
                "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "1412.4503",
                "title": "A Million Metaorder Analysis of Market Impact on the Bitcoin",
                "abstract": "We present a thorough empirical analysis of market impact on the Bitcoin/USD exchange market using a complete dataset that allows us to reconstruct more than one million metaorders. We empirically confirm the \"square-root law'' for market impact, which holds on four decades in spite of the quasi-absence of statistical arbitrage and market marking strategies. We show that the square-root impact holds during the whole trajectory of a metaorder and not only for the final execution price. We also attempt to decompose the order flow into an \"informed'' and \"uninformed'' component, the latter leading to an almost complete long-term decay of impact. This study sheds light on the hypotheses and predictions of several market impact models recently proposed in the literature and promotes heterogeneous agent models as promising candidates to explain price impact on the Bitcoin market -- and, we believe, on other markets as well."
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            },
            {
                "arxivId": "cond-mat/0501699",
                "title": "Volatility conditional on price trends",
                "abstract": "The influence of the past price behaviour on the realized volatility is investigated, showing that trending (driftless) prices lead to increased (decreased) realized volatility. This \u2018volatility induced by trend\u2019 constitutes a new stylized fact. The past price behaviour is measured by the product of two non-overlapping returns (of the form r \u00d7 L[r] where L is the lag operator), and is different from the usual heteroskedasticity. The effect is studied empirically using USD/CHF foreign exchange data, in a large range of time horizons. On the modelling side, a set of ARCH based processes are modified in order to include the \u2018volatility induced by trend\u2019 effect, and their forecasting performances are compared. The aim is to understand the role and importance of the various terms that can be included in such a model. For a better forecast, it is shown that the main factor is the shape of the memory kernel (i.e. power law), and the next most important factor is the trend effect. The subtle role of mean reversion is also discussed."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2311.10801",
        "category": "q-fin",
        "title": "Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools",
        "abstract": "Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNing framework with Maskable stOck REpresentation to handle PM with CSPs through one-shot training in a global stock pool (GSP). Specifically, we first introduce a mechanism to mask out the representation of the stocks outside the target pool. Second, we learn meaningful stock representations through a self-supervised masking and reconstruction process. Third, a re-weighting mechanism is designed to make the portfolio concentrate on favorable stocks and neglect the stocks outside the target pool. Through extensive experiments on 8 subset stock pools of the US stock market, we demonstrate that EarnMore significantly outperforms 14 state-of-the-art baselines in terms of 6 popular financial metrics with over 40% improvement on profit.",
        "references": [
            {
                "arxivId": "2312.15730",
                "title": "Deep Reinforcement Learning for Quantitative Trading",
                "abstract": "Artificial Intelligence (AI) and Machine Learning (ML) are transforming the domain of Quantitative Trading (QT) through the deployment of advanced algorithms capable of sifting through extensive financial datasets to pinpoint lucrative investment openings. AI-driven models, particularly those employing ML techniques such as deep learning and reinforcement learning, have shown great prowess in predicting market trends and executing trades at a speed and accuracy that far surpass human capabilities. Its capacity to automate critical tasks, such as discerning market conditions and executing trading strategies, has been pivotal. However, persistent challenges exist in current QT methods, especially in effectively handling noisy and high-frequency financial data. Striking a balance between exploration and exploitation poses another challenge for AI-driven trading agents. To surmount these hurdles, our proposed solution, QTNet, introduces an adaptive trading model that autonomously formulates QT strategies through an intelligent trading agent. Incorporating deep reinforcement learning (DRL) with imitative learning methodologies, we bolster the proficiency of our model. To tackle the challenges posed by volatile financial datasets, we conceptualize the QT mechanism within the framework of a Partially Observable Markov Decision Process (POMDP). Moreover, by embedding imitative learning, the model can capitalize on traditional trading tactics, nurturing a balanced synergy between discovery and utilization. For a more realistic simulation, our trading agent undergoes training using minute-frequency data sourced from the live financial market. Experimental findings underscore the model's proficiency in extracting robust market features and its adaptability to diverse market conditions."
            },
            {
                "arxivId": "2301.08871",
                "title": "Ti-MAE: Self-Supervised Masked Time Series Autoencoders",
                "abstract": "Multivariate Time Series forecasting has been an increasingly popular topic in various applications and scenarios. Recently, contrastive learning and Transformer-based models have achieved good performance in many long-term series forecasting tasks. However, there are still several issues in existing methods. First, the training paradigm of contrastive learning and downstream prediction tasks are inconsistent, leading to inaccurate prediction results. Second, existing Transformer-based models which resort to similar patterns in historical time series data for predicting future values generally induce severe distribution shift problems, and do not fully leverage the sequence information compared to self-supervised methods. To address these issues, we propose a novel framework named Ti-MAE, in which the input time series are assumed to follow an integrate distribution. In detail, Ti-MAE randomly masks out embedded time series data and learns an autoencoder to reconstruct them at the point-level. Ti-MAE adopts mask modeling (rather than contrastive learning) as the auxiliary task and bridges the connection between existing representation learning and generative Transformer-based methods, reducing the difference between upstream and downstream forecasting tasks while maintaining the utilization of original time series data. Experiments on several public real-world datasets demonstrate that our framework of masked autoencoding could learn strong representations directly from the raw data, yielding better performance in time series forecasting and classification tasks."
            },
            {
                "arxivId": "2211.14730",
                "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
                "abstract": "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST."
            },
            {
                "arxivId": "2211.09117",
                "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
                "abstract": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTHl4/mage."
            },
            {
                "arxivId": "2103.10860",
                "title": "Universal Trading for Order Execution with Oracle Policy Distillation",
                "abstract": "As a fundamental problem in algorithmic trading, order execution aims at fulfilling a specific trading order, either liquidation or acquirement, for a given instrument. Towards effective execution strategy, recent years have witnessed the shift from the analytical view with model-based market assumptions to model-free perspective, i.e., reinforcement learning, due to its nature of sequential decision optimization. However, the noisy and yet imperfect market information that can be leveraged by the policy has made it quite challenging to build up sample efficient reinforcement learning methods to achieve effective order execution. In this paper, we propose a novel universal trading policy optimization framework to bridge the gap between the noisy yet imperfect market states and the optimal action sequences for order execution. Particularly, this framework leverages a policy distillation method that can better guide the learning of the common policy towards practically optimal execution by an oracle teacher with perfect information to approximate the optimal trading strategy. The extensive experiments have shown significant improvements of our method over various strong baselines, with reasonable trading actions."
            },
            {
                "arxivId": "2012.12620",
                "title": "Commission Fee is not Enough: A Hierarchical Reinforced Framework for Portfolio Management",
                "abstract": "Portfolio management via reinforcement learning is at the forefront of fintech research, which explores how to optimally reallocate a fund into different financial assets over the long term by trial-and-error. Existing methods are impractical since they usually assume each reallocation can be finished immediately and thus ignoring the price slippage as part of the trading cost. To address these issues, we propose a hierarchical reinforced stock trading system for portfolio management (HRPM). Concretely, we decompose the trading process into a hierarchy of portfolio management over trade execution and train the corresponding policies. The high-level policy gives portfolio weights at a lower frequency to maximize the long-term profit and invokes the low-level policy to sell or buy the corresponding shares within a short time window at a higher frequency to minimize the trading cost. We train two levels of policies via a pre-training scheme and an iterative training scheme for data efficiency. Extensive experimental results in the U.S. market and the China market demonstrate that HRPM achieves significant improvement against many state-of-the-art approaches."
            },
            {
                "arxivId": "2002.05780",
                "title": "Reinforcement-Learning based Portfolio Management with Augmented Asset Movement Prediction States",
                "abstract": "Portfolio management (PM) is a fundamental financial planning task that aims to achieve investment goals such as maximal profits or minimal risks. Its decision process involves continuous derivation of valuable information from various data sources and sequential decision optimization, which is a prospective research direction for reinforcement learning (RL). In this paper, we propose SARL, a novel State-Augmented RL framework for PM. Our framework aims to address two unique challenges in financial PM: (1) data heterogeneity \u2013 the collected information for each asset is usually diverse, noisy and imbalanced (e.g., news articles); and (2) environment uncertainty \u2013 the financial market is versatile and non-stationary. To incorporate heterogeneous data and enhance robustness against environment uncertainty, our SARL augments the asset information with their price movement prediction as additional states, where the prediction can be solely based on financial data (e.g., asset prices) or derived from alternative sources such as news. Experiments on two real-world datasets, (i) Bitcoin market and (ii) HighTech stock market with 7-year Reuters news articles, validate the effectiveness of SARL over existing PM approaches, both in terms of accumulated profits and risk-adjusted profits. Moreover, extensive simulations are conducted to demonstrate the importance of our proposed state augmentation, providing new insights and boosting performance significantly over standard RL-based PM method and other baselines."
            },
            {
                "arxivId": "1804.04216",
                "title": "Market Making via Reinforcement Learning",
                "abstract": "Market making is a fundamental trading problem in which an agent provides liquidity by continually offering to buy and sell a security. The problem is challenging due to inventory risk, the risk of accumulating an unfavourable position and ultimately losing money. In this paper, we develop a high-fidelity simulation of limit order book markets, and use it to design a market making agent using temporal-difference reinforcement learning. We use a linear combination of tile codings as a value function approximator, and design a custom reward function that controls inventory risk. We demonstrate the effectiveness of our approach by showing that our agent outperforms both simple benchmark strategies and a recent online learning approach from the literature."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2401.08610",
        "category": "q-fin",
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities and Risks",
        "abstract": "Lido, the leading Liquid Staking Derivative (LSD) provider on Ethereum, allows users to stake an arbitrary amount of ETH to receive stETH, which can be integrated with Decentralized Finance (DeFi) protocols such as Aave. The composability between Lido and Aave enables a novel strategy called\"leverage staking\", where users stake ETH on Lido to acquire stETH, utilize stETH as collateral on Aave to borrow ETH, and then restake the borrowed ETH on Lido. Users can iteratively execute this process to optimize potential returns based on their risk profile. This paper systematically studies the opportunities and risks associated with leverage staking. We are the first to formalize the leverage staking strategy within the Lido-Aave ecosystem. Our empirical study identifies 262 leverage staking positions on Ethereum, with an aggregated staking amount of 295,243 ETH (482M USD). We discover that 90.13% of leverage staking positions have achieved higher returns than conventional staking. Furthermore, we perform stress tests to evaluate the risk introduced by leverage staking under extreme conditions. We find that leverage staking significantly amplifies the risk of cascading liquidations. We hope this paper can inform and encourage the development of robust risk management approaches to protect the Lido-Aave LSD ecosystem.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2402.17148",
        "category": "q-fin",
        "title": "Time series generation for option pricing on quantum computers using tensor network",
        "abstract": "Finance, especially option pricing, is a promising industrial field that might benefit from quantum computing. While quantum algorithms for option pricing have been proposed, it is desired to devise more efficient implementations of costly operations in the algorithms, one of which is preparing a quantum state that encodes a probability distribution of the underlying asset price. In particular, in pricing a path-dependent option, we need to generate a state encoding a joint distribution of the underlying asset price at multiple time points, which is more demanding. To address these issues, we propose a novel approach using Matrix Product State (MPS) as a generative model for time series generation. To validate our approach, taking the Heston model as a target, we conduct numerical experiments to generate time series in the model. Our findings demonstrate the capability of the MPS model to generate paths in the Heston model, highlighting its potential for path-dependent option pricing on quantum computers.",
        "references": [
            {
                "arxivId": "2112.05937",
                "title": "Inverse-coefficient black-box quantum state preparation",
                "abstract": "Black-box quantum state preparation is a fundamental building block for many higher-level quantum algorithms. The basic task of black-box state preparation is to transduce the data encoded as computational basis of quantum state into the amplitude. In the present work, we address the problem of transducing the reciprocal of the data, not the data itself into the amplitude, which is called the inverse-coefficient problem. This algorithm can be used directly as a subroutine in the matrix inversion algorithms. Furthermore, we extend this approach to address the more general nonlinear-coefficient problem in black-box state preparation. Our algorithm is based on the technique of inequality test. It can greatly relieve the need to do quantum arithmetic and the error is only resulted from the truncated error of binary string. The present algorithms enrich the algorithm library of black-box quantum state preparation and will be useful ingredients of quantum algorithm to implement non-linear quantum state transformations."
            },
            {
                "arxivId": "2111.07933",
                "title": "Quantum algorithms for approximate function loading",
                "abstract": "Loading classical data into quantum computers represents an essential stage in many relevant quantum algorithms, especially in the field of quantum machine learning. Therefore, the inefficiency of this loading process means a major bottleneck for the application of these algorithms. Here, we introduce two approximate quantum-state preparation methods for the NISQ era inspired by the Grover-Rudolph algorithm, which partially solve the problem of loading real functions. Indeed, by allowing for an infidelity $\\epsilon$ and under certain smoothness conditions, we prove that the complexity of the implementation of the Grover-Rudolph algorithm without ancillary qubits, first introduced by M\\\"ott\\\"onen $\\textit{et al}$, results into $\\mathcal{O}(2^{k_0(\\epsilon)})$, with $n$ the number of qubits and $k_0(\\epsilon)$ asymptotically independent of $n$. This leads to a dramatic reduction in the number of required two-qubit gates. Aroused by this result, we also propose a variational algorithm capable of loading functions beyond the aforementioned smoothness conditions. Our variational Ansatz is explicitly tailored to the landscape of the function, leading to a quasi-optimized number of hyperparameters. This allows us to achieve high fidelity in the loaded state with high speed convergence for the studied examples."
            },
            {
                "arxivId": "2106.12974",
                "title": "Tensor networks for unsupervised machine learning",
                "abstract": "Modeling the joint distribution of high-dimensional data is a central task in unsupervised machine learning. In recent years, many interests have been attracted to developing learning models based on tensor networks, which have the advantages of a principle understanding of the expressive power using entanglement properties, and as a bridge connecting classical computation and quantum computation. Despite the great potential, however, existing tensor network models for unsupervised machine learning only work as a proof of principle, as their performance is much worse than the standard models such as restricted Boltzmann machines and neural networks. In this Letter, we present autoregressive matrix product states (AMPS), a tensor network model combining matrix product states from quantum many-body physics and autoregressive modeling from machine learning. Our model enjoys the exact calculation of normalized probability and unbiased sampling. We demonstrate the performance of our model using two applications, generative modeling on synthetic and real-world data, and reinforcement learning in statistical physics. Using extensive numerical experiments, we show that the proposed model significantly outperforms the existing tensor network models and the restricted Boltzmann machines, and is competitive with state-of-the-art neural network models."
            },
            {
                "arxivId": "2011.06492",
                "title": "Prospects and challenges of quantum finance",
                "abstract": "Quantum computers are expected to have substantial impact on the finance industry, as they will be able to solve certain problems considerably faster than the best known classical algorithms. In this article we describe such potential applications of quantum computing to finance, starting with the state-of-the-art and focusing in particular on recent works by the QC Ware team. We consider quantum speedups for Monte Carlo methods, portfolio optimization, and machine learning. For each application we describe the extent of quantum speedup possible and estimate the quantum resources required to achieve a practical speedup. The near-term relevance of these quantum finance algorithms varies widely across applications - some of them are heuristic algorithms designed to be amenable to near-term prototype quantum computers, while others are proven speedups which require larger-scale quantum computers to implement. We also describe powerful ways to bring these speedups closer to experimental feasibility - in particular describing lower depth algorithms for Monte Carlo methods and quantum machine learning, as well as quantum annealing heuristics for portfolio optimization. This article is targeted at financial professionals and no particular background in quantum computation is assumed."
            },
            {
                "arxivId": "2009.10709",
                "title": "Fast Black-Box Quantum State Preparation",
                "abstract": "Quantum state preparation is an important ingredient for other higher-level quantum algorithms, such as Hamiltonian simulation, or for loading distributions into a quantum device to be used e.g. in the context of optimization tasks such as machine learning. Starting with a generic \"black box\" method devised by Grover in 2000, which employs amplitude amplification to load coefficients calculated by an oracle, there has been a long series of results and improvements with various additional conditions on the amplitudes to be loaded, culminating in Sanders et al.'s work which avoids almost all arithmetic during the preparation stage.In this work, we construct an optimized black box state loading scheme with which various important sets of coefficients can be loaded significantly faster than in O(N) rounds of amplitude amplification, up to only O(1) many. We achieve this with two variants of our algorithm. The first employs a modification of the oracle from Sanders et al., which requires fewer ancillas (log2\u2061g vs g+2 in the bit precision g), and fewer non-Clifford operations per amplitude amplification round within the context of our algorithm. The second utilizes the same oracle, but at slightly increased cost in terms of ancillas (g+log2\u2061g) and non-Clifford operations per amplification round. As the number of amplitude amplification rounds enters as multiplicative factor, our black box state loading scheme yields an up to exponential speedup as compared to prior methods. This speedup translates beyond the black box case."
            },
            {
                "arxivId": "2007.01467",
                "title": "Quantum pricing with a smile: implementation of local volatility model on quantum computer",
                "abstract": null
            },
            {
                "arxivId": "2006.14510",
                "title": "Quantum Computing for Finance: State-of-the-Art and Future Prospects",
                "abstract": "This article outlines our point of view regarding the applicability, state-of-the-art, and potential of quantum computing for problems in finance. We provide an introduction to quantum computing as well as a survey on problem classes in finance that are computationally challenging classically and for which quantum computing algorithms are promising. In the main part, we describe in detail quantum algorithms for specific applications arising in financial services, such as those involving simulation, optimization, and machine learning problems. In addition, we include demonstrations of quantum algorithms on IBM Quantum back-ends and discuss the potential benefits of quantum algorithms for problems in financial services. We conclude with a summary of technical challenges and future prospects."
            },
            {
                "arxivId": "1909.06619",
                "title": "Quantum-inspired algorithms for multivariate analysis: from interpolation to partial differential equations",
                "abstract": "In this work we study the encoding of smooth, differentiable multivariate functions in quantum registers, using quantum computers or tensor-network representations. We show that a large family of distributions can be encoded as low-entanglement states of the quantum register. These states can be efficiently created in a quantum computer, but they are also efficiently stored, manipulated and probed using Matrix-Product States techniques. Inspired by this idea, we present eight quantum-inspired numerical analysis algorithms, that include Fourier sampling, interpolation, differentiation and integration of partial derivative equations. These algorithms combine classical ideas \u2013 finite-differences, spectral methods \u2013 with the efficient encoding of quantum registers, and well known algorithms, such as the Quantum Fourier Transform. When these heuristic methods work, they provide an exponential speed-up over other classical algorithms, such as Monte Carlo integration, finite-difference and fast Fourier transforms (FFT). But even when they don't, some of these algorithms can be translated back to a quantum computer to implement a similar task."
            },
            {
                "arxivId": "1908.07958",
                "title": "Encoding of matrix product states into quantum circuits of one- and two-qubit gates",
                "abstract": "Matrix product state (MPS) belongs to the most important mathematical models in, for example, condensed matter physics and quantum information sciences. However, to realize an $N$-qubit MPS with large $N$ and large entanglement on a quantum platform is extremely challenging, since it requires high-level qudits or multi-body gates of two-level qubits to carry the entanglement. In this work, an efficient method that accurately encodes a given MPS into a quantum circuit with only one- and two-qubit gates is proposed. The idea is to construct the unitary matrix product operators that optimally disentangle the MPS to a product state. These matrix product operators form the quantum circuit that evolves a product state to the targeted MPS with a high fidelity. Our benchmark on the ground-state MPS's of the strongly-correlated spin models show that the constructed quantum circuits can encode the MPS's with much fewer qubits than the sizes of the MPS's themselves. This method paves a feasible and efficient path to realizing quantum many-body states and other MPS-based models as quantum circuits on the near-term quantum platforms."
            },
            {
                "arxivId": "1905.02666",
                "title": "Option Pricing using Quantum Computers",
                "abstract": "We present a methodology to price options and portfolios of options on a gate-based quantum computer using amplitude estimation, an algorithm which provides a quadratic speedup compared to classical Monte Carlo methods. The options that we cover include vanilla options, multi-asset options and path-dependent options such as barrier options. We put an emphasis on the implementation of the quantum circuits required to build the input states and operators needed by amplitude estimation to price the different option types. Additionally, we show simulation results to highlight how the circuits that we implement price the different option contracts. Finally, we examine the performance of option pricing circuits on quantum hardware using the IBM Q Tokyo quantum device. We employ a simple, yet effective, error mitigation scheme that allows us to significantly reduce the errors arising from noisy two-qubit gates."
            },
            {
                "arxivId": "1904.00043",
                "title": "Quantum Generative Adversarial Networks for learning and loading random distributions",
                "abstract": null
            },
            {
                "arxivId": "1805.00109",
                "title": "Quantum computational finance: Monte Carlo pricing of financial derivatives",
                "abstract": "Financial derivatives are contracts that can have a complex payoff dependent upon underlying benchmark assets. In this work, we present a quantum algorithm for the Monte Carlo pricing of financial derivatives. We show how the relevant probability distributions can be prepared in quantum superposition, the payoff functions can be implemented via quantum circuits, and the price of financial derivatives can be extracted via quantum measurements. We show how the amplitude estimation algorithm can be applied to achieve a quadratic quantum speedup in the number of steps required to obtain an estimate for the price with high confidence. This work provides a starting point for further research at the interface of quantum computing and finance."
            },
            {
                "arxivId": "1709.01662",
                "title": "Unsupervised Generative Modeling Using Matrix Product States",
                "abstract": "Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard datasets including the Bars and Stripes, random binary patterns and the MNIST handwritten digits to illustrate the abilities, features and drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work sheds light on many interesting directions of future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to be realized on quantum devices."
            },
            {
                "arxivId": "1504.06987",
                "title": "Quantum speedup of Monte Carlo methods",
                "abstract": "Monte Carlo methods use random sampling to estimate numerical quantities which are hard to compute deterministically. One important example is the use in statistical physics of rapidly mixing Markov chains to approximately compute partition functions. In this work, we describe a quantum algorithm which can accelerate Monte Carlo methods in a very general setting. The algorithm estimates the expected output value of an arbitrary randomized or quantum subroutine with bounded variance, achieving a near-quadratic speedup over the best possible classical algorithm. Combining the algorithm with the use of quantum walks gives a quantum speedup of the fastest known classical algorithms with rigorous performance bounds for computing partition functions, which use multiple-stage Markov chain Monte Carlo techniques. The quantum algorithm can also be used to estimate the total variation distance between probability distributions efficiently."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2402.17164",
        "category": "q-fin",
        "title": "Withdrawal Success Optimization in a Pooled Annuity Fund",
        "abstract": "Consider a closed pooled annuity fund investing in n assets with discrete-time rebalancing. At time 0, each annuitant makes an initial contribution to the fund, committing to a predetermined schedule of withdrawals. Require annuitants to be homogeneous in the sense that their initial contributions and predetermined withdrawal schedules are identical, and their mortality distributions are identical and independent. Under the forementioned setup, the probability for a particular annuitant to complete the prescribed withdrawals until death is maximized over progressively measurable portfolio weight functions. Applications consider fund portfolios that mix two assets: the S&P Composite Index and an inflation-protected bond. The maximum probability is computed for annually rebalanced schedules consisting of an initial investment and then equal annual withdrawals until death. A considerable increase in the maximum probability is achieved by increasing the number of annuitants initially in the pool. For example, when the per-annuitant initial contribution and annual withdrawal amount are held constant, starting with 20 annuitants instead of just 1 can increase the maximum probability (measured on a scale from 0 to 1) by as much as .15.",
        "references": [
            {
                "arxivId": "1311.5120",
                "title": "ACTUARIAL FAIRNESS AND SOLIDARITY IN POOLED ANNUITY FUNDS",
                "abstract": "Abstract Various types of structures that enable a group of individuals to pool their mortality risk have been proposed in the literature. Collectively, the structures are called pooled annuity funds. Since the pooled annuity funds propose different methods of pooling mortality risk, we investigate the connections between them and find that they are genuinely different for a finite heterogeneous membership profile. We discuss the importance of actuarial fairness, defined as the expected benefits equalling the contributions for each member, in the context of pooling mortality risk and comment on whether actuarial unfairness can be seen as solidarity between members. We show that, with a finite number of members in the fund, the group self-annuitization scheme is not actuarially fair: some members subsidize the other members. The implication is that the members who are subsidizing the others may obtain a higher expected benefit by joining a fund with a more favorable membership profile. However, we find that the subsidies are financially significant only for very small or highly heterogeneous membership profiles."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2402.17194",
        "category": "q-fin",
        "title": "The Random Forest Model for Analyzing and Forecasting the US Stock Market in the Context of Smart Finance",
        "abstract": "The stock market is a crucial component of the financial market, playing a vital role in wealth accumulation for investors, financing costs for listed companies, and the stable development of the national macroeconomy. Significant fluctuations in the stock market can damage the interests of stock investors and cause an imbalance in the industrial structure, which can interfere with the macro level development of the national economy. The prediction of stock price trends is a popular research topic in academia. Predicting the three trends of stock pricesrising, sideways, and falling can assist investors in making informed decisions about buying, holding, or selling stocks. Establishing an effective forecasting model for predicting these trends is of substantial practical importance. This paper evaluates the predictive performance of random forest models combined with artificial intelligence on a test set of four stocks using optimal parameters. The evaluation considers both predictive accuracy and time efficiency.",
        "references": [
            {
                "arxivId": "2312.12872",
                "title": "Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms",
                "abstract": "This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies. Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images. The successful experiences in the field of computer vision provide strong support for training deep learning algorithms. The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection. In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements. Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems. This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries."
            },
            {
                "arxivId": "2008.12918",
                "title": "Zero-Resource Knowledge-Grounded Dialogue Generation",
                "abstract": "While neural conversation models have shown great potentials towards generating informative and engaging responses via introducing external knowledge, learning such a model often requires knowledge-grounded dialogues that are difficult to obtain. To overcome the data challenge and reduce the cost of building a knowledge-grounded dialogue system, we explore the problem under a zero-resource setting by assuming no context-knowledge-response triples are needed for training. To this end, we propose representing the knowledge that bridges a context and a response and the way that the knowledge is expressed as latent variables, and devise a variational approach that can effectively estimate a generation model from a dialogue corpus and a knowledge corpus that are independent with each other. Evaluation results on three benchmarks of knowledge-grounded dialogue generation indicate that our model can achieve comparable performance with state-of-the-art methods that rely on knowledge-grounded dialogues for training, and exhibits a good generalization ability over different topics and different datasets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2402.17523",
        "category": "q-fin",
        "title": "Navigating Complexity: Constrained Portfolio Analysis in High Dimensions with Tracking Error and Weight Constraints",
        "abstract": "This paper analyzes the statistical properties of constrained portfolio formation in a high dimensional portfolio with a large number of assets. Namely, we consider portfolios with tracking error constraints, portfolios with tracking error jointly with weight (equality or inequality) restrictions, and portfolios with only weight restrictions. Tracking error is the portfolio's performance measured against a benchmark (an index usually), {\\color{black}{and weight constraints refers to specific allocation of assets within the portfolio, which often come in the form of regulatory requirement or fund prospectus.}} We show how these portfolios can be estimated consistently in large dimensions, even when the number of assets is larger than the time span of the portfolio. We also provide rate of convergence results for weights of the constrained portfolio, risk of the constrained portfolio and the Sharpe Ratio of the constrained portfolio. To achieve those results we use a new machine learning technique that merges factor models with nodewise regression in statistics. Simulation results and empirics show very good performance of our method.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-28.json",
        "arxivId": "2402.17526",
        "category": "q-fin",
        "title": "Political Pandering and Bureaucratic Influence",
        "abstract": "This paper examines the impact of bureaucracy on policy implementation in environments where electoral incentives generate pandering. A two-period model is developed to analyze the interactions between politicians and bureaucrats, who are categorized as either aligned -- sharing the voters' preferences over policies -- or intent on enacting policies that favor elite groups. The findings reveal equilibria in which aligned politicians resort to pandering, whereas aligned bureaucrats either support or oppose such behavior. The analysis further indicates that, depending on parameters, any level of bureaucratic influence can maximize the voters' welfare, ranging from scenarios with an all-powerful to a toothless bureaucracy.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2103.10872",
        "category": "q-fin",
        "title": "Optimal Clearing Payments in a Financial Contagion Model",
        "abstract": "Financial networks are characterized by complex structures of mutual obligations. These obligations are fulfilled entirely or in part (when defaults occur) via a mechanism called clearing, which determines a set of payments that settle the claims by respecting rules such as limited liability, absolute priority, and proportionality (pro-rated payments). In the presence of shocks on the financial system, however, the clearing mechanism may lead to cascaded defaults and eventually to financial disaster. In this paper, we first study the clearing model under pro-rated payments of Eisenberg and Noe, and we derive novel necessary and sufficient conditions for the uniqueness of the clearing payments, valid for an arbitrary topology of the financial network. Then, we argue that the proportionality rule is one of the factors responsible for cascaded defaults, and that the overall system loss can be reduced if this rule is lifted. The proposed approach thus shifts the focus from the individual interest to the overall system's interest to control and contain adverse effects of cascaded failures, and we show that clearing payments in this setting can be computed by solving suitable convex optimization problems.",
        "references": [
            {
                "arxivId": "2201.12898",
                "title": "Clearing Payments in Dynamic Financial Networks",
                "abstract": "This paper proposes a novel dynamical model for determining clearing payments in financial networks. We extend the classical Eisenberg-Noe model of financial contagion to multiple time periods, allowing financial operations to continue after possible initial pseudo defaults, thus permitting nodes to recover and eventually fulfil their liabilities. Optimal clearing payments in our model are computed by solving a suitable linear program, both in the full matrix payments case and in the pro-rata constrained case. We prove that the proposed model obeys the priority of debt claims requirement, that is, each node at every step either pays its liabilities in full, or it pays out all its balance. In the pro-rata case, the optimal dynamic clearing payments are unique, and can be determined via a time-decoupled sequential optimization approach."
            },
            {
                "arxivId": "2005.09066",
                "title": "Multi-period liability clearing via convex optimal control",
                "abstract": null
            },
            {
                "arxivId": "1912.04815",
                "title": "Equilibria and Systemic Risk in Saturated Networks",
                "abstract": "We undertake a fundamental study of network equilibria modeled as solutions of fixed-point equations for monotone linear functions with saturation nonlinearities. The considered model extends one originally proposed to study systemic risk in networks of financial institutions interconnected by mutual obligations. It is one of the simplest continuous models accounting for shock propagation phenomena and cascading failure effects. This model also characterizes Nash equilibria of constrained quadratic network games with strategic complementarities. We first derive explicit expressions for network equilibria and prove necessary and sufficient conditions for their uniqueness, encompassing and generalizing results available in the literature. Then, we study jump discontinuities of the network equilibria when the exogenous flows cross certain regions of measure 0 representable as graphs of continuous functions. Finally, we discuss some implications of our results in the two main motivating applications. In financial networks, this bifurcation phenomenon is responsible for how small shocks in the assets of a few nodes can trigger major aggregate losses to the system and cause the default of several agents. In constrained quadratic network games, it induces a blow-up behavior of the sensitivity of Nash equilibria with respect to the individual benefits."
            },
            {
                "arxivId": "1304.5590",
                "title": "Distributed Constrained Optimization by Consensus-Based Primal-Dual Perturbation Method",
                "abstract": "Various distributed optimization methods have been developed for solving problems which have simple local constraint sets and whose objective function is the sum of local cost functions of distributed agents in a network. Motivated by emerging applications in smart grid and distributed sparse regression, this paper studies distributed optimization methods for solving general problems which have a coupled global cost function and have inequality constraints. We consider a network scenario where each agent has no global knowledge and can access only its local mapping and constraint functions. To solve this problem in a distributed manner, we propose a consensus-based distributed primal-dual perturbation (PDP) algorithm. In the algorithm, agents employ the average consensus technique to estimate the global cost and constraint functions via exchanging messages with neighbors, and meanwhile use a local primal-dual perturbed subgradient method to approach a global optimum. The proposed PDP method not only can handle smooth inequality constraints but also non-smooth constraints such as some sparsity promoting constraints arising in sparse optimization. We prove that the proposed PDP algorithm converges to an optimal primal-dual solution of the original problem, under standard problem and network assumptions. Numerical results illustrating the performance of the proposed algorithm for a distributed demand response control problem in smart grid are also presented."
            },
            {
                "arxivId": "1304.2268",
                "title": "Gossips and Prejudices: Ergodic Randomized Dynamics in Social Networks",
                "abstract": "In this paper we study a new model of opinion dynamics in social networks, which has two main features. First, agents asynchronously interact in pairs, and these pairs are chosen according to a random process: following recent literature, we refer to this communication model as \u201cgossiping\u201d. Second, agents are not completely open-minded, but instead take into account their initial opinions, which may be thought of as their \u201cprejudices\u201d. In the literature, such agents are often called \u201cstubborn\u201d. We show that the opinions of the agents fail to converge, but persistently undergo ergodic oscillations, which asymptotically concentrate around a mean distribution of opinions. This mean value is exactly the limit of the synchronous dynamics of the expected opinions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2304.02356",
        "category": "q-fin",
        "title": "Unifying Market Microstructure and Dynamic Asset Pricing",
        "abstract": "We introduce a discrete binary tree for pricing contingent claims with the underlying security prices exhibiting history dependence characteristic of that induced by market microstructure phenomena. Example dependencies considered include moving average or autoregressive behavior. Our model is market-complete, arbitrage-free, and preserves all of the parameters governing the historical (natural world) price dynamics when passing to an equivalent martingale (risk-neutral) measure. Specifically, this includes the instantaneous mean and variance of the asset return and the instantaneous probabilities for the direction of asset price movement. We believe this is the first paper to demonstrate the ability to include market microstructure effects in dynamic asset/option pricing in a market-complete, no-arbitrage, format.",
        "references": [
            {
                "arxivId": "2011.08343",
                "title": "Option Pricing Incorporating Factor Dynamics in Complete Markets",
                "abstract": "Using the Donsker\u2013Prokhorov invariance principle, we extend the Kim\u2013Stoyanov\u2013Rachev\u2013Fabozzi option pricing model to allow for variably-spaced trading instances, an important consideration for short-sellers of options. Applying the Cherny\u2013Shiryaev\u2013Yor invariance principles, we formulate a new binomial path-dependent pricing model for discrete- and continuous-time complete markets where the stock price dynamics depends on the log-return dynamics of a market influencing factor. In the discrete case, we extend the results of this new approach to a financial market with informed traders employing a statistical arbitrage strategy involving trading of forward contracts. Our findings are illustrated with numerical examples employing US financial market data. Our work provides further support for the conclusion that any option pricing model must preserve valuable information on the instantaneous mean log-return, the probability of the stock\u2019s upturn movement (per trading interval), and other market microstructure features."
            },
            {
                "arxivId": "2006.02596",
                "title": "OPTION PRICING IN MARKETS WITH INFORMED TRADERS",
                "abstract": "The objective of this paper is to introduce the theory of option pricing for markets with informed traders within the framework of dynamic asset pricing theory. We introduce new models for option pricing for informed traders in complete markets, where we consider traders with information on the stock price direction and stock return mean. The Black\u2013Scholes\u2013Merton option pricing theory is extended for markets with informed traders, where price processes are following continuous-diffusions. By doing so, the discontinuity puzzle in option pricing is resolved. Using market option data, we estimate the implied surface of the probability for a stock upturn, the implied mean stock return surface, and implied trader information intensity surface."
            },
            {
                "arxivId": "1712.03566",
                "title": "Enhancing binomial and trinomial equity option pricing models",
                "abstract": null
            },
            {
                "arxivId": "1612.01979",
                "title": "Multi-Purpose Binomial Model: Fitting all Moments to the Underlying Geometric Brownian Motion",
                "abstract": null
            },
            {
                "arxivId": "1209.5254",
                "title": "BINARY MARKETS UNDER TRANSACTION COSTS",
                "abstract": "The goal of this work is to study binary market models with transaction costs, and to characterize their arbitrage opportunities. It has been already shown that the absence of arbitrage is related to the existence of \u03bb-consistent price systems (\u03bb-CPS), and, for this reason, we aim to provide conditions under which such systems exist. More precisely, we give a characterization for the smallest transaction cost \u03bbc (called \"critical\" \u03bb) starting from which one can construct a \u03bb-CPS. We also provide an expression for the set $\\mathcal{M}(\\lambda)$ of all probability measures inducing \u03bb-CPS. We show in particular that in the transition phase \"\u03bb = \u03bbc\" these sets are empty if and only if the frictionless market admits arbitrage opportunities. As an application, we obtain an explicit formula for \u03bbc depending only on the parameters of the model for homogeneous and also for some semi-homogeneous binary markets."
            },
            {
                "arxivId": "math/0702085",
                "title": "On a non-classical invariance principle",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2308.15672",
        "category": "q-fin",
        "title": "Asymptotics for short maturity Asian options in jump-diffusion models with local volatility",
        "abstract": "We present a study of the short maturity asymptotics for Asian options in a jump-diffusion model with a local volatility component, where the jumps are modeled as a compound Poisson process. The analysis for out-of-the-money Asian options is extended to models with L\\'evy jumps, including the exponential L\\'{e}vy model as a special case. Both fixed and floating strike Asian options are considered. Explicit results are obtained for the first-order asymptotics of the Asian options prices for a few popular models in the literature: the Merton jump-diffusion model, the double-exponential jump model, and the Variance Gamma model. We propose an analytical approximation for Asian option prices which satisfies the constraints from the short-maturity asymptotics, and test it against Monte Carlo simulations. The asymptotic results are in good agreement with numerical simulations for sufficiently small maturity.",
        "references": [
            {
                "arxivId": "1710.03160",
                "title": "Short Maturity Forward Start Asian Options in Local Volatility Models",
                "abstract": "ABSTRACT We study the short maturity asymptotics for prices of forward start Asian options under the assumption that the underlying asset follows a local volatility model. We obtain asymptotics for the cases of out-of-the-money, in-the-money, and at-the-money, considering both fixed strike and floating Asian options. The exponential decay of the price of an out-of-the-money forward start Asian option is handled using large deviations theory, and is controlled by a rate function which is given by a double-layer optimization problem. In the Black-Scholes model, the calculation of the rate function is simplified further to the solution of a non-linear equation. We obtain closed form for the rate function, as well as its asymptotic behavior when the strike is extremely large, small, or close to the initial price of the underlying asset."
            },
            {
                "arxivId": "1706.02408",
                "title": "MOST-LIKELY-PATH IN ASIAN OPTION PRICING UNDER LOCAL VOLATILITY MODELS",
                "abstract": "This paper addresses the problem of approximating the price of options on discrete and continuous arithmetic averages of the underlying, i.e. discretely and continuously monitored Asian options, in local volatility models. A \u201cpath-integral\u201d-type expression for option prices is obtained using a Brownian bridge representation for the transition density between consecutive sampling times and a Laplace asymptotic formula. In the limit where the sampling time window approaches zero, the option price is found to be approximated by a constrained variational problem on paths in time-price space. We refer to the optimizing path as the most-likely path (MLP). An approximation for the implied normal volatility follows accordingly. The small-time asymptotics and the existence of the MLP are also rigorously recovered using large deviation theory."
            },
            {
                "arxivId": "1609.07559",
                "title": "Short Maturity Asian Options in Local Volatility Models",
                "abstract": "We present a rigorous study of the short maturity asymptotics for Asian options with continuous-time averaging, under the assumption that the underlying asset follows a local volatility model. The asymptotics for out-of-the-money, in-the-money, and at-the-money cases are derived, considering both fixed strike and floating strike Asian options. The asymptotics for the out-of-the-money case involves a non-trivial variational problem which is solved completely. We present an analytical approximation for Asian options prices, and demonstrate good numerical agreement of the asymptotic results with the results of Monte Carlo simulations and benchmark test cases in the Black-Scholes model for option parameters relevant in practical applications."
            },
            {
                "arxivId": "0712.3485",
                "title": "Smart expansion and fast calibration for jump diffusions",
                "abstract": "Using Malliavin calculus techniques, we derive an analytical formula for the price of European options, for any model including local volatility and Poisson jump processes. We show that the accuracy of the formula depends on the smoothness of the payoff function. Our approach relies on an asymptotic expansion related to small diffusion and small jump frequency/size. Our formula has excellent accuracy (the error on implied Black\u2013Scholes volatilities for call options is smaller than 2\u00a0bp for various strikes and maturities). Additionally, model calibration becomes very rapid."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.17941",
        "category": "q-fin",
        "title": "Neural Networks for Portfolio-Level Risk Management: Portfolio Compression, Static Hedging, Counterparty Credit Risk Exposures and Impact on Capital Requirement",
        "abstract": "In this paper, we present an artificial neural network framework for portfolio compression of a large portfolio of European options with varying maturities (target portfolio) by a significantly smaller portfolio of European options with shorter or same maturity (compressed portfolio), which also represents a self-replicating static hedge portfolio of the target portfolio. For the proposed machine learning architecture, which is consummately interpretable by choice of design, we also define the algorithm to learn model parameters by providing a parameter initialisation technique and leveraging the optimisation methodology proposed in Lokeshwar and Jain (2024), which was initially introduced to price Bermudan options. We demonstrate the convergence of errors and the iterative evolution of neural network parameters over the course of optimization process, using selected target portfolio samples for illustration. We demonstrate through numerical examples that the Exposure distributions and Exposure profiles (Expected Exposure and Potential Future Exposure) of the target portfolio and compressed portfolio align closely across future risk horizons under risk-neutral and real-world scenarios. Additionally, we benchmark the target portfolio's Financial Greeks (Delta, Gamma, and Vega) against the compressed portfolio at future time horizons across different market scenarios generated by Monte-Carlo simulations. Finally, we compare the regulatory capital requirement under the standardised approach for counterparty credit risk of the target portfolio against the compressed portfolio and highlight that the capital requirement for the compact portfolio substantially reduces.",
        "references": [
            {
                "arxivId": "2302.00728",
                "title": "Data-driven Approach for Static Hedging of Exchange Traded Options",
                "abstract": "This paper presents a data-driven interpretable machine learning algorithm for semi-static hedging of Exchange Traded options, considering transaction costs with efficient run-time. Further, we provide empirical evidence on the performance of hedging longer-term National Stock Exchange (NSE) Index options using a self-replicating portfolio of shorter-term options and cash position, achieved by the automated algorithm, under different modeling assumptions and market conditions, including Covid period. We also systematically assess the model's performance using the Superior Predictive Ability (SPA) test by benchmarking against the static hedge proposed by Peter Carr and Liuren Wu and industry-standard dynamic hedging. We finally perform a thorough Profit and Loss (PnL) attribution analysis on the target option and hedge portfolios (dynamic and static) to discern the factors explaining the superior performance of static hedging."
            },
            {
                "arxivId": "1911.11362",
                "title": "Neural Network for Pricing and Universal Static Hedging of Contingent Claims",
                "abstract": "We present here a regress later based Monte Carlo approach that uses neural networks for pricing high-dimensional contingent claims. The choice of specific architecture of the neural networks used in the proposed algorithm provides for interpretability of the model, a feature that is often desirable in the financial context. Specifically, the interpretation leads us to demonstrate that any contingent claim -- possibly high dimensional and path-dependent -- under the Markovian and the no-arbitrage assumptions, can be semi-statically hedged using a portfolio of short maturity options. We show how the method can be used to obtain an upper and lower bound to the true price, where the lower bound is obtained by following a sub-optimal policy, while the upper bound by exploiting the dual formulation. Unlike other duality based upper bounds where one typically has to resort to nested simulation for constructing super-martingales, the martingales in the current approach come at no extra cost, without the need for any sub-simulations. We demonstrate through numerical examples the simplicity and efficiency of the method for both pricing and semi-static hedging of path-dependent options"
            },
            {
                "arxivId": "1705.07155",
                "title": "Compressing Over-the-Counter Markets",
                "abstract": "Over-the-counter markets are at the center of the global reform of the financial system. We show how the size and structure of these markets can undergo rapid and extensive changes when participants engage in portfolio compression, which is an optimization technology that exploits multilateral netting opportunities. We find that tightly knit and concentrated trading structures, as featured by many large over-the-counter markets, are especially susceptible to reductions of notional amounts and network reconfigurations resulting from compression activities. Using a unique transaction-level data set on credit-default-swaps markets, we estimate reduction levels, suggesting that the adoption of this technology can account for a large share of the historical development observed in these markets since the global financial crisis. Finally, we test the effect of a mandate to centrally clear over the counter markets in terms of size and structure. When participants engage in both central clearing and portfolio compression with the clearinghouse, we find large netting failures if clearinghouses proliferate. Allowing for compression across clearinghouses by and large offsets this adverse effect."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "math/0207260",
                "title": "Optimal portfolio selection and compression in an incomplete market",
                "abstract": "We investigate an optimal investment problem with a general performance criterion which, in particular, includes discontinuous functions. Prices are modelled as diffusions and the market is incomplete. We find an explicit solution for the case of limited diversification of the portfolio, i.e. for the portfolio compression problem. By this we mean that any admissible strategy may include no more than m different stocks concurrently, where m may be less than the total number n of available stocks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18014",
        "category": "q-fin",
        "title": "Set-valued Star-Shaped Risk Measures",
        "abstract": "In this paper, we introduce a new class of set-valued risk measures, named set-valued star-shaped risk measures. Motivated by the results of scalar monetary and star-shaped risk measures, this paper investigates the representation theorems in the set-valued framework. It is demonstrated that set-valued risk measures can be represented as the union of a family of set-valued convex risk measures, and set-valued normalized star-shaped risk measures can be represented as the union of a family of set-valued normalized convex risk measures. The link between set-valued risk measures and set-valued star-shaped risk measures is also established.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18047",
        "category": "q-fin",
        "title": "Prosocial and Financial Incentives for Biodiversity Conservation: A Field Experiment Using a Smartphone App",
        "abstract": "Ascertaining the number, type, and location of plant, insect, and animal species is essential for biodiversity conservation. However, comprehensively monitoring the situation only through public fixed-point surveys is challenging, and therefore information voluntarily provided by citizens assists in ascertaining the species distribution. To effectively encourage the citizens' data sharing behavior, this study proposed a prosocial incentive scheme in which, if they provide species information, donations are made to activities for saving endangered species. We conducted a field experiment with users (N=830) of a widely-used Japanese smartphone app to which they post species photos and measured the incentive's effect on their posting behavior. In addition, we measured the effect of a financial incentive scheme that provides monetary rewards for posting species photos and compared the two incentives' effects. The analyses revealed that while the prosocial incentive did not increase the number of posts on average, it did change the contents of posts, increasing the proportion of posts on rare species. On the contrary, the financial incentive statistically significantly increased the number of posts, in particular, on less rare and invasive species. Our findings suggest that the prosocial and financial incentives could stimulate different motivations and encourage different posting behaviors.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18119",
        "category": "q-fin",
        "title": "Modeling and Analysis of Crypto-Backed Over-Collateralized Stable Derivatives in DeFi",
        "abstract": "In decentralized finance (DeFi), stablecoins like DAI are designed to offer a stable value amidst the fluctuating nature of cryptocurrencies. We examine the class of crypto-backed stable derivatives, with a focus on mechanisms for price stabilization, which is exemplified by the well-known stablecoin DAI from MakerDAO. For simplicity, we focus on a single-collateral setting. We introduce a belief parameter to the simulation model of DAI in a previous work (DAISIM), reflecting market sentiments about the value and stability of DAI, and show that it better matches the expected behavior when this parameter is set to a sufficiently high value. We also propose a simple mathematical model of DAI price to explain its stability and dependency on ETH price. Finally, we analyze possible risk factors associated with these stable derivatives to provide valuable insights for stakeholders in the DeFi ecosystem.",
        "references": [
            {
                "arxivId": "2006.12388",
                "title": "Stablecoins 2.0: Economic Foundations and Risk-based Models",
                "abstract": "Stablecoins are one of the most widely capitalized type of cryptocurrency. However, their risks vary significantly according to their design and are often poorly understood. We seek to provide a sound foundation for stablecoin theory, with a risk-based functional characterization of the economic structure of stablecoins. First, we match existing economic models to the disparate set of custodial systems. Next, we characterize the unique risks that emerge in non-custodial stablecoins and develop a model framework that unifies existing models from economics and computer science. We further discuss how this modeling framework is applicable to a wide array of cryptoeconomic systems, including cross-chain protocols, collateralized lending, and decentralized exchanges. These unique risks yield unanswered research questions that will form the crux of research in decentralized finance going forward."
            },
            {
                "arxivId": "1906.06037",
                "title": "What is Stablecoin?: A Survey on Price Stabilization Mechanisms for Decentralized Payment Systems",
                "abstract": "Since the first theoretical concept of blockchains was proposed, over 100 digital currencies have been issued by online platformers as cryptocurrencies and traded by online consumers mainly in emerging countries. From the perspective of online payment systems, several studies have regarded blockchains as decentralized payment systems (DPSs), enabling international payment with lower cost and higher traceability with sophisticated peer-to-peer protocols in contrast to other centralized systems. Despite the advantages, DPSs are not chosen by the owners of online shops due to the high volatility of cryptocurrency prices. Stablecoins are cryptocurrencies with price stabilization mechanisms to match the price of another currency with lower volatility. Our motivation is to gather various price stabilization mechanisms for the purpose of comparing them from the perspective of implementation and enterprise usage. After dividing the methods into four collateral types (fiat, crypto, commodity, and non-collateralized) and two layers (protocol and application), we show that non-collateralized stablecoin on the application layer is the simplest approach for implementation. Moreover, we discuss their connection with traditional economic studies on Hayek money, Seigniorage Share, and Tobin tax. Some current stablecoin projects are also discussed and compared. This is the first survey of stablecoins to the best of our knowledge."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18135",
        "category": "q-fin",
        "title": "Manager Characteristics and SMEs' Restructuring Decisions: In-Court vs. Out-of-Court Restructuring",
        "abstract": "This study aims to empirically investigate the impact of managers' characteristics on their choice between in-court and out-of-court restructuring. Based on the theory of upper echelons, we tested the preferences of 342 managers of financially distressed French firms regarding restructuring decisions. The overall findings of this study provide empirical support for the upper echelons theory. Specifically, managers with a long tenure and those with a high level of education are less likely to restructure before the court and are more likely to restructure privately. The findings also indicate that managers' age and gender do not significantly affect their choice between in-court and out-of-court restructuring. This study contributes to the literature on bankruptcy and corporate restructuring by turning the focus from firm characteristics to manager characteristics to explain restructuring decisions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18435",
        "category": "q-fin",
        "title": "Stochastic User Equilibrium Model with a Bounded Perceived Travel Time",
        "abstract": "Stochastic User Equilibrium (SUE) models depict the perception differences in traffic assignment problems. According to the assumption of an unbounded perceived travel time distribution, the conventional SUE problems result in a positive choice probability for all available routes, regardless of their unappealing travel time. This study provides an eUnit-SUE model to relax this assumption. The eUnit model is derived from a bounded probability distribution. This closed-form model aligns with an exponentiated random utility maximization (ERUM) paradigm with the exponentiated uniform distributed random error, where the lower and upper bounds endogeneously determine the route usage. Specifically, a Beckmann-type mathematical programming formulation is presented for the eUnit-SUE problem. The equivalency and uniqueness properties are rigorously proven. Numerical examples reveal that the eUnit bound range between the lower and upper bounds greatly affects the SUE assignment results. A larger bound range increases not only the number of routes in the choice set but also the degree of dispersion in the assignment results due to a larger route-specific perception variance. The misperception is contingent upon the disparity between the shortest and longest travel times and the bounds. As the bound range decreases, the shortest route receives significant flow allocation, and the assignment result approaches the deterministic user equilibrium (DUE) flow pattern.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18452",
        "category": "q-fin",
        "title": "Social Learning with Intrinsic Preferences",
        "abstract": "Despite strong evidence for peer effects, little is known about how individuals balance intrinsic preferences and social learning in different choice environments. Using a combination of experiments and discrete choice modeling, we show that intrinsic preferences and social learning jointly influence participants' decisions, but their relative importance varies across choice tasks and environments. Intrinsic preferences guide participants' decisions in a subjective choice task, while social learning determines participants' decisions in a task with an objectively correct solution. A choice environment in which people expect to be rewarded for their choices reinforces the influence of intrinsic preferences, whereas an environment in which people expect to be punished for their choices reinforces conformist social learning. We use simulations to discuss the implications of these findings for the polarization of behavior.",
        "references": [
            {
                "arxivId": "1507.04544",
                "title": "Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC",
                "abstract": null
            },
            {
                "arxivId": "1212.4693",
                "title": "A General Metric for Riemannian Manifold Hamiltonian Monte Carlo",
                "abstract": null
            },
            {
                "arxivId": "1004.2316",
                "title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory",
                "abstract": "In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2\u03bb/n, where \u03bb is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-02-29.json",
        "arxivId": "2402.18459",
        "category": "q-fin",
        "title": "In-Person, Hybrid or Remote? Employers' Perspectives on the Future of Work Post-Pandemic",
        "abstract": "We present an employer-side perspective on remote work through the pandemic using data from top executives of 129 employers in North America. Our analysis suggests that at least some of the pandemic-accelerated changes to the work location landscape will likely stick; with some form of hybrid work being the norm. However, the patterns will vary by department (HR/legal/sales/IT, etc.) and by sector of operations. Top three concerns among employers include: supervision and mentoring, reduction in innovation, and creativity; and the top three benefits include their ability to retain / recruit talent, positive impact on public image and their ability to compete. An Ordered Probit model of the expected April 2024 work location strategy revealed that those in transportation, warehousing, and manufacturing sectors, those with a fully in-person approach to work pre-COVID, and those with a negative outlook towards the impact of remote work are likely to be more in-person-centered, while those with fully remote work approach in April 2020 are likely to be less in-person-centered. Lastly, we present data on resumption of business travel, in-person client interactions and changes in office space reconfigurations that employers have made since the beginning of the pandemic.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-01.json",
        "arxivId": "1903.11198",
        "category": "q-fin",
        "title": "Parallel Experimentation and Competitive Interference on Online Advertising Platforms",
        "abstract": "This paper studies the measurement of advertising effects on online platforms when parallel experimentation occurs, that is, when multiple advertisers experiment concurrently. It provides a framework that makes precise how parallel experimentation affects the experiment's value: while ignoring parallel experimentation yields an estimate of the average effect of advertising in-place, which has limited value in decision-making in an environment with variable advertising competition, accounting for parallel experimentation captures the actual uncertainty advertisers face due to competitive actions. It then implements an experimental design that enables the estimation of these effects on JD.com, a large e-commerce platform that is also a publisher of digital ads. Using traditional and kernel-based estimators, it shows that not accounting for competitive actions can result in the advertiser inaccurately estimating the advertising lift by a factor of two or higher, which can be consequential for decision-making.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-01.json",
        "arxivId": "2207.04856",
        "category": "q-fin",
        "title": "Research Joint Ventures: The Role of Financial Constraints",
        "abstract": "This paper provides a novel theory of research joint ventures for financially constrained firms. When firms choose R&D portfolios, an RJV can help to coordinate research efforts, reducing investments in duplicate projects. This can free up resources, increase the variety of pursued projects and thereby increase the probability of discovering the innovation. RJVs improve innovation outcomes when market competition is weak and external financing conditions are bad. An RJV may increase the innovation probability and nevertheless lower total R&D costs. RJVs that increase innovation also increase consumer surplus and tend to be profitable, but innovation-reducing RJVs also exist. Finally, we compare RJVs to innovation-enhancing mergers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-01.json",
        "arxivId": "2303.14947",
        "category": "q-fin",
        "title": "Measuring Self-Preferencing on Digital Platforms",
        "abstract": "Digital platforms use recommendations to facilitate exchanges between platform actors, such as trade between buyers and sellers. Aiming to protect consumers and guarantee fair competition on platforms, legislators increasingly require that recommendations on market-dominating platforms be free from self-preferencing. That is, platforms that also act as sellers (e.g., Amazon) or information providers (e.g., Google) must not prefer their own offers over comparable third-party offers. Yet, successful enforcement of self-preferencing bans -- to the potential benefit of consumers and third-party actors -- requires defining and measuring self-preferencing across a platform. In the context of recommendations through search results, this research contributes by i) conceptualizing a\"recommendation\"as an offer's level of search engine visibility across an entire platform (instead of its position in specific search queries, as in previous research); ii) discussing two tests for self-preferencing, and iii) implementing them in two empirical studies across three international Amazon marketplaces. Contrary to consumer expectations and emerging literature, our analysis finds almost no evidence for self-preferencing. A survey reveals that even if Amazon were proven to engage in self-preferencing, most consumers would not change their shopping behavior on the platform -- highlighting Amazon's significant market power and suggesting the need for robust protections for sellers and consumers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-01.json",
        "arxivId": "2402.18764",
        "category": "q-fin",
        "title": "An Analytical Approach to (Meta)Relational Models Theory, and its Application to Triple Bottom Line (Profit, People, Planet) -- Towards Social Relations Portfolio Management",
        "abstract": "Investigating the optimal nature of social interactions among generic actors (e.g., people or firms), aiming to achieve specifically-agreed objectives, has been the subject of extensive academic research. Using the relational models theory - comprehensively describing all social interactions among actors as combinations of only four forms of sociality: communal sharing, authority ranking, equality matching, and market pricing - the common approach within the literature revolves around qualitative assessments of the sociality models' configurations most effective in realizing predefined purposes, at times supplemented by empirical data. In this treatment, we formulate this question as a mathematical optimization problem, in order to quantitatively determine the best possible configurations of sociality forms between dyadic actors which would optimize their mutually-agreed objectives. For this purpose, we develop an analytical framework for quantifying the (meta)relational models theory, and mathematically demonstrate that combining the four sociality forms within a specific meaningful social interaction inevitably prompts an inherent tension among them, through a single elementary and universal metarelation. In analogy with financial portfolio management, we subsequently introduce the concept of Social Relations Portfolio (SRP) management, and propose a generalizable procedural methodology capable of quantitatively identifying the efficient SRP for any objective involving meaningful social relations. As an important illustration, the methodology is applied to the Triple Bottom Line paradigm to derive its efficient SRP, guiding practitioners in precisely measuring, monitoring, reporting and (proactively) steering stakeholder management efforts regarding Corporate Social Responsibility (CSR) and Environmental, Social and Governance (ESG) within and / or across organizations.",
        "references": [
            {
                "arxivId": "1312.4644",
                "title": "A Generic Model of Dyadic Social Relationships",
                "abstract": "We introduce a model of dyadic social interactions and establish its correspondence with relational models theory (RMT), a theory of human social relationships. RMT posits four elementary models of relationships governing human interactions, singly or in combination: Communal Sharing, Authority Ranking, Equality Matching, and Market Pricing. To these are added the limiting cases of asocial and null interactions, whereby people do not coordinate with reference to any shared principle. Our model is rooted in the observation that each individual in a dyadic interaction can do either the same thing as the other individual, a different thing or nothing at all. To represent these three possibilities, we consider two individuals that can each act in one out of three ways toward the other: perform a social action X or Y, or alternatively do nothing. We demonstrate that the relationships generated by this model aggregate into six exhaustive and disjoint categories. We propose that four of these categories match the four relational models, while the remaining two correspond to the asocial and null interactions defined in RMT. We generalize our results to the presence of N social actions. We infer that the four relational models form an exhaustive set of all possible dyadic relationships based on social coordination. Hence, we contribute to RMT by offering an answer to the question of why there could exist just four relational models. In addition, we discuss how to use our representation to analyze data sets of dyadic social interactions, and how social actions may be valued and matched by the agents."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-01.json",
        "arxivId": "2402.19203",
        "category": "q-fin",
        "title": "On non-negative solutions of stochastic Volterra equations with jumps and non-Lipschitz coefficients",
        "abstract": "We consider one-dimensional stochastic Volterra equations with jumps for which we establish conditions upon the convolution kernel and coefficients for the strong existence and pathwise uniqueness of a non-negative c\\`adl\\`ag solution. By using the approach recently developed in arXiv:2302.07758, we show the strong existence by using a nonnegative approximation of the equation whose convergence is proved via a variant of the Yamada--Watanabe approximation technique. We apply our results to L\\'evy-driven stochastic Volterra equations. In particular, we are able to define a Volterra extension of the so-called alpha-stable Cox--Ingersoll--Ross process, which is especially used for applications in Mathematical Finance.",
        "references": [
            {
                "arxivId": "2302.07758",
                "title": "Nonnegativity preserving convolution kernels. Application to Stochastic Volterra Equations in closed convex domains and their approximation",
                "abstract": "This work defines and studies convolution kernels that preserve nonnegativity. When the past dynamics of a process is integrated with a convolution kernel like in Stochastic Volterra Equations or in the jump intensity of Hawkes processes, this property allows to get the nonnegativity of the integral. We give characterizations of these kernels and show in particular that completely monotone kernels preserve nonnegativity. We then apply these results to analyze the stochastic invariance of a closed convex set by Stochastic Volterra Equations. We also get a comparison result in dimension one. Last, when the kernel is a positive linear combination of decaying exponential functions, we present a second order approximation scheme for the weak error that stays in the closed convex domain under suitable assumptions. We apply these results to the rough Heston model and give numerical illustrations."
            },
            {
                "arxivId": "2210.12393",
                "title": "The Rough Hawkes Heston Stochastic Volatility Model",
                "abstract": "We study an extension of the Heston stochastic volatility model that incorporates rough volatility and jump clustering phenomena. In our model, named the rough Hawkes Heston stochastic volatility model, the spot variance is a rough Hawkes-type process proportional to the intensity process of the jump component appearing in the dynamics of the spot variance itself and the log returns. The model belongs to the class of affine Volterra models. In particular, the Fourier-Laplace transform of the log returns and the square of the volatility index can be computed explicitly in terms of solutions of deterministic Riccati-Volterra equations, which can be efficiently approximated using a multi-factor approximation technique. We calibrate a parsimonious specification of our model characterized by a power kernel and an exponential law for the jumps. We show that our parsimonious setup is able to simultaneously capture, with a high precision, the behavior of the implied volatility smile for both S&P 500 and VIX options. In particular, we observe that in our setting the usual shift in the implied volatility of VIX options is explained by a very low value of the power in the kernel. Our findings demonstrate the relevance, under an affine framework, of rough volatility and self-exciting jumps in order to capture the joint evolution of the S&P 500 and VIX."
            },
            {
                "arxivId": "1911.02906",
                "title": "Multiple yield curve modelling with CBI processes",
                "abstract": "We develop a modelling framework for multiple yield curves driven by continuous-state branching processes with immigration (CBI processes). Exploiting the self-exciting behavior of CBI jump processes, this approach can reproduce the relevant empirical features of spreads between different interbank rates. In particular, we introduce multi-curve models driven by a flow of tempered alpha-stable CBI processes. Such models are especially parsimonious and tractable, and can generate contagion effects among different spreads. We provide a complete analytical framework, including a detailed study of discounted exponential moments of CBI processes. The proposed approach allows for explicit valuation formulae for all linear interest rate derivatives and semi-closed formulae for non-linear derivatives via Fourier techniques and quantization. We show that a simple specification of the model can be successfully calibrated to market data."
            },
            {
                "arxivId": "1812.01914",
                "title": "The Alpha\u2010Heston stochastic volatility model",
                "abstract": "We introduce an affine extension of the Heston model, called the \u03b1 \u2010Heston model, where the instantaneous variance process contains a jump part driven by \u03b1 \u2010stable processes with \u03b1\u2208(1,2] . In this framework, we examine the implied volatility and its asymptotic behavior for both asset and VIX options. Furthermore, we study the jump clustering phenomenon observed on the market. We provide a jump cluster decomposition for the variance process where each cluster is induced by a \u201cmother jump\u201d representing a triggering shock followed by \u201csecondary jumps\u201d characterizing the contagion impact."
            },
            {
                "arxivId": "1804.08070",
                "title": "On a positivity preserving numerical scheme for jump-extended CIR process: the alpha-stable case",
                "abstract": null
            },
            {
                "arxivId": "1712.09220",
                "title": "On the Euler\u2013Maruyama scheme for spectrally one-sided L\u00e9vy driven SDEs with H\u00f6lder continuous coefficients",
                "abstract": null
            },
            {
                "arxivId": "1609.02108",
                "title": "The characteristic function of rough Heston models",
                "abstract": "It has been recently shown that rough volatility models, where the volatility is driven by a fractional Brownian motion with small Hurst parameter, provide very relevant dynamics in order to reproduce the behavior of both historical and implied volatilities. However, due to the non\u2010Markovian nature of the fractional Brownian motion, they raise new issues when it comes to derivatives pricing. Using an original link between nearly unstable Hawkes processes and fractional volatility models, we compute the characteristic function of the log\u2010price in rough Heston models. In the classical Heston model, the characteristic function is expressed in terms of the solution of a Riccati equation. Here, we show that rough Heston models exhibit quite a similar structure, the Riccati equation being replaced by a fractional Riccati equation."
            },
            {
                "arxivId": "1602.05541",
                "title": "Alpha-CIR model with branching processes in sovereign interest rate modeling",
                "abstract": null
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            },
            {
                "arxivId": "1301.3243",
                "title": "Asymptotic properties of estimators in a stable Cox\u2013Ingersoll\u2013Ross model",
                "abstract": null
            },
            {
                "arxivId": "0802.0933",
                "title": "Stochastic equations of non-negative processes with jumps",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-01.json",
        "arxivId": "2402.19421",
        "category": "q-fin",
        "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines",
        "abstract": "In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their\"cognitive\"processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines.",
        "references": [
            {
                "arxivId": "2306.15033",
                "title": "Sea Change in Software Development: Economic and Productivity Analysis of the AI-Powered Developer Lifecycle",
                "abstract": "This study examines the impact of GitHub Copilot on a large sample of Copilot users (n=934,533). The analysis shows that users on average accept nearly 30% of the suggested code, leading to increased productivity. Furthermore, our research demonstrates that the acceptance rate rises over time and is particularly high among less experienced developers, providing them with substantial benefits. Additionally, our estimations indicate that the adoption of generative AI productivity tools could potentially contribute to a $1.5 trillion increase in global GDP by 2030. Moreover, our investigation sheds light on the diverse contributors in the generative AI landscape, including major technology companies, startups, academia, and individual developers. The findings suggest that the driving force behind generative AI software innovation lies within the open-source ecosystem, particularly in the United States. Remarkably, a majority of repositories on GitHub are led by individual developers. As more developers embrace these tools and acquire proficiency in the art of prompting with generative AI, it becomes evident that this novel approach to software development has forged a unique inextricable link between humans and artificial intelligence. This symbiotic relationship has the potential to shape the construction of the world's software for future generations."
            },
            {
                "arxivId": "2305.12763",
                "title": "The emergence of economic rationality of GPT.",
                "abstract": "As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory. We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms."
            },
            {
                "arxivId": "2212.04037",
                "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "abstract": "Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance."
            },
            {
                "arxivId": "2209.07858",
                "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
                "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models."
            },
            {
                "arxivId": "2204.07705",
                "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\u2014training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models."
            },
            {
                "arxivId": "2203.02155",
                "title": "Training language models to follow instructions with human feedback",
                "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
            },
            {
                "arxivId": "2112.09332",
                "title": "WebGPT: Browser-assisted question-answering with human feedback",
                "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit."
            },
            {
                "arxivId": "2005.11401",
                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
            },
            {
                "arxivId": "2001.09768",
                "title": "Artificial Intelligence, Values, and Alignment",
                "abstract": null
            },
            {
                "arxivId": "1806.00069",
                "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
                "abstract": "There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "1910.14023",
        "category": "q-fin",
        "title": "Firm Entry and Exit with Unbounded Productivity Growth",
        "abstract": "In Hopenhayn's (1992) entry-exit model productivity is bounded, implying that the predicted firm size distribution cannot match the power law tail observable in the data. In this paper we remove the boundedness assumption and, in this more general setting, provide an exact characterization of existence of stationary equilibria, as well as a novel sufficient condition for existence based on treating production as a Lyapunov function. We also provide new representations of the rate of entry and aggregate supply. Finally, we prove that the firm size distribution has a power law tail under a very broad set of productivity growth specifications.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2203.05593",
        "category": "q-fin",
        "title": "Labor Demand on a Tight Leash",
        "abstract": "We develop a labor demand model that encompasses pre-match hiring cost arising from tight labor markets. Through the lens of the model, we study the effect of labor market tightness on firms' labor demand by applying novel shift-share instruments to the universe of German firms. In line with theory, we find that a doubling in tightness reduces firms' employment by 5 percent. Taking into account the resulting search externalities, the wage elasticity of firms' labor demand reduces from -0.7 to -0.5 through reallocation effects. In light of our results, pre-match hiring cost amount to 40 percent of annual wage payments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2306.03303",
        "category": "q-fin",
        "title": "Global universal approximation of functional input maps on weighted spaces",
        "abstract": "We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family to map the input weighted space to the hidden layer, on which a non-linear scalar activation function is applied to each neuron, and finally return the output via some linear readouts. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result on weighted spaces for continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and emphasize that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gaussian processes. This paves a way towards uncertainty quantification for signature kernel regression.",
        "references": [
            {
                "arxivId": "2310.02322",
                "title": "Signature Methods in Stochastic Portfolio Theory",
                "abstract": "In the context of stochastic portfolio theory we introduce a novel class of portfolios which we call linear path-functional portfolios. These are portfolios which are determined by certain transformations of linear functions of a collections of feature maps that are non-anticipative path functionals of an underlying semimartingale. As main example for such feature maps we consider the signature of the (ranked) market weights. We prove that these portfolios are universal in the sense that every continuous, possibly path-dependent, portfolio function of the market weights can be uniformly approximated by signature portfolios. We also show that signature portfolios can approximate the growth-optimal portfolio in several classes of non-Markovian market models arbitrarily well and illustrate numerically that the trained signature portfolios are remarkably close to the theoretical growth-optimal portfolios. Besides these universality features, the main numerical advantage lies in the fact that several optimization tasks like maximizing (expected) logarithmic wealth or mean-variance optimization within the class of linear path-functional portfolios reduce to a convex quadratic optimization problem, thus making it computationally highly tractable. We apply our method also to real market data based on several indices. Our results point towards out-performance on the considered out-of-sample data, also in the presence of transaction costs."
            },
            {
                "arxivId": "2304.00490",
                "title": "Infinite-dimensional reservoir computing",
                "abstract": "Reservoir computing approximation and generalization bounds are proved for a new concept class of input/output systems that extends the so-called generalized Barron functionals to a dynamic context. This new class is characterized by the readouts with a certain integral representation built on infinite-dimensional state-space systems. It is shown that this class is very rich and possesses useful features and universal approximation properties. The reservoir architectures used for the approximation and estimation of elements in the new class are randomly generated echo state networks with either linear or ReLU activation functions. Their readouts are built using randomly generated neural networks in which only the output layer is trained (extreme learning machines or random feature neural networks). The results in the paper yield a fully implementable recurrent neural network-based learning algorithm with provable convergence guarantees that do not suffer from the curse of dimensionality."
            },
            {
                "arxivId": "2302.01362",
                "title": "Signature SDEs from an affine and polynomial perspective",
                "abstract": "Signature stochastic differential equations (SDEs) constitute a large class of stochastic processes, here driven by Brownian motions, whose characteristics are entire or real-analytic functions of their own signature, i.e. of iterated integrals of the process with itself, and allow therefore for a generic path dependence. We show that their prolongation with the corresponding signature is an affine and polynomial process taking values in subsets of group-like elements of the extended tensor algebra. By relying on the duality theory for affine and polynomial processes we obtain explicit formulas in terms of novel and proper notions of converging power series for the Fourier-Laplace transform and the expected value of entire functions of the signature process. The coefficients of these power series are solutions of extended tensor algebra valued Riccati and linear ordinary differential equations (ODEs), respectively, whose vector fields can be expressed in terms of the entire characteristics of the corresponding SDEs. In other words, we construct a class of stochastic processes, which is universal within It\\^o processes with path-dependent characteristics and which allows for a relatively explicit characterization of the Fourier-Laplace transform and hence the full law on path space. We also analyze the special case of one-dimensional signature SDEs, which correspond to classical SDEs with real-analytic characteristics. Finally, the practical feasibility of this affine and polynomial approach is illustrated by several numerical examples."
            },
            {
                "arxivId": "2209.10166",
                "title": "Chaotic Hedging with Iterated Integrals and Neural Networks",
                "abstract": "In this paper, we extend the Wiener-Ito chaos decomposition to the class of diffusion processes, whose drift and diffusion coefficient are of linear growth. By omitting the orthogonality in the chaos expansion, we are able to show that every $p$-integrable functional, for $p \\in [1,\\infty)$, can be represented as sum of iterated integrals of the underlying process. Using a truncated sum of this expansion and (possibly random) neural networks for the integrands, whose parameters are learned in a machine learning setting, we show that every financial derivative can be approximated arbitrarily well in the $L^p$-sense. Since the hedging strategy of the approximating option can be computed in closed form, we obtain an efficient algorithm that can replicate any integrable financial derivative with short runtime."
            },
            {
                "arxivId": "2202.08653",
                "title": "Convex monotone semigroups and their generators with respect to $\\Gamma$-convergence",
                "abstract": "We study semigroups of convex monotone operators on spaces of continuous functions and their behaviour with respect to $\\Gamma$-convergence. In contrast to the linear theory, the domain of the generator is, in general, not invariant under the semigroup. To overcome this issue, we consider different versions of invariant Lipschitz sets which turn out to be suitable domains for weaker notions of the generator. The so-called $\\Gamma$-generator is defined as the time derivative with respect to $\\Gamma$-convergence in the space of upper semicontinuous functions. Under suitable assumptions, we show that the $\\Gamma$-generator uniquely characterizes the semigroup and is determined by its evaluation at smooth functions. Furthermore, we provide Chernoff approximation results for convex monotone semigroups and show that approximation schemes based on the same infinitesimal behaviour lead to the same semigroup. Our results are applied to semigroups related to stochastic optimal control problems in finite and infinite-dimensional settings as well as Wasserstein perturbations of transition semigroups."
            },
            {
                "arxivId": "2201.02441",
                "title": "Applications of Signature Methods to Market Anomaly Detection",
                "abstract": "Anomaly detection is the process of identifying abnormal instances or events in data sets which deviate from the norm significantly. In this study, we propose a signatures based machine learning algorithm to detect rare or unexpected items in a given data set of time series type. We present applications of signature or randomized signature as feature extractors for anomaly detection algorithms; additionally we provide an easy, representation theoretic justification for the construction of randomized signatures. Our first application is based on synthetic data and aims at distinguishing between real and fake trajectories of stock prices, which are indistinguishable by visual inspection. We also show a real life application by using transaction data from the cryptocurrency market. In this case, we are able to identify pump and dump attempts organized on social networks with F1 scores up to 88% by means of our unsupervised learning algorithm, thus achieving results that are close to the state-of-the-art in the field based on supervised learning."
            },
            {
                "arxivId": "2111.01207",
                "title": "Sig-wasserstein GANs for time series generation",
                "abstract": "Synthetic data is an emerging technology that can significantly accelerate the development and deployment of AI machine learning pipelines. In this work, we develop high-fidelity time-series generators, the SigWGAN, by combining continuous-time stochastic models with the newly proposed signature W1 metric. The former are the Logsig-RNN models based on the stochastic differential equations, whereas the latter originates from the universal and principled mathematical features to characterize the measure induced by time series. SigWGAN allows turning computationally challenging GAN min-max problem into supervised learning while generating high fidelity samples. We validate the proposed model on both synthetic data generated by popular quantitative risk models and empirical financial data. Codes are available at https://github.com/SigCGANs/Sig-Wasserstein-GANs.git"
            },
            {
                "arxivId": "2109.13512",
                "title": "Neural networks in Fr\u00e9chet spaces",
                "abstract": null
            },
            {
                "arxivId": "2107.00447",
                "title": "Weighted signature kernels",
                "abstract": "Suppose that $\\gamma$ and $\\sigma$ are two continuous bounded variation paths which take values in a finite-dimensional inner product space $V$. Recent papers have introduced the truncated and the untruncated signature kernel of $\\gamma$ and $\\sigma$, and showed how these concepts can be used in classification and prediction tasks involving multivariate time series. In this paper, we introduce general signature kernels and show how they can be interpreted, in many examples, as an average of PDE solutions, and hence how they can be computed using suitable quadrature rules. We extend this analysis to derive closed-form formulae for expressions involving the expected (Stratonovich) signature of Brownian motion. In doing so, we articulate a novel connection between signature kernels and the hyperbolic development map, the latter of which has been a broadly useful tool in the analysis of the signature. As an application we evaluate the use of different general signature kernels as the basis for non-parametric goodness-of-fit tests to Wiener measure on path space."
            },
            {
                "arxivId": "2105.11053",
                "title": "Arbitrage-Free Neural-SDE Market Models",
                "abstract": "Modelling joint dynamics of liquid vanilla options is crucial for arbitrage-free pricing of illiquid derivatives and managing risks of option trade books. This paper develops a nonparametric model for the European options book respecting underlying financial constraints and while being practically implementable. We derive a state space for prices which are free from static (or model-independent) arbitrage and study the inference problem where a model is learnt from discrete time series data of stock and option prices. We use neural networks as function approximators for the drift and diffusion of the modelled SDE system, and impose constraints on the neural nets such that no-arbitrage conditions are preserved. In particular, we give methods to calibrate neural SDE models which are guaranteed to satisfy a set of linear inequalities. We validate our approach with numerical experiments using data generated from a Heston stochastic local volatility model."
            },
            {
                "arxivId": "2105.00778",
                "title": "Optimal stopping with signatures",
                "abstract": "We propose a new method for solving optimal stopping problems (such as American option pricing in finance) under minimal assumptions on the underlying stochastic process $X$. We consider classic and randomized stopping times represented by linear and non-linear functionals of the rough path signature $\\mathbb{X}^{<\\infty}$ associated to $X$, and prove that maximizing over these classes of signature stopping times, in fact, solves the original optimal stopping problem. Using the algebraic properties of the signature, we can then recast the problem as a (deterministic) optimization problem depending only on the (truncated) expected signature $\\mathbb{E}\\left[ \\mathbb{X}^{\\le N}_{0,T} \\right]$. By applying a deep neural network approach to approximate the non-linear signature functionals, we can efficiently solve the optimal stopping problem numerically. The only assumption on the process $X$ is that it is a continuous (geometric) random rough path. Hence, the theory encompasses processes such as fractional Brownian motion, which fail to be either semi-martingales or Markov processes, and can be used, in particular, for American-type option pricing in fractional models, e.g. on financial or electricity markets."
            },
            {
                "arxivId": "2102.03657",
                "title": "Neural SDEs as Infinite-Dimensional GANs",
                "abstract": "Stochastic differential equations (SDEs) are a staple of mathematical modelling of temporal dynamics. However, a fundamental limitation has been that such models have typically been relatively inflexible, which recent work introducing Neural SDEs has sought to solve. Here, we show that the current classical approach to fitting SDEs may be approached as a special case of (Wasserstein) GANs, and in doing so the neural and classical regimes may be brought together. The input noise is Brownian motion, the output samples are time-evolving paths produced by a numerical solver, and by parameterising a discriminator as a Neural Controlled Differential Equation (CDE), we obtain Neural SDEs as (in modern machine learning parlance) continuous-time generative time series models. Unlike previous work on this problem, this is a direct extension of the classical approach without reference to either prespecified statistics or density functions. Arbitrary drift and diffusions are admissible, so as the Wasserstein loss has a unique global minima, in the infinite data limit any SDE may be learnt. Example code has been made available as part of the \\texttt{torchsde} repository."
            },
            {
                "arxivId": "2010.14615",
                "title": "Discrete-Time Signatures and Randomness in Reservoir Computing",
                "abstract": "A new explanation of the geometric nature of the reservoir computing (RC) phenomenon is presented. RC is understood in the literature as the possibility of approximating input\u2013output systems with randomly chosen recurrent neural systems and a trained linear readout layer. Light is shed on this phenomenon by constructing what is called strongly universal reservoir systems as random projections of a family of state-space systems that generate Volterra series expansions. This procedure yields a state-affine reservoir system with randomly generated coefficients in a dimension that is logarithmically reduced with respect to the original system. This reservoir system is able to approximate any element in the fading memory filters class just by training a different linear readout for each different filter. Explicit expressions for the probability distributions needed in the generation of the projected reservoir system are stated, and bounds for the committed approximation error are provided."
            },
            {
                "arxivId": "2007.04154",
                "title": "Robust Pricing and Hedging via Neural SDEs",
                "abstract": "Mathematical modelling is ubiquitous in the financial industry and drives key decision processes. Any given model provides only a crude approximation to reality and the risk of using an inadequate model is hard to detect and quantify. By contrast, modern data science techniques are opening the door to more robust and data-driven model selection mechanisms. However, most machine learning models are \"black-boxes\" as individual parameters do not have meaningful interpretation. The aim of this paper is to combine the above approaches achieving the best of both worlds. Combining neural networks with risk models based on classical stochastic differential equations (SDEs), we find robust bounds for prices of derivatives and the corresponding hedging strategies while incorporating relevant market data. The resulting model called neural SDE is an instantiation of generative models and is closely linked with the theory of causal optimal transport. Neural SDEs allow consistent calibration under both the risk-neutral and the real-world measures. Thus the model can be used to simulate market scenarios needed for assessing risk profiles and hedging strategies. We develop and analyse novel algorithms needed for efficient use of neural SDEs. We validate our approach with numerical experiments using both local and stochastic volatility models."
            },
            {
                "arxivId": "2006.14794",
                "title": "The Signature Kernel Is the Solution of a Goursat PDE",
                "abstract": "Recently, there has been an increased interest in the development of kernel methods for learning with sequential data. The signature kernel is a learning tool with potential to handle irregularly sampled, multivariate time series. In\"Kernels for sequentially ordered data\"the authors introduced a kernel trick for the truncated version of this kernel avoiding the exponential complexity that would have been involved in a direct computation. Here we show that for continuously differentiable paths, the signature kernel solves a hyperbolic PDE and recognize the connection with a well known class of differential equations known in the literature as Goursat problems. This Goursat PDE only depends on the increments of the input sequences, does not require the explicit computation of signatures and can be solved efficiently using state-of-the-arthyperbolic PDE numerical solvers, giving a kernel trick for the untruncated signature kernel, with the same raw complexity as the method from\"Kernels for sequentially ordered data\", but with the advantage that the PDE numerical scheme is well suited for GPU parallelization, which effectively reduces the complexity by a full order of magnitude in the length of the input sequences. In addition, we extend the previous analysis to the space of geometric rough paths and establish, using classical results from rough path theory, that the rough version of the signature kernel solves a rough integral equation analogous to the aforementioned Goursat PDE. Finally, we empirically demonstrate the effectiveness of our PDE kernel as a machine learning tool in various machine learning applications dealing with sequential data. We release the library sigkernel publicly available at https://github.com/crispitagorico/sigkernel."
            },
            {
                "arxivId": "2006.02341",
                "title": "Non-Euclidean Universal Approximation",
                "abstract": "Modifications to a neural network's input and output layers are often required to accommodate the specificities of most practical learning tasks. However, the impact of such changes on architecture's approximation capabilities is largely not understood. We present general conditions describing feature and readout maps that preserve an architecture's ability to approximate any continuous functions uniformly on compacts. As an application, we show that if an architecture is capable of universal approximation, then modifying its final layer to produce binary values creates a new architecture capable of deterministically approximating any classifier. In particular, we obtain guarantees for deep CNNs, deep ffNN, and universal Gaussian processes. Our results also have consequences within the scope of geometric deep learning. Specifically, when the input and output spaces are Hadamard manifolds, we obtain geometrically meaningful feature and readout maps satisfying our criteria. Consequently, commonly used non-Euclidean regression models between spaces of symmetric positive definite matrices are extended to universal DNNs. The same result allows us to show that the hyperbolic feed-forward networks, used for hierarchical learning, are universal. Our result is also used to show that the common practice of randomizing all but the last two layers of a DNN produces a universal family of functions with probability one."
            },
            {
                "arxivId": "2006.00218",
                "title": "Sig-SDEs model for quantitative finance",
                "abstract": "Mathematical models, calibrated to data, have become ubiquitous to make key decision processes in modern quantitative finance. In this work, we propose a novel framework for data-driven model selection by integrating a classical quantitative setup with a generative modelling approach. Leveraging the properties of the signature, a well-known path-transform from stochastic analysis that recently emerged as leading machine learning technology for learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new perspective on neural SDEs and can be calibrated to exotic financial products that depend, in a non-linear way, on the whole trajectory of asset prices. Furthermore, we our approach enables to consistently calibrate under the pricing measure Q and real-world measure P. Finally, we demonstrate the ability of Sig-SDE to simulate future possible market scenarios needed for computing risk profiles or hedging strategies. Importantly, this new model is underpinned by rigorous mathematical analysis, that under appropriate conditions provides theoretical guarantees for convergence of the presented algorithms."
            },
            {
                "arxivId": "2005.02505",
                "title": "A Generative Adversarial Network Approach to Calibration of Local Stochastic Volatility Models",
                "abstract": "We propose a fully data-driven approach to calibrate local stochastic volatility (LSV) models, circumventing in particular the ad hoc interpolation of the volatility surface. To achieve this, we parametrize the leverage function by a family of feed-forward neural networks and learn their parameters directly from the available market option prices. This should be seen in the context of neural SDEs and (causal) generative adversarial networks: we generate volatility surfaces by specific neural SDEs, whose quality is assessed by quantifying, possibly in an adversarial manner, distances to market prices. The minimization of the calibration functional relies strongly on a variance reduction technique based on hedging and deep hedging, which is interesting in its own right: it allows the calculation of model prices and model implied volatilities in an accurate way using only small sets of sample paths. For numerical illustration we implement a SABR-type LSV model and conduct a thorough statistical performance analysis on many samples of implied volatility smiles, showing the accuracy and stability of the method."
            },
            {
                "arxivId": "1910.03193",
                "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
                "abstract": null
            },
            {
                "arxivId": "1907.08529",
                "title": "A note on Machado-Bishop theorem in weighted spaces with applications",
                "abstract": null
            },
            {
                "arxivId": "1905.00728",
                "title": "Optimal Execution with Rough Path Signatures",
                "abstract": "We present a method for obtaining approximate solutions to the problem of optimal execution, based on a signature method. The framework is general, only requiring that the price process is a geometric rough path and the price impact function is a continuous function of the trading speed. Following an approximation of the optimisation problem, we are able to calculate an optimal solution for the trading speed in the space of linear functions on a truncation of the signature of the price process. We provide strong numerical evidence illustrating the accuracy and flexibility of the approach. Our numerical investigation both examines cases where exact solutions are known, demonstrating that the method accurately approximates these solutions, and models where exact solutions are not known. In the latter case, we obtain favourable comparisons with standard execution strategies."
            },
            {
                "arxivId": "1905.00711",
                "title": "Non-parametric Pricing and Hedging of Exotic Derivatives",
                "abstract": "ABSTRACT In the spirit of Arrow\u2013Debreu, we introduce a family of financial derivatives that act as primitive securities in that exotic derivatives can be approximated by their linear combinations. We call these financial derivatives signature payoffs. We show that signature payoffs can be used to non-parametrically price and hedge exotic derivatives in the scenario where one has access to price data for other exotic payoffs. The methodology leads to a computationally tractable and accurate algorithm for pricing and hedging using market prices of a basket of exotic derivatives that has been tested on real and simulated market prices, obtaining good results."
            },
            {
                "arxivId": "1810.10971",
                "title": "Signature Moments to Characterize Laws of Stochastic Processes",
                "abstract": "The normalized sequence of moments characterizes the law of any finite-dimensional random variable. We prove an analogous result for path-valued random variables, that is stochastic processes, by using the normalized sequence of signature moments. We use this to define a metric for laws of stochastic processes. This metric can be efficiently estimated from finite samples, even if the stochastic processes themselves evolve in high-dimensional state spaces. As an application, we provide a non-parametric two-sample hypothesis test for laws of stochastic processes."
            },
            {
                "arxivId": "1806.00797",
                "title": "Echo state networks are universal",
                "abstract": null
            },
            {
                "arxivId": "1804.10450",
                "title": "Generalized Feller processes and Markovian lifts of stochastic Volterra processes: the affine case",
                "abstract": null
            },
            {
                "arxivId": "1705.01714",
                "title": "Optimal Approximation with Sparsely Connected Deep Neural Networks",
                "abstract": "We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in $L^2(\\mathbb{R}^d)$. In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy. Additionally, we prove that our lower bounds are achievable for a broad family of function classes. Specifically, all function classes that are optimally approximated by a general class of representation systems---so-called affine systems---can be approximated by deep neural networks with minimal connectivity and memory requirements. Affine systems encompass a wealth of representation systems from applied harmonic analysis such as wavelets, ridgelets, curvelets, shearlets, $\\alpha$-shearlets, and more generally $\\alpha$-molecules. Our central result elucidates a remarkable universality property of neural networks and shows that they achieve the optimum approximation properties of all affine systems combined. As a specific example, we consider the class of $\\alpha^{-1}$-cartoon-like functions, which is approximated optimally by $\\alpha$-shearlets. We also explain how our results can be extended to the case of functions on low-dimensional immersed manifolds. Finally, we present numerical experiments demonstrating that the standard stochastic gradient descent algorithm generates deep neural networks providing close-to-optimal approximation rates at minimal connectivity. Moreover, these results indicate that stochastic gradient descent can actually learn approximations that are sparse in the representation systems optimally sparsifying the function class the network is trained on."
            },
            {
                "arxivId": "1601.08169",
                "title": "Kernels for sequentially ordered data",
                "abstract": "We present a novel framework for kernel learning with sequential data of any kind, such as time series, sequences of graphs, or strings. Our approach is based on signature features which can be seen as an ordered variant of sample (cross-)moments; it allows to obtain a \"sequentialized\" version of any static kernel. The sequential kernels are efficiently computable for discrete sequences and are shown to approximate a continuous moment form in a sampling sense. \nA number of known kernels for sequences arise as \"sequentializations\" of suitable static kernels: string kernels may be obtained as a special case, and alignment kernels are closely related up to a modification that resolves their open non-definiteness issue. Our experiments indicate that our signature-based sequential kernel framework may be a promising approach to learning with sequential data, such as time series, that allows to avoid extensive manual pre-processing."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "1406.7871",
                "title": "The Signature of a Rough Path: Uniqueness",
                "abstract": null
            },
            {
                "arxivId": "1307.3580",
                "title": "Characteristic functions of measures on geometric rough paths",
                "abstract": "We define a characteristic function for probability measures on the signatures of geometric rough paths. We determine sufficient conditions under which a random variable is uniquely determined by its expected signature, thus partially solving the analogue of the moment problem. We furthermore study analyticity properties of the characteristic function and prove a method of moments for weak convergence of random variables. We apply our results to signature arising from Levy, Gaussian and Markovian rough paths."
            },
            {
                "arxivId": "1205.1832",
                "title": "Differential Equations Driven by \u03a0-Rough Paths",
                "abstract": "Abstract This paper revisits the concept of rough paths of inhomogeneous degree of smoothness (geometric \u03a0-rough paths in our terminology) sketched by Lyons in 1998. Although geometric \u03a0-rough paths can be treated as p-rough paths for a sufficiently large p, and the theory of integration of Lip \u03b3 one-forms (\u03b3 > p\u20131) along geometric p-rough paths applies, we prove the existence of integrals of one-forms under weaker conditions. Moreover, we consider differential equations driven by geometric \u03a0-rough paths and give sufficient conditions for existence and uniqueness of solution."
            },
            {
                "arxivId": "1011.2651",
                "title": "A Semigroup Point Of View On Splitting Schemes For Stochastic (Partial) Differential Equations",
                "abstract": "We construct normed spaces of real-valued functions with controlled growth on possibly infinite-dimensional state spaces such that semigroups of positive, bounded operators $(P_t)_{t\\ge 0}$ thereon with $\\lim_{t\\to 0+}P_t f(x)=f(x)$ are in fact strongly continuous. This result applies to prove optimal rates of convergence of splitting schemes for stochastic (partial) differential equations with linearly growing characteristics and for sets of functions with controlled growth. Applications are general Da Prato-Zabczyk type equations and the HJM equations from interest rate theory."
            },
            {
                "arxivId": "1004.1380",
                "title": "Change of variable formulas for non-anticipative functionals on path space \u2729",
                "abstract": null
            },
            {
                "arxivId": "1002.2446",
                "title": "Functional Ito calculus and stochastic integral representation of martingales",
                "abstract": "We develop a non-anticipative calculus for functionals of a continuous semimartingale, using an extension of the Ito formula to path-dependent functionals which possess certain directional derivatives. The construction is based on a pathwise derivative, introduced by B Dupire, for functionals on the space of right-continuous functions with left limits. We show that this functional derivative admits a suitable extension to the space of square-integrable martingales. This extension denes a weak derivative which is shown to be the inverse of the Ito integral and which may be viewed as a non-anticipative \\lifting\" of the Malliavin derivative. These results lead to a constructive martingale representation formula for Ito processes. By contrast with the Clark-Haussmann-Ocone formula, this representation only involves nonanticipative quantities which may be computed pathwise."
            },
            {
                "arxivId": "math/0511708",
                "title": "Kolmogorov equations in infinite dimensions: Well-posedness and regularity of solutions, with applications to stochastic generalized Burgers equations",
                "abstract": "We develop a new method to uniquely solve a large class of heat equations, so-called Kolmogorov equations in infinitely many variables. The equations are analyzed in spaces of sequentially weakly continuous functions weighted by proper (Lyapunov type) functions. This way for the first time the solutions are constructed everywhere without exceptional sets for equations with possibly nonlocally Lipschitz drifts. Apart from general analytic interest, the main motivation is to apply this to uniquely solve martingale problems in the sense of Stroock-Varadhan given by stochastic partial differential equations from hydrodynamics, such as the stochastic Navier-Stokes equations. In this paper this is done in the case of the stochastic generalized Burgers equation. Uniqueness is shown in the sense of Markov flows."
            },
            {
                "arxivId": "math/0507536",
                "title": "Uniqueness for the signature of a path of bounded variation and the reduced path group",
                "abstract": "We introduce the notions of tree-like path and tree-like equivalence between paths and prove that the latter is an equivalence relation for paths of finite length. We show that the equivalence classes form a group with some similarity to a free group, and that in each class there is one special tree reduced path. The set of these paths is the Reduced Path Group. It is a continuous analogue to the group of reduced words. The signature of the path is a power series whose coefficients are definite iterated integrals of the path. We identify the paths with trivial signature as the tree-like paths, and prove that two paths are in tree-like equivalence if and only if they have the same signature. In this way, we extend Chen's theorems on the uniqueness of the sequence of iterated integrals associated with a piecewise regular path to finite length paths and identify the appropriate extended meaning for reparameterisation in the general setting. It is suggestive to think of this result as a non-commutative analogue of the result that integrable functions on the circle are determined, up to Lebesgue null sets, by their Fourier coefficients. As a second theme we give quantitative versions of Chen's theorem in the case of lattice paths and paths with continuous derivative, and as a corollary derive results on the triviality of exponential products in the tensor algebra."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2310.00321",
        "category": "q-fin",
        "title": "Modeling the yield curve of Burundian bond market by parametric models",
        "abstract": "The term structure of interest rates (yield curve) is a critical facet of financial analytics, impacting various investment and risk management decisions. It is used by the central bank to conduct and monitor its monetary policy. That instrument reflects the anticipation of inflation and the risk by investors. The rates reported on yield curve are the cornerstone of valuation of all assets. To provide such tool for Burundi financial market, we collected the auction reports of treasury securities from the website of the Central Bank of Burundi. Then, we computed the zero-coupon rates, and estimated actuarial rates of return by applying the Nelson-Siegel and Svensson models. This paper conducts a rigorous comparative analysis of these two prominent parametric yield curve models and finds that the Nelson-Siegel model is the optimal choice for modeling the Burundian yield curve. The findings contribute to the body of knowledge on yield curve modeling, enhancing its precision and applicability in financial markets. Furthermore, this research holds implications for investment strategies, risk management, second market pricing, financial decision-making, and the forthcoming establishment of the Burundian stock market.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2402.17359",
        "category": "q-fin",
        "title": "Limit Order Book Simulations: A Review",
        "abstract": "Limit Order Books (LOBs) serve as a mechanism for buyers and sellers to interact with each other in the financial markets. Modelling and simulating LOBs is quite often necessary for calibrating and fine-tuning the automated trading strategies developed in algorithmic trading research. The recent AI revolution and availability of faster and cheaper compute power has enabled the modelling and simulations to grow richer and even use modern AI techniques. In this review we examine the various kinds of LOB simulation models present in the current state of the art. We provide a classification of the models on the basis of their methodology and provide an aggregate view of the popular stylized facts used in the literature to test the models. We additionally provide a focused study of price impact's presence in the models since it is one of the more crucial phenomena to model in algorithmic trading. Finally, we conduct a comparative analysis of various qualities of fits of these models and how they perform when tested against empirical data.",
        "references": [
            {
                "arxivId": "2306.12806",
                "title": "Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness",
                "abstract": "Limit order books are a fundamental and widespread market mechanism. This paper investigates the use of conditional generative models for order book simulation. For developing a trading agent, this approach has drawn recent attention as an alternative to traditional backtesting, due to its ability to react to the presence of the trading agent. We explore the dependence of a state-of-the-art conditional generative adversarial network (CGAN) upon its input features, highlighting both strengths and weaknesses. To do this, we use \u201cadversarial attacks\u201d on the model\u2019s features and its mechanism. We then show how these insights can be used to improve the CGAN, both in terms of its realism and robustness. We finish by laying out a roadmap for future work."
            },
            {
                "arxivId": "2201.10173",
                "title": "Modeling bid and ask price dynamics with an extended Hawkes process and its empirical applications for high-frequency stock market data",
                "abstract": "This study proposes a versatile model for the dynamics of the best bid and ask prices using an extended Hawkes process. The model incorporates the zero intensities of the spreadnarrowing processes at the minimum bid-ask spread, spread-dependent intensities, possible negative excitement, and nonnegative intensities. We apply the model to high-frequency best bid and ask price data from US stock markets. The empirical findings demonstrate a spread-narrowing tendency, excitations of the intensities caused by previous events, the impact of flash crashes, characteristic trends in fast trading over time, and the different features of market participants in the various exchanges."
            },
            {
                "arxivId": "2105.00521",
                "title": "Order flow and price formation",
                "abstract": "I present an overview of some recent advancements on the empirical analysis and theoretical modeling of the process of price formation in financial markets as the result of the arrival of orders in a limit order book exchange. After discussing critically the possible modeling approaches and the observed stylized facts of order flow, I consider in detail market impact and transaction cost of trades executed incrementally over an extended period of time, by comparing model predictions and recent extensive empirical results. I also discuss how the simultaneous presence of many algorithmic trading executions affects the quality and cost of trading."
            },
            {
                "arxivId": "2102.08811",
                "title": "Deep Learning for Market by Order Data",
                "abstract": "ABSTRACT Market by order (MBO) data \u2013 a detailed feed of individual trade instructions for a given stock on an exchange \u2013 is arguably one of the most granular sources of microstructure information. While limit order books (LOBs) are implicitly derived from it, MBO data is largely neglected by current academic literature, which focuses primarily on LOB modelling. In this paper, we demonstrate the utility of MBO data for forecasting high-frequency price movements, providing an orthogonal source of information to LOB snapshots and expanding the universe of alpha discovery. We provide the first predictive analysis on MBO data by carefully introducing the data structure and presenting a specific normalization scheme to consider level information in order books and to allow model training with multiple instruments. Through forecasting experiments using deep neural networks, we show that while MBO-driven and LOB-driven models individually provide similar performance, ensembles of the two can lead to improvements in forecasting accuracy \u2013 indicating that MBO data is additive to LOB-based features."
            },
            {
                "arxivId": "2004.10632",
                "title": "Order book dynamics with liquidity fluctuations: limit theorems and large deviations",
                "abstract": "We propose a class of stochastic models for a dynamics of limit order book with different type of liquidities. Within this class of models we study the one where a spread decreases uniformly, belonging to the class of processes known as a population processes with uniform catastrophes. The law of large numbers (LLN), central limit theorem (CLT) and large deviations (LD) are proved for our model with uniform catastrophes. Our results allow us to satisfactorily explain the volatility and local trends in the prices, relevant empirical characteristics that are observed in this type of markets. Furthermore, it shows us how these local trends and volatility are determined by the typical values of the bid-ask spread. In addition, we use our model to show how large deviations occur in the spread and prices, such as those observed in flash crashes."
            },
            {
                "arxivId": "1912.04941",
                "title": "Get real: realism metrics for robust limit order book market simulations",
                "abstract": "Market simulation is an increasingly important method for evaluating and training trading strategies and testing \"what if\" scenarios. The extent to which results from these simulations can be trusted depends on how realistic the environment is for the strategies being tested. As a step towards providing benchmarks for realistic simulated markets, we enumerate measurable stylized facts of limit order book (LOB) markets across multiple asset classes from the literature. We apply these metrics to data from real markets and compare the results to data originating from simulated markets. We illustrate their use in five different simulated market configurations: The first (market replay) is frequently used in practice to evaluate trading strategies; the other four are interactive agent based simulation (IABS) configurations which combine zero intelligence agents, and agents with limited strategic behavior. These simulated agents rely on an internal \"oracle\" that provides a fundamental value for the asset. In traditional IABS methods the fundamental originates from a mean reverting random walk. We show that markets exhibit more realistic behavior when the fundamental arises from historical market data. We further experimentally illustrate the effectiveness of IABS techniques as opposed to market replay."
            },
            {
                "arxivId": "1907.12025",
                "title": "Marked Hawkes process modeling of price dynamics and volatility estimation",
                "abstract": null
            },
            {
                "arxivId": "1906.05420",
                "title": "From asymptotic properties of general point processes to the ranking of financial agents",
                "abstract": "We propose a general non-linear order book model that is built from the individual behaviours of the agents. Our framework encompasses Markovian and Hawkes based models. Under mild assumptions, we prove original results on the ergodicity and diffusivity of such system. Then we provide closed form formulas for various quantities of interest: stationary distribution of the best bid and ask quantities, spread, liquidity fluctuations and price volatility. These formulas are expressed in terms of individual order flows of market participants. Our approach enables us to establish a ranking methodology for the market makers with respect to the quality of their trading."
            },
            {
                "arxivId": "1904.03058",
                "title": "A Stochastic Partial Differential Equation Model for Limit Order Book Dynamics",
                "abstract": "We propose an analytically tractable class of models for the dynamics of a limit order book, described as the solution of a stochastic partial differential equation (SPDE) with multiplicative noise. We provide conditions under which the model admits a finite dimensional realization driven by a (low-dimensional) Markov process, leading to efficient methods for estimation and computation. \n \nWe study two examples of parsimonious models in this class: a two-factor model and a model in which the order book depth is mean-reverting. For each model we perform a detailed analysis of the role of different parameters, study the dynamics of the price, order book depth, volume and order imbalance, provide an intuitive financial interpretation of the variables involved and show how the model reproduces statistical properties of price changes, market depth and order flow in limit order markets."
            },
            {
                "arxivId": "1901.08938",
                "title": "Queue-reactive Hawkes models for the order flow",
                "abstract": "In this work we introduce two variants of multivariate Hawkes models with an explicit dependency on various queue sizes aimed at modeling the stochastic time evolution of a limit order book. The models we propose thus integrate the influence of both the current book state and the past order flow. The first variant considers the flow of order arrivals at a specific price level as independent from the other one and describes this flow by adding a Hawkes component to the arrival rates provided by the continuous time Markov \"Queue Reactive\" model of Huang et al. Empirical calibration using Level-I order book data from Eurex future assets (Bund and DAX) show that the Hawkes term dramatically improves the pure \"Queue-Reactive\" model not only for the description of the order flow properties (as e.g. the statistics of inter-event times) but also with respect to the shape of the queue distributions. The second variant we introduce describes the joint dynamics of all events occurring at best bid and ask sides of some order book during a trading day. This model can be considered as a queue dependent extension of the multivariate Hawkes order-book model of Bacry et al. We provide an explicit way to calibrate this model either with a Maximum-Likelihood method or with a Least-Square approach. Empirical estimation from Bund and DAX level-I order book data allow us to recover the main features of Hawkes interactions uncovered in Bacry et al. but also to unveil their joint dependence on bid and ask queue sizes. We notably find that while the market order or mid-price changes rates can mainly be functions on the volume imbalance this is not the case for the arrival rate of limit or cancel orders. Our findings also allows us to clearly bring to light various features that distinguish small and large tick assets."
            },
            {
                "arxivId": "1809.08060",
                "title": "State-dependent Hawkes processes and their application to limit order book modelling",
                "abstract": "We study statistical aspects of state-dependent Hawkes processes, which are an extension of Hawkes processes where a self- and cross-exciting counting process and a state process are fully coupled, interacting with each other. The excitation kernel of the counting process depends on the state process that, reciprocally, switches state when there is an event in the counting process. We first establish the existence and uniqueness of state-dependent Hawkes processes and explain how they can be simulated. Then we develop maximum likelihood estimation methodology for parametric specifications of the process. We apply state-dependent Hawkes processes to high-frequency limit order book data, allowing us to build a novel model that captures the feedback loop between the order flow and the shape of the limit order book. We estimate two specifications of the model, using the bid\u2013ask spread and the queue imbalance as state variables, and find that excitation effects in the order flow are strongly state-dependent. Additionally, we find that the endogeneity of the order flow, measured by the magnitude of excitation, is also state-dependent, being more pronounced in disequilibrium states of the limit order book."
            },
            {
                "arxivId": "1808.07107",
                "title": "Limit Order Books, Diffusion Approximations and Reflected SPDEs: From Microscopic to Macroscopic Models",
                "abstract": "ABSTRACT Motivated by a zero-intelligence approach, the aim of this paper is to connect the microscopic (discrete price and volume), mesoscopic (discrete price and continuous volume) and macroscopic (continuous price and volume) frameworks for the modelling of limit order books, with a view to providing a natural probabilistic description of their behaviour in a high- to ultra high-frequency setting. Starting with a microscopic framework, we first examine the limiting behaviour of the order book process when order arrival and cancellation rates are sent to infinity and when volumes are considered to be of infinitesimal size. We then consider the transition between this mesoscopic model and a macroscopic model for the limit order book, obtained by letting the tick size tend to zero. The macroscopic limit can then be described using reflected SPDEs which typically arise in stochastic interface models. We then use financial data to discuss a possible calibration procedure for the model and illustrate numerically how it can reproduce observed behaviour of prices. This could then be used as a market simulator for short-term price prediction or for testing optimal execution strategies."
            },
            {
                "arxivId": "1806.05101",
                "title": "Order-book modelling and market making strategies",
                "abstract": "Market making is one of the most important aspects of algorithmic trading, and it has been studied quite extensively from a theoretical point of view. The practical implementation of so-called \"optimal strategies\" however suffers from the failure of most order book models to faithfully reproduce the behaviour of real market participants. \nThis paper is twofold. First, some important statistical properties of order driven markets are identified, advocating against the use of purely Markovian order book models. Then, market making strategies are designed and their performances are compared, based on simulation as well as backtesting. We find that incorporating some simple non-Markovian features in the limit order book greatly improves the performances of market making strategies in a realistic context."
            },
            {
                "arxivId": "1803.06917",
                "title": "Universal features of price formation in financial markets: perspectives from deep learning",
                "abstract": "Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary relation between order flow history and the direction of price moves. The universal price formation model exhibits a remarkably stable out-of-sample accuracy across a wide range of stocks and time periods. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. The universal model\u2014trained on data from all stocks\u2014outperforms asset-specific models trained on time series of any given stock. This weighs in favor of pooling together financial data from various stocks, rather than designing asset- or sector-specific models, as is currently commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations improves forecast accuracy, indicating that there is path-dependence in price dynamics."
            },
            {
                "arxivId": "1708.07394",
                "title": "Second order approximations for limit order books",
                "abstract": null
            },
            {
                "arxivId": "1707.03003",
                "title": "Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling",
                "abstract": "Tick is a statistical learning library for Python~3, with a particular emphasis on time-dependent models, such as point processes, and tools for generalized linear models and survival analysis. The core of the library is an optimization module providing model computational classes, solvers and proximal operators for regularization. tick relies on a C++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from this https URL"
            },
            {
                "arxivId": "1509.02017",
                "title": "An estimation procedure for the Hawkes process",
                "abstract": "In this paper, we present a nonparametric estimation procedure for the multivariate Hawkes point process. The timeline is cut into bins and\u2014for each component process\u2014the number of points in each bin is counted. As a consequence of earlier results in Kirchner [Stoch. Process. Appl., 2016, 162, 2494\u20132525], the distribution of the resulting \u2018bin-count sequences\u2019 can be approximated by an integer-valued autoregressive model known as the (multivariate) INAR(p) model. We represent the INAR(p) model as a standard vector-valued linear autoregressive time series with white-noise innovations (VAR(p)). We establish consistency and asymptotic normality for conditional least-squares estimation of the VAR(p), respectively, the INAR(p) model. After appropriate scaling, these time-series estimates yield estimates for the underlying multivariate Hawkes process as well as corresponding variance estimates. The estimates depend on a bin-size and a support s. We discuss the impact and the choice of these parameters. All results are presented in such a way that computer implementation, e.g. in R, is straightforward. Simulation studies confirm the effectiveness of our estimation procedure. In the second part of the paper, we present a data example where the method is applied to bivariate event-streams in financial limit-order-book data. We fit a bivariate Hawkes model on the joint process of limit and market order arrivals. The analysis exhibits a remarkably asymmetric relation between the two component processes: incoming market orders excite the limit-order flow heavily whereas the market-order flow is hardly affected by incoming limit orders. For the estimated excitement functions, we observe power-law shapes, inhibitory effects for lags under 0.003 s, second periodicities and local maxima at 0.01, 0.1 and 0.5 s."
            },
            {
                "arxivId": "1502.04592",
                "title": "Hawkes Processes in Finance",
                "abstract": "In this paper we propose an overview of the recent academic literature devoted to the applications of Hawkes processes in finance. Hawkes processes constitute a particular class of multivariate point processes that has become very popular in empirical high-frequency finance this last decade. After a reminder of the main definitions and properties that characterize Hawkes processes, we review their main empirical applications to address many different problems in high-frequency finance. Because of their great flexibility and versatility, we show that they have been successfully involved in issues as diverse as estimating the volatility at the level of transaction data, estimating the market stability, accounting for systemic risk contagion, devising optimal execution strategies or capturing the dynamics of the full order book."
            },
            {
                "arxivId": "1401.4636",
                "title": "Dynamic Equilibrium Limit Order Book Model and Optimal Execution Problem",
                "abstract": "In this paper we propose a dynamic model of Limit Order Book (LOB). The main feature of our model is that the shape of the LOB is determined endogenously by an expected utility function via a competitive equilibrium argument. Assuming zero resilience, the resulting equilibrium density of the LOB is random, nonlinear, and time inhomogeneous. Consequently, the liquidity cost can be defined dynamically in a natural way. \nWe next study an optimal execution problem in our model. We verify that the value function satisfies the Dynamic Programming Principle, and is a viscosity solution to the corresponding Hamilton-Jacobi-Bellman equation which is in the form of an integro-partial-differential quasi-variational inequality. We also prove the existence and analyze the structure of the optimal strategy via a verification theorem argument, assuming that the PDE has a classical solution."
            },
            {
                "arxivId": "1312.0563",
                "title": "Simulating and Analyzing Order Book Data: The Queue-Reactive Model",
                "abstract": "Through the analysis of a dataset of ultra high frequency order book updates, we introduce a model which accommodates the empirical properties of the full order book together with the stylized facts of lower frequency financial data. To do so, we split the time interval of interest into periods in which a well chosen reference price, typically the midprice, remains constant. Within these periods, we view the limit order book as a Markov queuing system. Indeed, we assume that the intensities of the order flows only depend on the current state of the order book. We establish the limiting behavior of this model and estimate its parameters from market data. Then, to design a relevant model for the whole period of interest, we use a stochastic mechanism that allows to switch from one period of constant reference price to another. Beyond enabling to reproduce accurately the behavior of market data, we show that our framework can be very useful for practitioners, notably as a market simulator or as a tool for the transaction cost analysis of complex trading algorithms."
            },
            {
                "arxivId": "1301.5007",
                "title": "Ergodicity and Scaling Limit of a Constrained Multivariate Hawkes Process",
                "abstract": "We introduce a multivariate Hawkes process with constraints on its conditional density. It is a multivariate point process with conditional intensity similar to that of a multivariate Hawkes process but certain events are forbidden with respect to boundary conditions on a multidimensional constraint variable, whose evolution is driven by the point process. We study this process in the special case where the fertility function is exponential so that the process is entirely described by an underlying Markov chain, which includes the constraint variable. Some conditions on the parameters are established to ensure the ergodicity of the chain. Moreover, scaling limits are derived for the integrated point process. This study is primarily motivated by the stochastic modelling of a limit order book for high frequency financial data analysis."
            },
            {
                "arxivId": "1202.6412",
                "title": "Order Book Dynamics in Liquid Markets: Limit Theorems and Diffusion Approximations",
                "abstract": "We propose a model for the dynamics of a limit order book in a liquid market where buy and sell orders are submitted at high frequency. We derive a functional central limit theorem for the joint dynamics of the bid and ask queues and show that, when the frequency of order arrivals is large, the intraday dynamics of the limit order book may be approximated by a Markovian jump-diffusion process in the positive orthant, whose characteristics are explicitly described in terms of the statistical properties of the underlying order flow. This result allows to obtain tractable analytical approximations for various quantities of interest, such as the probability of a price increase or the distribution of the duration until the next price move, conditional on the state of the order book. Our results allow for a wide range of distributional assumptions and temporal dependence in the order flow and apply to a wide class of stochastic models proposed for order book dynamics, including models based on Poisson point processes, self-exciting point processes and models of the ACD-GARCH family."
            },
            {
                "arxivId": "1012.0349",
                "title": "Limit order books",
                "abstract": "Abstract Limit order books (LOBs) match buyers and sellers in more than half of the world\u2019s financial markets. This survey highlights the insights that have emerged from the wealth of empirical and theoretical studies of LOBs. We examine the findings reported by statistical analyses of historical LOB data and discuss how several LOB models provide insight into certain aspects of the mechanism. We also illustrate that many such models poorly resemble real LOBs and that several well-established empirical facts have yet to be reproduced satisfactorily. Finally, we identify several key unresolved questions about LOBs."
            },
            {
                "arxivId": "1003.3796",
                "title": "\"Market making\" behaviour in an order book model and its impact on the bid-ask spread",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0309233",
                "title": "The Predictive Power of Zero Intelligence in Financial Markets",
                "abstract": "Standard models in economics stress the role of intelligent agents who maximize utility. However, there may be situations where constraints imposed by market institutions dominate strategic agent behavior. We use data from the London Stock Exchange to test a simple model in which minimally intelligent agents place orders to trade at random. The model treats the statistical mechanics of order placement, price formation, and the accumulation of revealed supply and demand within the context of the continuous double auction and yields simple laws relating order-arrival rates to statistical properties of the market. We test the validity of these laws in explaining cross-sectional variation for 11 stocks. The model explains 96% of the variance of the gap between the best buying and selling prices (the spread) and 76% of the variance of the price diffusion rate, with only one free parameter. We also study the market impact function, describing the response of quoted prices to the arrival of new orders. The nondimensional coordinates dictated by the model approximately collapse data from different stocks onto a single curve. This work is important from a practical point of view, because it demonstrates the existence of simple laws relating prices to order flows and, in a broader context, suggests there are circumstances where the strategic behavior of agents may be dominated by other considerations."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00006",
        "category": "q-fin",
        "title": "Local sensitivity analysis of heating degree day and cooling degree day temperature derivatives prices",
        "abstract": "We study the local sensitivity of heating degree day (HDD) and cooling degree day (CDD) temperature futures and option prices with respect to perturbations in the deseasonalized temperature or in one of its derivatives up to a certain order determined by the continuous-time autoregressive process modelling the deseasonalized temperature in the HDD and CDD indexes. We also consider an empirical case where a CAR process of autoregressive order 3 is fitted to New York temperatures and we perform a study of the local sensitivity of these financial contracts and a posterior analysis of the results.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00009",
        "category": "q-fin",
        "title": "Randomized Control in Performance Analysis and Empirical Asset Pricing",
        "abstract": "The present article explores the application of randomized control techniques in empirical asset pricing and performance evaluation. It introduces geometric random walks, a class of Markov chain Monte Carlo methods, to construct flexible control groups in the form of random portfolios adhering to investor constraints. The sampling-based methods enable an exploration of the relationship between academically studied factor premia and performance in a practical setting. In an empirical application, the study assesses the potential to capture premias associated with size, value, quality, and momentum within a strongly constrained setup, exemplified by the investor guidelines of the MSCI Diversified Multifactor index. Additionally, the article highlights issues with the more traditional use case of random portfolios for drawing inferences in performance evaluation, showcasing challenges related to the intricacies of high-dimensional geometry.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00139",
        "category": "q-fin",
        "title": "Optimal positioning in derivative securities in incomplete markets",
        "abstract": "This paper analyzes a problem of optimal static hedging using derivatives in incomplete markets. The investor is assumed to have a risk exposure to two underlying assets. The hedging instruments are vanilla options written on a single underlying asset. The hedging problem is formulated as a utility maximization problem whereby the form of the optimal static hedge is determined. Among our results, a semi-analytical solution for the optimizer is found through variational methods for exponential, power/logarithmic, and quadratic utility. When vanilla options are available for each underlying asset, the optimal solution is related to the fixed points of a Lipschitz map. In the case of exponential utility, there is only one such fixed point, and subsequent iterations of the map converge to it.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00273",
        "category": "q-fin",
        "title": "ARED: Argentina Real Estate Dataset",
        "abstract": "The Argentinian real estate market presents a unique case study characterized by its unstable and rapidly shifting macroeconomic circumstances over the past decades. Despite the existence of a few datasets for price prediction, there is a lack of mixed modality datasets specifically focused on Argentina. In this paper, the first edition of ARED is introduced. A comprehensive real estate price prediction dataset series, designed for the Argentinian market. This edition contains information solely for Jan-Feb 2024. It was found that despite the short time range captured by this zeroth edition (44 days), time dependent phenomena has been occurring mostly on a market level (market as a whole). Nevertheless future editions of this dataset, will most likely contain historical data. Each listing in ARED comprises descriptive features, and variable-length sets of images.",
        "references": [
            {
                "arxivId": "2308.10609",
                "title": "ST-RAP: A Spatio-Temporal Framework for Real Estate Appraisal",
                "abstract": "In this paper, we introduce ST-RAP, a novel Spatio-Temporal framework for Real estate APpraisal. ST-RAP employs a hierarchical architecture with a heterogeneous graph neural network to encapsulate temporal dynamics and spatial relationships simultaneously. Through comprehensive experiments on a large-scale real estate dataset, ST-RAP outperforms previous methods, demonstrating the significant benefits of integrating spatial and temporal aspects in real estate appraisal. Our code and dataset are available at https://github.com/dojeon-ai/STRAP."
            },
            {
                "arxivId": "2012.07436",
                "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
                "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem."
            },
            {
                "arxivId": "2001.02908",
                "title": "Spatial-Temporal Transformer Networks for Traffic Flow Forecasting",
                "abstract": "Traffic forecasting has emerged as a core component of intelligent transportation systems. However, timely accurate traffic forecasting, especially long-term forecasting, still remains an open challenge due to the highly nonlinear and dynamic spatial-temporal dependencies of traffic flows. In this paper, we propose a novel paradigm of Spatial-Temporal Transformer Networks (STTNs) that leverages dynamical directed spatial dependencies and long-range temporal dependencies to improve the accuracy of long-term traffic forecasting. Specifically, we present a new variant of graph neural networks, named spatial transformer, by dynamically modeling directed spatial dependencies with self-attention mechanism to capture realtime traffic conditions as well as the directionality of traffic flows. Furthermore, different spatial dependency patterns can be jointly modeled with multi-heads attention mechanism to consider diverse relationships related to different factors (e.g. similarity, connectivity and covariance). On the other hand, the temporal transformer is utilized to model long-range bidirectional temporal dependencies across multiple time steps. Finally, they are composed as a block to jointly model the spatial-temporal dependencies for accurate traffic prediction. Compared to existing works, the proposed model enables fast and scalable training over a long range spatial-temporal dependencies. Experiment results demonstrate that the proposed model achieves competitive results compared with the state-of-the-arts, especially forecasting long-term traffic flows on real-world PeMS-Bay and PeMSD7(M) datasets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00471",
        "category": "q-fin",
        "title": "Idiosyncratic Risk, Government Debt and Inflation",
        "abstract": "How does public debt matter for price stability? If it is useful for the private sector to insure idiosyncratic risk, government debt expansions can increase the natural rate of interest and create inflation. As I demonstrate using a tractable model, this holds in the presence of an active Taylor rule and does not require the absence of future fiscal consolidation. Further analysis using a full-blown 2-asset HANK model reveals the quantitative magnitude of the mechanism to crucially depend on the structure of the asset market: under standard assumptions, the effect of public debt on the natural rate is either overly strong or overly weak. Employing a parsimonious way to overcome this issue, my framework suggests relevant effects of public debt on inflation under active monetary policy: In particular, persistently elevated public debt may make it harder to go the last\"mile of disinflation\"unless central banks explicitly take its effect on the neutral rate into account.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00523",
        "category": "q-fin",
        "title": "Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin",
        "abstract": "Exploring transactions within the Bitcoin blockchain entails examining the transfer of bitcoins among several hundred million entities. However, it is often impractical and resource-consuming to study such a vast number of entities. Consequently, entity clustering serves as an initial step in most analytical studies. This process often employs heuristics grounded in the practices and behaviors of these entities. In this research, we delve into the examination of two widely used heuristics, alongside the introduction of four novel ones. Our contribution includes the introduction of the \\textit{clustering ratio}, a metric designed to quantify the reduction in the number of entities achieved by a given heuristic. The assessment of this reduction ratio plays an important role in justifying the selection of a specific heuristic for analytical purposes. Given the dynamic nature of the Bitcoin system, characterized by a continuous increase in the number of entities on the blockchain, and the evolving behaviors of these entities, we extend our study to explore the temporal evolution of the clustering ratio for each heuristic. This temporal analysis enhances our understanding of the effectiveness of these heuristics over time.",
        "references": [
            {
                "arxivId": "2311.12491",
                "title": "Heuristics for Detecting CoinJoin Transactions on the Bitcoin Blockchain",
                "abstract": "This research delves into the intricacies of Bitcoin, a decentralized peer-to-peer network, and its associated blockchain, which records all transactions since its inception. While this ensures integrity and transparency, the transparent nature of Bitcoin potentially compromises users' privacy rights. To address this concern, users have adopted CoinJoin, a method that amalgamates multiple transaction intents into a single, larger transaction to bolster transactional privacy. This process complicates individual transaction tracing and disrupts many established blockchain analysis heuristics. Despite its significance, limited research has been conducted on identifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin implementations such as JoinMarket, Wasabi, and Whirlpool, each presenting distinct challenges due to their unique transaction structures. This study delves deeply into the open-source implementations of these protocols, aiming to develop refined heuristics for identifying their transactions on the blockchain. Our exhaustive analysis covers transactions up to block 760,000, offering a comprehensive insight into CoinJoin transactions and their implications for Bitcoin blockchain analysis."
            },
            {
                "arxivId": "2104.09979",
                "title": "Bitcoin Address Clustering Method Based on Multiple Heuristic Conditions",
                "abstract": "We analyzed the associations between Bitcoin transactions and addresses to cluster address and further find groups of addresses controlled by the same entity. It revealed the vulnerabilities of Bitcoin anonymity mechanism, which could be used by the law enforcement agencies to track and crack down illegal transactions. However, single heuristic method and incomplete heuristic conditions were difficult to cluster a large number of addresses comprehensively and accurately. Therefore, this paper reviewed a variety of heuristics, and used multiple heuristics comprehensively to cluster addresses to improve the degree of address aggregation and address recall rate, which laid a foundation for further inferring of entity identity."
            },
            {
                "arxivId": "1710.08158",
                "title": "Tracking Bitcoin Users Activity Using Community Detection on a Network of Weak Signals",
                "abstract": null
            },
            {
                "arxivId": "1107.4524",
                "title": "An Analysis of Anonymity in the Bitcoin System",
                "abstract": "Anonymity in Bit coin, a peer-to-peer electronic currency system, is a complicated issue. Within the system, users are identified by public-keys only. An attacker wishing to de-anonymize its users will attempt to construct the one to-many mapping between users and public-keys and associate information external to the system with the users. Bitcoinfrustrates this attack by storing the mapping of a user to his or her public-keys on that user's node only and by allowing each user to generate as many public-keys as required. In this paper we consider the topological structure of two networks derived from Bitcoin's public transaction history. We show that the two networks have a non-trivial topological structure, provide complementary views of the Bit coin system and have implications for anonymity. We combine these structures with external information and techniques such as context discovery and flow analysis to investigate an alleged theft of Bit coins, which, at the time of the theft, had a market value of approximately half a million U.S. dollars."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00653",
        "category": "q-fin",
        "title": "Modelling Global Fossil CO2 Emissions with a Lognormal Distribution: A Climate Policy Tool",
        "abstract": "Carbon dioxide (CO2) emissions have emerged as a critical issue with profound impacts on the environment, human health, and the global economy. The steady increase in atmospheric CO2 levels, largely due to human activities such as burning fossil fuels and deforestation, has become a major contributor to climate change and its associated catastrophic effects. To tackle this pressing challenge, a coordinated global effort is needed, which necessitates a deep understanding of emissions patterns and trends. In this paper, we explore the use of statistical modelling, specifically the lognormal distribution, as a framework for comprehending and predicting CO2 emissions. We build on prior research that suggests a complex distribution of emissions and seek to test the hypothesis that a simpler distribution can still offer meaningful insights for policy-makers. We utilize data from three comprehensive databases and analyse six candidate distributions (exponential, Fisk, gamma, lognormal, Lomax, Weibull) to identify a suitable model for global fossil CO2 emissions. Our findings highlight the adequacy of the lognormal distribution in characterizing emissions across all countries and years studied. Furthermore, to provide additional support for this distribution, we provide statistical evidence supporting the applicability of Gibrat's law to those CO2 emissions. Finally, we employ the lognormal model to predict emission parameters for the coming years and propose two policies for reducing total fossil CO2 emissions. Our research aims to provide policy-makers with accurate and detailed information to support effective climate change mitigation strategies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00707",
        "category": "q-fin",
        "title": "Dimensionality reduction techniques to support insider trading detection",
        "abstract": "Identification of market abuse is an extremely complicated activity that requires the analysis of large and complex datasets. We propose an unsupervised machine learning method for contextual anomaly detection, which allows to support market surveillance aimed at identifying potential insider trading activities. This method lies in the reconstruction-based paradigm and employs principal component analysis and autoencoders as dimensionality reduction techniques. The only input of this method is the trading position of each investor active on the asset for which we have a price sensitive event (PSE). After determining reconstruction errors related to the trading profiles, several conditions are imposed in order to identify investors whose behavior could be suspicious of insider trading related to the PSE. As a case study, we apply our method to investor resolved data of Italian stocks around takeover bids.",
        "references": [
            {
                "arxivId": "2212.05912",
                "title": "A machine learning approach to support decision in insider trading detection",
                "abstract": "Identifying market abuse activity from data on investors' trading activity is very challenging both for the data volume and for the low signal to noise ratio. Here we propose two complementary unsupervised machine learning methods to support market surveillance aimed at identifying potential insider trading activities. The first one uses clustering to identify, in the vicinity of a price sensitive event such as a takeover bid, discontinuities in the trading activity of an investor with respect to his/her own past trading history and on the present trading activity of his/her peers. The second unsupervised approach aims at identifying (small) groups of investors that act coherently around price sensitive events, pointing to potential insider rings, i.e. a group of synchronised traders displaying strong directional trading in rewarding position in a period before the price sensitive event. As a case study, we apply our methods to investor resolved data of Italian stocks around takeover bids."
            },
            {
                "arxivId": "2212.03637",
                "title": "Unsupervised Anomaly Detection in Time-series: An Extensive Evaluation and Analysis of State-of-the-art Methods",
                "abstract": "Unsupervised anomaly detection in time-series has been extensively investigated in the literature. Notwithstanding the relevance of this topic in numerous application fields, a complete and extensive evaluation of recent state-of-the-art techniques is still missing. Few efforts have been made to compare existing unsupervised time-series anomaly detection methods rigorously. However, only standard performance metrics, namely precision, recall, and F1-score are usually considered. Essential aspects for assessing their practical relevance are therefore neglected. This paper proposes an original and in-depth evaluation study of recent unsupervised anomaly detection techniques in time-series. Instead of relying solely on standard performance metrics, additional yet informative metrics and protocols are taken into account. In particular, (1) more elaborate performance metrics specifically tailored for time-series are used; (2) the model size and the model stability are studied; (3) an analysis of the tested approaches with respect to the anomaly type is provided; and (4) a clear and unique protocol is followed for all experiments. Overall, this extensive analysis aims to assess the maturity of state-of-the-art time-series anomaly detection, give insights regarding their applicability under real-world setups and provide to the community a more complete evaluation protocol."
            },
            {
                "arxivId": "2209.11686",
                "title": "Anomaly Detection on Financial Time Series by Principal Component Analysis and Neural Networks",
                "abstract": "A major concern when dealing with financial time series involving a wide variety ofmarket risk factors is the presence of anomalies. These induce a miscalibration of the models used toquantify and manage risk, resulting in potential erroneous risk measures. We propose an approachthat aims to improve anomaly detection in financial time series, overcoming most of the inherentdifficulties. Valuable features are extracted from the time series by compressing and reconstructingthe data through principal component analysis. We then define an anomaly score using a feedforwardneural network. A time series is considered to be contaminated when its anomaly score exceeds agiven cutoff value. This cutoff value is not a hand-set parameter but rather is calibrated as a neuralnetwork parameter throughout the minimization of a customized loss function. The efficiency of theproposed approach compared to several well-known anomaly detection algorithms is numericallydemonstrated on both synthetic and real data sets, with high and stable performance being achievedwith the PCA NN approach. We show that value-at-risk estimation errors are reduced when theproposed anomaly detection model is used with a basic imputation approach to correct the anomaly."
            },
            {
                "arxivId": "2208.00181",
                "title": "How Covid mobility restrictions modified the population of investors in Italian stock markets  (L\u2019evoluzione della composizione del retail trading sul mercato azionario italiano a seguito delle restrizioni imposte dalla pandemia da Covid)  CONSOB - Scuola Normale Superiore di Pisa",
                "abstract": "This paper investigates how Covid mobility restrictions impacted the population of investors of the Italian stock market. The analysis tracks the trading activity of individual investors in Italian stocks in the period January 2019-September 2021, investigating how their composition and the trading activity changed around the Covid-19 lockdown period (March 9 - May 19, 2020) and more generally in the period of the pandemic. The results pinpoint that the lockdown restriction was accompanied by a surge in interest toward stock market, as testi\ufb01ed by the trading volume by households. Given the generically falling prices during the lockdown, the households, which are typically contrarian, were net buyers, even if less than expected from their trading activity in 2019. This can be explained by the arrival, during the lockdown, of a group of \u223c 185k new investors (i.e. which had never traded since January 2019) which were on average ten year younger and with a larger fraction of males than the pre-lockdown investors. By looking at the gross P&L, there is clear evidence that these new investors were more skilled in trading. There are thus indications that the lockdown, and more generally the Covid pandemic, created a sort of regime change in the population of \ufb01nancial investors."
            },
            {
                "arxivId": "2105.00733",
                "title": "The Doge of Wall Street: Analysis and Detection of Pump and Dump Cryptocurrency Manipulations",
                "abstract": "Cryptocurrencies are increasingly popular. Even people who are not experts have started to invest in these assets, and nowadays, cryptocurrency exchanges process transactions for over 100 billion US dollars per month. Despite this, many cryptocurrencies have low liquidity and are highly prone to market manipulation. This paper performs an in-depth analysis of two market manipulations organized by communities over the Internet: The pump and dump and the crowd pump. The pump and dump scheme is a fraud as old as the stock market. Now, it has new vitality in the loosely regulated market of cryptocurrencies. Groups of highly coordinated people systematically arrange this scam, usually on Telegram and Discord. We monitored these groups for more than 3 years, detecting around 900 individual events. We report on three case studies related to pump and dump groups. We leverage our unique dataset of the verified pump and dumps to build a machine learning model able to detect a pump and dump in 25 seconds from the moment it starts, achieving the results of 94.5% of F1-score. Then, we move on to the crowd pump, a new phenomenon that hit the news in the first months of 2021, when a Reddit community inflated the price of the GameStop stocks (GME) by over 1,900% on Wall Street, the world\u2019s largest stock exchange. Later, other Reddit communities replicated the operation on the cryptocurrency markets. The targets were DogeCoin (DOGE) and Ripple (XRP). We reconstruct how these operations developed and discuss differences and analogies with the standard pump and dump. We believe this study helps understand a widespread phenomenon affecting cryptocurrency markets. The detection algorithms we develop effectively detect these events in real-time and helps investors stay out of the market when these frauds are in action."
            },
            {
                "arxivId": "1404.4679",
                "title": "Graph based anomaly detection and description: a survey",
                "abstract": null
            },
            {
                "arxivId": "1008.1414",
                "title": "Statistically Validated Networks in Bipartite Complex Systems",
                "abstract": "Many complex systems present an intrinsic bipartite structure where elements of one set link to elements of the second set. In these complex systems, such as the system of actors and movies, elements of one set are qualitatively different than elements of the other set. The properties of these complex systems are typically investigated by constructing and analyzing a projected network on one of the two sets (for example the actor network or the movie network). Complex systems are often very heterogeneous in the number of relationships that the elements of one set establish with the elements of the other set, and this heterogeneity makes it very difficult to discriminate links of the projected network that are just reflecting system's heterogeneity from links relevant to unveil the properties of the system. Here we introduce an unsupervised method to statistically validate each link of a projected network against a null hypothesis that takes into account system heterogeneity. We apply the method to a biological, an economic and a social complex system. The method we propose is able to detect network structures which are very informative about the organization and specialization of the investigated systems, and identifies those relationships between elements of the projected network that cannot be explained simply by system heterogeneity. We also show that our method applies to bipartite systems in which different relationships might have different qualitative nature, generating statistically validated networks in which such difference is preserved."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-04.json",
        "arxivId": "2403.00746",
        "category": "q-fin",
        "title": "A time-stepping deep gradient flow method for option pricing in (rough) diffusion models",
        "abstract": "We develop a novel deep learning approach for pricing European options in diffusion models, that can efficiently handle high-dimensional problems resulting from Markovian approximations of rough volatility models. The option pricing partial differential equation is reformulated as an energy minimization problem, which is approximated in a time-stepping fashion by deep artificial neural networks. The proposed scheme respects the asymptotic behavior of option prices for large levels of moneyness, and adheres to a priori known bounds for option prices. The accuracy and efficiency of the proposed method is assessed in a series of numerical examples, with particular focus in the lifted Heston model.",
        "references": [
            {
                "arxivId": "2401.06740",
                "title": "A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models",
                "abstract": "We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model."
            },
            {
                "arxivId": "2306.02708",
                "title": "From elephant to goldfish (and back): memory in stochastic Volterra processes",
                "abstract": "We propose a new theoretical framework that exploits convolution kernels to transform a Volterra path-dependent (non-Markovian) stochastic process into a standard (Markovian) diffusion process. This transformation is achieved by embedding a Markovian\"memory process\"within the dynamics of the non-Markovian process. We discuss existence and path-wise regularity of solutions for the stochastic Volterra equations introduced and we provide a financial application to volatility modeling. We also propose a numerical scheme for simulating the processes. The numerical scheme exhibits a strong convergence rate of 1/2, which is independent of the roughness parameter of the volatility process. This is a significant improvement compared to Euler schemes used in similar models. We propose a new theoretical framework that exploits convolution kernels to transform a Volterra path-dependent (non-Markovian) stochastic process into a standard (Markovian) diffusion process. This transformation is achieved by embedding a Markovian\"memory process\"(the goldfish) within the dynamics of the non-Markovian process (the elephant). Most notably, it is also possible to go back, i.e., the transformation is reversible. We discuss existence and path-wise regularity of solutions for the stochastic Volterra equations introduced and we propose a numerical scheme for simulating the processes, which exhibits a remarkable convergence rate of $1/2$. In particular, in the fractional kernel case, the strong convergence rate is independent of the roughness parameter, which is a positive novelty in contrast with what happens in the available Euler schemes in the literature in rough volatility models."
            },
            {
                "arxivId": "2305.01035",
                "title": "Random Neural Networks for Rough Volatility",
                "abstract": "We construct a deep learning-based numerical algorithm to solve path-dependent partial differential equations arising in the context of rough volatility. Our approach is based on interpreting the PDE as a solution to an SPDE, building upon recent insights by Bayer, Qiu and Yao, and on constructing a neural network of reservoir type as originally developed by Gonon, Grigoryeva, Ortega. The reservoir approach allows us to formulate the optimisation problem as a simple least-square regression for which we prove theoretical convergence properties."
            },
            {
                "arxivId": "2109.14851",
                "title": "The Deep Minimizing Movement Scheme",
                "abstract": null
            },
            {
                "arxivId": "2004.10571",
                "title": "Large and moderate deviations for stochastic Volterra systems",
                "abstract": null
            },
            {
                "arxivId": "1912.01309",
                "title": "Deep Nitsche Method: Deep Ritz Method with Essential Boundary Conditions",
                "abstract": "We propose a method due to Nitsche (Deep Nitsche Method) from 1970s to deal with the essential boundary conditions encountered in the deep learning-based numerical method without significant extra computational costs. The method inherits several advantages from Deep Ritz Method~\\cite{EYu:2018} while successfully overcomes the difficulties in treatment of the essential boundary conditions. We illustrate the method on several representative problems posed in at most 100 dimensions with complicated boundary conditions. The numerical results clearly show that the Deep Nitsche Method is naturally nonlinear, naturally adaptive and has the potential to work on rather high dimensions."
            },
            {
                "arxivId": "1906.02551",
                "title": "Deep Curve-Dependent PDEs for Affine Rough Volatility",
                "abstract": "We introduce a new deep-learning based algorithm to evaluate options in affine rough stochastic volatility models. We show that the pricing function is the solution to a curve-dependent PDE (CPDE), depending on forward curves rather than the whole path of the process, for which we develop a numerical scheme based on deep learning techniques. Numerical simulations suggest that the latter is extremely efficient, and provides a good alternative to classical Monte Carlo simulations."
            },
            {
                "arxivId": "1901.09647",
                "title": "Deep learning volatility: a deep neural network perspective on pricing and calibration in (rough) volatility models",
                "abstract": "We present a neural network-based calibration method that performs the calibration task within a few milliseconds for the full implied volatility surface. The framework is consistently applicable throughout a range of volatility models\u2014including second-generation stochastic volatility models and the rough volatility family\u2014and a range of derivative contracts. Neural networks in this work are used in an off-line approximation of complex pricing functions, which are difficult to represent or time-consuming to evaluate by other means. The form in which information from available data is extracted and used influences network performance: The grid-based algorithm used for calibration is inspired by representing the implied volatility and option prices as a collection of pixels. We highlight how this perspective opens new horizons for quantitative modelling. The calibration bottleneck posed by a slow pricing of derivative contracts is lifted, and stochastic volatility models (classical and rough) can be handled in great generality as the framework also allows taking the forward variance curve as an input. We demonstrate the calibration performance both on simulated and historical data, on different derivative contracts and on a number of example models of increasing complexity, and also showcase some of the potentials of this approach towards model recognition. The algorithm and examples are provided in the Github repository GitHub: NN-StochVol-Calibrations."
            },
            {
                "arxivId": "1710.07481",
                "title": "A regularity structure for rough volatility",
                "abstract": "A new paradigm has emerged recently in financial modeling: rough (stochastic) volatility. First observed by Gatheral et al. in high\u2010frequency data, subsequently derived within market microstructure models, rough volatility captures parsimoniously key\u2010stylized facts of the entire implied volatility surface, including extreme skews (as observed earlier by Al\u00f2s et al.) that were thought to be outside the scope of stochastic volatility models. On the mathematical side, Markovianity and, partially, semimartingality are lost. In this paper, we show that Hairer's regularity structures, a major extension of rough path theory, which caused a revolution in the field of stochastic partial differential equations, also provide a new and powerful tool to analyze rough volatility models."
            },
            {
                "arxivId": "1708.07469",
                "title": "DGM: A deep learning algorithm for solving partial differential equations",
                "abstract": null
            },
            {
                "arxivId": "1708.01121",
                "title": "Asymptotic behaviour of randomised fractional volatility models",
                "abstract": "Abstract We study the asymptotic behaviour of a class of small-noise diffusions driven by fractional Brownian motion, with random starting points. Different scalings allow for different asymptotic properties of the process (small-time and tail behaviours in particular). In order to do so, we extend some results on sample path large deviations for such diffusions. As an application, we show how these results characterise the small-time and tail estimates of the implied volatility for rough volatility models, recently proposed in mathematical finance."
            },
            {
                "arxivId": "1707.02568",
                "title": "Solving high-dimensional partial differential equations using deep learning",
                "abstract": "Significance Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the \u201ccurse of dimensionality.\u201d This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships. Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the \u201ccurse of dimensionality.\u201d This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black\u2013Scholes equation, the Hamilton\u2013Jacobi\u2013Bellman equation, and the Allen\u2013Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships."
            },
            {
                "arxivId": "1703.05132",
                "title": "Short-time near-the-money skew in rough fractional volatility models",
                "abstract": "We consider rough stochastic volatility models where the driving noise of volatility has fractional scaling, in the \u2018rough\u2019 regime of Hurst parameter . This regime recently attracted a lot of attention both from the statistical and option pricing point of view. With focus on the latter, we sharpen the large deviation results of Forde-Zhang [Asymptotics for rough stochastic volatility models. SIAM J. Financ. Math., 2017, 8(1), 114\u2013145] in a way that allows us to zoom-in around the money while maintaining full analytical tractability. More precisely, this amounts to proving higher order moderate deviation estimates, only recently introduced in the option pricing context. This in turn allows us to push the applicability range of known at-the-money skew approximation formulae from CLT type log-moneyness deviations of order (works of Al\u00f2s, Le\u00f3n & Vives and Fukasawa) to the wider moderate deviations regime."
            },
            {
                "arxivId": "1609.02108",
                "title": "The characteristic function of rough Heston models",
                "abstract": "It has been recently shown that rough volatility models, where the volatility is driven by a fractional Brownian motion with small Hurst parameter, provide very relevant dynamics in order to reproduce the behavior of both historical and implied volatilities. However, due to the non\u2010Markovian nature of the fractional Brownian motion, they raise new issues when it comes to derivatives pricing. Using an original link between nearly unstable Hawkes processes and fractional volatility models, we compute the characteristic function of the log\u2010price in rough Heston models. In the classical Heston model, the characteristic function is expressed in terms of the solution of a Riccati equation. Here, we show that rough Heston models exhibit quite a similar structure, the Riccati equation being replaced by a fractional Riccati equation."
            },
            {
                "arxivId": "1507.03004",
                "title": "Hybrid scheme for Brownian semistationary processes",
                "abstract": null
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            },
            {
                "arxivId": "1404.5140",
                "title": "High-Order Compact Finite Difference Scheme for Option Pricing in Stochastic Volatility Models",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2104.04264",
        "category": "q-fin",
        "title": "Risks of heterogeneously persistent higher moments",
        "abstract": "Using intraday data for the cross-section of individual stocks, we show that both transitory and persistent fluctuations in realized market and average idiosyncratic volatility, skewness and kurtosis are differentially priced in the cross-section of asset returns, implying a heterogeneous persistence structure of different sources of higher moment risks. Specifically, we find that idiosyncratic transitory shocks to volatility as well as idiosyncratic persistent shocks to skewness contain strong commonalities that are relevant to investors.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2206.13679",
        "category": "q-fin",
        "title": "Diversification Quotients: Quantifying Diversification via Risk Measures",
        "abstract": "We establish the first axiomatic theory for diversification indices using six intuitive axioms: non-negativity, location invariance, scale invariance, rationality, normalization, and continuity. The unique class of indices satisfying these axioms, called the diversification quotients (DQs), are defined based on a parametric family of risk measures. A further axiom of portfolio convexity pins down DQ based on coherent risk measures. DQ has many attractive properties, and it can address several theoretical and practical limitations of existing indices. In particular, for the popular risk measures Value-at-Risk and Expected Shortfall, the corresponding DQ admits simple formulas and it is efficient to optimize in portfolio selection. Moreover, it can properly capture tail heaviness and common shocks, which are neglected by traditional diversification indices. When illustrated with financial data, DQ is intuitive to interpret, and its performance is competitive against other diversification indices.",
        "references": [
            {
                "arxivId": "2301.03517",
                "title": "Diversification quotients based on VaR and ES",
                "abstract": null
            },
            {
                "arxivId": "2012.11972",
                "title": "Acceptability maximization",
                "abstract": "The aim of this paper is to study the optimal investment problem by using coherent acceptability indices (CAIs) as a tool to measure the portfolio performance. We call this problem the acceptability maximization. First, we study the one-period (static) case, and propose a numerical algorithm that approximates the original problem by a sequence of risk minimization problems. The results are applied to several important CAIs, such as the gain-to-loss ratio, the risk-adjusted return on capital and the tail-value-at-risk based CAI. In the second part of the paper we investigate the acceptability maximization in a discrete time dynamic setup. Using robust representations of CAIs in terms of a family of dynamic coherent risk measures (DCRMs), we establish an intriguing dichotomy: if the corresponding family of DCRMs is recursive (i.e. strongly time consistent) and assuming some recursive structure of the market model, then the acceptability maximization problem reduces to just a one period problem and the maximal acceptability is constant across all states and times. On the other hand, if the family of DCRMs is not recursive, which is often the case, then the acceptability maximization problem ordinarily is a time-inconsistent stochastic control problem, similar to the classical mean-variance criteria. To overcome this form of time-inconsistency, we adapt to our setup the set-valued Bellman's principle recently proposed in [23] applied to two particular dynamic CAIs - the dynamic risk-adjusted return on capital and the dynamic gain-to-loss ratio. The obtained theoretical results are illustrated via numerical examples that include, in particular, the computation of the intermediate mean-risk efficient frontiers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2206.15098",
        "category": "q-fin",
        "title": "Talent Hoarding in Organizations",
        "abstract": "Most organizations rely on managers to identify talented workers. However, managers who are evaluated on team performance have an incentive to hoard workers. This study provides the first empirical evidence of talent hoarding using personnel records and survey evidence from a large manufacturing firm. Talent hoarding is reported by three-fourths of managers, is detectable in managerial decisions, and occurs more frequently when hoarding incentives are stronger. Using quasi-random variation in exposure to talent hoarding, I demonstrate that hoarding deters workers from applying to new positions, inhibiting worker career progression and altering the allocation of talent in the firm.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2208.01353",
        "category": "q-fin",
        "title": "On the implied volatility of Asian options under stochastic volatility models",
        "abstract": "In this paper we study the short-time behavior of the at-the-money implied volatility for arithmetic Asian options with fixed strike price. The asset price is assumed to follow the Black-Scholes model with a general stochastic volatility process. Using techniques of the Malliavin calculus such as the anticipating Ito's formula we first compute the level of the implied volatility of the option when the maturity converges to zero. Then, we find and short maturity asymptotic formula for the skew of the implied volatility that depends on the roughness of the volatility model. We apply our general results to the SABR model and the rough Bergomi model, and provide some numerical simulations that confirm the accurateness of the asymptotic formula for the skew.",
        "references": [
            {
                "arxivId": "1702.03382",
                "title": "SHORT MATURITY ASIAN OPTIONS FOR THE CEV MODEL",
                "abstract": "We present a rigorous study of the short maturity asymptotics for Asian options with continuous-time averaging, under the assumption that the underlying asset follows the constant elasticity of variance (CEV) model. The leading order short maturity limit of the Asian option prices under the CEV model is obtained in closed form. We propose an analytical approximation for the Asian options prices which reproduces the exact short maturity asymptotics, and demonstrate good numerical agreement of the asymptotic results with Monte Carlo simulations and benchmark test cases for option parameters relevant for practical applications."
            },
            {
                "arxivId": "1609.07559",
                "title": "Short Maturity Asian Options in Local Volatility Models",
                "abstract": "We present a rigorous study of the short maturity asymptotics for Asian options with continuous-time averaging, under the assumption that the underlying asset follows a local volatility model. The asymptotics for out-of-the-money, in-the-money, and at-the-money cases are derived, considering both fixed strike and floating strike Asian options. The asymptotics for the out-of-the-money case involves a non-trivial variational problem which is solved completely. We present an analytical approximation for Asian options prices, and demonstrate good numerical agreement of the asymptotic results with the results of Monte Carlo simulations and benchmark test cases in the Black-Scholes model for option parameters relevant in practical applications."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2307.11846",
        "category": "q-fin",
        "title": "Social and individual learning in the Minority Game",
        "abstract": "We study the roles of social and individual learning on outcomes of the Minority Game model of a financial market. Social learning occurs via agents adopting the strategies of their neighbours within a social network, while individual learning results in agents changing their strategies without input from other agents. In particular, we show how social learning can undermine efficiency of the market due to negative frequency dependent selection and loss of strategy diversity. The latter of which can lock the population into a maximally inefficient state. We show how individual learning can rescue a population engaged in social learning from such inefficiencies.",
        "references": [
            {
                "arxivId": "1406.1137",
                "title": "Crowds on Wall Street: Extracting Value from Social Investing Platforms",
                "abstract": "For decades, the world of financial advisors has been dom- inated by large investment banks such as Goldman Sachs. In recent years, user-contributed investment services suc h as SeekingAlpha and StockTwits have grown to millions of users. In this paper, we seek to understand the quality and impact of content on social investment platforms, by empiri- cally analyzing complete datasets of SeekingAlpha articles (9 years) and StockTwits messages (4 years). We develop sen- timent analysis tools and correlate contributed content to the historical performance of relevant stocks. While SeekingAl- pha articles and StockTwits messages provide minimal corre- lation to stock performance in aggregate, a subset of authors contribute more valuable (predictive) content. We show that these authors can be identified via both empirical methods or by user interactions, and investments using their analysis sig- nificantly outperform broader markets. Finally, we conduct a user survey that sheds light on users views of SeekingAlpha content and stock manipulation. In this paper, we seek to understand the quality and impact of opinions and analysis shared on social investment platforms. We target the two primary yet quite different social invest- ment platforms, SeekingAlpha and StockTwits, and analyze the potential for investment returns following their recom - mendations versus the market baseline, the S&P 500 stock market index. We seek to understand how expertise of con- tributors can affect the quality and utility of contributed con- tent, using SeekingAlpha as an \"expert\" model (all content is contributed by less than 0.27% of users) and StockTwits as a \"peer\" model (any user can contribute). Our work makes four key contributions. First, we gather longitudinal datasets from both platforms since their inception (9 years of data for SeekingAlpha, 4 years for StockTwits). We develop sentiment analyzers on each dataset, using a mixture of keyword processing and machine learning classifiers. Validation shows our methods achieve high accuracy in extracting sentiments towards in- dividual stocks (85.5% for SeekingAlpha, 76.2% for Stock- Twits). Second, we analyze correlation between content sentiment from both services with stock returns at different time scal es. We show that content from both SeekingAlpha and Stock- Twits provide minimal forward correlation with stock perfor- mance. While the average article provides little value, we find that a subset of \"top authors\" in SeekingAlpha contribut e content that shows significantly higher correlation with fu ture stock performance. Third, we evaluate the hypothetical performance of simple investment strategies following top authors from both plat- forms. We show that investment strategies based on stock sentiment from top SeekingAlpha authors perform exception- ally well and significantly outperform broader markets. In contrast, strategies relying on StockTwits generally unde rper- form relative to broader markets. In addition, we show that we can identify top authors without historical stock market data, using only user interactions with their articles as a g uide. Fourth, we conduct a large scale survey of SeekingAlpha users and contributors to understand their usage, reliance , and trust in the SeekingAlpha service. Results show that despite seeing potentially intentionally misleading or manipulat ive articles, most users still rely heavily on the site content f or investment advice. Most consider SeekingAlpha unique, and"
            },
            {
                "arxivId": "1312.0690",
                "title": "Self-organization and phase transition in financial markets with multiple choices",
                "abstract": null
            },
            {
                "arxivId": "0811.1479",
                "title": "Minority Games",
                "abstract": null
            },
            {
                "arxivId": "physics/0502140",
                "title": "Inter-pattern speculation: beyond minority, majority and $-games",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0402028",
                "title": "Evolutionary minority game: the roles of response time and mutation threshold.",
                "abstract": "In the evolutionary minority game, agents are allowed to evolve their strategies (\"mutate\") based on past experience. We explore the dependence of the system's global behavior on the response time and the mutation threshold of the agents. We find that the precise values of these parameters determine if the strategy distribution of the population has a U shape, inverse U shape, or W shape. It is shown that in a free society (market), highly adaptive agents (with short response times) perform best. In addition, \"patient\" agents (with high mutation thresholds) outperform \"nervous\" ones."
            },
            {
                "arxivId": "cond-mat/0212635",
                "title": "The evolutionary minority game with local coordination",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0209525",
                "title": "Intelligent minority game with genetic crossover strategies",
                "abstract": null
            },
            {
                "arxivId": "physics/0203038",
                "title": "Time series analysis for minority game simulations of financial markets",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0107236",
                "title": "Harms and benefits from social imitation",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0101326",
                "title": "Stylized Facts of Financial Markets and Market Crashes in Minority Games",
                "abstract": "We present and study a Minority Game based model of a financial market where adaptive agents\u2014the speculators\u2014interact with deterministic agents\u2014called producers. Speculators trade only if they detect predictable patterns which grant them a positive gain. Indeed the average number of active speculators grows with the amount of information that producers inject into the market. Transitions between equilibrium and out of equilibrium behavior are observed when the relative number of speculators to the complexity of information or to the number of producers are changed. When the system is out of equilibrium, stylized facts arise, such as fat tailed distribution of returns and volatility clustering. Without speculators, the price follows a random walk; this implies that stylized facts arise because of the presence of speculators. Furthermore, if speculators abandon price taking behavior, stylized facts disappear."
            },
            {
                "arxivId": "cond-mat/0011042",
                "title": "From Minority Games to real markets",
                "abstract": "We address the question of market efficiency using the Minority Game (MG) model. First we show that removing unrealistic features of the MG leads to models which reproduce a scaling behaviour close to what is observed in real markets. In particular we find that (i) fat tails and clustered volatility arise at the phase transition point and that (ii) the crossover to random walk behaviour of prices is a finite-size effect. This, on one hand, suggests that markets operate close to criticality, where the market is marginally efficient. On the other it allows one to measure the distance from criticality of real markets, using cross-over times. The artificial market described by the MG is then studied as an ecosystem with different species of traders. This clarifies the nature of the interaction and the particular role played by the various populations."
            },
            {
                "arxivId": "cond-mat/0006098",
                "title": "Social organization in the Minority Game model",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0003379",
                "title": "Theory of the evolutionary minority game",
                "abstract": "We present a theory describing a recently introduced model of an evolving, adaptive system in which agents compete to be in the minority. The agents themselves are able to evolve their strategies over time in an attempt to improve their performance. The theory explicitly demonstrates the self-interaction, or market impact, that agents in such systems experience."
            },
            {
                "arxivId": "adap-org/9912001",
                "title": "Frontiers of finance: evolution and efficient markets.",
                "abstract": "In this review article, we explore several recent advances in the quantitative modeling of financial markets. We begin with the Efficient Markets Hypothesis and describe how this controversial idea has stimulated a number of new directions of research, some focusing on more elaborate mathematical models that are capable of rationalizing the empirical facts, others taking a completely different tack in rejecting rationality altogether. One of the most promising directions is to view financial markets from a biological perspective and, specifically, within an evolutionary framework in which markets, instruments, institutions, and investors interact and evolve dynamically according to the \"law\" of economic selection. Under this view, financial agents compete and adapt, but they do not necessarily do so in an optimal fashion. Evolutionary and ecological models of financial markets is truly a new frontier whose exploration has just begun."
            },
            {
                "arxivId": "adap-org/9906001",
                "title": "Evolution in minority games. (II). Games with variable strategy spaces",
                "abstract": null
            },
            {
                "arxivId": "adap-org/9903008",
                "title": "Evolution in minority games. (I). Games with a fixed strategy space",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9810142",
                "title": "Self-Organized Segregation within an Evolving Population",
                "abstract": "An evolving population, in which individual members (``agents'') adapt their behavior according to past experience, is of central importance to many disciplines. Because of their limited knowledge and capabilities, agents are forced to make decisions based on inductive, rather than deductive, thinking. We show that a population of competing agents with similar capabilities and knowledge will tend to self-segregate into opposing groups characterized by extreme behavior. Cautious agents perform poorly and tend to become rare."
            },
            {
                "arxivId": "cond-mat/9710096",
                "title": "Evolutionary prisoner's dilemma game on a square lattice",
                "abstract": "A simplified prisoner's game is studied on a square lattice when the players interacting with their neighbors can follow two strategies: to cooperate $(C)$ or to defect $(D)$ unconditionally. The players updated in random sequence have a chance to adopt one of the neighboring strategies with a probability depending on the payoff difference. Using Monte Carlo simulations and dynamical cluster techniques, we study the density $c$ of cooperators in the stationary state. This system exhibits a continuous transition between the two absorbing states when varying the value of temptation to defect. In the limits $\\stackrel{\\ensuremath{\\rightarrow}}{c}0$ and 1 we have observed critical transitions belonging to the universality class of directed percolation."
            },
            {
                "arxivId": "adap-org/9708006",
                "title": "Emergence of cooperation and organization in an evolutionary game",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2401.08323",
        "category": "q-fin",
        "title": "Dynamic portfolio selection under generalized disappointment aversion",
        "abstract": "This paper addresses the continuous-time portfolio selection problem under generalized disappointment aversion (GDA). The implicit definition of the certainty equivalent within GDA preferences introduces time inconsistency to this problem. We provide the sufficient and necessary condition for a strategy to be an equilibrium by a fully nonlinear integral equation. Investigating the existence and uniqueness of the solution to the integral equation, we establish the existence and uniqueness of the equilibrium. Our findings indicate that under disappointment aversion preferences, non-participation in the stock market is the unique equilibrium. The semi-analytical equilibrium strategies obtained under the constant relative risk aversion utility functions reveal that, under GDA preferences, the investment proportion in the stock market consistently remains smaller than the investment proportion under classical expected utility theory. The numerical analysis shows that the equilibrium strategy's monotonicity concerning the two parameters of GDA preference aligns with the monotonicity of the degree of risk aversion.",
        "references": [
            {
                "arxivId": "2312.15173",
                "title": "Equilibrium stochastic control with implicitly defined objective functions",
                "abstract": "This paper considers a class of stochastic control problems with implicitly defined objective functions, which are the sources of time-inconsistency. We study the closed-loop equilibrium solutions in a general controlled diffusion framework. First, we provide a sufficient and necessary condition for a strategy to be an equilibrium. Then, we apply the result to discuss two problems of dynamic portfolio selection for a class of betweenness preferences, allowing for closed convex constraints on portfolio weights and borrowing cost, respectively. The equilibrium portfolio strategies are explicitly characterized in terms of the solutions of some first-order ordinary differential equations for the case of deterministic market coefficients."
            },
            {
                "arxivId": "2311.06745",
                "title": "Dynamic portfolio selection for nonlinear law-dependent preferences",
                "abstract": "This paper addresses the portfolio selection problem for nonlinear law-dependent preferences in continuous time, which inherently exhibit time inconsistency. Employing the method of stochastic maximum principle, we establish verification theorems for equilibrium strategies, accommodating both random market coefficients and incomplete markets. We derive the first-order condition (FOC) for the equilibrium strategies, using a notion of functional derivatives with respect to probability distributions. Then, with the help of the FOC we obtain the equilibrium strategies in closed form for two classes of implicitly defined preferences: CRRA and CARA betweenness preferences, with deterministic market coefficients. Finally, to show applications of our theoretical results to problems with random market coefficients, we examine the weighted utility. We reveal that the equilibrium strategy can be described by a coupled system of Quadratic Backward Stochastic Differential Equations (QBSDEs). The well-posedness of this system is generally open but is established under the special structures of our problem."
            },
            {
                "arxivId": "2006.01979",
                "title": "Consistent Investment of Sophisticated Rank-Dependent Utility Agents in Continuous Time",
                "abstract": "We study portfolio selection in a complete continuous-time market where the preference is dictated by the rank-dependent utility. As such a model is inherently time inconsistent due to the underlying probability weighting, we study the investment behavior of sophisticated consistent planners who seek (subgame perfect) intra-personal equilibrium strategies. We provide sufficient conditions under which an equilibrium strategy is a replicating portfolio of a final wealth. We derive this final wealth profile explicitly, which turns out to be in the same form as in the classical Merton model with the market price of risk process properly scaled by a deterministic function in time. We present this scaling function explicitly through the solution to a highly nonlinear and singular ordinary differential equation, whose existence of solutions is established. Finally, we give a necessary and sufficient condition for the scaling function to be smaller than 1 corresponding to an effective reduction in risk premium due to probability weighting."
            },
            {
                "arxivId": "2002.12572",
                "title": "Me, myself and I: A general theory of non-Markovian time-inconsistent stochastic control for sophisticated agents",
                "abstract": "We develop a theory for continuous-time non-Markovian stochastic control problems which are inherently time-inconsistent. Their distinguishing feature is that the classical Bellman optimality principle no longer holds. Our formulation is cast within the framework of a controlled non-Markovian forward stochastic differential equation, and a general objective functional setting. We adopt a game-theoretic approach to study such problems, meaning that we seek for \\emph{sub-game perfect Nash equilibrium} points. As a first novelty of this work, we introduce and motivate a new definition of equilibrium that allows us to establish rigorously an \\emph{extended dynamic programming principle}, in the same spirit as in the classical theory. This in turn allows us to introduce a system of backward stochastic differential equations analogous to the classical HJB equation. We prove that this system is fundamental, in the sense that its well-posedness is both necessary and sufficient to characterise the value function and equilibria. As a final step we provide an existence and uniqueness result. Some examples and extensions of our results are also presented."
            },
            {
                "arxivId": "1912.01281",
                "title": "Time-Inconsistent Consumption-Investment Problems in Incomplete Markets under General Discount Functions",
                "abstract": "In this paper we study a time-inconsistent consumption-investment problem with random endowments in a possibly incomplete market under general discount functions. We provide a necessary condition and a verification theorem for an open-loop equilibrium consumption-investment pair in terms of a coupled forward-backward stochastic differential equation. Moreover, we prove the uniqueness of the open-loop equilibrium pair by showing that the original time-inconsistent problem is equivalent to an associated time-consistent one."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2402.15828",
        "category": "q-fin",
        "title": "Pricing of geometric Asian options in the Volterra-Heston model",
        "abstract": "Geometric Asian options are a type of options where the payoff depends on the geometric mean of the underlying asset over a certain period of time. This paper is concerned with the pricing of such options for the class of Volterra-Heston models, covering the rough Heston model. We are able to derive semi-closed formulas for the prices of geometric Asian options with fixed and floating strikes for this class of stochastic volatility models. These formulas require the explicit calculation of the conditional joint Fourier transform of the logarithm of the stock price and the logarithm of the geometric mean of the stock price over time. Linking our problem to the theory of affine Volterra processes, we find a representation of this Fourier transform as a suitably constructed stochastic exponential, which depends on the solution of a Riccati-Volterra equation. Finally we provide a numerical study for our results in the rough Heston model.",
        "references": [
            {
                "arxivId": "2103.11734",
                "title": "American Options in the Volterra Heston Model",
                "abstract": "We price American options using kernel-based approximations of the Volterra Heston model. We choose these approximations because they allow simulation-based techniques for pricing. We prove the convergence of American option prices in the approximating sequence of models towards the prices in the Volterra Heston model. A crucial step in the proof is to exploit the a\ufb03ne structure of the model in order to establish explicit formulas and convergence results for the conditional Fourier\u2013Laplace transform of the log price and an adjusted version of the forward variance. We illustrate with numerical examples our convergence result and the behavior of American option prices with respect to certain parameters of the model."
            },
            {
                "arxivId": "1802.01641",
                "title": "Volatility Options in Rough Volatility Models",
                "abstract": "We discuss the pricing and hedging of volatility options in some rough volatility models. First, we develop efficient Monte Carlo methods and asymptotic approximations for computing option prices and hedge ratios in models where log-volatility follows a Gaussian Volterra process. While providing a good fit for European options, these models are unable to reproduce the VIX option smile observed in the market, and are thus not suitable for VIX products. To accommodate these, we introduce the class of modulated Volterra processes, and show that they successfully capture the VIX smile."
            },
            {
                "arxivId": "1703.05049",
                "title": "Perfect hedging in rough Heston models",
                "abstract": "Rough volatility models are known to reproduce the behavior of historical volatility data while at the same time fitting the volatility surface remarkably well, with very few parameters. However, managing the risks of derivatives under rough volatility can be intricate since the dynamics involve fractional Brownian motion. We show in this paper that surprisingly enough, explicit hedging strategies can be obtained in the case of rough Heston models. The replicating portfolios contain the underlying asset and the forward variance curve, and lead to perfect hedging (at least theoretically). From a probabilistic point of view, our study enables us to disentangle the infinite-dimensional Markovian structure associated to rough volatility models."
            },
            {
                "arxivId": "1609.02108",
                "title": "The characteristic function of rough Heston models",
                "abstract": "It has been recently shown that rough volatility models, where the volatility is driven by a fractional Brownian motion with small Hurst parameter, provide very relevant dynamics in order to reproduce the behavior of both historical and implied volatilities. However, due to the non\u2010Markovian nature of the fractional Brownian motion, they raise new issues when it comes to derivatives pricing. Using an original link between nearly unstable Hawkes processes and fractional volatility models, we compute the characteristic function of the log\u2010price in rough Heston models. In the classical Heston model, the characteristic function is expressed in terms of the solution of a Riccati equation. Here, we show that rough Heston models exhibit quite a similar structure, the Riccati equation being replaced by a fractional Riccati equation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00770",
        "category": "q-fin",
        "title": "Blockchain Metrics and Indicators in Cryptocurrency Trading",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00772",
        "category": "q-fin",
        "title": "Do Weibo Platform Experts Perform Better at Predicting Stock Market?",
        "abstract": null,
        "references": [
            {
                "arxivId": "1901.10496",
                "title": "Impact of Training Dataset Size on Neural Answer Selection Models",
                "abstract": null
            },
            {
                "arxivId": "1301.3781",
                "title": "Efficient Estimation of Word Representations in Vector Space",
                "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00775",
        "category": "q-fin",
        "title": "Detecting Anomalous Events in Object-centric Business Processes via Graph Neural Networks",
        "abstract": "Detecting anomalies is important for identifying inefficiencies, errors, or fraud in business processes. Traditional process mining approaches focus on analyzing 'flattened', sequential, event logs based on a single case notion. However, many real-world process executions exhibit a graph-like structure, where events can be associated with multiple cases. Flattening event logs requires selecting a single case identifier which creates a gap with the real event data and artificially introduces anomalies in the event logs. Object-centric process mining avoids these limitations by allowing events to be related to different cases. This study proposes a novel framework for anomaly detection in business processes that exploits graph neural networks and the enhanced information offered by object-centric process mining. We first reconstruct and represent the process dependencies of the object-centric event logs as attributed graphs and then employ a graph convolutional autoencoder architecture to detect anomalous events. Our results show that our approach provides promising performance in detecting anomalies at the activity type and attributes level, although it struggles to detect anomalies in the temporal order of events.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00777",
        "category": "q-fin",
        "title": "Combating Financial Crimes with Unsupervised Learning Techniques: Clustering and Dimensionality Reduction for Anti-Money Laundering",
        "abstract": "Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of financial systems. One keychallenge in AML is identifying high-risk groups based on their behavior. Unsupervised learning, particularly clustering, is a promising solution for this task. However, the use of hundreds of features todescribe behavior results in a highdimensional dataset that negatively impacts clustering performance.In this paper, we investigate the effectiveness of combining clustering method agglomerative hierarchicalclustering with four dimensionality reduction techniques -Independent Component Analysis (ICA), andKernel Principal Component Analysis (KPCA), Singular Value Decomposition (SVD), Locality Preserving Projections (LPP)- to overcome the issue of high-dimensionality in AML data and improve clusteringresults. This study aims to provide insights into the most effective way of reducing the dimensionality ofAML data and enhance the accuracy of clustering-based AML systems. The experimental results demonstrate that KPCA outperforms other dimension reduction techniques when combined with agglomerativehierarchical clustering. This superiority is observed in the majority of situations, as confirmed by threedistinct validation indices.",
        "references": [
            {
                "arxivId": "hep-ph/9509307",
                "title": "SVD APPROACH TO DATA UNFOLDING",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00782",
        "category": "q-fin",
        "title": "Ploutos: Towards interpretable stock movement prediction with financial large language model",
        "abstract": "Recent advancements in large language models (LLMs) have opened new pathways for many domains. However, the full potential of LLMs in financial investments remains largely untapped. There are two main challenges for typical deep learning-based methods for quantitative finance. First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and interpretability, which impedes their application in scenarios where the justification for predictions is essential. To solve the above challenges, we propose Ploutos, a novel financial LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen contains multiple primary experts that can analyze different modal data, such as text and numbers, and provide quantitative strategies from different perspectives. Then PloutosGPT combines their insights and predictions and generates interpretable rationales. To generate accurate and faithful rationales, the training strategy of PloutosGPT leverage rearview-mirror prompting mechanism to guide GPT-4 to generate rationales, and a dynamic token weighting mechanism to finetune LLM by increasing key tokens weight. Extensive experiments show our framework outperforms the state-of-the-art methods on both prediction accuracy and interpretability.",
        "references": [
            {
                "arxivId": "2109.07958",
                "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
                "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00785",
        "category": "q-fin",
        "title": "Applying News and Media Sentiment Analysis for Generating Forex Trading Signals",
        "abstract": "The objective of this research is to examine how sentiment analysis can be employed to generate trading signals for the Foreign Exchange (Forex) market.The author assessed sentiment in social media posts and news articles pertaining to the United States Dollar (USD) using a combination of methods: lexicon-based analysis and the Naive Bayes machine learning algorithm.The findings indicate that sentiment analysis proves valuable in forecasting market movements and devising trading signals. Notably, its effectiveness is consistent across different market conditions.The author concludes that by analyzing sentiment expressed in news and social media, traders can glean insights into prevailing market sentiments towards the USD and other pertinent countries, thereby aiding trading decision-making. This study underscores the importance of weaving sentiment analysis into trading strategies as a pivotal tool for predicting market dynamics.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00796",
        "category": "q-fin",
        "title": "Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting",
        "abstract": "In this paper, we explore the application of Gaussian Processes (GPs) for predicting mean-reverting time series with an underlying structure, using relatively unexplored functional and augmented data structures. While many conventional forecasting methods concentrate on the short-term dynamics of time series data, GPs offer the potential to forecast not just the average prediction but the entire probability distribution over a future trajectory. This is particularly beneficial in financial contexts, where accurate predictions alone may not suffice if incorrect volatility assessments lead to capital losses. Moreover, in trade selection, GPs allow for the forecasting of multiple Sharpe ratios adjusted for transaction costs, aiding in decision-making. The functional data representation utilized in this study enables longer-term predictions by leveraging information from previous years, even as the forecast moves away from the current year's training data. Additionally, the augmented representation enriches the training set by incorporating multiple targets for future points in time, facilitating long-term predictions. Our implementation closely aligns with the methodology outlined in, which assessed effectiveness on commodity futures. However, our testing methodology differs. Instead of real data, we employ simulated data with similar characteristics. We construct a testing environment to evaluate both data representations and models under conditions of increasing noise, fat tails, and inappropriate kernels-conditions commonly encountered in practice. By simulating data, we can compare our forecast distribution over time against a full simulation of the actual distribution of our test set, thereby reducing the inherent uncertainty in testing time series models on real data. We enable feature prediction through augmentation and employ sub-sampling to ensure the feasibility of GPs.",
        "references": [
            {
                "arxivId": "2401.15700",
                "title": "AI-based Personalization and Trust in Digital Finance",
                "abstract": "Personalized services bridge the gap between a financial institution and its customers and are built on trust. The more we trust the product, the keener we are to disclose our personal information in order to receive a highly personalized service that maximizes consumer value. Artificial Intelligence (AI) can help financial institutions tailor relevant products and services to their customers as well as improve their credit risk management, compliance, and fraud detection capabilities by incorporating chatbots and face recognition systems. The present study has analyzed sixteen research papers using the PRISMA model to perform a Systematic Literature Review (SLR). It has identified five research gaps and corresponding questions to analyze the present scenario. One of the gaps is credit risk detection for improved personalization and trust. Finally, an AI-based credit risk detection model has been built using four supervised machine learning classifiers viz., Support Vector Machine, Random Forest, Decision Tree, and Logistic Regression. Performance comparison shows an optimal performance of the model giving accuracy of ~89%, precision of ~88%, recall of ~89%, specificity of ~89%, F1_score of ~88%, and AUC of 0.77 for the Random Forest classifier. This model is foreseen to be most suitable for envisaging customer characteristics for which personalized credit risk mitigation strategies are particularly effective as compared to other existing works presented in this study."
            },
            {
                "arxivId": "2311.16958",
                "title": "From Simulations to Reality: Enhancing Multi-Robot Exploration for Urban Search and Rescue",
                "abstract": "In this study, we present a novel hybrid algorithm, combining Levy Flight (LF) and Particle Swarm Optimization (PSO) (LF-PSO), tailored for efficient multi-robot exploration in unknown environments with limited communication and no global positioning information. The research addresses the growing interest in employing multiple autonomous robots for exploration tasks, particularly in scenarios such as Urban Search and Rescue (USAR) operations. Multiple robots offer advantages like increased task coverage, robustness, flexibility, and scalability. However, existing approaches often make assumptions such as search area, robot positioning, communication restrictions, and target information that may not hold in real-world situations. The hybrid algorithm leverages LF, known for its effectiveness in large space exploration with sparse targets, and incorporates inter-robot repulsion as a social component through PSO. This combination enhances area exploration efficiency. We redefine the local best and global best positions to suit scenarios without continuous target information. Experimental simulations in a controlled environment demonstrate the algorithm's effectiveness, showcasing improved area coverage compared to traditional methods. In the process of refining our approach and testing it in complex, obstacle-rich environments, the presented work holds promise for enhancing multi-robot exploration in scenarios with limited information and communication capabilities."
            },
            {
                "arxivId": "2308.10908",
                "title": "MLOps: A Review",
                "abstract": "Recently, Machine Learning (ML) has become a widely accepted method for significant progress that is rapidly evolving. Since it employs computational methods to teach machines and produce acceptable answers. The significance of the Machine Learning Operations (MLOps) methods, which can provide acceptable answers for such problems, is examined in this study. To assist in the creation of software that is simple to use, the authors research MLOps methods. To choose the best tool structure for certain projects, the authors also assess the features and operability of various MLOps methods. A total of 22 papers were assessed that attempted to apply the MLOps idea. Finally, the authors admit the scarcity of fully effective MLOps methods based on which advancements can self-regulate by limiting human engagement."
            },
            {
                "arxivId": "2308.11632",
                "title": "Alternative Agriculture Land-Use Transformation Pathways by Partial-Equilibrium Agricultural Sector Model: A Mathematical Approach",
                "abstract": "Humanity's progress in combating hunger, poverty, and child mortality is marred by escalating environmental degradation due to rising greenhouse gas emissions and climate change impacts. Despite positive developments, ecosystems are suffering globally. Regional strategies for mitigating and adapting to climate change must be viewed from a global perspective. The 2015 UN Sustainable Development Goals reflect the challenge of balancing social and environmental aspects for sustainable development. Agriculture, vital for food production, also threatens Earth systems. A study examines the interplay of land-use impacts, modeling crop and livestock trade, and their effects on climate, biodiversity, water, and land using a Partial-Equilibrium Agricultural Sector Model. Different scenarios involving taxing externalities related to Earth processes were tested. Results show synergies in reducing emissions, biodiversity loss, water use, and phosphorus pollution, driven by shifts in crop management. Nitrogen application and deforestation scenarios exhibit weaker synergies and more conflicts. The study offers insights into SDG interactions and the potential for sustainable farming."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.00819",
        "category": "q-fin",
        "title": "Jump Detection in High-frequency Order Prices",
        "abstract": "We propose methods to infer jumps of a semi-martingale, which describes long-term price dynamics based on discrete, noisy, high-frequency observations. Different to the classical model of additive, centered market microstructure noise, we consider one-sided microstructure noise for order prices in a limit order book. We develop methods to estimate, locate and test for jumps using local order statistics. We provide a local test and show that we can consistently estimate price jumps. The main contribution is a global test for jumps. We establish the asymptotic properties and optimality of this test. We derive the asymptotic distribution of a maximum statistic under the null hypothesis of no jumps based on extreme value theory. We prove consistency under the alternative hypothesis. The rate of convergence for local alternatives is determined and shown to be much faster than optimal rates for the standard market microstructure noise model. This allows the identification of smaller jumps. In the process, we establish uniform consistency for spot volatility estimation under one-sided microstructure noise. A simulation study sheds light on the finite-sample implementation and properties of our new statistics and draws a comparison to a popular method for market microstructure noise. We showcase how our new approach helps to improve jump detection in an empirical analysis of intra-daily limit order book data.",
        "references": [
            {
                "arxivId": "2301.01965",
                "title": "Inference on the intraday spot volatility from high-frequency order prices with irregular microstructure noise",
                "abstract": "\n We consider estimation of the spot volatility in a stochastic boundary model with one-sided microstructure noise for high-frequency limit order prices. Based on discrete, noisy observations of an It\u00f4 semimartingale with jumps and general stochastic volatility, we present a simple and explicit estimator using local order statistics. We establish consistency and stable central limit theorems as asymptotic properties. The asymptotic analysis builds upon an expansion of tail probabilities for the order statistics based on a generalized arcsine law. In order to use the involved distribution of local order statistics for a bias correction, an efficient numerical algorithm is developed. We demonstrate the finite-sample performance of the estimation in a Monte Carlo simulation."
            },
            {
                "arxivId": "2103.14525",
                "title": "Gumbel convergence of the maximum of convoluted half-normally distributed random variables",
                "abstract": "In this note, we establish the convergence in distribution of the maxima of i.i.d. random variables to the Gumbel distribution with the associated normalizing sequences for several examples that are related to the normal distribution. Motivated by tests for jumps in high-frequency data, our main interest is in the half-normal distribution and the sum or difference of two independent half-normally distributed random variables. Since the half-normal distribution is neither stable nor symmetric, these examples are non-obvious generalizations. It is shown that the sum and difference of two independent half-normally distributed random variables and other examples yield distributions with tail behaviours that relate to the normal case. It turns out that the Gumbel convergence for all such distributions can be proved following similar steps. We illustrate the results in Monte Carlo simulations."
            },
            {
                "arxivId": "1709.02502",
                "title": "Testing if the market microstructure noise is fully explained by the informational content of some variables from the limit order book",
                "abstract": null
            },
            {
                "arxivId": "1704.06537",
                "title": "Estimation of the Discontinuous Leverage Effect: Evidence from the NASDAQ Order Book",
                "abstract": "An extensive empirical literature documents a generally negative correlation, named the ?leverage effect,? between asset returns and changes of volatility. It is more challenging to establish such a return-volatility relationship for jumps in high-frequency data. We propose new nonparametric methods to assess and test for a discontinuous leverage effect ? i.e. a relation between contemporaneous jumps in prices and volatility ? in high-frequency data with market microstructure noise. We present local tests and estimators for price jumps and volatility jumps. Five years of transaction data from 320 NASDAQ firms display no negative relation between price and volatility cojumps. We show, however, that there is a strong relation between price-volatility cojumps if one conditions on the sign of price jumps and whether the price jumps are market-wide or idiosyncratic."
            },
            {
                "arxivId": "1707.02419",
                "title": "Estimating the Spot Covariation of Asset Prices\u2014Statistical Theory and Empirical Evidence",
                "abstract": "ABSTRACT We propose a new estimator for the spot covariance matrix of a multi-dimensional continuous semimartingale log asset price process, which is subject to noise and nonsynchronous observations. The estimator is constructed based on a local average of block-wise parametric spectral covariance estimates. The latter originate from a local method of moments (LMM), which recently has been introduced by Bibinger et al.. We prove consistency and a point-wise stable central limit theorem for the proposed spot covariance estimator in a very general setup with stochastic volatility, leverage effects, and general noise distributions. Moreover, we extend the LMM estimator to be robust against autocorrelated noise and propose a method to adaptively infer the autocorrelations from the data. Based on simulations we provide empirical guidance on the effective implementation of the estimator and apply it to high-frequency data of a cross-section of Nasdaq blue chip stocks. Employing the estimator to estimate spot covariances, correlations, and volatilities in normal but also unusual periods yields novel insights into intraday covariance and correlation dynamics. We show that intraday (co-)variations (i) follow underlying periodicity patterns, (ii) reveal substantial intraday variability associated with (co-)variation risk, and (iii) can increase strongly and nearly instantaneously if new information arrives. Supplementary materials for this article are available online."
            },
            {
                "arxivId": "1408.3768",
                "title": "Volatility estimation under one-sided errors with applications to limit order books",
                "abstract": "For a semi-martingale $X_t$, which forms a stochastic boundary, a rate-optimal estimator for its quadratic variation $\\langle X, X \\rangle_t$ is constructed based on observations in the vicinity of $X_t$. The problem is embedded in a Poisson point process framework, which reveals an interesting connection to the theory of Brownian excursion areas. We derive $n^{-1/3}$ as optimal convergence rate in a high-frequency framework with $n$ observations (in mean). We discuss a potential application for the estimation of the integrated squared volatility of an efficient price process $X_t$ from intra-day order book quotes."
            },
            {
                "arxivId": "1403.7889",
                "title": "Time endogeneity and an optimal weight function in pre-averaging covariance estimation",
                "abstract": null
            },
            {
                "arxivId": "1305.6430",
                "title": "Adaptive function estimation in nonparametric regression with one-sided errors",
                "abstract": "We consider the model of nonregular nonparametric regression where smoothness constraints are imposed on the regression function $f$ and the regression errors are assumed to decay with some sharpness level at their endpoints. The aim of this paper is to construct an adaptive estimator for the regression function $f$. In contrast to the standard model where local averaging is fruitful, the nonregular conditions require a substantial different treatment based on local extreme values. We study this model under the realistic setting in which both the smoothness degree $\\beta>0$ and the sharpness degree $\\mathfrak {a}\\in(0,\\infty)$ are unknown in advance. We construct adaptation procedures applying a nested version of Lepski's method and the negative Hill estimator which show no loss in the convergence rates with respect to the general $L_q$-risk and a logarithmic loss with respect to the pointwise risk. Optimality of these rates is proved for $\\mathfrak{a}\\in(0,\\infty)$. Some numerical simulations and an application to real data are provided."
            },
            {
                "arxivId": "1303.6146",
                "title": "Estimating the Quadratic Covariation Matrix from Noisy Observations: Local Method of Moments and Efficiency",
                "abstract": "An efficient estimator is constructed for the quadratic covariation or integrated covolatility matrix of a multivariate continuous martingale based on noisy and non-synchronous observations under high-frequency asymptotics. Our approach relies on an asymptotically equivalent continuous-time observation model where a local generalised method of moments in the spectral domain turns out to be optimal. Asymptotic semiparametric efficiency is established in the CramA\u00a9r-Rao sense. Main findings are that non-synchronicity of observation times has no impact on the asymptotics and that major efficiency gains are possible under correlation. Simulations illustrate the finite-sample behaviour."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.01012",
        "category": "q-fin",
        "title": "Hilbert Space-Valued LQG Mean Field Games: An Infinite-Dimensional Analysis",
        "abstract": "This paper presents a comprehensive study of Hilbert space-valued Linear-Quadratic-Gaussian (LQG) mean field games (MFGs), generalizing the classic LQG mean field game theory to scenarios where the state equations are driven by infinite-dimensional stochastic equations. In this framework, state and control processes take values in separable Hilbert spaces. Moreover, the state equations involve infinite dimensional noises, namely $Q$-Wiener processes. All agents are coupled through the average state of the population appearing in their linear dynamics and quadratic cost functional. In addition, the diffusion coefficient of each agent involves the state, control, and the average state processes. We first study the well-posedness of a system of coupled infinite-dimensional stochastic evolution equations, which forms the foundation of MFGs in Hilbert spaces. Next, we develop the Nash Certainty Equivalence principle and obtain a unique Nash equilibrium for the limiting Hilbert space-valued MFG. Finally, we establish the $\\epsilon$-Nash property for the finite-player game in Hilbert space.",
        "references": [
            {
                "arxivId": "1811.07338",
                "title": "Well-posedness of Stochastic Riccati Equations and Closed-Loop Solvability for Stochastic Linear Quadratic Optimal Control Problems",
                "abstract": "We study the closed-loop solvability of a stochastic linear quadratic optimal control problem for systems governed by stochastic evolution equations. This solvability is established by means of solvability of the corresponding Riccati equation, which is implied by the uniform convexity of the quadratic cost functional. At last, conditions ensuring the uniform convexity of the cost functional are discussed. 2010 Mathematics Subject Classification. 93E20, 49N10, 49N35."
            },
            {
                "arxivId": "1810.07551",
                "title": "Convex Analysis for LQG Systems with Applications to Major Minor LQG Mean Field Game Systems",
                "abstract": null
            },
            {
                "arxivId": "1807.07088",
                "title": "A mean-field game approach to price formation in electricity markets",
                "abstract": "Here, we introduce a price-formation model where a large number of small players can store and trade electricity. Our model is a constrained mean-field game (MFG) where the price is a Lagrange multiplier for the supply vs. demand balance condition. We establish the existence of a unique solution using a fixed-point argument. In particular, we show that the price is well-defined and it is a Lipschitz function of time. Then, we study linear-quadratic models that can be solved explicitly and compare our model with real data."
            },
            {
                "arxivId": "1807.04795",
                "title": "Mean Field Game with Delay: A Toy Model",
                "abstract": "We study a toy model of linear-quadratic mean field game with delay. We \u201clift\u201d the delayed dynamic into an infinite dimensional space, and recast the mean field game system which is made of a forward Kolmogorov equation and a backward Hamilton-Jacobi-Bellman equation. We identify the corresponding master equation. A solution to this master equation is computed, and we show that it provides an approximation to a Nash equilibrium of the finite player game."
            },
            {
                "arxivId": "1607.06373",
                "title": "Systemic Risk and Stochastic Games with Delay",
                "abstract": null
            },
            {
                "arxivId": "1404.5741",
                "title": "Linear-Quadratic Mean Field Games",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.01088",
        "category": "q-fin",
        "title": "Justifying the Volatility of S&P 500 Daily Returns",
        "abstract": "Over the past 60 years, there has been a gradual increase in the volatility of daily returns for the S&P 500 Index. Hypothetically, suppose that market forces determine daily volatility such that a daily leveraged S&P 500 fund cannot outperform a standard S&P 500 fund in the long run. Then this hypothetical volatility happens to support the increase in volatility seen in the S&P 500 index. On this basis, it appears that the classic argument of the market portfolio being unbeatable in the long run is determining the volatility of S&P 500 daily returns. Moreover, it follows that the long-term volatility of the daily returns for the S&P 500 Index should continue to increase until passing a particular threshold. If, on the other hand, this hypothesis about market forces increasing volatility is invalid, then there is room for daily leveraged S&P 500 funds to outperform their unleveraged counterparts in the long run.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.01360",
        "category": "q-fin",
        "title": "\"Digitwashing\": The Gap between Words and Deeds in Digital Transformation and Stock Price Crash Risk",
        "abstract": "The contrast between companies'\"fleshy\"promises and the\"skeletal\"performance in digital transformation may lead to a higher risk of stock price crash. This paper selects a sample of Shanghai and Shenzhen A-share listed companies from 2010 to 2021, empirically analyses the specific impact of the gap between words and deeds in digital transformation (GDT) on the stock price crash risk, and explores the possible causes of GDT. We found that GDT significantly increases the stock price crash risk, and this finding is still valid after a series of robustness tests. In a further study, a deeper examination of the causes of GDT reveals that firms' perceptions of economic policy uncertainty significantly increase GDT, and the effect is more pronounced in the sample of loss-making firms. At the same time, the results of the heterogeneity test suggest that investors are more tolerant of state-owned enterprises when they are in the GDT situation. Taken together, we provide a concrete bridge between the two measures of digital transformation - digital text frequency and digital technology share - and offer new insights to enhance capital market stability.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.01361",
        "category": "q-fin",
        "title": "Bandit Profit-maximization for Targeted Marketing",
        "abstract": "We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs. Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only noisy evaluations of chosen points on the demand curves. We prove a regret upper bound of $\\widetilde{\\mathcal{O}}\\big(nT^{3/4}\\big)$ and a lower bound of $\\Omega\\big((nT)^{3/4}\\big)$ for monotonic demand curves, and a regret bound of $\\widetilde{\\Theta}\\big(nT^{2/3}\\big)$ for demands curves that are monotonic in price and concave in the ancillary variables.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.01468",
        "category": "q-fin",
        "title": "Properties of the entropic risk measure EVaR in relation to selected distributions",
        "abstract": "Entropic Value-at-Risk (EVaR) measure is a convenient coherent risk measure. Due to certain difficulties in finding its analytical representation, it was previously calculated explicitly only for the normal distribution. We succeeded to overcome these difficulties and to calculate Entropic Value-at-Risk (EVaR) measure for Poisson, compound Poisson, Gamma, Laplace, exponential, chi-squared, inverse Gaussian distribution and normal inverse Gaussian distribution with the help of Lambert function that is a special function, generally speaking, with two branches.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-05.json",
        "arxivId": "2403.01770",
        "category": "q-fin",
        "title": "Experimenting with Generative AI: Does ChatGPT Really Increase Everyone's Productivity?",
        "abstract": "Generative AI technologies such as ChatGPT, Gemini, and MidJourney have made remarkable progress in recent years. Recent literature has documented ChatGPT's positive impact on productivity in areas where it has strong expertise, attributable to extensive training datasets, such as the English language and Python/SQL programming. However, there is still limited literature regarding ChatGPT's performance in areas where its capabilities could still be further enhanced. This paper aims to fill this gap. We conducted an experiment in which economics students were asked to perform writing analysis tasks in a non-English language (specifically, Thai) and math&data analysis tasks using a less frequently used programming package (specifically, Stata). The findings suggest that, on average, participants performed better using ChatGPT in terms of scores and time taken to complete the tasks. However, a detailed examination reveals that 34% of participants saw no improvement in writing analysis tasks, and 42% did not improve in math&data analysis tasks when employing ChatGPT. Further investigation indicated that higher-ability students, as proxied by their econometrics grades, were the ones who performed worse in writing analysis tasks when using ChatGPT. We also found evidence that students with better digital skills performed better with ChatGPT. This research provides insights on the impact of generative AI. Thus, stakeholders can make informed decisions to implement appropriate policy frameworks or redesign educational systems. It also highlights the critical role of human skills in addressing and complementing the limitations of technology.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-06.json",
        "arxivId": "2403.02572",
        "category": "q-fin",
        "title": "Fill Probabilities in a Limit Order Book with State-Dependent Stochastic Order Flows",
        "abstract": "This paper focuses on computing the fill probabilities for limit orders positioned at various price levels within the limit order book, which play a crucial role in optimizing executions. We adopt a generic stochastic model to capture the dynamics of the order book as a series of queueing systems. This generic model is state-dependent and also incorporates stylized factors. We subsequently derive semi-analytical expressions to compute the relevant probabilities within the context of state-dependent stochastic order flows. These probabilities cover various scenarios, including the probability of a change in the mid-price, the fill probabilities of orders posted at the best quotes, and those posted at a price level deeper than the best quotes in the book, before the opposite best quote moves. These expressions can be further generalized to accommodate orders posted even deeper in the order book, although the associated probabilities are typically very small in such cases. Lastly, we conduct extensive numerical experiments using real order book data from the foreign exchange spot market. Our findings suggest that the model is tractable and possesses the capability to effectively capture the dynamics of the limit order book. Moreover, the derived formulas and numerical methods demonstrate reasonably good accuracy in estimating the fill probabilities.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-06.json",
        "arxivId": "2403.02726",
        "category": "q-fin",
        "title": "Bias in Generative AI",
        "abstract": "This study analyzed images generated by three popular generative artificial intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators. Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances. Firstly, we found that all three AI generators exhibited bias against women and African Americans. Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society. Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances. For example, women were depicted as younger with more smiles and happiness, while men were depicted as older with more neutral expressions and anger, posing a risk that generative AI models may unintentionally depict women as more submissive and less competent than men. Such nuanced biases, by their less overt nature, might be more problematic as they can permeate perceptions unconsciously and may be more difficult to rectify. Although the extent of bias varied depending on the model, the direction of bias remained consistent in both commercial and open-source AI generators. As these tools become commonplace, our study highlights the urgency to identify and mitigate various biases in generative AI, reinforcing the commitment to ensuring that AI technologies benefit all of humanity for a more inclusive future.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2309.07488",
        "category": "q-fin",
        "title": "Long-Term Mean-Variance Optimization Under Mean-Reverting Equity Returns",
        "abstract": "This paper studies the mean-variance optimal portfolio choice of an investor pre-committed to a deterministic investment policy in continuous time in a market with mean-reversion in the risk-free rate and the equity risk-premium. In the tradition of Markowitz, optimal policies are restricted to a subclass of factor exposures in which losses cannot exceed initial capital and it is shown that the optimal policy is characterized by an Euler-Lagrange equation derived by the method of Calculus of Variations. It is a main result, that the Euler-Lagrange equation can be recast into a matrix differential equation by an integral transformation of the factor exposure and that the solution to the characteristic equation can be parametrized by the eigenvalues of the associated lambda-matrix, hence, the optimization problem is equivalent to a spectral problem. Finally, explicit solutions to the optimal policy are provided by application of suitable boundary conditions and it is demonstrated that - if in fact the equity risk-premium is slowly mean-reverting - then investors committing to long investment horizons realize better risk-return trade-offs than investors with shorter investment horizons.",
        "references": [
            {
                "arxivId": "2201.05103",
                "title": "Analysis of a five-factor capital market model",
                "abstract": "In this paper we analyse the five-factor capital market model of Munk et al. (2004). The model features a Vasicek interest rate model, an equity index with mean-reverting excess return and an index for realized inflation with meanreverting expectation. The primary aim of the analysis is to facilitate so-called exact simulation from the model on a set of discrete time points. It turns out that this can be achieved by sampling from a (degenerate) seven-dimensional normal distribution. We derive the distributional results necessary and describe how to overcome the rank deficiency of the variance-covariance matrix in practice. The tradeable assets in the original model consist of cash, nominal bonds and stocks. We extend the investment universe to also include inflation bonds by deriving the arbitrage free break-even inflation (BEI) curve for a three-parameter specification of the two market prices of inflation risk. Finally, we provide a number of auxiliary results regarding the dynamics of constant-maturity nominal and inflation bond indices, the distribution of the stock index in nominal and real terms, and the distribution of the Sharpe ratio for individual assets and portfolios with an application to factor investing."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2310.02322",
        "category": "q-fin",
        "title": "Signature Methods in Stochastic Portfolio Theory",
        "abstract": "In the context of stochastic portfolio theory we introduce a novel class of portfolios which we call linear path-functional portfolios. These are portfolios which are determined by certain transformations of linear functions of a collections of feature maps that are non-anticipative path functionals of an underlying semimartingale. As main example for such feature maps we consider the signature of the (ranked) market weights. We prove that these portfolios are universal in the sense that every continuous, possibly path-dependent, portfolio function of the market weights can be uniformly approximated by signature portfolios. We also show that signature portfolios can approximate the growth-optimal portfolio in several classes of non-Markovian market models arbitrarily well and illustrate numerically that the trained signature portfolios are remarkably close to the theoretical growth-optimal portfolios. Besides these universality features, the main numerical advantage lies in the fact that several optimization tasks like maximizing (expected) logarithmic wealth or mean-variance optimization within the class of linear path-functional portfolios reduce to a convex quadratic optimization problem, thus making it computationally highly tractable. We apply our method also to real market data based on several indices. Our results point towards out-performance on the considered out-of-sample data, also in the presence of transaction costs.",
        "references": [
            {
                "arxivId": "2308.15135",
                "title": "Signature Trading: A Path-Dependent Extension of the Mean-Variance Framework with Exogenous Signals",
                "abstract": "In this article we introduce a portfolio optimisation framework, in which the use of rough path signatures (Lyons, 1998) provides a novel method of incorporating path-dependencies in the joint signal-asset dynamics, naturally extending traditional factor models, while keeping the resulting formulas lightweight and easily interpretable. We achieve this by representing a trading strategy as a linear functional applied to the signature of a path (which we refer to as\"Signature Trading\"or\"Sig-Trading\"). This allows the modeller to efficiently encode the evolution of past time-series observations into the optimisation problem. In particular, we derive a concise formulation of the dynamic mean-variance criterion alongside an explicit solution in our setting, which naturally incorporates a drawdown control in the optimal strategy over a finite time horizon. Secondly, we draw parallels between classical portfolio stategies and Sig-Trading strategies and explain how the latter leads to a pathwise extension of the classical setting via the\"Signature Efficient Frontier\". Finally, we give examples when trading under an exogenous signal as well as examples for momentum and pair-trading strategies, demonstrated both on synthetic and market data. Our framework combines the best of both worlds between classical theory (whose appeal lies in clear and concise formulae) and between modern, flexible data-driven methods that can handle more realistic datasets. The advantage of the added flexibility of the latter is that one can bypass common issues such as the accumulation of heteroskedastic and asymmetric residuals during the optimisation phase. Overall, Sig-Trading combines the flexibility of data-driven methods without compromising on the clarity of the classical theory and our presented results provide a compelling toolbox that yields superior results for a large class of trading strategies."
            },
            {
                "arxivId": "2303.17671",
                "title": "Neural signature kernels as infinite-width-depth-limits of controlled ResNets",
                "abstract": "Motivated by the paradigm of reservoir computing, we consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations (Neural CDEs), a unified architecture which enconpasses both RNNs and ResNets. We show that in the infinite-width-depth limit and under proper scaling, these architectures converge weakly to Gaussian processes indexed on some spaces of continuous paths and with kernels satisfying certain partial differential equations (PDEs) varying according to the choice of activation function, extending the results of Hayou (2022); Hayou&Yang (2023) to the controlled and homogeneous case. In the special, homogeneous, case where the activation is the identity, we show that the equation reduces to a linear PDE and the limiting kernel agrees with the signature kernel of Salvi et al. (2021a). We name this new family of limiting kernels neural signature kernels. Finally, we show that in the infinite-depth regime, finite-width controlled ResNets converge in distribution to Neural CDEs with random vector fields which, depending on whether the weights are shared across layers, are either time-independent and Gaussian or behave like a matrix-valued Brownian motion."
            },
            {
                "arxivId": "2301.13235",
                "title": "Joint calibration to SPX and VIX options with signature-based models",
                "abstract": "We consider a stochastic volatility model where the dynamics of the volatility are described by linear functions of the (time extended) signature of a primary underlying process, which is supposed to be some multidimensional continuous semimartingale. Under the additional assumption that this primary process is of polynomial type, we obtain closed form expressions for the VIX squared, exploiting the fact that the truncated signature of a polynomial process is again a polynomial process. Adding to such a primary process the Brownian motion driving the stock price, allows then to express both the log-price and the VIX squared as linear functions of the signature of the corresponding augmented process. This feature can then be e\ufb03ciently used for pricing and calibration purposes. Indeed, as the signature samples can be easily precomputed, the calibration task can be split into an o\ufb04ine sampling and a standard optimization. For both the SPX and VIX options we obtain highly accurate calibration results, showing that this model class allows to solve the joint calibration problem without adding jumps or rough volatility."
            },
            {
                "arxivId": "2211.15628",
                "title": "Ergodic robust maximization of asymptotic growth under stochastic volatility",
                "abstract": "We consider an asymptotic robust growth problem under model uncertainty and in the presence of (non-Markovian) stochastic covariance. We fix two inputs representing the instantaneous covariance for the asset process $X$, which depends on an additional stochastic factor process $Y$, as well as the invariant density of $X$ together with $Y$. The stochastic factor process $Y$ has continuous trajectories but is not even required to be a semimartingale. Our setup allows for drift uncertainty in $X$ and model uncertainty for the local dynamics of $Y$. This work builds upon a recent paper of Kardaras&Robertson, where the authors consider an analogous problem, however, without the additional stochastic factor process. Under suitable, quite weak assumptions we are able to characterize the robust optimal trading strategy and the robust optimal growth rate. The optimal strategy is shown to be functionally generated and, remarkably, does not depend on the factor process $Y$. Our result provides a comprehensive answer to a question proposed by Fernholz in 2002. Mathematically, we use a combination of partial differential equation (PDE), calculus of variations and generalized Dirichlet form techniques."
            },
            {
                "arxivId": "2208.02293",
                "title": "Universal approximation theorems for continuous functions of c\\`adl\\`ag paths and L\\'evy-type signature models",
                "abstract": "We prove a universal approximation theorem that allows to approximate continuous functionals of c\\`adl\\`ag (rough) paths uniformly in time and on compact sets of paths via linear functionals of their time-extended signature. Our main motivation to treat this question comes from signature-based models for finance that allow for the inclusion of jumps. Indeed, as an important application, we define a new class of universal signature models based on an augmented L\\'evy process, which we call L\\'evy-type signature models. They extend continuous signature models for asset prices as proposed e.g. by Arribas et al.(2020) in several directions, while still preserving universality and tractability properties. To analyze this, we first show that the signature process of a generic multivariate L\\'evy process is a polynomial process on the extended tensor algebra and then use this for pricing and hedging approaches within L\\'evy-type signature models."
            },
            {
                "arxivId": "2208.07251",
                "title": "Signature-based validation of real-world economic scenarios",
                "abstract": "\n Motivated by insurance applications, we propose a new approach for the validation of real-world economic scenarios. This approach is based on the statistical test developed by Chevyrev and Oberhauser ((2022) Journal of Machine Learning Research, 23(176), 1\u201342.) and relies on the notions of signature and maximum mean distance. This test allows to check whether two samples of stochastic processes paths come from the same distribution. Our contribution is to apply this test to a variety of stochastic processes exhibiting different pathwise properties (H\u00f6lder regularity, autocorrelation, and regime switches) and which are relevant for the modelling of stock prices and stock volatility as well as of inflation in view of actuarial applications."
            },
            {
                "arxivId": "2207.13136",
                "title": "Signature-Based Models: Theory and Calibration",
                "abstract": "We consider asset price models whose dynamics are described by linear functions of the (time extended) signature of a primary underlying process, which can range from a (market-inferred) Brownian motion to a general multidimensional continuous semimartingale. The framework is universal in the sense that classical models can be approximated arbitrarily well and that the model's parameters can be learned from all sources of available data by simple methods. We provide conditions guaranteeing absence of arbitrage as well as tractable option pricing formulas for so-called sig-payoffs, exploiting the polynomial nature of generic primary processes. One of our main focus lies on calibration, where we consider both time-series and implied volatility surface data, generated from classical stochastic volatility models and also from S&P500 index market data. For both tasks the linearity of the model turns out to be the crucial tractability feature which allows to get fast and accurate calibrations results."
            },
            {
                "arxivId": "2206.03742",
                "title": "Market-to-book ratio in stochastic portfolio theory",
                "abstract": null
            },
            {
                "arxivId": "2201.02441",
                "title": "Applications of Signature Methods to Market Anomaly Detection",
                "abstract": "Anomaly detection is the process of identifying abnormal instances or events in data sets which deviate from the norm significantly. In this study, we propose a signatures based machine learning algorithm to detect rare or unexpected items in a given data set of time series type. We present applications of signature or randomized signature as feature extractors for anomaly detection algorithms; additionally we provide an easy, representation theoretic justification for the construction of randomized signatures. Our first application is based on synthetic data and aims at distinguishing between real and fake trajectories of stock prices, which are indistinguishable by visual inspection. We also show a real life application by using transaction data from the cryptocurrency market. In this case, we are able to identify pump and dump attempts organized on social networks with F1 scores up to 88% by means of our unsupervised learning algorithm, thus achieving results that are close to the state-of-the-art in the field based on supervised learning."
            },
            {
                "arxivId": "2111.01207",
                "title": "Sig-wasserstein GANs for time series generation",
                "abstract": "Synthetic data is an emerging technology that can significantly accelerate the development and deployment of AI machine learning pipelines. In this work, we develop high-fidelity time-series generators, the SigWGAN, by combining continuous-time stochastic models with the newly proposed signature W1 metric. The former are the Logsig-RNN models based on the stochastic differential equations, whereas the latter originates from the universal and principled mathematical features to characterize the measure induced by time series. SigWGAN allows turning computationally challenging GAN min-max problem into supervised learning while generating high fidelity samples. We validate the proposed model on both synthetic data generated by popular quantitative risk models and empirical financial data. Codes are available at https://github.com/SigCGANs/Sig-Wasserstein-GANs.git"
            },
            {
                "arxivId": "2109.13512",
                "title": "Neural networks in Fr\u00e9chet spaces",
                "abstract": null
            },
            {
                "arxivId": "2109.01843",
                "title": "Model\u2010free portfolio theory: A rough path approach",
                "abstract": "Based on a rough path foundation, we develop a model\u2010free approach to stochastic portfolio theory (SPT). Our approach allows to handle significantly more general portfolios compared to previous model\u2010free approaches based on F\u00f6llmer integration. Without the assumption of any underlying probabilistic model, we prove a pathwise formula for the relative wealth process, which reduces in the special case of functionally generated portfolios to a pathwise version of the so\u2010called master formula of classical SPT. We show that the appropriately scaled asymptotic growth rate of a far reaching generalization of Cover's universal portfolio based on controlled paths coincides with that of the best retrospectively chosen portfolio within this class. We provide several novel results concerning rough integration, and highlight the advantages of the rough path approach by showing that (nonfunctionally generated) log\u2010optimal portfolios in an ergodic It\u00f4 diffusion setting have the same asymptotic growth rate as Cover's universal portfolio and the best retrospectively chosen one."
            },
            {
                "arxivId": "2104.13669",
                "title": "Optimal Stopping via Randomized Neural Networks",
                "abstract": "This paper presents the benefits of using randomized neural networks instead of standard basis functions or deep neural networks to approximate the solutions of optimal stopping problems. The key idea is to use neural networks, where the parameters of the hidden layers are generated randomly and only the last layer is trained, in order to approximate the continuation value. Our approaches are applicable to high dimensional problems where the existing approaches become increasingly impractical. In addition, since our approaches can be optimized using simple linear regression, they are easy to implement and theoretical guarantees can be provided. We test our approaches for American option pricing on Black--Scholes, Heston and rough Heston models and for optimally stopping a fractional Brownian motion. In all cases, our algorithms outperform the state-of-the-art and other relevant machine learning approaches in terms of computation time while achieving comparable results. Moreover, we show that they can also be used to efficiently compute Greeks of American options."
            },
            {
                "arxivId": "2103.10925",
                "title": "Functional Portfolio Optimization in Stochastic Portfolio Theory",
                "abstract": "In this paper we develop a concrete and fully implementable approach to the optimization of functionally generated portfolios in stochastic portfolio theory. The main idea is to optimize over a family of rank-based portfolios parameterized by an exponentially concave function on the unit interval. This choice can be motivated by the long term stability of the capital distribution observed in large equity markets, and allows us to circumvent the curse of dimensionality. The resulting optimization problem, which is convex, is flexible as various regularizations and constraints can be imposed on the generating function. We prove that the optimization problem is well-posed and provide a stability estimate in terms of a Wasserstein metric of the input measure. We then give a careful treatment of its discretization and the optimization algorithm. Finally, we present empirical examples using CRSP data from the US stock market."
            },
            {
                "arxivId": "2010.12047",
                "title": "Fading memory echo state networks are universal",
                "abstract": null
            },
            {
                "arxivId": "2010.14615",
                "title": "Discrete-Time Signatures and Randomness in Reservoir Computing",
                "abstract": "A new explanation of the geometric nature of the reservoir computing (RC) phenomenon is presented. RC is understood in the literature as the possibility of approximating input\u2013output systems with randomly chosen recurrent neural systems and a trained linear readout layer. Light is shed on this phenomenon by constructing what is called strongly universal reservoir systems as random projections of a family of state-space systems that generate Volterra series expansions. This procedure yields a state-affine reservoir system with randomly generated coefficients in a dimension that is logarithmically reduced with respect to the original system. This reservoir system is able to approximate any element in the fading memory filters class just by training a different linear readout for each different filter. Explicit expressions for the probability distributions needed in the generation of the projected reservoir system are stated, and bounds for the committed approximation error are provided."
            },
            {
                "arxivId": "2006.14498",
                "title": "A Data-Driven Market Simulator for Small Data Environments",
                "abstract": "Neural network based data-driven market simulation unveils a new and flexible way of modelling financial time series without imposing assumptions on the underlying stochastic dynamics. Though in this sense generative market simulation is model-free, the concrete modelling choices are nevertheless decisive for the features of the simulated paths. We give a brief overview of currently used generative modelling approaches and performance evaluation metrics for financial time series, and address some of the challenges to achieve good results in the latter. We also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach. While most generative models tend to rely on large amounts of training data, we present here a generative model that works reliably in environments where the amount of available training data is notoriously small. Furthermore, we show how a rough paths perspective combined with a parsimonious Variational Autoencoder framework provides a powerful way for encoding and evaluating financial time series in such environments where available training data is scarce. Finally, we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our Market Generator to deep hedging."
            },
            {
                "arxivId": "1905.00711",
                "title": "Non-parametric Pricing and Hedging of Exotic Derivatives",
                "abstract": "ABSTRACT In the spirit of Arrow\u2013Debreu, we introduce a family of financial derivatives that act as primitive securities in that exotic derivatives can be approximated by their linear combinations. We call these financial derivatives signature payoffs. We show that signature payoffs can be used to non-parametrically price and hedge exotic derivatives in the scenario where one has access to price data for other exotic payoffs. The methodology leads to a computationally tractable and accurate algorithm for pricing and hedging using market prices of a basket of exotic derivatives that has been tested on real and simulated market prices, obtaining good results."
            },
            {
                "arxivId": "1904.08925",
                "title": "The Impact of Proportional Transaction Costs on Systematically Generated Portfolios",
                "abstract": "The effect of proportional transaction costs on systematically generated portfolios is studied empirically. The performance of several portfolios (the index tracking portfolio, the equally-weighted portfolio, the entropy-weighted portfolio, and the diversity-weighted portfolio) in the presence of dividends and transaction costs is examined under different configurations involving the trading frequency, constituent list size, and renewing frequency. Moreover, a method to smooth transaction costs is proposed."
            },
            {
                "arxivId": "1810.10971",
                "title": "Signature Moments to Characterize Laws of Stochastic Processes",
                "abstract": "The normalized sequence of moments characterizes the law of any finite-dimensional random variable. We prove an analogous result for path-valued random variables, that is stochastic processes, by using the normalized sequence of signature moments. We use this to define a metric for laws of stochastic processes. This metric can be efficiently estimated from finite samples, even if the stochastic processes themselves evolve in high-dimensional state spaces. As an application, we provide a non-parametric two-sample hypothesis test for laws of stochastic processes."
            },
            {
                "arxivId": "1809.10123",
                "title": "Trading strategies generated pathwise by functions of market weights",
                "abstract": null
            },
            {
                "arxivId": "1806.10910",
                "title": "Machine learning with controllable quantum dynamics of a nuclear spin ensemble in a solid",
                "abstract": "We experimentally demonstrate quantum machine learning using NMR based on a framework of quantum reservoir computing. Reservoir computing is for exploiting natural nonlinear dynamics with large degrees of freedom, which is called a reservoir, for a machine learning purpose. Here we propose a concrete physical implementation of a quantum reservoir using controllable dynamics of a nuclear spin ensemble in a molecular solid. In this implementation, we demonstrate learning of nonlinear functions with binary or continuous variable inputs with low mean squared errors. Our implementation and demonstration paves a road toward exploiting quantum computational supremacy in NMR ensemble systems for information processing with reachable technologies."
            },
            {
                "arxivId": "1806.00797",
                "title": "Echo state networks are universal",
                "abstract": null
            },
            {
                "arxivId": "1801.06425",
                "title": "Ergodic robust maximization of asymptotic growth",
                "abstract": "We consider the problem of robustly maximizing the growth rate of investor wealth in the presence of model uncertainty. Possible models are all those under which the assets' region $E$ and instantaneous covariation $c$ are known, and where additionally the assets are stable in that their occupancy time measures converge to a law with density $p$. This latter assumption is motivated by the observed stability of ranked relative market capitalizations for equity markets. We seek to identify the robust optimal growth rate, as well as a trading strategy which achieves this rate in all models. Under minimal assumptions upon $(E,c,p)$, we identify the robust growth rate with the Donsker-Varadhan rate function from occupancy time Large Deviations theory. We also prove existence of, and explicitly identify, the optimal trading strategy. We then apply our results in the case of drift uncertainty for ranked relative market capitalizations. Assuming regularity under symmetrization for the covariance and limiting density of the ranked capitalizations, we explicitly identify the robust optimal trading strategy in this setting."
            },
            {
                "arxivId": "1705.03647",
                "title": "Polynomial processes in stochastic portfolio theory",
                "abstract": null
            },
            {
                "arxivId": "1611.09631",
                "title": "Cover's universal portfolio, stochastic portfolio theory, and the num\u00e9raire portfolio",
                "abstract": "Cover's celebrated theorem states that the long\u2010run yield of a properly chosen \u201cuniversal\u201d portfolio is almost as good as that of the best retrospectively chosen constant rebalanced portfolio. The \u201cuniversality\u201d refers to the fact that this result is model\u2010free, that is, not dependent on an underlying stochastic process. We extend Cover's theorem to the setting of stochastic portfolio theory: the market portfolio is taken as the num\u00e9raire, and the rebalancing rule need not be constant anymore but may depend on the current state of the stock market. By fixing a stochastic model of the stock market this model\u2010free result is complemented by a comparison with the num\u00e9raire portfolio. Roughly speaking, under appropriate assumptions the asymptotic growth rate coincides for the three approaches mentioned in the title of this paper. We present results in both discrete and continuous time."
            },
            {
                "arxivId": "1606.03325",
                "title": "Model-Free Portfolio Theory and Its Functional Master Formula",
                "abstract": "We use pathwise Ito calculus to prove two strictly pathwise versions of the master formula in Fernholz' stochastic portfolio theory. Our first version is set within the framework of Follmer's pathwise Ito calculus and works for portfolios generated from functions that may depend on the current states of the market portfolio and an additional path of finite variation. The second version is formulated within the functional pathwise Ito calculus of Dupire (2009) and Cont \\& Fournie (2010) and allows for portfolio-generating functionals that may depend additionally on the entire path of the market portfolio. Our results are illustrated by several examples and shown to work on empirical market data."
            },
            {
                "arxivId": "1603.08245",
                "title": "Trading strategies generated by Lyapunov functions",
                "abstract": null
            },
            {
                "arxivId": "1601.08169",
                "title": "Kernels for sequentially ordered data",
                "abstract": "We present a novel framework for kernel learning with sequential data of any kind, such as time series, sequences of graphs, or strings. Our approach is based on signature features which can be seen as an ordered variant of sample (cross-)moments; it allows to obtain a \"sequentialized\" version of any static kernel. The sequential kernels are efficiently computable for discrete sequences and are shown to approximate a continuous moment form in a sampling sense. \nA number of known kernels for sequences arise as \"sequentializations\" of suitable static kernels: string kernels may be obtained as a special case, and alignment kernels are closely related up to a modification that resolves their open non-definiteness issue. Our experiments indicate that our signature-based sequential kernel framework may be a promising approach to learning with sequential data, such as time series, that allows to avoid extensive manual pre-processing."
            },
            {
                "arxivId": "1406.7871",
                "title": "The Signature of a Rough Path: Uniqueness",
                "abstract": null
            },
            {
                "arxivId": "1309.0260",
                "title": "Learning from the past, predicting the statistics for the future, learning an evolving system",
                "abstract": "We bring the theory of rough paths to the study of non-parametric statistics on streamed data. We discuss the problem of regression where the input variable is a stream of information, and the dependent response is also (potentially) a stream. A certain graded feature set of a stream, known in the rough path literature as the signature, has a universality that allows formally, linear regression to be used to characterise the functional relationship between independent explanatory variables and the conditional distribution of the dependent response. This approach, via linear regression on the signature of the stream, is almost totally general, and yet it still allows explicit computation. The grading allows truncation of the feature set and so leads to an efficient local description for streams (rough paths). In the statistical context this method offers potentially significant, even transformational dimension reduction. By way of illustration, our approach is applied to stationary time series including the familiar AR model and ARCH model. In the numerical examples we examined, our predictions achieve similar accuracy to the Gaussian Process (GP) approach with much lower computational cost especially when the sample size is large."
            },
            {
                "arxivId": "1110.2481",
                "title": "On a Chen\u2013Fliess approximation for diffusion functionals",
                "abstract": null
            },
            {
                "arxivId": "0803.1877",
                "title": "The num\u00e9raire portfolio in semimartingale financial models",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2403.03410",
        "category": "q-fin",
        "title": "Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression",
        "abstract": "The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information. With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies. However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis. This risk posed as significant challenges for coin investors that could result in substantial investment losses. The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment. Forecasting, is one of the methods used to predict the future value of these crypto coins. By utilizing the models of Long Short Term Memory, Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most suitable for predicting crypto currency prices. The mean square error is employed as a benchmark for the comparison. By applying those three constructed algorithm models, the Support Vector Machine uses a linear kernel to produce the smallest mean square error compared to the Long Short Term Memory and Polynomial Regression algorithm models, with a mean square error value of 0.02.",
        "references": [
            {
                "arxivId": "2401.02469",
                "title": "Modern Computing: Vision and Challenges",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2403.03597",
        "category": "q-fin",
        "title": "The 'Must Stock' Challenge in Academic Publishing: Pricing Implications of Transformative Agreements",
        "abstract": "The high relevance of top-notch academic journals turns them into 'must stock' products that assign its often commercial owners with extraordinary market power. Intended to tackle this, university consortia around the globe negotiate so-called 'transformative agreements' with many publishing houses. It shall pave the way towards standard open-access publishing. While several contract designs exist, the 'publish-and-read' (PAR) scheme is the one that comes closest to the ideal of an entirely open access environment: Publishers are paid a fixed case-by-case rate for each publication, which includes a fee for their extensive libraries. In turn, all subscription payments are waived. I theoretically derive that this contract design benefits the included publishers regardless of whether the number of publications in these publishers' journals grows or declines. Consequently, widespread PAR contracts are likely to raise entry barriers for new (open-access) competitors even further. Intending to lower costs for the universities, their libraries, and, ultimately, the taxpayers, this PAR fee contract design of transformative agreements might cause the opposite.",
        "references": [
            {
                "arxivId": "2402.18255",
                "title": "How open are hybrid journals included in transformative agreements?",
                "abstract": "The ongoing controversy surrounding transformative agreements, which aim to transition journal publishing to full open access, highlight the need for large-scale studies assessing the impact of these agreements on hybrid open access. By combining publicly available data from various sources, including cOAlition S Journal Checker, Crossref, and OpenAlex, this study presents a novel approach that analyses over 700 agreements. Results suggest a strong growth in open access between 2018 and 2022 from 4.3% to 15%. During this period, 11,189 hybrid journals provided open access to 742,369 out of 8,146,958 articles, representing a five-year open access proportion of 9.1%. Authors who could make use of transformative agreements at the time of publication contributed 328,957 open access articles. In 2022, 143,615 out of 249,511 open access articles in hybrid journals or 58% were enabled by transformative agreements. This trend was largely driven by the three commercial publishers Elsevier, Springer Nature, and Wiley, but the open access uptake varied substantially across journals, publishers, disciplines, and country affiliations. In particular, the OECD and BRICS areas revealed different publication trends. In conclusion, this study suggests that current levels of implementation of transformative agreements is insufficient to bring about a large-scale transition to full open access."
            },
            {
                "arxivId": "2105.12078",
                "title": "No deal: German researchers\u2019 publishing and citing behaviors after Big Deal negotiations with Elsevier",
                "abstract": "Abstract In 2014, a union of German research organizations established Projekt DEAL, a national-level project to negotiate licensing agreements with large scientific publishers. Negotiations between DEAL and Elsevier began in 2016, and broke down without a successful agreement in 2018; during this time, around 200 German research institutions canceled their license agreements with Elsevier, leading Elsevier to restrict journal access at those institutions. We investigated the effect on researchers\u2019 publishing and citing behaviors from a bibliometric perspective, using a data set of \u223c400,000 articles published by researchers at DEAL institutions during 2012\u20132020. We further investigated these effects with respect to the timing of contract cancellations, research disciplines, collaboration patterns, and article open-access status. We find evidence for a decrease in Elsevier\u2019s market share of articles from DEAL institutions, with the largest year-on-year market share decreases occurring from 2018 to 2020 following the implementation of access restrictions. We also observe year-on-year decreases in the proportion of citations, although the decrease is smaller. We conclude that negotiations with Elsevier and access restrictions have led to some reduced willingness to publish in Elsevier journals, but that researchers are not strongly affected in their ability to cite Elsevier articles, implying that researchers use other methods to access scientific literature."
            },
            {
                "arxivId": "2012.07675",
                "title": "Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases",
                "abstract": null
            },
            {
                "arxivId": "1912.12646",
                "title": "Scholarly journal publishing in transition- from restricted to open access",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2403.03610",
        "category": "q-fin",
        "title": "Paying for Privacy: Pay-or-Tracking Walls",
        "abstract": "Prestigious news publishers, and more recently, Meta, have begun to request that users pay for privacy. Specifically, users receive a notification banner, referred to as a pay-or-tracking wall, that requires them to (i) pay money to avoid being tracked or (ii) consent to being tracked. These walls have invited concerns that privacy might become a luxury. However, little is known about pay-or-tracking walls, which prevents a meaningful discussion about their appropriateness. This paper conducts several empirical studies and finds that top EU publishers use pay-or-tracking walls. Their implementations involve various approaches, including bundling the pay option with advertising-free access or additional content. The price for not being tracked exceeds the advertising revenue that publishers generate from a user who consents to being tracked. Notably, publishers' traffic does not decline when implementing a pay-or-tracking wall and most users consent to being tracked; only a few users pay. In short, pay-or-tracking walls seem to provide the means for expanding the practice of tracking. Publishers profit from pay-or-tracking walls and may observe a revenue increase of 16.4% due to tracking more users than under a cookie consent banner.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2403.03649",
        "category": "q-fin",
        "title": "The Cost of Coming Out",
        "abstract": "The fear of social stigma and discrimination leads many individuals worldwide to hesitate in openly disclosing their sexual orientation. Due to the large costs of concealing identity, it is crucial to understand the extent of anti-LGB sentiments and reactions to coming out. However, disclosing one's sexual orientation is a personal choice, complicating data access and introducing endogeneity issues. This paper tackles these challenges by using an innovative data source from a popular online video game together with a natural experiment. We exploit exogenous variation in the identity of a playable character to identify the effects of disclosure on players' revealed preferences for that character. Leveraging detailed daily data, we monitor players' preferences for the character across diverse regions globally and employ synthetic control methods to isolate the effect of the disclosure on players' preferences. Our findings reveal a substantial and persistent negative impact of coming out. To strengthen the plausibility of social stigma as the primary explanation for the estimated effects, we systematically address and eliminate several alternative game-related channels.",
        "references": [
            {
                "arxivId": "2201.01194",
                "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-07.json",
        "arxivId": "2403.03785",
        "category": "q-fin",
        "title": "A machine learning workflow to address credit default prediction",
        "abstract": "Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention. In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management. In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations. The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task. We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling missing values, and making data uniform for models working with different data types. Next, we train several families of learning models, introducing ensemble techniques to build more robust models and hyperparameter optimization via multi-objective genetic algorithms to consider both predictive accuracy and financial aspects. Our research aims at contributing to the FinTech industry in providing a tool to move toward more accurate and reliable credit risk assessment, benefiting both lenders and borrowers.",
        "references": [
            {
                "arxivId": "2101.01494",
                "title": "Weight-of-evidence through shrinkage and spline binning for interpretable nonlinear classification",
                "abstract": null
            },
            {
                "arxivId": "2002.09437",
                "title": "Calibrating Deep Neural Networks using Focal Loss",
                "abstract": "Miscalibration - a mismatch between a model's confidence and its correctness - of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al., 2017] allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases. Code is available at this https URL"
            },
            {
                "arxivId": "2001.08025",
                "title": "Optimal binning: mathematical programming formulation",
                "abstract": "The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target. We present a rigorous and extensible mathematical programming formulation to solving the optimal binning problem for a binary, continuous and multi-class target type, incorporating constraints not previously addressed. For all three target types, we introduce a convex mixed-integer programming formulation. Several algorithmic enhancements such as automatic determination of the most suitable monotonic trend via a Machine-Learning-based classifier and implementation aspects are thoughtfully discussed. The new mathematical programming formulations are carefully implemented in the open-source python library OptBinning."
            },
            {
                "arxivId": "1907.10902",
                "title": "Optuna: A Next-generation Hyperparameter Optimization Framework",
                "abstract": "The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/)."
            },
            {
                "arxivId": "2002.09931",
                "title": "The value of big data for credit scoring: Enhancing financial inclusion using mobile phone data and social network analytics",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2009.00868",
        "category": "q-fin",
        "title": "Post-Last Exit Time Process and its Application to Loss-Given-Default Distribution",
        "abstract": "We study a linear diffusion process after its last exit time from a certain regular point. Rather than treating the process as newly born at the last exit time, we view the whole path and separate the original process before and after the last exit time. This enables us not only to identify the transition semigroup, boundary behavior, entrance law, and reverse of the post-last exit time process, but also to establish a financial model for estimating the loss-given-default distribution of corporate debt (an all-time important open problem).",
        "references": [
            {
                "arxivId": "1701.04565",
                "title": "Time reversal and last passage time of diffusions with applications to credit risk management",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2110.14046",
        "category": "q-fin",
        "title": "Open Markets and Hybrid Jacobi Processes",
        "abstract": "We propose a unified approach to several problems in Stochastic Portfolio Theory (SPT), which is a framework for equity markets with a large number $d$ of stocks. Our approach combines open markets, where trading is confined to the top $N$ capitalized stocks as well as the market portfolio consisting of all $d$ assets, with a parametric family of models which we call hybrid Jacobi processes. We provide a detailed analysis of ergodicity, particle collisions, and boundary attainment, and use these results to study the associated financial markets. Their properties include (1) stability of the capital distribution curve and (2) unleveraged and explicit growth optimal strategies. The sub-class of rank Jacobi models are additionally shown to (3) serve as the worst-case model for a robust asymptotic growth problem under model ambiguity and (4) exhibit stability in the large-$d$ limit. Our definition of an open market is a relaxation of existing definitions which is essential to make the analysis tractable.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2301.13595",
        "category": "q-fin",
        "title": "Local Volatility in Interest Rate Models",
        "abstract": "Local Volatility (LV) is a very powerful tool for market modeling. This tool can be used to generate arbitrage-free scenarios calibrated to all available options. Here we demonstrate how to implement LV in order to reproduce most swaption prices within a single model. There was a good agreement between market prices and Monte Carlo prices for all tenors and maturities from 2 to 20 years. Note that due to the use of a normal distribution in the scenario generation process, the volatility of short-term swaptions cannot be generated accurately.",
        "references": [
            {
                "arxivId": "1105.3359",
                "title": "Asymptotic Expansion for the Normal Implied Volatility in Local Volatility Models",
                "abstract": "We study the dynamics of the normal implied volatility in a local volatility model, using a small-time expansion in powers of maturity T. At leading order in this expansion, the asymptotics of the normal implied volatility is similar, up to a different definition of the moneyness, to that of the log-normal volatility. This relation is preserved also to order O(T) in the small-time expansion, and differences with the log-normal case appear first at O(T^2). The results are illustrated on a few examples of local volatility models with analytical local volatility, finding generally good agreement with exact or numerical solutions. We point out that the asymptotic expansion can fail if applied naively for models with nonanalytical local volatility, for example which have discontinuous derivatives. Using perturbation theory methods, we show that the ATM normal implied volatility for such a model contains a term ~ \\sqrt{T}, with a coefficient which is proportional with the jump of the derivative."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2308.05564",
        "category": "q-fin",
        "title": "Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity Returns",
        "abstract": "Skew-t copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate skew-t factor copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. We show that intraday predictive densities from the skew-t copula are more accurate than from some other copula models, while portfolio selection strategies based on the estimated pairwise tail dependencies improve performance relative to the benchmark index.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2309.03403",
        "category": "q-fin",
        "title": "Sources of capital growth",
        "abstract": "Data from national accounts show no effect of change in net saving or consumption, in ratio to market-value capital, on change in growth rate of market-value capital (capital acceleration). Thus it appears that capital growth and acceleration arrive without help from net saving or consumption restraint. We explore ways in which this is possible, and discuss implications for economic teaching and public policy",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2402.11864",
        "category": "q-fin",
        "title": "The Price of Information",
        "abstract": "When an investor is faced with the option to purchase additional information regarding an asset price, how much should she pay? To address this question, we solve for the indifference price of information in a setting where a trader maximizes her expected utility of terminal wealth over a finite time horizon. If she does not purchase the information, then she solves a partial information stochastic control problem, while, if she does purchase the information, then she pays a cost and receives partial information about the asset's trajectory. We further demonstrate that when the investor can purchase the information at any stopping time prior to the end of the trading horizon, she chooses to do so at a deterministic time.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2403.04104",
        "category": "q-fin",
        "title": "Testing Business Cycle Theories: Evidence from the Great Recession",
        "abstract": "Empirical business cycle studies using cross-country data usually cannot achieve causal relationships while within-country studies mostly focus on the bust period. We provide the first causal investigation into the boom period of the 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research design, we show that credit expansion in private-label mortgages causes a differentially stronger boom (2000-2006) and bust (2007-2010) cycle in the house-related industries in the high net-export-growth areas. Most importantly, our unique research design enables us to perform the most comprehensive tests on theories (hypotheses) regarding the business cycle. We show that the following theories (hypotheses) cannot explain the cause of the 1999-2010 U.S. business cycle: the speculative euphoria hypothesis, the real business cycle theory, the collateral-driven credit cycle theory, the business uncertainty theory, and the extrapolative expectation theory.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-08.json",
        "arxivId": "2403.04674",
        "category": "q-fin",
        "title": "Calibrated rank volatility stabilized models for large equity markets",
        "abstract": "In the framework of stochastic portfolio theory we introduce rank volatility stabilized models for large equity markets over long time horizons. These models are rank-based extensions of the volatility stabilized models introduced by Fernholz&Karatzas in 2005. On the theoretical side we establish global existence of the model and ergodicity of the induced ranked market weights. We also derive explicit expressions for growth-optimal portfolios and show the existence of relative arbitrage with respect to the market portfolio. On the empirical side we calibrate the model to sixteen years of CRSP US equity data matching (i) rank-based volatilities, (ii) stock turnover as measured by market weight collisions, (iii) the average market rate of return and (iv) the capital distribution curve. Assessment of model fit and error analysis is conducted both in and out of sample. To the best of our knowledge this is the first model exhibiting relative arbitrage that has statistically been shown to have a good quantitative fit with the empirical features (i)-(iv). We additionally simulate trajectories of the calibrated model and compare them to historical trajectories, both in and out of sample.",
        "references": [
            {
                "arxivId": "2310.02322",
                "title": "Signature Methods in Stochastic Portfolio Theory",
                "abstract": "In the context of stochastic portfolio theory we introduce a novel class of portfolios which we call linear path-functional portfolios. These are portfolios which are determined by certain transformations of linear functions of a collections of feature maps that are non-anticipative path functionals of an underlying semimartingale. As main example for such feature maps we consider the signature of the (ranked) market weights. We prove that these portfolios are universal in the sense that every continuous, possibly path-dependent, portfolio function of the market weights can be uniformly approximated by signature portfolios. We also show that signature portfolios can approximate the growth-optimal portfolio in several classes of non-Markovian market models arbitrarily well and illustrate numerically that the trained signature portfolios are remarkably close to the theoretical growth-optimal portfolios. Besides these universality features, the main numerical advantage lies in the fact that several optimization tasks like maximizing (expected) logarithmic wealth or mean-variance optimization within the class of linear path-functional portfolios reduce to a convex quadratic optimization problem, thus making it computationally highly tractable. We apply our method also to real market data based on several indices. Our results point towards out-performance on the considered out-of-sample data, also in the presence of transaction costs."
            },
            {
                "arxivId": "2211.15628",
                "title": "Ergodic robust maximization of asymptotic growth under stochastic volatility",
                "abstract": "We consider an asymptotic robust growth problem under model uncertainty and in the presence of (non-Markovian) stochastic covariance. We fix two inputs representing the instantaneous covariance for the asset process $X$, which depends on an additional stochastic factor process $Y$, as well as the invariant density of $X$ together with $Y$. The stochastic factor process $Y$ has continuous trajectories but is not even required to be a semimartingale. Our setup allows for drift uncertainty in $X$ and model uncertainty for the local dynamics of $Y$. This work builds upon a recent paper of Kardaras&Robertson, where the authors consider an analogous problem, however, without the additional stochastic factor process. Under suitable, quite weak assumptions we are able to characterize the robust optimal trading strategy and the robust optimal growth rate. The optimal strategy is shown to be functionally generated and, remarkably, does not depend on the factor process $Y$. Our result provides a comprehensive answer to a question proposed by Fernholz in 2002. Mathematically, we use a combination of partial differential equation (PDE), calculus of variations and generalized Dirichlet form techniques."
            },
            {
                "arxivId": "2211.02990",
                "title": "Efficient Convex PCA with applications to Wasserstein geodesic PCA and ranked data",
                "abstract": "Convex PCA, which was introduced by Bigot et al., is a dimension reduction methodology for data with values in a convex subset of a Hilbert space. This setting arises naturally in many applications, including distributional data in the Wasserstein space of an interval, and ranked compositional data under the Aitchison geometry. Our contribution in this paper is threefold. First, we present several new theoretical results including consistency as well as continuity and differentiability of the objective function in the finite dimensional case. Second, we develop a numerical implementation of finite dimensional convex PCA when the convex set is polyhedral, and show that this provides a natural approximation of Wasserstein geodesic PCA. Third, we illustrate our results with two financial applications, namely distributions of stock returns ranked by size and the capital distribution curve, both of which are of independent interest in stochastic portfolio theory."
            },
            {
                "arxivId": "2110.14046",
                "title": "Open Markets and Hybrid Jacobi Processes",
                "abstract": "We propose a unified approach to several problems in Stochastic Portfolio Theory (SPT), which is a framework for equity markets with a large number $d$ of stocks. Our approach combines open markets, where trading is confined to the top $N$ capitalized stocks as well as the market portfolio consisting of all $d$ assets, with a parametric family of models which we call hybrid Jacobi processes. We provide a detailed analysis of ergodicity, particle collisions, and boundary attainment, and use these results to study the associated financial markets. Their properties include (1) stability of the capital distribution curve and (2) unleveraged and explicit growth optimal strategies. The sub-class of rank Jacobi models are additionally shown to (3) serve as the worst-case model for a robust asymptotic growth problem under model ambiguity and (4) exhibit stability in the large-$d$ limit. Our definition of an open market is a relaxation of existing definitions which is essential to make the analysis tractable."
            },
            {
                "arxivId": "2103.10925",
                "title": "Functional Portfolio Optimization in Stochastic Portfolio Theory",
                "abstract": "In this paper we develop a concrete and fully implementable approach to the optimization of functionally generated portfolios in stochastic portfolio theory. The main idea is to optimize over a family of rank-based portfolios parameterized by an exponentially concave function on the unit interval. This choice can be motivated by the long term stability of the capital distribution observed in large equity markets, and allows us to circumvent the curse of dimensionality. The resulting optimization problem, which is convex, is flexible as various regularizations and constraints can be imposed on the generating function. We prove that the optimization problem is well-posed and provide a stability estimate in terms of a Wasserstein metric of the input measure. We then give a careful treatment of its discretization and the optimization algorithm. Finally, we present empirical examples using CRSP data from the US stock market."
            },
            {
                "arxivId": "2009.08533",
                "title": "Robust asymptotic growth in stochastic portfolio theory under long\u2010only constraints",
                "abstract": "We consider the problem of maximizing the asymptotic growth rate of an investor under drift uncertainty in the setting of stochastic portfolio theory (SPT). As in the work of Kardaras and Robertson we take as inputs (i) aMarkovian volatility matrix c(x)$c(x)$ and (ii) an invariant density p(x)$p(x)$ for the market weights, but we additionally impose long\u2010only constraints on the investor. Our principal contribution is proving a uniqueness and existence result for the class of concave functionally generated portfolios and developing a finite dimensional approximation, which can be used to numerically find the optimum. In addition to the general results outlined above, we propose the use of a broad class of models for the volatility matrix c(x)$c(x)$ , which can be calibrated to data and, under which, we obtain explicit formulas of the optimal unconstrained portfolio for any invariant density."
            },
            {
                "arxivId": "2003.13601",
                "title": "Relative arbitrage: Sharp time horizons and motion by curvature",
                "abstract": "We characterize the minimal time horizon over which any equity market with d\u22652 stocks and sufficient intrinsic volatility admits relative arbitrage with respect to the market portfolio. If d\u2208{2,3} , the minimal time horizon can be computed explicitly, its value being zero if d=2 and 3/(2\u03c0) if d=3 . If d\u22654 , the minimal time horizon can be characterized via the arrival time function of a geometric flow of the unit simplex in Rd that we call the minimum curvature flow."
            },
            {
                "arxivId": "1912.13110",
                "title": "Open markets",
                "abstract": "An open market is a subset of a larger equity market, composed of a certain fixed number of top\u2010capitalization stocks. Though the number of stocks in the open market is fixed, their composition changes over time, as each company's rank by market capitalization fluctuates. When one is allowed to invest also in a money market, an open market resembles the entire \u201cclosed\u201d equity market in the sense that the market viability (lack of arbitrage) is equivalent to the existence of a num\u00e9raire portfolio (which cannot be outperformed). When access to the money market is prohibited, the class of portfolios shrinks significantly in open markets; in such a setting, we discuss the Capital Asset Pricing Model, how to construct functionally generated portfolios, and the concept of universal portfolio."
            },
            {
                "arxivId": "1904.08925",
                "title": "The Impact of Proportional Transaction Costs on Systematically Generated Portfolios",
                "abstract": "The effect of proportional transaction costs on systematically generated portfolios is studied empirically. The performance of several portfolios (the index tracking portfolio, the equally-weighted portfolio, the entropy-weighted portfolio, and the diversity-weighted portfolio) in the presence of dividends and transaction costs is examined under different configurations involving the trading frequency, constituent list size, and renewing frequency. Moreover, a method to smooth transaction costs is proposed."
            },
            {
                "arxivId": "1809.03769",
                "title": "Diversification, Volatility, and Surprising Alpha",
                "abstract": "It has been widely observed that capitalization-weighted indexes can be beaten by surprisingly simple, systematic investment strategies. Indeed, in the U.S. stock market, equal-weighted portfolios, random-weighted portfolios, and other naive, non- optimized portfolios tend to outperform a capitalization-weighted index over the long term. This outperformance is generally attributed to beneficial factor exposures. Here, we provide a deeper, more general explanation of this phenomenon by decomposing portfolio log-returns into an average growth and an excess growth component. Using a rank-based empirical study we argue that the excess growth component plays the major role in explaining the outperformance of naive portfolios. In particular, individual stock growth rates are not as critical as is traditionally assumed."
            },
            {
                "arxivId": "1801.06425",
                "title": "Ergodic robust maximization of asymptotic growth",
                "abstract": "We consider the problem of robustly maximizing the growth rate of investor wealth in the presence of model uncertainty. Possible models are all those under which the assets' region $E$ and instantaneous covariation $c$ are known, and where additionally the assets are stable in that their occupancy time measures converge to a law with density $p$. This latter assumption is motivated by the observed stability of ranked relative market capitalizations for equity markets. We seek to identify the robust optimal growth rate, as well as a trading strategy which achieves this rate in all models. Under minimal assumptions upon $(E,c,p)$, we identify the robust growth rate with the Donsker-Varadhan rate function from occupancy time Large Deviations theory. We also prove existence of, and explicitly identify, the optimal trading strategy. We then apply our results in the case of drift uncertainty for ranked relative market capitalizations. Assuming regularity under symmetrization for the covariance and limiting density of the ranked capitalizations, we explicitly identify the robust optimal trading strategy in this setting."
            },
            {
                "arxivId": "1603.08245",
                "title": "Trading strategies generated by Lyapunov functions",
                "abstract": null
            },
            {
                "arxivId": "1302.3870",
                "title": "A second-order stock market model",
                "abstract": null
            },
            {
                "arxivId": "1206.2305",
                "title": "THE NUM\u00c9RAIRE PROPERTY AND LONG\u2010TERM GROWTH OPTIMALITY FOR DRAWDOWN\u2010CONSTRAINED INVESTMENTS",
                "abstract": "We consider the portfolio choice problem for a long\u2010run investor in a general continuous semimartingale model. We combine the decision criterion of pathwise growth optimality with a flexible specification of attitude toward risk, encoded by a linear drawdown constraint imposed on admissible wealth processes. We define the constrained num\u00e9raire property through the notion of expected relative return and prove that drawdown\u2010constrained num\u00e9raire portfolio exists and is unique, but may depend on the investment horizon. However, when sampled at the times of its maximum and asymptotically as the time\u2010horizon becomes distant, the drawdown\u2010constrained num\u00e9raire portfolio is given explicitly through a model\u2010independent transformation of the unconstrained num\u00e9raire portfolio. The asymptotically growth\u2010optimal strategy is obtained as limit of num\u00e9raire strategies on finite horizons."
            },
            {
                "arxivId": "0909.0065",
                "title": "HYBRID ATLAS MODELS",
                "abstract": "We study Atlas-type models of equity markets with local characteristics that depend on both name and rank, and in ways that induce a stable capital distribution. Ergodic properties and rankings of processes are examined with reference to the theory of reflected Brownian motions in polyhedral domains. In the context of such models we discuss properties of various investment strategies, including the so-called growth-optimal and universal portfolios."
            },
            {
                "arxivId": "math/0602521",
                "title": "Atlas models of equity markets",
                "abstract": "Atlas-type models are constant-parameter models of uncorrelated stocks for equity markets with a stable capital distribution, in which the growth rates and variances depend on rank. The simplest such model assigns the same, constant variance to all stocks; zero rate of growth to all stocks but the smallest; and positive growth rate to the smallest, the Atlas stock. In this paper we study the basic properties of this class of models, as well as the behavior of various portfolios in their midst. Of particular interest are portfolios that do not contain the Atlas stock."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2104.04233",
        "category": "q-fin",
        "title": "Functional quantization of rough volatility and applications to volatility derivatives",
        "abstract": "We develop a product functional quantization of rough volatility. Since the optimal quantizers can be computed offline, this new technique, built on the insightful works by [Luschgy, H. and Pag\u00e8s, G., Functional quantization of Gaussian processes. J. Funct. Anal., 2002, 196(2), 486\u2013531; Luschgy, H. and Pag\u00e8s, G., High-resolution product quantization for Gaussian processes under sup-norm distortion. Bernoulli, 2007, 13(3), 653\u2013671; Pag\u00e8s, G., Quadratic optimal functional quantization of stochastic processes and numerical applications. In Monte Carlo and Quasi-Monte Carlo Methods 2006, pp. 101\u2013142, 2007 (Springer: Berlin Heidelberg)], becomes a strong competitor in the new arena of numerical tools for rough volatility. We concentrate our numerical analysis on the pricing of options on the VIX and realized variance in the rough Bergomi model [Bayer, C., Friz, P.K. and Gatheral, J., Pricing under rough volatility. Quant. Finance, 2016, 16(6), 887\u2013904] and compare our results to other benchmarks recently suggested.",
        "references": [
            {
                "arxivId": "2203.09298",
                "title": "Weak Error Rates of Numerical Schemes for Rough Volatility",
                "abstract": "Simulation of rough volatility models involves discretization of stochastic integrals where the integrand is a function of a (correlated) fractional Brownian motion of Hurst index $H \\in (0,1/2)$. We obtain results on the rate of convergence for the weak error of such approximations, in the special cases when either the integrand is the fBm itself, or the test function is cubic. Our result states that the convergence is of order $(3H+ \\frac{1}{2}) \\wedge 1$ for exact left-point discretization, and of order $H+\\frac{1}{2}$ for the hybrid scheme with well-chosen weights."
            },
            {
                "arxivId": "2203.02943",
                "title": "On the weak convergence rate in the discretization of rough volatility models",
                "abstract": "We study the weak convergence rate in the discretization of rough volatility models. After showing a lower bound $2H$ under a general model, where $H$ is the Hurst index of the volatility process, we give a sharper bound $H + 1/2$ under a linear model."
            },
            {
                "arxivId": "2009.01219",
                "title": "WEAK ERROR RATES FOR OPTION PRICING UNDER LINEAR ROUGH VOLATILITY",
                "abstract": "In quantitative finance, modeling the volatility structure of underlying assets is vital to pricing options. Rough stochastic volatility models, such as the rough Bergomi model [C.\u00a0Bayer, P.\u00a0K. Friz & J.\u00a0Gatheral (2016) Pricing under rough volatility, Quantitative Finance 16 (6), 887\u2013904, doi:10.1080/14697688.2015.1099717], seek to fit observed market data based on the observation that the log-realized variance behaves like a fractional Brownian motion with small Hurst parameter, [Formula: see text], over reasonable timescales. Both time series of asset prices and option-derived price data indicate that [Formula: see text] often takes values close to [Formula: see text] or less, i.e. rougher than Brownian motion. This change improves the fit to both option prices and time series of underlying asset prices while maintaining parsimoniousness. However, the non-Markovian nature of the driving fractional Brownian motion in rough volatility models poses severe challenges for theoretical and numerical analyses and for computational practice. While the explicit Euler method is known to converge to the solution of the rough Bergomi and similar models, its strong rate of convergence is only [Formula: see text]. We prove rate [Formula: see text] for the weak convergence of the Euler method for the rough Stein\u2013Stein model, which treats the volatility as a linear function of the driving fractional Brownian motion, and, surprisingly, we prove rate one for the case of quadratic payoff functions. Indeed, the problem of weak convergence for rough volatility models is very subtle; we provide examples demonstrating the rate of convergence for payoff functions that are well approximated by second-order polynomials, as weighted by the law of the fractional Brownian motion, may be hard to distinguish from rate one empirically. Our proof uses Talay\u2013Tubaro expansions and an affine Markovian representation of the underlying and is further supported by numerical experiments. These convergence results provide a first step toward deriving weak rates for the rough Bergomi model, which treats the volatility as a nonlinear function of the driving fractional Brownian motion."
            },
            {
                "arxivId": "2007.02113",
                "title": "Markovian Approximation of the Rough Bergomi Model for Monte Carlo Option Pricing",
                "abstract": "The recently developed rough Bergomi (rBergomi) model is a rough fractional stochastic volatility (RFSV) model which can generate a more realistic term structure of at-the-money volatility skews compared with other RFSV models. However, its non-Markovianity brings mathematical and computational challenges for model calibration and simulation. To overcome these difficulties, we show that the rBergomi model can be well-approximated by the forward-variance Bergomi model with wisely chosen weights and mean-reversion speed parameters (aBergomi), which has the Markovian property. We establish an explicit bound on the L2-error between the respective kernels of these two models, which is explicitly controlled by the number of terms in the aBergomi model. We establish and describe the affine structure of the rBergomi model, and show the convergence of the affine structure of the aBergomi model to the one of the rBergomi model. We demonstrate the efficiency and accuracy of our method by implementing a classical Markovian Monte Carlo simulation scheme for the aBergomi model, which we compare to the hybrid scheme of the rBergomi model."
            },
            {
                "arxivId": "2002.09215",
                "title": "Volatility has to be rough",
                "abstract": "Under power-law blow-up of the short ATM skew, volatility must be rough in a viable market for the underlying asset"
            },
            {
                "arxivId": "1905.04852",
                "title": "Is Volatility Rough",
                "abstract": "Rough volatility models are continuous time stochastic volatility models where the volatility process is driven by a fractional Brownian motion with the Hurst parameter smaller than half, and have attracted much attention since a seminal paper titled \"Volatility is rough\" was posted on SSRN in 2014 showing that the log realized volatility time series of major stock indices have the same scaling property as such a rough fractional Brownian motion has. We however find by simulations that the impressive approach tends to suggest the same roughness irrespectively whether the volatility is actually rough or not; an overlooked estimation error of latent volatility often results in an illusive scaling property. Motivated by this preliminary finding, here we develop a statistical theory for a continuous time fractional stochastic volatility model to examine whether the Hurst parameter is indeed estimated smaller than half, that is, whether the volatility is really rough. We construct a quasi-likelihood estimator and apply it to realized volatility time series. Our quasi-likelihood is based on the error distribution of the realized volatility and a Whittle-type approximation to the auto-covariance of the log-volatility process. We prove the consistency of our estimator under high frequency asymptotics, and examine by simulations its finite sample performance. Our empirical study suggests that the volatility is indeed rough; actually it is even rougher than considered in the literature."
            },
            {
                "arxivId": "1802.01641",
                "title": "Volatility Options in Rough Volatility Models",
                "abstract": "We discuss the pricing and hedging of volatility options in some rough volatility models. First, we develop efficient Monte Carlo methods and asymptotic approximations for computing option prices and hedge ratios in models where log-volatility follows a Gaussian Volterra process. While providing a good fit for European options, these models are unable to reproduce the VIX option smile observed in the market, and are thus not suitable for VIX products. To accommodate these, we introduce the class of modulated Volterra processes, and show that they successfully capture the VIX smile."
            },
            {
                "arxivId": "1711.03078",
                "title": "Functional Central Limit Theorems for Rough Volatility",
                "abstract": "The non-Markovian nature of rough volatility makes Monte Carlo methods challenging, and it is in fact a major challenge to develop fast and accurate simulation algorithms. We provide an efficient one for stochastic Volterra processes, based on an extension of Donsker\u2019s approximation of Brownian motion to the fractional Brownian case with arbitrary Hurst exponent $H \\in (0,1)$\n H\n \u2208\n (\n 0\n ,\n 1\n )\n . Some of the most relevant consequences of this \u2018rough Donsker (rDonsker) theorem\u2019 are functional weak convergence results in Skorokhod space for discrete approximations of a large class of rough stochastic volatility models. This justifies the validity of simple and easy-to-implement Monte Carlo methods, for which we provide detailed numerical recipes. We test these against the current benchmark hybrid scheme and find remarkable agreement (for a large range of values of\u00a0$H$\n H\n ). Our rDonsker theorem further provides a weak convergence proof for the hybrid scheme itself and allows constructing binomial trees for rough volatility models, the first available scheme (in the rough volatility context) for early exercise options such as American or Bermudan options."
            },
            {
                "arxivId": "1708.02563",
                "title": "Turbocharging Monte Carlo pricing for the rough Bergomi model",
                "abstract": "The rough Bergomi model, introduced by Bayer et al. [Quant. Finance, 2016, 16(6), 887\u2013904], is one of the recent rough volatility models that are consistent with the stylised fact of implied volatility surfaces being essentially time-invariant, and are able to capture the term structure of skew observed in equity markets. In the absence of analytical European option pricing methods for the model, we focus on reducing the runtime-adjusted variance of Monte Carlo implied volatilities, thereby contributing to the model\u2019s calibration by simulation. We employ a novel composition of variance reduction methods, immediately applicable to any conditionally log-normal stochastic volatility model. Assuming one targets implied volatility estimates with a given degree of confidence, thus calibration RMSE, the results we demonstrate equate to significant runtime reductions\u2014roughly 20 times on average, across different correlation regimes."
            },
            {
                "arxivId": "1706.05291",
                "title": "Pathwise large deviations for the rough Bergomi model",
                "abstract": "Abstract Introduced recently in mathematical finance by Bayer et al. (2016), the rough Bergomi model has proved particularly efficient to calibrate option markets. We investigate some of its probabilistic properties, in particular proving a pathwise large deviations principle for a small-noise version of the model. The exponential function (continuous but superlinear) as well as the drift appearing in the volatility process fall beyond the scope of existing results, and a dedicated analysis is needed."
            },
            {
                "arxivId": "1701.04260",
                "title": "On VIX futures in the rough Bergomi model",
                "abstract": "Abstract The rough Bergomi model introduced by Bayer et al. [Quant. Finance, 2015, 1\u201318] has been outperforming conventional Markovian stochastic volatility models by reproducing implied volatility smiles in a very realistic manner, in particular for short maturities. We investigate here the dynamics of the VIX and the forward variance curve generated by this model, and develop efficient pricing algorithms for VIX futures and options. We further analyse the validity of the rough Bergomi model to jointly describe the VIX and the SPX, and present a joint calibration algorithm based on the hybrid scheme by Bennedsen et al. [Finance Stoch., forthcoming]."
            },
            {
                "arxivId": "1507.03004",
                "title": "Hybrid scheme for Brownian semistationary processes",
                "abstract": null
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            },
            {
                "arxivId": "1409.3071",
                "title": "Representations and Inequalities for Generalized Hypergeometric Functions",
                "abstract": null
            },
            {
                "arxivId": "0912.3168",
                "title": "Representation formulae for the fractional Brownian motion",
                "abstract": null
            },
            {
                "arxivId": "0706.4450",
                "title": "Quadratic optimal functional quantization of stochastic processes and numerical applications",
                "abstract": null
            },
            {
                "arxivId": "math/0511208",
                "title": "High-resolution product quantization for Gaussian processes under sup-norm distortion",
                "abstract": "We derive high-resolution upper bounds for optimal product quantization of pathwise contionuous Gaussian processes respective to the supremum norm on [0,T] d . Moreover, we describe a product quantization design which attains this bound. This is achieved under very general assumptions on random series expansions of the process. It turns out that product quantization is asymptotically only slightly worse than optimal functional quantization. The results are applied e.g. to fractional Brownian sheets and the Ornstein-Uhlenbeck process."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2108.02648",
        "category": "q-fin",
        "title": "Optimal Consumption with Loss Aversion and Reference to Past Spending Maximum",
        "abstract": "This paper studies an optimal consumption problem for a loss-averse agent with reference to past consumption maximum. To account for loss aversion on relative consumption, an S-shaped utility is adopted that measures the difference between the non-negative consumption rate and a fraction of the historical spending peak. We consider the concave envelope of the utility with respect to consumption, allowing us to focus on an auxiliary HJB variational inequality on the strength of concavification principle and dynamic programming arguments. By applying the dual transform and smooth-fit conditions, the auxiliary HJB variational inequality is solved in piecewise closed-form and some thresholds of the wealth variable are obtained. The optimal consumption and investment control can be derived in the piecewise feedback form. The rigorous verification proofs on optimality and concavification principle are provided. Some numerical sensitivity analysis and financial implications are also presented.",
        "references": [
            {
                "arxivId": "2206.03116",
                "title": "Optimal Consumption and Life Insurance Under Shortfall Aversion and a Drawdown Constraint",
                "abstract": "This paper studies a life-cycle optimal portfolio-consumption problem when the consumption performance is measured by a shortfall aversion preference with an additional drawdown constraint on consumption rate. Meanwhile, the agent also dynamically chooses her life insurance premium to maximize the expected bequest at the death time. By using dynamic programming arguments and the dual transform, we solve the HJB variational inequality explicitly in a piecewise form across different regions and derive some thresholds of the wealth variable for the piecewise optimal feedback controls. Taking advantage of our analytical results, we are able to numerically illustrate some quantitative impacts on optimal consumption and life insurance by model parameters and discuss their financial implications."
            },
            {
                "arxivId": "2006.13661",
                "title": "Optimal Tracking Portfolio with a Ratcheting Capital Benchmark",
                "abstract": "This paper studies the finite horizon portfolio management by optimally tracking a ratcheting capital benchmark process. To formulate such an optimal tracking problem, we envision that the fund manager can dynamically inject capital into the portfolio account such that the total capital dominates the nondecreasing benchmark floor process at each intermediate time. The control problem is to minimize the cost of the accumulative capital injection. We first transform the original problem with floor constraints into an unconstrained control problem, however, under a running maximum cost. By identifying a controlled state process with reflection, we next transform the problem further into an equivalent auxiliary problem, which leads to a nonlinear Hamilton-Jacobi-Bellman (HJB) with a Neumann boundary condition. By employing the dual transform, the probabilistic representation approach and some stochastic flow arguments, the existence of the unique classical solution to the dual HJB is established. The verification theorem is carefully proved, which gives the complete characterization of the primal value function and the feedback optimal portfolio."
            },
            {
                "arxivId": "1806.07499",
                "title": "Optimal Dividend Distribution Under Drawdown and Ratcheting Constraints on Dividend Rates",
                "abstract": "We consider the optimal dividend problem under a habit formation constraint that prevents the dividend rate to fall below a certain proportion of its historical maximum, the so-called drawdown constraint. This is an extension of the optimal Duesenberry's ratcheting consumption problem, studied by Dybvig (1995) [Review of Economic Studies 62(2), 287-313], in which consumption is assumed to be nondecreasing. Our problem differs from Dybvig's also in that the time of ruin could be finite in our setting, whereas ruin was impossible in Dybvig's work. We formulate our problem as a stochastic control problem with the objective of maximizing the expected discounted utility of the dividend stream until bankruptcy, in which risk preferences are embodied by power utility. We semi-explicitly solve the corresponding Hamilton-Jacobi-Bellman variational inequality, which is a nonlinear free-boundary problem. The optimal (excess) dividend rate $c^*_t$ - as a function of the company's current surplus $X_t$ and its historical running maximum of the (excess) dividend rate $z_t$ - is as follows: There are constants $0 w^* z_t$, it is optimal to increase the dividend rate above $z_t$, and (5) it is optimal to increase $z_t$ via singular control as needed to keep $X_t \\le w^* z_t$. Because, the maximum (excess) dividend rate will eventually be proportional to the running maximum of the surplus, \"mountains will have to move\" before we increase the dividend rate beyond its historical maximum."
            },
            {
                "arxivId": "1210.5205",
                "title": "The Merton Problem with a Drawdown Constraint on Consumption",
                "abstract": "In this paper, we work in the framework of the Merton problem but we impose a drawdown constraint on the consumption process. This means that consumption can never fall below a fixed proportion of the running maximum of past consumption. In terms of economic motivation, this constraint represents a type of habit formation where the investor is reluctant to let his standard of living fall too far from the maximum standard achieved to date. We use techniques from stochastic optimal control and duality theory to obtain our candidate value function and optimal controls, which are then verified."
            },
            {
                "arxivId": "0709.2830",
                "title": "BEHAVIORAL PORTFOLIO SELECTION IN CONTINUOUS TIME",
                "abstract": "This paper formulates and studies a general continuous\u2010time behavioral portfolio selection model under Kahneman and Tversky's (cumulative) prospect theory, featuring S\u2010shaped utility (value) functions and probability distortions. Unlike the conventional expected utility maximization model, such a behavioral model could be easily mis\u2010formulated (a.k.a. ill\u2010posed) if its different components do not coordinate well with each other. Certain classes of an ill\u2010posed model are identified. A systematic approach, which is fundamentally different from the ones employed for the utility model, is developed to solve a well\u2010posed model, assuming a complete market and general It\u00f4 processes for asset prices. The optimal terminal wealth positions, derived in fairly explicit forms, possess surprisingly simple structure reminiscent of a gambling policy betting on a good state of the world while accepting a fixed, known loss in case of a bad one. An example with a two\u2010piece CRRA utility is presented to illustrate the general results obtained, and is solved completely for all admissible parameters. The effect of the behavioral criterion on the risky allocations is finally discussed."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2201.12731",
        "category": "q-fin",
        "title": "Optimal Support for Distressed Subsidiaries -- a Systemic Risk Perspective",
        "abstract": "We consider a network of bank holdings, where every holding has two subsidiaries of different types. A subsidiary can trade with another holding's subsidiary of the same type. Holdings support their subsidiaries up to a certain level when they would otherwise fail to honor their financial obligations. We investigate the spread of contagion in this banking network when the number of bank holdings is large, and find the final number of defaulted subsidiaries under different rules for the holding support. We also consider resilience of this multilayered network to small shocks. Our work sheds light onto the role that holding structures can play in the amplification of financial stress. We find that depending on the capitalization of the network, a holding structure can be beneficial as compared to smaller separated entities. In other instances, it can be harmful and actually increase contagion. We illustrate our results in a numerical case study and also determine the optimal level of holding support from a regulator perspective.",
        "references": [
            {
                "arxivId": "2205.14782",
                "title": "Percolation in Random Graphs of Unbounded Rank",
                "abstract": "Bootstrap percolation in (random) graphs is a contagion dynamics among a set of vertices with certain threshold levels. The process is started by a set of initially infected vertices, and an initially uninfected vertex with threshold $k$ gets infected as soon as the number of its infected neighbors exceeds $k$. This process has been studied extensively in so called \\textit{rank one} models. These models can generate random graphs with heavy-tailed degree sequences but they are not capable of clustering. In this paper, we treat a class of random graphs of unbounded rank which allow for extensive clustering. Our main result determines the final fraction of infected vertices as the fixed point of a non-linear operator defined on a suitable function space. We propose an algorithm that facilitates neural networks to calculate this fixed point efficiently. We further derive criteria based on the Fr\\'echet derivative of the operator that allows one to determine whether small infections spread through the entire graph or rather stay local."
            },
            {
                "arxivId": "2006.08110",
                "title": "Suffocating Fire Sales",
                "abstract": "Fire sales are among the major drivers of market instability in modern financial systems. Due to iterated distressed selling and the associated price impact, initial shocks to some institutions can be amplified dramatically through the network induced by portfolio overlaps. In this paper we develop models that allow us to investigate central characteristics that drive or hinder the propagation of distress. We investigate single systems as well as ensembles of systems that are alike, where similarity is measured in terms of the empirical distribution of all defining properties of a system. This asymptotic approach ensures a great deal of robustness to statistical uncertainty and temporal fluctuations, and we give various applications. A natural characterization of systems resilient to fire sales emerges, and we provide explicit criteria that regulators may exploit in order to assess the stability of any system. Moreover, we propose risk management guidelines in form of minimal capital requirements, and we investigate the effect of portfolio diversification and portfolio overlap. We test our results by Monte Carlo simulations for exemplary configurations."
            },
            {
                "arxivId": "2005.05364",
                "title": "A Repo Model of Fire Sales with VWAP and LOB Pricing Mechanisms",
                "abstract": "We consider a network of banks that optimally choose a strategy of asset liquidations and borrowing in order to cover short term obligations. The borrowing is done in the form of collateralized repurchase agreements, the haircut level of which depends on the total liquidations of all the banks. Similarly the fire-sale price of the asset obtained by each of the banks depends on the amount of assets liquidated by the bank itself and by other banks. By nature of this setup, banks' behavior is considered as a Nash equilibrium. This paper provides two forms for market clearing to occur: through a common closing price and through an application of the limit order book. The main results of this work are providing sufficient conditions for existence and uniqueness of the clearing solutions (i.e., liquidations, borrowing, fire sale prices, and haircut levels)."
            },
            {
                "arxivId": "1610.09542",
                "title": "Managing Default Contagion in Inhomogeneous Financial Networks",
                "abstract": "The aim of this paper is to quantify and manage systemic risk caused by default contagion in the interbank market. We model the market as a random directed network, where the vertices represent financial institutions and the weighted edges monetary exposures between them. Our model captures the strong degree of heterogeneity observed in empirical data and the parameters can easily be fitted to real data sets. One of our main results allows us to determine the impact of local shocks, where initially some banks default, to the entire system and the wider economy. Here the impact is measured by some index of total systemic importance of all eventually defaulted institutions. As a central application, we characterize resilient and non-resilient cases. In particular, for the prominent case where the network has a degree sequence without second moment, we show that a small number of initially defaulted banks can trigger a substantial default cascade. Our results complement and extend significantly earlier findings derived in the configuration model where the existence of a second moment of the degree distribution is assumed. As a second main contribution, paralleling regulatory discussions, we determine minimal capital requirements for financial institutions sufficient to make the network resilient to small shocks. An appealing feature of these capital requirements is that they can be determined locally by each institution without knowing the complete network structure as they basically only depend on the institution's exposures to its counterparties."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2206.13341",
        "category": "q-fin",
        "title": "A mean field game approach to equilibrium consumption under external habit formation",
        "abstract": "This paper studies the equilibrium consumption under external habit formation in a large population of agents. We first formulate problems under two types of conventional habit formation preferences, namely linear and multiplicative external habit formation, in a mean field game framework. In a log-normal market model with the asset specialization, we characterize one mean field equilibrium in analytical form in each problem, allowing us to understand some quantitative properties of the equilibrium strategy and conclude some financial implications caused by consumption habits from a mean-field perspective. In each problem with n agents, we construct an approximate Nash equilibrium for the n-player game using the obtained mean field equilibrium when n is sufficiently large. The explicit convergence order in each problem can also be obtained.",
        "references": [
            {
                "arxivId": "2106.00581",
                "title": "$N$-player and Mean-field Games in It\\^{o}-diffusion Markets with Competitive or Homophilous Interaction",
                "abstract": "In It\u00f4-diffusion environments, we introduce and analyze N -player and common-noise mean-field games in the context of optimal portfolio choice in a common market. The players invest in a finite horizon and also interact, driven either by competition or homophily. We study an incomplete market model in which the players have constant individual risk tolerance coefficients (CARA utilities). We also consider the general case of random individual risk tolerances and analyze the related games in a complete market setting. This randomness makes the problem substantially more complex as it leads to (N or a continuum of) auxiliary \u201cindividual\u201d It\u00f4-diffusion markets. For all cases, we derive explicit or closed-form solutions for the equilibrium stochastic processes, the optimal state processes, and the values of the games."
            },
            {
                "arxivId": "2011.10166",
                "title": "Retirement decision with addictive habit persistence in a jump diffusion market",
                "abstract": "This paper investigates the optimal retirement decision, investment, and consumption strategies in a market with jump diffusion, taking into account habit persistence and stock-wage correlation. Our analysis considers multiple stocks and a finite time framework, intending to determine the retirement boundary of the ``wealth-habit-wage\"triplet $(x, h, w)$. To achieve this, we use the habit reduction method and a duality approach to obtain the retirement boundary of the primal variables and feedback forms of optimal strategies. { When dealing with the dual problem, we address technical challenges in the proof of integral equation characterization of optimal retirement boundary using a $C^1$ version of It$\\hat{\\rm o}$'s formula.} Our results show that when the so-called ``de facto wealth\"exceeds a critical proportion of wage, an immediate retirement is the optimal choice for the agent. Additionally, we find that the introduction of jump risks allows for the possibility of discontinuous investment strategies within the working region, which is a novel and insightful finding. Our numerical results effectively illustrate these findings by varying the parameters."
            },
            {
                "arxivId": "2006.07684",
                "title": "Mean Field Exponential Utility Game: A Probabilistic Approach",
                "abstract": "We study an $N$-player and a mean field exponential utility game. Each player manages two stocks; one is driven by an individual shock and the other is driven by a common shock. Moreover, each player is concerned not only with her own terminal wealth but also with the relative performance of her competitors. We use the probabilistic approach to study these two games. We show the unique equilibrium of the $N$-player game and the mean field game can be characterized by a novel multi-dimensional FBSDE with quadratic growth and a novel mean-field FBSDEs, respectively. The well-posedness result and the convergence result are established."
            },
            {
                "arxivId": "1905.11782",
                "title": "Many-player games of optimal consumption and investment under relative performance criteria",
                "abstract": null
            },
            {
                "arxivId": "1308.2172",
                "title": "Mean Field Games and Systemic Risk",
                "abstract": "We propose a simple model of inter-bank borrowing and lending where the evolution of the log-monetary reserves of $N$ banks is described by a system of diffusion processes coupled through their drifts in such a way that stability of the system depends on the rate of inter-bank borrowing and lending. Systemic risk is characterized by a large number of banks reaching a default threshold by a given time horizon. Our model incorporates a game feature where each bank controls its rate of borrowing/lending to a central bank. The optimization reflects the desire of each bank to borrow from the central bank when its monetary reserve falls below a critical level or lend if it rises above this critical level which is chosen here as the average monetary reserve. Borrowing from or lending to the central bank is also subject to a quadratic cost at a rate which can be fixed by the regulator. We solve explicitly for Nash equilibria with finitely many players, and we show that in this model the central bank acts as a clearing house, adding liquidity to the system without affecting its systemic risk. We also study the corresponding Mean Field Game in the limit of large number of banks in the presence of a common noise."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2210.02957",
        "category": "q-fin",
        "title": "Vice versa: The decoupling of content and topic heterogeneity in collusion research",
        "abstract": "Collusive practices continue to be a significant threat to competition and consumer welfare. It should be of utmost importance for academic research to provide the theoretical and empirical foundations to antitrust authorities and enable them to develop proper tools to encounter new collusive practices. Utilizing topical natural language machine learning techniques allows me to analyze the evolution of economic research on collusion over the past two decades in a novel way. It enables me to review some 800 publications systematically. I extract the underlying topics from the papers and conduct a large set of uni\u2010 and multivariate time series and regression analyses on their individual prevalences. I detect a notable tendency towards monocultures in topics and an endogenous constriction of the topic variety. In contrast, the overall contents and issues addressed by these papers have grown remarkably. This caused a decoupling: Nowadays, more datasets and cartel cases are studied but with a smaller research\u00a0scope.",
        "references": [
            {
                "arxivId": "2007.11604",
                "title": "Understanding the temporal evolution of COVID-19 research through machine learning and natural language processing",
                "abstract": null
            },
            {
                "arxivId": "1301.3781",
                "title": "Efficient Estimation of Word Representations in Vector Space",
                "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2304.10802",
        "category": "q-fin",
        "title": "An extended Merton problem with relaxed benchmark tracking",
        "abstract": "This paper studies a Merton's optimal portfolio and consumption problem in an extended formulation incorporating the tracking of a benchmark process described by a geometric Brownian motion. We consider a relaxed tracking formulation such that the wealth process compensated by a fictitious capital injection outperforms the benchmark at all times. The fund manager aims to maximize the expected utility of consumption deducted by the cost of the capital injection, where the latter term can also be regarded as the expected largest shortfall of the wealth with reference to the benchmark. By introducing an auxiliary state process with reflection, we formulate and tackle an equivalent stochastic control problem by means of the dual transform and probabilistic representation, where the dual PDE can be solved explicitly. On the strength of the closed-form results, we can derive and verify the optimal feedback control for the primal control problem, allowing us to discuss some new and interesting financial implications induced by the additional risk-taking from the capital injection and the goal of tracking.",
        "references": [
            {
                "arxivId": "2006.15384",
                "title": "Optimal asset allocation for outperforming a stochastic benchmark target",
                "abstract": "We propose a data-driven Neural Network (NN) optimization framework to determine the optimal multi-period dynamic asset allocation strategy for outperforming a general stochastic target. We formulate the problem as an optimal stochastic control with an asymmetric, distribution shaping, objective function. The proposed framework is illustrated with the asset allocation problem in the accumulation phase of a defined contribution pension plan, with the goal of achieving a higher terminal wealth than a stochastic benchmark. We demonstrate that the data-driven approach is capable of learning an adaptive asset allocation strategy directly from historical market returns, without assuming any parametric model of the financial market dynamics. The optimal adaptive strategy outperforms the benchmark constant proportion strategy, achieving a higher terminal wealth with a 90% probability, a 46% higher median terminal wealth, and a significantly more right-skewed terminal wealth distribution."
            },
            {
                "arxivId": "1802.03954",
                "title": "On Dynamic Programming Principle for Stochastic Control Under Expectation Constraints",
                "abstract": null
            },
            {
                "arxivId": "0805.0618",
                "title": "Risk Aversion and Portfolio Selection in a Continuous-Time Model",
                "abstract": "The comparative statics of the optimal portfolios across individuals is carried out for the Black-Scholes market model. It turns out that the indirect utility functions inherit the order of risk aversion (in the Arrow-Pratt sense) from the von Neumann-Morgenstern utility functions, and therefore, a more risk-averse agent would invest less wealth (in absolute value) in the risky asset."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2306.16208",
        "category": "q-fin",
        "title": "Continuous-time q-learning for mean-field control problems",
        "abstract": "This paper studies the q-learning, recently coined as the continuous time counterpart of Q-learning by Jia and Zhou (2023), for continuous time Mckean-Vlasov control problems in the setting of entropy-regularized reinforcement learning. In contrast to the single agent's control problem in Jia and Zhou (2023), the mean-field interaction of agents renders the definition of the q-function more subtle, for which we reveal that two distinct q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as the first-order approximation of the integrated Q-function introduced in Gu, Guo, Wei and Xu (2023), which can be learnt by a weak martingale condition involving test policies; and (ii) the essential q-function (denoted by $q_e$) that is employed in the policy improvement iterations. We show that two q-functions are related via an integral representation under all test policies. Based on the weak martingale condition and our proposed searching method of test policies, some model-free learning algorithms are devised. In two examples, one in LQ control framework and one beyond LQ control framework, we can obtain the exact parameterization of the optimal value function and q-functions and illustrate our algorithms with simulation experiments.",
        "references": [
            {
                "arxivId": "2303.06993",
                "title": "Actor-Critic learning for mean-field control in continuous time",
                "abstract": "We study policy gradient for mean-field control in continuous time in a reinforcement learning setting. By considering randomised policies with entropy regularisation, we derive a gradient expectation representation of the value function, which is amenable to actor-critic type algorithms, where the value functions and the policies are learnt alternately based on observation samples of the state and model-free estimation of the population state distribution, either by offline or online learning. In the linear-quadratic mean-field framework, we obtain an exact parametrisation of the actor and critic functions defined on the Wasserstein space. Finally, we illustrate the results of our algorithms with some numerical experiments on concrete examples."
            },
            {
                "arxivId": "2111.11232",
                "title": "Policy Gradient and Actor-Critic Learning in Continuous Time and Space: Theory and Algorithms",
                "abstract": "We study policy gradient (PG) for reinforcement learning in continuous time and space under the regularized exploratory formulation developed by Wang et al. (2020). We represent the gradient of the value function with respect to a given parameterized stochastic policy as the expected integration of an auxiliary running reward function that can be evaluated using samples and the current value function. This effectively turns PG into a policy evaluation (PE) problem, enabling us to apply the martingale approach recently developed by Jia and Zhou (2021) for PE to solve our PG problem. Based on this analysis, we propose two types of the actor-critic algorithms for RL, where we learn and update value functions and policies simultaneously and alternatingly. The first type is based directly on the aforementioned representation which involves future trajectories and hence is offline. The second type, designed for online learning, employs the first-order condition of the policy gradient and turns it into martingale orthogonality conditions. These conditions are then incorporated using stochastic approximation when updating policies. Finally, we demonstrate the algorithms by simulations in two concrete examples."
            },
            {
                "arxivId": "2108.02731",
                "title": "Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach",
                "abstract": "One of the challenges for multiagent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial information of the entire system. Whereas exciting progress has been made to analyze decentralized MARL with the network of agents for social networks and team video games, little is known theoretically for decentralized MARL with the network of states for modeling self-driving vehicles, ride-sharing, and data and traffic routing. This paper proposes a framework of localized training and decentralized execution to study MARL with the network of states. Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that agents can execute afterward the learned decentralized policies, which depend only on agents\u2019 current states. The theoretical analysis consists of three key components: the first is the reformulation of the MARL system as a networked Markov decision process with teams of agents, enabling updating the associated team Q-function in a localized fashion; the second is the Bellman equation for the value function and the appropriate Q-function on the probability measure space; and the third is the exponential decay property of the team Q-function, facilitating its approximation with efficient sample efficiency and controllable error. The theoretical analysis paves the way for a new algorithm LTDE-Neural-AC, in which the actor\u2013critic approach with overparameterized neural networks is proposed. The convergence and sample complexity are established and shown to be scalable with respect to the sizes of both agents and states. To the best of our knowledge, this is the first neural network\u2013based MARL algorithm with network structure and provable convergence guarantee. Funding: X. Wei is partially supported by NSFC no. 12201343. R. Xu is partially supported by the NSF CAREER award DMS-2339240."
            },
            {
                "arxivId": "2012.14772",
                "title": "Optimal control of path-dependent McKean\u2013Vlasov SDEs in infinite-dimension",
                "abstract": "We study the optimal control of path-dependent McKean-Vlasov equations valued in Hilbert spaces motivated by non Markovian mean-field models driven by stochastic PDEs. We first establish the well-posedness of the state equation, and then we prove the dynamic programming principle (DPP) in such a general framework. The crucial law invariance property of the value function V is rigorously obtained, which means that V can be viewed as a function on the Wasserstein space of probability measures on the set of continuous functions valued in Hilbert space. We then define a notion of pathwise measure derivative, which extends the Wasserstein derivative due to Lions [41], and prove a related functional It{\\^o} formula in the spirit of Dupire [24] and Wu and Zhang [51]. The Master Bellman equation is derived from the DPP by means of a suitable notion of viscosity solution. We provide different formulations and simplifications of such a Bellman equation notably in the special case when there is no dependence on the law of the control."
            },
            {
                "arxivId": "2010.00145",
                "title": "Entropy Regularization for Mean Field Games with Learning",
                "abstract": "Entropy regularization has been extensively adopted to improve the efficiency, the stability, and the convergence of algorithms in reinforcement learning. This paper analyzes both quantitatively and qualitatively the impact of entropy regularization for mean field games (MFGs) with learning in a finite time horizon. Our study provides a theoretical justification that entropy regularization yields time-dependent policies and, furthermore, helps stabilizing and accelerating convergence to the game equilibrium. In addition, this study leads to a policy-gradient algorithm with exploration in MFG. With this algorithm, agents are able to learn the optimal exploration scheduling, with stable and fast convergence to the game equilibrium."
            },
            {
                "arxivId": "1911.07314",
                "title": "Dynamic Programming Principles for Mean-Field Controls with Learning",
                "abstract": "Multiagent systems\u2014such as recommendation systems, ride-sharing platforms, food-delivery systems, and data-routing centers\u2014are areas of rapid technology development that require constant improvements to address the lack of efficiency and curse of dimensionality. In the paper \u201cDynamic Programming Principles for Mean-Field Controls with Learning,\u201d we show that multiagent systems with mean-field approximation and learning can be recast as general forms of reinforcement learning problems, where the state variable is replaced by the probability distribution. This reformulation paves the way for developing efficient value-based and policy-based algorithms for mean-field controls with learning. It is also the first step toward future theoretical development of learning problem with mean-field controls."
            },
            {
                "arxivId": "1704.06440",
                "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
                "abstract": "Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that \"soft\" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\\epsilon$-greedy exploration schedule."
            },
            {
                "arxivId": "1609.08064",
                "title": "Limit Theory for Controlled McKean-Vlasov Dynamics",
                "abstract": "This paper rigorously connects the problem of optimal control of McKean-Vlasov dynamics with large systems of interacting controlled state processes. Precisely, the empirical distributions of near-optimal control-state pairs for the $n$-state systems, as $n$ tends to infinity, admit limit points in distribution (if the objective functions are suitably coercive), and every such limit is supported on the set of optimal control-state pairs for the McKean-Vlasov problem. Conversely, any distribution on the set of optimal control-state pairs for the McKean-Vlasov problem can be realized as a limit in this manner. Arguments are based on controlled martingale problems, which lend themselves naturally to existence proofs; along the way it is shown that a large class of McKean-Vlasov control problems admit optimal Markovian controls."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2308.04400",
        "category": "q-fin",
        "title": "Limited substitutability, relative price changes and the uplifting of public natural capital values",
        "abstract": "As the global economy continues to grow, ecosystem services tend to stagnate or decline. Economic theory has shown how such shifts in relative scarcities can be reflected in the appraisal of public projects and environmental-economic accounting, but empirical evidence has been lacking to put the theory into practice. To estimate the relative price change in ecosystem services that can be used to make such adjustments, we perform a global meta-analysis of environmental valuation studies to derive income elasticities of willingness to pay (WTP) for ecosystem services as a proxy for the degree of limited substitutability. Based on 861 income-WTP pairs, we estimate an income elasticity of WTP of around 0.79 (95-CI: 0.60 to 0.97). Combining these results with a global data set on shifts in the relative scarcity of ecosystem services, we estimate relative price change of ecosystem services of around 2.2 percent per year. In an application to natural capital valuation of non-timber forest ecosystem services by the World Bank, we show that their natural capital value should be uplifted by more than 50 percent (95-CI: 32 to 78 percent), materially elevating the role of public natural capital. We discuss implications for relative price adjustments in policy appraisal and for improving estimates of comprehensive national accounts.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2309.02338",
        "category": "q-fin",
        "title": "Sustainability assessment of Low Earth Orbit (LEO) satellite broadband mega-constellations",
        "abstract": "The growth of megaconstellations is rapidly increasing the number of rocket launches. While Low Earth Orbit (LEO) broadband satellites help to connect unconnected communities and achieve the Sustainable Development Goals (SDGs), there are also significant environmental emissions impacts from burning rocket fuels. We present sustainability analytics for phase 1 of the three main LEO constellations including Amazon Kuiper (3,236 satellites), Eutelsat Group`s OneWeb (648 satellites), and SpaceX Starlink (4,425 satellites). We find that LEO megaconstellations provide substantially improved broadband speeds for rural and remote communities, but are roughly 6-8 times more emissions intensive (250 kg CO2eq/subscriber/year) than comparative terrestrial mobile broadband. In the worst-case emissions scenario, this rises to 12-14 times more (469 kg CO2eq/subscriber/year). Policy makers must carefully consider the trade-off between connecting unconnected communities to further the SDGs and mitigating the growing space sector environmental footprint, particularly regarding phase 2 plans to launch an order-of-magnitude more satellites.",
        "references": [
            {
                "arxivId": "2203.08933",
                "title": "The Digital Divide in Canada and the Role of LEO Satellites in Bridging the Gap",
                "abstract": "Overcoming the digital divide in rural and remote areas has always been a big challenge for Canada with its huge geographical area. In 2016, the Canadian Radio-television and Telecommunications Commission announced broadband Internet as a basic service available for all Canadians. However, approximately one million Canadians still did not have access to broadband services as of 2020. The COVID-19 pandemic has made the situation more challenging, as social, economic, and educational activities have increasingly been transferred online. The condition is more unfavorable for Indigenous communities. A key challenge in deploying rural and remote broadband Internet is to plan and implement high-capacity backbones, which are now available only in denser urban areas. For any Internet provider, it is almost impossible to make a viable business proposal in these areas. For example, the vast land of the Northwest Territories', Yukon's, and Nunavut's diverse geographical features present obstacles for broadband infrastructure. In this article, we investigate the digital divide in Canada with a focus on rural and remote areas. In so doing, we highlight two potential solutions using low Earth orbit (LEO) constellations to deliver broadband Internet in rural and remote areas to address the access inequality and the digital divide. The first solution involves integrating LEO constellations as a backbone for the existing 4G/5G telecommunications network. This solution uses satellites in a LEO constellation to provide a backhaul network connecting the 4G/5G access network to its core network. The 3rd Generation Partnership Project already specifies how to integrate LEO satellite networks into the 4G/5G network, and the Canadian satellite operator Telesat has already showcased this solution with one terrestrial operator, TIM Brasil, in their 4G network. In this way, users can seamlessly access broadband Internet via their mobile terminals. The second solution is based on the direct use of LEO constellations, such as Starlink, which are now operating in Canada, to deliver broadband Internet. As LEO satellites fly lower, their round-trip latency is lower, and the user terminals can receive Internet signals as long as they are pointing at the sky. An in-depth discussion of both solutions is presented in this work."
            },
            {
                "arxivId": "2012.06182",
                "title": "Point-to-Point Communication in Integrated Satellite-Aerial 6G Networks: State-of-the-Art and Future Challenges",
                "abstract": "This paper surveys the literature on point-to-point (P2P) links for integrated satellite-aerial networks, which are envisioned to be among the key enablers of the sixth-generation (6G) of wireless networks vision. The paper first outlines the unique characteristics of such integrated large-scale complex networks, often denoted by spatial networks, and focuses on two particular space-air infrastructures, namely, satellites networks and high-altitude platforms (HAPs). The paper then classifies the connecting P2P communications links as satellite-to-satellite links at the same layer (SSLL), satellite-to-satellite links at different layers (SSLD), and HAP-to-HAP links (HHL). The paper surveys each layer of such spatial networks separately, and highlights the possible natures of the connecting links (i.e., radio-frequency or free-space optics) with a dedicated survey of the existing link-budget results. The paper, afterwards, presents the prospective merit of realizing such an integrated satellite-HAP network towards providing broadband services in under-served and remote areas. Finally, the paper sheds light on several future research directions in the context of spatial networks, namely large-scale network optimization, intelligent offloading, smart platforms, energy efficiency, multiple access schemes, distributed spatial networks, and routing."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-11.json",
        "arxivId": "2403.05222",
        "category": "q-fin",
        "title": "Matching under Imperfectly Transferable Utility",
        "abstract": "In this paper, we examine matching models with imperfectly transferable utility (ITU). We provide motivating examples, discuss the theoretical foundations of ITU matching models and present methods for estimating them. We also explore connected topics and provide an overview of the related literature. This paper has been submitted as a draft chapter for the Handbook of the Economics of Matching, edited by Che, Chiappori and Salani\\'e.",
        "references": [
            {
                "arxivId": "2309.11416",
                "title": "Existence of a Competitive Equilibrium with Substitutes, with Applications to Matching and Discrete Choice Models",
                "abstract": "We propose new results for the existence and uniqueness of a general nonparametric and nonseparable competitive equilibrium with substitutes. These results ensure the invertibility of a general competitive system. The existing literature has focused on the uniqueness of a competitive equilibrium assuming that existence holds. We introduce three properties that our supply system must satisfy: weak substitutes, pivotal substitutes, and responsiveness. These properties are sufficient to ensure the existence of an equilibrium, thus providing the existence counterpart to Berry, Gandhi, and Haile (2013)'s uniqueness results. For two important classes of models, bipartite matching models with full assignment and discrete choice models, we show that both models can be reformulated as a competitive system such that our existence and uniqueness results can be readily applied. We also provide an algorithm to compute the unique competitive equilibrium. Furthermore, we argue that our results are particularly useful for studying imperfectly transferable utility matching models with full assignment and non-additive random utility models."
            },
            {
                "arxivId": "2102.02071",
                "title": "Matching Function Equilibria with Partial Assignment: Existence, Uniqueness and Estimation",
                "abstract": "In this paper, we argue that models coming from a variety of fields share a common structure that we call matching function equilibria with partial assignment. This structure revolves around an aggregate matching function and a system of nonlinear equations. This encompasses search and matching models, matching models with transferable, non-transferable and imperfectly transferable utility, and matching with peer effects. We provide a proof of existence and uniqueness of an equilibrium as well as an efficient algorithm to compute it. We show how to estimate parametric versions of these models by maximum likelihood. We also propose an approach to construct counterfactuals without estimating the matching functions for a subclass of models. We illustrate our estimation approach by analyzing the impact of the elimination of the Social Security Student Benefit Program in 1982 on the marriage market in the United States."
            },
            {
                "arxivId": "2102.06547",
                "title": "Like Attract Like? A Structural Comparison of Homogamy across Same-Sex and Different-Sex Households",
                "abstract": "In this paper, we extend Gary Becker\u2019s empirical analysis of the marriage market to same-sex couples. We build an equilibrium model of the same-sex marriage market that allows for straightforward identification of the gains of marriage. We estimate the model with 2008\u201312 American Community Survey data on California and find that positive assortative mating is weaker for same-sex couples than for different-sex couples with respect to age and race. Positive assortative mating on education is stronger among female same-sex couples but comparable for male same-sex and different-sex couples. As regards labor market outcomes, our results suggest that specialization within the household mainly applies to different-sex couples."
            },
            {
                "arxivId": "2211.07416",
                "title": "Collective models and the marriage market",
                "abstract": "In this paper, I develop an integrated approach to collective models and matching models of the marriage market. In the collective framework, both household formation and the intra-household allocation of bargaining power are taken as given. This is no longer the case in the present contribution, where both are endogenous to the determination of equilibrium on the marriage market. I characterize a class of \"proper'' collective models which can be embedded into a general matching framework with imperfectly transferable utility. In such models, the bargaining sets are parametrized by an analytical device called distance function, which plays a key role both for writing down the usual stability conditions and for estimation. In general, however, distance functions are not known in closed-form. I provide an efficient method for computing distance functions, that works even with the most complex collective models. Finally, I illustrate my results with a proof-of-concept application that uses PSID data. I identify the sharing rule and its distribution and consider two counterfactual experiments of women empowerment, both of which are shown to affect the sharing rule and the investment of couples in public goods."
            },
            {
                "arxivId": "1609.06349",
                "title": "A review of matrix scaling and Sinkhorn's normal form for matrices and positive maps",
                "abstract": "Given a nonnegative matrix $A$, can you find diagonal matrices $D_1,~D_2$ such that $D_1AD_2$ is doubly stochastic? The answer to this question is known as Sinkhorn's theorem. It has been proved with a wide variety of methods, each presenting a variety of possible generalisations. Recently, generalisations such as to positive maps between matrix algebras have become more and more interesting for applications. This text gives a review of over 70 years of matrix scaling. The focus lies on the mathematical landscape surrounding the problem and its solution as well as the generalisation to positive maps and contains hardly any nontrivial unpublished results."
            },
            {
                "arxivId": "1508.05114",
                "title": "The nonlinear Bernstein-Schr\\\"odinger equation in Economics",
                "abstract": "In this paper we relate the Equilibrium Assignment Problem (EAP), which is underlying in several economics models, to a system of nonlinear equations that we call the \"nonlinear Bernstein-Schr\\\"odinger system\", which is well-known in the linear case, but whose nonlinear extension does not seem to have been studied. We apply this connection to derive an existence result for the EAP, and an efficient computational method."
            },
            {
                "arxivId": "2106.02371",
                "title": "Cupid\u2019s Invisible Hand: Social Surplus and Identification in Matching Models",
                "abstract": "We investigate a model of one-to-one matching with transferable utility when some of the characteristics of the players are unobservable to the analyst. We allow for a wide class of distributions of unobserved heterogeneity, subject only to a separability assumption that generalizes Choo and Siow (2006). We first show that the stable matching maximizes a social gain function that trades off the average surplus due to the observable characteristics and a generalized entropy term that reflects the impact of matching on unobserved characteristics. We use this result to derive simple closed-form formulae that identify the joint surplus in every possible match and the equilibrium utilities of all participants, given any known distribution of unobserved heterogeneity. If transfers are observed, then the pre-transfer utilities of both partners are also identified. We also present a very fast algorithm that computes the optimal matching for any specification of the joint surplus. We conclude by discussing some empirical approaches suggested by these results."
            },
            {
                "arxivId": "2102.07476",
                "title": "Personality Traits and the Marriage Market",
                "abstract": "Which and how many attributes are relevant for the sorting of agents in a matching market? This paper addresses these questions by constructing indices of mutual attractiveness that aggregate information about agents\u2019 attributes. The first k indices for agents on each side of the market provide the best approximation of the matching surplus by a k-dimensional model. The methodology is applied on a unique Dutch household survey containing information about education, height, body mass index, health, attitude toward risk, and personality traits of spouses."
            },
            {
                "arxivId": "1012.1904",
                "title": "Unique equilibria and substitution effects in a stochastic model of the marriage market",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2210.08569",
        "category": "q-fin",
        "title": "Limited or Biased: Modeling Sub-Rational Human Investors in Financial Markets",
        "abstract": "Human decision-making in real-life deviates significantly from the optimal decisions made by fully rational agents, primarily due to computational limitations or psychological biases. While existing studies in behavioral finance have discovered various aspects of human sub-rationality, there lacks a comprehensive framework to transfer these findings into an adaptive human model applicable across diverse financial market scenarios. In this study, we introduce a flexible model that incorporates five different aspects of human sub-rationality using reinforcement learning. Our model is trained using a high-fidelity multi-agent market simulator, which overcomes limitations associated with the scarcity of labeled data of individual investors. We evaluate the behavior of sub-rational human investors using hand-crafted market scenarios and SHAP value analysis, showing that our model accurately reproduces the observations in the previous studies and reveals insights of the driving factors of human behavior. Finally, we explore the impact of sub-rationality on the investor's Profit and Loss (PnL) and market quality. Our experiments reveal that bounded-rational and prospect-biased human behaviors improve liquidity but diminish price efficiency, whereas human behavior influenced by myopia, optimism, and pessimism reduces market liquidity.",
        "references": [
            {
                "arxivId": "2402.08755",
                "title": "LLM-driven Imitation of Subrational Behavior : Illusion or Reality?",
                "abstract": "Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework."
            },
            {
                "arxivId": "2111.06956",
                "title": "Human irrationality: both bad and good for reward inference",
                "abstract": "Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference. We find that wrongly modeling a systematically irrational human as noisy-rational performs a lot worse than correctly capturing these biases -- so much so that it can be better to skip inference altogether and stick to the prior! More importantly, we show that an irrational human, when correctly modelled, can communicate more information about the reward than a perfectly rational human can. That is, if a robot has the correct model of a human's irrationality, it can make an even stronger inference than it ever could if the human were rational. Irrationality fundamentally helps rather than hinder reward inference, but it needs to be correctly accounted for."
            },
            {
                "arxivId": "2111.00094",
                "title": "Profit equitably: an investigation of market maker's impact on equitable outcomes",
                "abstract": "We look at discovering the impact of market microstructure on equitability for market participants at public exchanges such as the New York Stock Exchange or NASDAQ. Are these environments equitable venues for low-frequency participants (such as retail investors)? In particular, can market makers contribute to equitability for these agents? We use a simulator to assess the effect a market marker can have on equality of outcomes for consumer or retail traders by adjusting its parameters. Upon numerically quantifying market equitability by the entropy of the price returns distribution of consumer agents, we demonstrate that market makers indeed support equitability and that a negative correlation is observed between the profits of the market maker and equitability. We then use multi objective reinforcement learning to concurrently optimize for the two objectives of consumer agent equitability and market maker profitability, which leads us to learn policies that facilitate lower market volatility and tighter spreads for comparable profit levels."
            },
            {
                "arxivId": "2110.14771",
                "title": "ABIDES-gym: gym environments for multi-agent discrete event simulation and application to financial markets",
                "abstract": "Model-free Reinforcement Learning (RL) requires the ability to sample trajectories by taking actions in the original problem environment or a simulated version of it. Breakthroughs in the field of RL have been largely facilitated by the development of dedicated open source simulators with easy to use frameworks such as OpenAI Gym and its Atari environments. In this paper we propose to use the OpenAI Gym framework on discrete event time based Discrete Event Multi-Agent Simulation (DEMAS). We introduce a general technique to wrap a DEMAS simulator into the Gym framework. We expose the technique in detail and implement it using the simulator ABIDES as a base. We apply this work by specifically using the markets extension of ABIDES, ABIDES-Markets, and develop two benchmark financial markets OpenAI Gym environments for training daily investor and execution agents.1 As a result, these two environments describe classic financial problems with a complex interactive market behavior response to the experimental agent's action."
            },
            {
                "arxivId": "1912.04941",
                "title": "Get real: realism metrics for robust limit order book market simulations",
                "abstract": "Market simulation is an increasingly important method for evaluating and training trading strategies and testing \"what if\" scenarios. The extent to which results from these simulations can be trusted depends on how realistic the environment is for the strategies being tested. As a step towards providing benchmarks for realistic simulated markets, we enumerate measurable stylized facts of limit order book (LOB) markets across multiple asset classes from the literature. We apply these metrics to data from real markets and compare the results to data originating from simulated markets. We illustrate their use in five different simulated market configurations: The first (market replay) is frequently used in practice to evaluate trading strategies; the other four are interactive agent based simulation (IABS) configurations which combine zero intelligence agents, and agents with limited strategic behavior. These simulated agents rely on an internal \"oracle\" that provides a fundamental value for the asset. In traditional IABS methods the fundamental originates from a mean reverting random walk. We show that markets exhibit more realistic behavior when the fundamental arises from historical market data. We further experimentally illustrate the effectiveness of IABS techniques as opposed to market replay."
            },
            {
                "arxivId": "1906.12010",
                "title": "How to Evaluate Trading Strategies: Single Agent Market Replay or Multiple Agent Interactive Simulation?",
                "abstract": "We show how a multi-agent simulator can support two important but distinct methods for assessing a trading strategy: Market Replay and Interactive Agent-Based Simulation (IABS). Our solution is important because each method offers strengths and weaknesses that expose or conceal flaws in the subject strategy. A key weakness of Market Replay is that the simulated market does not substantially adapt to or respond to the presence of the experimental strategy. IABS methods provide an artificial market for the experimental strategy using a population of background trading agents. Because the background agents attend to market conditions and current price as part of their strategy, the overall market is responsive to the presence of the experimental strategy. Even so, IABS methods have their own weaknesses, primarily that it is unclear if the market environment they provide is realistic. We describe our approach in detail, and illustrate its use in an example application: The evaluation of market impact for various size orders."
            },
            {
                "arxivId": "1906.08253",
                "title": "When to Trust Your Model: Model-Based Policy Optimization",
                "abstract": "Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely."
            },
            {
                "arxivId": "1906.02312",
                "title": "Risk-Sensitive Compact Decision Trees for Autonomous Execution in Presence of Simulated Market Response",
                "abstract": "We demonstrate an application of risk-sensitive reinforcement learning to optimizing execution in limit order book markets. We represent taking order execution decisions based on limit order book knowledge by a Markov Decision Process; and train a trading agent in a market simulator, which emulates multi-agent interaction by synthesizing market response to our agent's execution decisions from historical data. Due to market impact, executing high volume orders can incur significant cost. We learn trading signals from market microstructure in presence of simulated market response and derive explainable decision-tree-based execution policies using risk-sensitive Q-learning to minimize execution cost subject to constraints on cost variance."
            },
            {
                "arxivId": "1904.12066",
                "title": "ABIDES: Towards High-Fidelity Market Simulation for AI Research",
                "abstract": "We introduce ABIDES, an Agent-Based Interactive Discrete Event Simulation environment. ABIDES is designed from the ground up to support AI agent research in market applications. While simulations are certainly available within trading firms for their own internal use, there are no broadly available high-fidelity market simulation environments. We hope that the availability of such a platform will facilitate AI research in this important area. ABIDES currently enables the simulation of tens of thousands of trading agents interacting with an exchange agent to facilitate transactions. It supports configurable pairwise network latencies between each individual agent as well as the exchange. Our simulator's message-based design is modeled after NASDAQ's published equity trading protocols ITCH and OUCH. We introduce the design of the simulator and illustrate its use and configuration with sample code, validating the environment with example trading scenarios. The utility of ABIDES is illustrated through experiments to develop a market impact model. We close with discussion of future experimental problems it can be used to explore, such as the development of ML-based trading algorithms."
            },
            {
                "arxivId": "1902.06865",
                "title": "Hyperbolic Discounting and Learning over Multiple Horizons",
                "abstract": "Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process. The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences. In this work we revisit the fundamentals of discounting in RL and bridge this disconnect by implementing an RL agent that acts via hyperbolic discounting. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL. Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over a strong value-based RL agent, Rainbow."
            },
            {
                "arxivId": "1812.05905",
                "title": "Soft Actor-Critic Algorithms and Applications",
                "abstract": "Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks."
            },
            {
                "arxivId": "1805.08010",
                "title": "Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior",
                "abstract": "Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences."
            },
            {
                "arxivId": "1804.04216",
                "title": "Market Making via Reinforcement Learning",
                "abstract": "Market making is a fundamental trading problem in which an agent provides liquidity by continually offering to buy and sell a security. The problem is challenging due to inventory risk, the risk of accumulating an unfavourable position and ultimately losing money. In this paper, we develop a high-fidelity simulation of limit order book markets, and use it to design a market making agent using temporal-difference reinforcement learning. We use a linear combination of tile codings as a value function approximator, and design a custom reward function that controls inventory risk. We demonstrate the effectiveness of our approach by showing that our agent outperforms both simple benchmark strategies and a recent online learning approach from the literature."
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1512.05832",
                "title": "Learning the Preferences of Ignorant, Inconsistent Agents",
                "abstract": "\n \n An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.\n \n"
            },
            {
                "arxivId": "1312.5602",
                "title": "Playing Atari with Deep Reinforcement Learning",
                "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
            },
            {
                "arxivId": "1311.2097",
                "title": "Risk-Sensitive Reinforcement Learning",
                "abstract": "We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents\u2019 behaviors express key features of human behavior as predicted by prospect theory (Kahneman & Tversky, 1979), for example, different risk preferences for gains and losses, as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework, we apply it to quantify human behavior in a sequential investment task. We find that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses that is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals shows a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex, and insula that is not present if standard Q-values are used."
            },
            {
                "arxivId": "1204.1381",
                "title": "Price Jump Prediction in Limit Order Book",
                "abstract": "A limit order book provides information on available limit order prices and their volumes. Based on these quantities, we give an empirical result on the relationship between the bid-ask liquidity balance and trade sign and we show that liquidity balance on best bid/best ask is quite informative for predicting the future market order's direction. Moreover, we de ne price jump as a sell (buy) market order arrival which is executed at a price which is smaller (larger) than the best bid (best ask) price at the moment just after the precedent market order arrival. Features are then extracted related to limit order volumes, limit order price gaps, market order information and limit order event information. Logistic regression is applied to predict the price jump from the limit order book's feature. LASSO logistic regression is introduced to help us make variable selection from which we are capable to highlight the importance of di erent features in predicting the future price jump. In order to get rid of the intraday data seasonality, the analysis is based on two separated datasets: morning dataset and afternoon dataset. Based on an analysis on forty largest French stocks of CAC40, we nd that trade sign and market order size as well as the liquidity on the best bid (best ask) are consistently informative for predicting the incoming price jump."
            },
            {
                "arxivId": "1012.0349",
                "title": "Limit order books",
                "abstract": "Abstract Limit order books (LOBs) match buyers and sellers in more than half of the world\u2019s financial markets. This survey highlights the insights that have emerged from the wealth of empirical and theoretical studies of LOBs. We examine the findings reported by statistical analyses of historical LOB data and discuss how several LOB models provide insight into certain aspects of the mechanism. We also illustrate that many such models poorly resemble real LOBs and that several well-established empirical facts have yet to be reproduced satisfactorily. Finally, we identify several key unresolved questions about LOBs."
            },
            {
                "arxivId": "cond-mat/0203511",
                "title": "Statistical properties of stock order books: empirical results and models",
                "abstract": "Abstract We investigate several statistical properties of the order book of three liquid stocks of the Paris Bourse. The results are to a large degree independent of the stock studied. The most interesting features concern (i) the statistics of incoming limit order prices, which follows a power-law around the current price with a diverging mean; and (ii) the shape of the average order book, which can be quantitatively reproduced using a \u2018zero intelligence\u2019 numerical model and qualitatively predicted using a simple approximation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2212.14076",
        "category": "q-fin",
        "title": "Quantum-Inspired Tensor Neural Networks for Option Pricing",
        "abstract": "Recent advances in deep learning have enabled us to address the curse of dimensionality (COD) by solving problems in higher dimensions. A subset of such approaches of addressing the COD has led us to solving high-dimensional PDEs. This has resulted in opening doors to solving a variety of real-world problems ranging from mathematical finance to stochastic control for industrial applications. Although feasible, these deep learning methods are still constrained by training time and memory. Tackling these shortcomings, Tensor Neural Networks (TNN) demonstrate that they can provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. Besides TNN, we also introduce Tensor Network Initializer (TNN Init), a weight initialization scheme that leads to faster convergence with smaller variance for an equivalent parameter count as compared to a DNN. We benchmark TNN and TNN Init by applying them to solve the parabolic PDE associated with the Heston model, which is widely used in financial pricing theory.",
        "references": [
            {
                "arxivId": "2208.02235",
                "title": "Quantum-Inspired Tensor Neural Networks for Partial Differential Equations",
                "abstract": "Partial Differential Equations (PDEs) are used to model a variety of dynamical systems in science and engineering. Recent advances in deep learning have enabled us to solve them in a higher dimension by addressing the curse of dimensionality in new ways. However, deep learning methods are constrained by training time and memory. To tackle these shortcomings, we implement Tensor Neural Networks (TNN), a quantum-inspired neural network architecture that leverages Tensor Network ideas to improve upon deep learning approaches. We demonstrate that TNN provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. We benchmark TNN by applying them to solve parabolic PDEs, specifically the Black-Scholes-Barenblatt equation, widely used in financial pricing theory, empirically showing the advantages of TNN over DNN. Further examples, such as the Hamilton-Jacobi-Bellman equation, are also discussed."
            },
            {
                "arxivId": "2104.13669",
                "title": "Optimal Stopping via Randomized Neural Networks",
                "abstract": "This paper presents the benefits of using randomized neural networks instead of standard basis functions or deep neural networks to approximate the solutions of optimal stopping problems. The key idea is to use neural networks, where the parameters of the hidden layers are generated randomly and only the last layer is trained, in order to approximate the continuation value. Our approaches are applicable to high dimensional problems where the existing approaches become increasingly impractical. In addition, since our approaches can be optimized using simple linear regression, they are easy to implement and theoretical guarantees can be provided. We test our approaches for American option pricing on Black--Scholes, Heston and rough Heston models and for optimally stopping a fractional Brownian motion. In all cases, our algorithms outperform the state-of-the-art and other relevant machine learning approaches in terms of computation time while achieving comparable results. Moreover, we show that they can also be used to efficiently compute Greeks of American options."
            },
            {
                "arxivId": "2002.11794",
                "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",
                "abstract": "Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. \nThis leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models."
            },
            {
                "arxivId": "1910.07425",
                "title": "Modeling sequences with quantum states: a look under the hood",
                "abstract": "Classical probability distributions on sets of sequences can be modeled using quantum states. Here, we do so with a quantum state that is pure and entangled. Because it is entangled, the reduced densities that describe subsystems also carry information about the complementary subsystem. This is in contrast to the classical marginal distributions on a subsystem in which information about the complementary system has been integrated out and lost. A training algorithm based on the density matrix renormalization group (DMRG) procedure uses the extra information contained in the reduced densities and organizes it into a tensor network model. An understanding of the extra information contained in the reduced densities allow us to examine the mechanics of this DMRG algorithm and study the generalization error of the resulting model. As an illustration, we work with the even-parity dataset and produce an estimate for the generalization error as a function of the fraction of the dataset used in training."
            },
            {
                "arxivId": "1909.11532",
                "title": "Deep neural network framework based on backward stochastic differential equations for pricing and hedging American options in high dimensions",
                "abstract": "We propose a deep neural network framework for computing prices and deltas of American options in high dimensions. The architecture of the framework is a sequence of neural networks, where each network learns the difference of the price functions between adjacent timesteps. We introduce the least squares residual of the associated backward stochastic differential equation as the loss function. Our proposed framework yields prices and deltas for the entire spacetime, not only at a given point (e.g. t\u2009=\u20090). The computational cost of the proposed approach is quadratic in dimension, which addresses the curse of dimensionality issue that state-of-the-art approaches suffer. Our numerical simulations demonstrate these contributions, and show that the proposed neural network framework outperforms state-of-the-art approaches in high dimensions."
            },
            {
                "arxivId": "1907.06474",
                "title": "Neural network regression for Bermudan option pricing",
                "abstract": "Abstract The pricing of Bermudan options amounts to solving a dynamic programming principle, in which the main difficulty, especially in high dimension, comes from the conditional expectation involved in the computation of the continuation value. These conditional expectations are classically computed by regression techniques on a finite-dimensional vector space. In this work, we study neural networks approximations of conditional expectations. We prove the convergence of the well-known Longstaff and Schwartz algorithm when the standard least-square regression is replaced by a neural network approximation, assuming an efficient algorithm to compute this approximation. We illustrate the numerical efficiency of neural networks as an alternative to standard regression methods for approximating conditional expectations on several numerical examples."
            },
            {
                "arxivId": "1905.01426",
                "title": "Matrix Product State\u2013Based Quantum Classifier",
                "abstract": "Interest in quantum computing has increased significantly. Tensor network theory has become increasingly popular and widely used to simulate strongly entangled correlated systems. Matrix product state (MPS) is a well-designed class of tensor network states that plays an important role in processing quantum information. In this letter, we show that MPS, as a one-dimensional array of tensors, can be used to classify classical and quantum data. We have performed binary classification of the classical machine learning data set Iris encoded in a quantum state. We have also investigated its performance by considering different parameters on the ibmqx4 quantum computer and proved that MPS circuits can be used to attain better accuracy. Furthermore the learning ability of an MPS quantum classifier is tested to classify evapotranspiration (ETo) for the Patiala meteorological station located in northern Punjab (India), using three years of a historical data set (Agri). We have used different performance metrics of classification to measure its capability. Finally, the results are plotted and the degree of correspondence among values of each sample is shown."
            },
            {
                "arxivId": "1903.10742",
                "title": "Generative Tensor Network Classification Model for Supervised Machine Learning",
                "abstract": "Tensor network (TN) has recently triggered extensive interests in developing machine-learning models in quantum many-body Hilbert space. Here we purpose a generative TN classification (GTNC) approach for supervised learning. The strategy is to train the generative TN for each class of the samples to construct the classifiers. The classification is implemented by comparing the distance in the many-body Hilbert space. The numerical experiments by GTNC show impressive performance on the MNIST and Fashion-MNIST dataset. The testing accuracy is competitive to the state-of-the-art convolutional neural network while higher than the naive Bayes classifier (a generative classifier) and support vector machine. Moreover, GTNC is more efficient than the existing TN models that are in general discriminative. By investigating the distances in the many-body Hilbert space, we find that (a) the samples are naturally clustering in such a space; and (b) bounding the bond dimensions of the TN's to finite values corresponds to removing redundant information in the image recognition. These two characters make GTNC an adaptive and universal model of excellent performance."
            },
            {
                "arxivId": "1901.02217",
                "title": "Tree Tensor Networks for Generative Modeling",
                "abstract": "Matrix product states (MPSs), a tensor network designed for one-dimensional quantum systems, were recently proposed for generative modeling of natural data (such as images) in terms of the ``Born machine.'' However, the exponential decay of correlation in MPSs restricts its representation power heavily for modeling complex data such as natural images. In this work, we push forward the effort of applying tensor networks to machine learning by employing the tree tensor network (TTN), which exhibits balanced performance in expressibility and efficient training and sampling. We design the tree tensor network to utilize the two-dimensional prior of the natural images and develop sweeping learning and sampling algorithms which can be efficiently implemented utilizing graphical processing units. We apply our model to random binary patterns and the binary MNIST data sets of handwritten digits. We show that the TTN is superior to MPSs for generative modeling in keeping the correlation of pixels in natural images, as well as giving better log-likelihood scores in standard data sets of handwritten digits. We also compare its performance with state-of-the-art generative models such as variational autoencoders, restricted Boltzmann machines, and PixelCNN. Finally, we discuss the future development of tensor network states in machine learning problems."
            },
            {
                "arxivId": "1806.05964",
                "title": "From Probabilistic Graphical Models to Generalized Tensor Networks for Supervised Learning",
                "abstract": "Tensor networks have found a wide use in a variety of applications in physics and computer science, recently leading to both theoretical insights as well as practical algorithms in machine learning. In this work we explore the connection between tensor networks and probabilistic graphical models, and show that it motivates the definition of generalized tensor networks where information from a tensor can be copied and reused in other parts of the network. We discuss the relationship between generalized tensor network architectures used in quantum physics, such as string-bond states, and architectures commonly used in machine learning. We provide an algorithm to train these networks in a supervised-learning context and show that they overcome the limitations of regular tensor networks in higher dimensions, while keeping the computation efficient. A method to combine neural networks and tensor networks as part of a common deep learning architecture is also introduced. We benchmark our algorithm for several generalized tensor network architectures on the task of classifying images and sounds, and show that they outperform previously introduced tensor-network algorithms. The models we consider also have a natural implementation on a quantum computer and may guide the development of near-term quantum machine learning architectures."
            },
            {
                "arxivId": "1804.07010",
                "title": "Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations",
                "abstract": "Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions."
            },
            {
                "arxivId": "1801.00315",
                "title": "Learning relevant features of data with multi-scale tensor networks",
                "abstract": "Inspired by coarse-graining approaches used in physics, we show how similar algorithms can be adapted for data. The resulting algorithms are based on layered tree tensor networks and scale linearly with both the dimension of the input and the training set size. Computing most of the layers with an unsupervised algorithm, then optimizing just the top layer for supervised classification of the MNIST and fashion MNIST data sets gives very good results. We also discuss mixing a prior guess for supervised weights together with an unsupervised representation of the data, yielding a smaller number of features nevertheless able to give good performance."
            },
            {
                "arxivId": "1711.10566",
                "title": "Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations",
                "abstract": "We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial differential equations. Depending on whether the available data is scattered in space-time or arranged in fixed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The effectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible fluid flow, and the propagation of nonlinear shallow-water waves."
            },
            {
                "arxivId": "1711.10561",
                "title": "Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations",
                "abstract": "We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters."
            },
            {
                "arxivId": "1710.04833",
                "title": "Machine learning by unitary tensor network of hierarchical tree structure",
                "abstract": "The resemblance between the methods used in quantum-many body physics and in machine learning has drawn considerable attention. In particular, tensor networks (TNs) and deep learning architectures bear striking similarities to the extent that TNs can be used for machine learning. Previous results used one-dimensional TNs in image recognition, showing limited scalability and flexibilities. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multi-scale entanglement renormalization ansatz. This approach introduces mathematical connections among quantum many-body physics, quantum information theory, and machine learning. While keeping the TN unitary in the training phase, TN states are defined, which encode classes of images into quantum many-body states. We study the quantum features of the TN states, including quantum entanglement and fidelity. We find these quantities could be properties that characterize the image classes, as well as the machine learning tasks."
            },
            {
                "arxivId": "1709.05963",
                "title": "Machine Learning Approximation Algorithms for High-Dimensional Fully Nonlinear Partial Differential Equations and Second-order Backward Stochastic Differential Equations",
                "abstract": null
            },
            {
                "arxivId": "1709.01662",
                "title": "Unsupervised Generative Modeling Using Matrix Product States",
                "abstract": "Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard datasets including the Bars and Stripes, random binary patterns and the MNIST handwritten digits to illustrate the abilities, features and drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work sheds light on many interesting directions of future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to be realized on quantum devices."
            },
            {
                "arxivId": "1707.02568",
                "title": "Solving high-dimensional partial differential equations using deep learning",
                "abstract": "Significance Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the \u201ccurse of dimensionality.\u201d This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships. Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the \u201ccurse of dimensionality.\u201d This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black\u2013Scholes equation, the Hamilton\u2013Jacobi\u2013Bellman equation, and the Allen\u2013Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships."
            },
            {
                "arxivId": "1706.04702",
                "title": "Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations",
                "abstract": null
            },
            {
                "arxivId": "1509.06569",
                "title": "Tensorizing Neural Networks",
                "abstract": "Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times."
            },
            {
                "arxivId": "1502.02963",
                "title": "An Analysis of the Heston Stochastic Volatility Model: Implementation and Calibration Using Matlab",
                "abstract": "This paper analyses the implementation and calibration of the Heston Stochastic\nVolatility Model. We first explain how characteristic functions can be used to esti\n-\nmate option prices. Then we consider the implementation of the Heston model,\nshowing that relatively simple solutions can lead to fast and accurate vanilla option\nprices. We also perform several calibration tests, using both local and global optimi\n-\nzation. Our analyses show that straightforward setups deliver good calibration re\n-\nsults. All calculations are carried out in Matlab and numerical examples are included\nin the paper to facilitate the understanding of mathematical concepts"
            },
            {
                "arxivId": "1409.1556",
                "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
                "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            {
                "arxivId": "1306.2164",
                "title": "A Practical Introduction to Tensor Networks: Matrix Product States and Projected Entangled Pair States",
                "abstract": null
            },
            {
                "arxivId": "math/0509295",
                "title": "Second\u2010order backward stochastic differential equations and fully nonlinear parabolic PDEs",
                "abstract": "For a d\u2010dimensional diffusion of the form dXt = \u03bc(Xt)dt + \u03c3(Xt)dWt and continuous functions f and g, we study the existence and uniqueness of adapted processes Y, Z, \u0393, and A solving the second\u2010order backward stochastic differential equation (2BSDE) $$dY_{t} = f(t,X_{t}, Y_{t}, Z_{t}, \\Gamma_{t}) dt + Z_t'\\circ dX_{t}, \\quad t \\in [0,T),$$ $$dZ_{t} = A_{t} dt + \\Gamma_{t}dX_{t}, \\quad t \\in [0,T),$$ $$Y_{T} = g(X_{T}).$$ If the associated PDE $$- v_{t}(t,x) + f(t,x,v(t,x), Dv(t,x), D^{2}v(t,x)) = 0,$$ $$(t,x) \\in [0,T) \\times {\\cal R}^{d}, \\quad v(T,x) = g(x),$$ has a sufficiently regular solution, then it follows directly from It\u00f4's formula that the processes $$v(t,X_{t}), Dv(t,X_{t}), D^{2}v(t,X_t), {\\cal L} Dv(t,X_{t}), \\quad t \\in [0,T],$$ solve the 2BSDE, where \ud835\udcc1 is the Dynkin operator of X without the drift term."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2304.09750",
        "category": "q-fin",
        "title": "Application of Tensor Neural Networks to Pricing Bermudan Swaptions",
        "abstract": "The Cheyette model is a quasi-Gaussian volatility interest rate model widely used to price interest rate derivatives such as European and Bermudan Swaptions for which Monte Carlo simulation has become the industry standard. In low dimensions, these approaches provide accurate and robust prices for European Swaptions but, even in this computationally simple setting, they are known to underestimate the value of Bermudan Swaptions when using the state variables as regressors. This is mainly due to the use of a finite number of predetermined basis functions in the regression. Moreover, in high-dimensional settings, these approaches succumb to the Curse of Dimensionality. To address these issues, Deep-learning techniques have been used to solve the backward Stochastic Differential Equation associated with the value process for European and Bermudan Swaptions; however, these methods are constrained by training time and memory. To overcome these limitations, we propose leveraging Tensor Neural Networks as they can provide significant parameter savings while attaining the same accuracy as classical Dense Neural Networks. In this paper we rigorously benchmark the performance of Tensor Neural Networks and Dense Neural Networks for pricing European and Bermudan Swaptions, and we show that Tensor Neural Networks can be trained faster than Dense Neural Networks and provide more accurate and robust prices than their Dense counterparts.",
        "references": [
            {
                "arxivId": "2303.09430",
                "title": "Global optimization of MPS in quantum-inspired numerical analysis",
                "abstract": "This work discusses the solution of partial differential equations (PDEs) using matrix product states (MPS). The study focuses on the search for the lowest eigenstates of a Hamiltonian equation, for which five algorithms are introduced: imaginary-time evolution, steepest gradient descent, an improved gradient descent, an implicitly restarted Arnoldi method, and density matrix renormalization group (DMRG) optimization. The first four methods are engineered using a framework of limited-precision linear algebra, where operations between MPS and matrix product operators (MPOs) are implemented with finite resources. All methods are benchmarked using the PDE for a quantum harmonic oscillator in up to two dimensions, over a regular grid with up to $2^{28}$ points. Our study reveals that all MPS-based techniques outperform exact diagonalization techniques based on vectors, with respect to memory usage. Imaginary-time algorithms are shown to underperform any type of gradient descent, both in terms of calibration needs and costs. Finally, Arnoldi like methods and DMRG asymptotically outperform all other methods, including exact diagonalization, as problem size increases, with an exponential advantage in memory and time usage."
            },
            {
                "arxivId": "2212.14703",
                "title": "Quantum simulation of partial differential equations via Schrodingerisation: technical details",
                "abstract": "We study a new method - called Schrodingerisation introduced in [Jin, Liu, Yu, arXiv: 2212.13969] - for solving general linear partial differential equations with quantum simulation. This method converts linear partial differential equations into a `Schrodingerised' or Hamiltonian system, using a new and simple transformation called the warped phase transformation. Here we provide more in-depth technical discussions and expand on this approach in a more detailed and pedagogical way. We apply this to more examples of partial differential equations, including heat, convection, Fokker-Planck, linear Boltzmann and Black-Scholes equations. This approach can also be extended to Schrodingerise general linear partial differential equations, including the Vlasov-Fokker-Planck equation and the Liouville representation equation for nonlinear ordinary differential equations."
            },
            {
                "arxivId": "2212.14076",
                "title": "Quantum-Inspired Tensor Neural Networks for Option Pricing",
                "abstract": "Recent advances in deep learning have enabled us to address the curse of dimensionality (COD) by solving problems in higher dimensions. A subset of such approaches of addressing the COD has led us to solving high-dimensional PDEs. This has resulted in opening doors to solving a variety of real-world problems ranging from mathematical finance to stochastic control for industrial applications. Although feasible, these deep learning methods are still constrained by training time and memory. Tackling these shortcomings, Tensor Neural Networks (TNN) demonstrate that they can provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. Besides TNN, we also introduce Tensor Network Initializer (TNN Init), a weight initialization scheme that leads to faster convergence with smaller variance for an equivalent parameter count as compared to a DNN. We benchmark TNN and TNN Init by applying them to solve the parabolic PDE associated with the Heston model, which is widely used in financial pricing theory."
            },
            {
                "arxivId": "2211.05567",
                "title": "Partial Differential Equations Meet Deep Neural Networks: A Survey",
                "abstract": "Many problems in science and engineering can be represented by a set of partial differential equations (PDEs) through mathematical modeling. Mechanism-based computation following PDEs has long been an essential paradigm for studying topics such as computational fluid dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. At the same time, solving PDEs efficiently has been a long-standing challenge. Generally, except for a few differential equations for which analytical solutions are directly available, many more equations must rely on numerical approaches such as the finite difference method, finite element method, finite volume method, and boundary element method to be solved approximately. These numerical methods usually divide a continuous problem domain into discrete points and then concentrate on solving the system at each of those points. Though the effectiveness of these traditional numerical methods, the vast number of iterative operations accompanying each step forward significantly reduces the efficiency. Recently, another equally important paradigm, data-based computation represented by deep learning, has emerged as an effective means of solving PDEs. Surprisingly, a comprehensive review for this interesting subfield is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks (DNNs) for PDEs. We discuss the literature published in this subfield over the past decades and present them in a common taxonomy, followed by an overview and classification of applications of these related methods in scientific research and engineering scenarios. The origin, developing history, character, sort, as well as the future trends in each potential direction of this subfield are also introduced."
            },
            {
                "arxivId": "1803.00704",
                "title": "Extension of PCA to Higher Order Data Structures: An Introduction to Tensors, Tensor Decompositions, and Tensor PCA",
                "abstract": "The widespread use of multisensor technology and the emergence of big data sets have brought the necessity to develop more versatile tools to represent higher order data with multiple aspects and high dimensionality. Data in the form of multidimensional arrays, also referred to as tensors, arise in a variety of applications including chemometrics, hyperspectral imaging, high-resolution videos, neuroimaging, biometrics, and social network analysis. Early multiway data analysis approaches reformatted such tensor data as large vectors or matrices and then resorted to dimensionality reduction methods developed for classical two-way analysis such as principal component analysis (PCA). However, one cannot discover hidden components within multiway data using conventional PCA. To this end, tensor decomposition methods which are flexible in the choice of the constraints and that extract more general latent components have been proposed. In this paper, we review the major tensor decomposition methods with a focus on problems targeted by classical PCA. In particular, we present tensor methods that aim to solve three important challenges typically addressed by PCA: dimensionality reduction, i.e., low-rank tensor approximation; supervised learning, i.e., learning linear subspaces for feature extraction; and robust low-rank tensor recovery. We also provide experimental results to compare different tensor models for both dimensionality reduction and supervised learning applications."
            },
            {
                "arxivId": "1708.00006",
                "title": "Tensor Networks in a Nutshell",
                "abstract": "Tensor network methods are taking a central role in modern quantum physics and beyond. They can provide an efficient approximation to certain classes of quantum states, and the associated graphical language makes it easy to describe and pictorially reason about quantum circuits, channels, protocols, open systems and more. Our goal is to explain tensor networks and some associated methods as quickly and as painlessly as possible. Beginning with the key definitions, the graphical tensor network language is presented through examples. We then provide an introduction to matrix product states. We conclude the tutorial with tensor contractions evaluating combinatorial counting problems. The first one counts the number of solutions for Boolean formulae, whereas the second is Penrose's tensor contraction algorithm, returning the number of $3$-edge-colorings of $3$-regular planar graphs."
            },
            {
                "arxivId": "1509.00216",
                "title": "Multireference linearized coupled cluster theory for strongly correlated systems using matrix product states.",
                "abstract": "We propose a multireference linearized coupled cluster theory using matrix product states (MPSs-LCC) which provides remarkably accurate ground-state energies, at a computational cost that has the same scaling as multireference configuration interaction singles and doubles, for a wide variety of electronic Hamiltonians. These range from first-row dimers at equilibrium and stretched geometries to highly multireference systems such as the chromium dimer and lattice models such as periodic two-dimensional 1-band and 3-band Hubbard models. The MPS-LCC theory shows a speed up of several orders of magnitude over the usual Density Matrix Renormalization Group (DMRG) algorithm while delivering energies in excellent agreement with converged DMRG calculations. Also, in all the benchmark calculations presented here, MPS-LCC outperformed the commonly used multi-reference quantum chemistry methods in some cases giving energies in excess of an order of magnitude more accurate. As a size-extensive method that can treat large active spaces, MPS-LCC opens up the use of multireference quantum chemical techniques in strongly correlated ab initio Hamiltonians, including two- and three-dimensional solids."
            },
            {
                "arxivId": "0907.2994",
                "title": "Tensor network decompositions in the presence of a global symmetry",
                "abstract": "Tensor network decompositions offer an efficient description of certain many-body states of a lattice system and are the basis of a wealth of numerical simulation algorithms. We discuss how to incorporate a global symmetry, given by a compact, completely reducible group G, in tensor network decompositions and algorithms. This is achieved by considering tensors that are invariant under the action of the group G. Each symmetric tensor decomposes into two types of tensors: degeneracy tensors, containing all the degrees of freedom, and structural tensors, which only depend on the symmetry group. In numerical calculations, the use of symmetric tensors ensures the preservation of the symmetry, allows selection of a specific symmetry sector, and significantly reduces computational costs. On the other hand, the resulting tensor network can be interpreted as a superposition of exponentially many spin networks. Spin networks are used extensively in loop quantum gravity, where they represent states of quantum geometry. Our work highlights their importance in the context of tensor network algorithms as well, thus setting the stage for cross-fertilization between these two areas of research."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2304.12245",
        "category": "q-fin",
        "title": "Inferring comparative advantage via entropy maximization",
        "abstract": "We revise the procedure proposed by Balassa to infer comparative advantage, which is a standard tool in Economics to analyze specialization (of countries, regions, etc). Balassa\u2019s approach compares a country\u2019s export of a given product with what would be expected from a benchmark based on the total volumes of countries and product flows. Based on results in the literature, we show that implementing Balassa\u2019s idea leads to conditions for estimating parameters conflicting with the information content of the model itself. Moreover, Balassa\u2019s approach does not implement any statistical validation. Hence, we propose an alternative procedure to overcome such a limitation, based upon the framework of entropy maximization and implementing a proper test of hypothesis: the \u2018key products\u2019 of a country are, now, the ones whose production is significantly larger than expected, under a null-model constraining the same amount of information defining Balassa\u2019s approach. What we found is that country diversification is always observed, regardless of the strictness of the validation procedure. Besides, the ranking of countries\u2019 fitnesses is only partially affected by the details of the validation scheme employed for the analysis while large differences are found to affect the rankings of product complexities. The routine for implementing the entropy-based filtering procedures employed here is freely available through the official Python Package Index PyPI.",
        "references": [
            {
                "arxivId": "2101.12625",
                "title": "Fast and scalable likelihood maximization for Exponential Random Graph Models with local constraints",
                "abstract": null
            },
            {
                "arxivId": "1810.05095",
                "title": "The statistical physics of real-world networks",
                "abstract": null
            },
            {
                "arxivId": "1707.05146",
                "title": "Unfolding the innovation system for the development of countries: coevolution of Science, Technology and Production",
                "abstract": null
            },
            {
                "arxivId": "1703.04090",
                "title": "Grand canonical validation of the bipartite international trade network.",
                "abstract": "Devising strategies for economic development in a globally competitive landscape requires a solid and unbiased understanding of countries' technological advancements and similarities among export products. Both can be addressed through the bipartite representation of the International Trade Network. In this paper, we apply the recently proposed grand canonical projection algorithm to uncover country and product communities. Contrary to past endeavors, our methodology, based on information theory, creates monopartite projections in an unbiased and analytically tractable way. Single links between countries or products represent statistically significant signals, which are not accounted for by null models such as the bipartite configuration model. We find stable country communities reflecting the socioeconomic distinction in developed, newly industrialized, and developing countries. Furthermore, we observe product clusters based on the aforementioned country groups. Our analysis reveals the existence of a complicated structure in the bipartite International Trade Network: apart from the diversification of export baskets from the most basic to the most exclusive products, we observe a statistically significant signal of an export specialization mechanism towards more sophisticated products."
            },
            {
                "arxivId": "1607.01735",
                "title": "A maximum entropy approach to separating noise from signal in bimodal affiliation networks",
                "abstract": "In practice, many empirical networks, including co-authorship and collocation networks are unimodal projections of a bipartite data structure where one layer represents entities, the second layer consists of a number of sets representing affiliations, attributes, groups, etc., and an inter-layer link indicates membership of an entity in a set. The edge weight in the unimodal projection, which we refer to as a co-occurrence network, counts the number of sets to which both end-nodes are linked. Interpreting such dense networks requires statistical analysis that takes into account the bipartite structure of the underlying data. Here we develop a statistical significance metric for such networks based on a maximum entropy null model which preserves both the frequency sequence of the individuals/entities and the size sequence of the sets. Solving the maximum entropy problem is reduced to solving a system of nonlinear equations for which fast algorithms exist, thus eliminating the need for expensive Monte-Carlo sampling techniques. We use this metric to prune and visualize a number of empirical networks."
            },
            {
                "arxivId": "1605.03133",
                "title": "Economic development and wage inequality: A complex system analysis",
                "abstract": "Adapting methods from complex system analysis, this paper analyzes the features of the complex relationship between wage inequality and the development and industrialization of a country. Development is understood as a combination of a monetary index, GDP per capita, and a recently introduced measure of a country\u2019s economic complexity: Fitness. Initially the paper looks at wage inequality on a global scale, over the time period 1990\u20132008. Our empirical results show that globally the movement of wage inequality along with the ongoing industrialization of countries has followed a longitudinally persistent pattern comparable to the one theorized by Kuznets in the fifties: countries with an average level of development suffer the highest levels of wage inequality. Next, the study narrows its focus on wage inequality within the United States. By using data on wages and employment in the approximately 3100 US counties over the time interval 1990\u20132014, it generalizes the Fitness-Complexity metric for geographic units and industrial sectors, and then investigates wage inequality between NAICS industries. The empirical time and scale dependencies are consistent with a relation between wage inequality and development driven by institutional factors comparing countries, and by change in the structural compositions of sectors in a homogeneous institutional environment, such as the counties of the United States."
            },
            {
                "arxivId": "1509.00607",
                "title": "Assessing Systemic Risk Due to Fire Sales Spillover Through Maximum Entropy Network Reconstruction",
                "abstract": "Monitoring and assessing systemic risk in financial markets is of great importance but it often requires data that are unavailable or available at a very low frequency. For this reason, systemic risk assessment with partial information is potentially very useful for regulators and other stakeholders. In this paper we consider systemic risk due to fire sales spillovers and portfolio rebalancing by using the risk metrics defined by Greenwood et al. (2015). By using a method based on the constrained minimization of the Cross Entropy, we show that it is possible to assess aggregated and single bank\u2019s systemicness and vulnerability, using only the information on the size of each bank and the capitalization of each investment asset. We also compare our approach with an alternative widespread application of the Maximum Entropy principle allowing to derive graph probability distributions and generating scenarios and we use it to propose a statistical test for a change in banks\u2019 vulnerability to systemic events."
            },
            {
                "arxivId": "1410.0249",
                "title": "On the convergence of the Fitness-Complexity algorithm",
                "abstract": null
            },
            {
                "arxivId": "1307.2104",
                "title": "Enhanced reconstruction of weighted networks from strengths and degrees",
                "abstract": "Network topology plays a key role in many phenomena, from the spreading of diseases to that of financial crises. Whenever the whole structure of a network is unknown, one must resort to reconstruction methods that identify the least biased ensemble of networks consistent with the partial information available. A challenging case, frequently encountered due to privacy issues in the analysis of interbank flows and Big Data, is when there is only local (node-specific) aggregate information available. For binary networks, the relevant ensemble is one where the degree (number of links) of each node is constrained to its observed value. However, for weighted networks the problem is much more complicated. While the na\u00efve approach prescribes to constrain the strengths (total link weights) of all nodes, recent counter-intuitive results suggest that in weighted networks the degrees are often more informative than the strengths. This implies that the reconstruction of weighted networks would be significantly enhanced by the specification of both strengths and degrees, a computationally hard and bias-prone procedure. Here we solve this problem by introducing an analytical and unbiased maximum-entropy method that works in the shortest possible time and does not require the explicit generation of reconstructed samples. We consider several real-world examples and show that, while the strengths alone give poor results, the additional knowledge of the degrees yields accurately reconstructed networks. Information-theoretic criteria rigorously confirm that the degree sequence, as soon as it is non-trivial, is irreducible to the strength sequence. Our results have strong implications for the analysis of motifs and communities and whenever the reconstructed ensemble is required as a null model to detect higher-order patterns."
            },
            {
                "arxivId": "0909.3890",
                "title": "The building blocks of economic complexity",
                "abstract": "For Adam Smith, wealth was related to the division of labor. As people and firms specialize in different activities, economic efficiency increases, suggesting that development is associated with an increase in the number of individual activities and with the complexity that emerges from the interactions between them. Here we develop a view of economic growth and development that gives a central role to the complexity of a country's economy by interpreting trade data as a bipartite network in which countries are connected to the products they export, and show that it is possible to quantify the complexity of a country's economy by characterizing the structure of this network. Furthermore, we show that the measures of complexity we derive are correlated with a country's level of income, and that deviations from this relationship are predictive of future growth. This suggests that countries tend to converge to the level of income dictated by the complexity of their productive structures, indicating that development efforts should focus on generating the conditions that would allow complexity to emerge to generate sustained growth and prosperity."
            },
            {
                "arxivId": "cond-mat/0609015",
                "title": "Maximum likelihood: extracting unbiased information from complex networks.",
                "abstract": "The choice of free parameters in network models is subjective, since it depends on what topological properties are being monitored. However, we show that the maximum likelihood (ML) principle indicates a unique, statistically rigorous parameter choice, associated with a well-defined topological feature. We then find that, if the ML condition is incompatible with the built-in parameter choice, network models turn out to be intrinsically ill defined or biased. To overcome this problem, we construct a class of safely unbiased models. We also propose an extension of these results that leads to the fascinating possibility to extract, only from topological data, the \"hidden variables\" underlying network organization, making them \"no longer hidden.\" We test our method on World Trade Web data, where we recover the empirical gross domestic product using only topological information."
            },
            {
                "arxivId": "cond-mat/0405566",
                "title": "Statistical mechanics of networks.",
                "abstract": "We study the family of network models derived by requiring the expected properties of a graph ensemble to match a given set of measurements of a real-world network, while maximizing the entropy of the ensemble. Models of this type play the same role in the study of networks as is played by the Boltzmann distribution in classical statistical mechanics; they offer the best prediction of network properties subject to the constraints imposed by a given set of observations. We give exact solutions of models within this class that incorporate arbitrary degree distributions and arbitrary but independent edge probabilities. We also discuss some more complex examples with correlated edges that can be solved approximately or exactly by adapting various familiar methods, including mean-field theory, perturbation theory, and saddle-point expansions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2308.07172",
        "category": "q-fin",
        "title": "Economic complexity and the sustainability transition: A review of data, methods, and literature",
        "abstract": "Economic Complexity (EC) methods have gained increasing popularity across fields and disciplines. In particular, the EC toolbox has proved particularly promising in the study of complex and interrelated phenomena, such as the transition towards a greener economy. Using the EC approach, scholars have been investigating the relationship between EC and sustainability, proposing to identify the distinguishing characteristics of green products and to assess the readiness of productive and technological structures for the sustainability transition. This article proposes to review and summarize the data, methods, and empirical literature that are relevant to the study of the sustainability transition from an EC perspective. We review three distinct but connected blocks of literature on EC and environmental sustainability. First, we survey the evidence linking measures of EC to indicators related to environmental sustainability. Second, we review articles that strive to assess the green competitiveness of productive systems. Third, we examine evidence on green technological development and its connection to non-green knowledge bases. Finally, we summarize the findings for each block and identify avenues for further research in this recent and growing body of empirical literature.",
        "references": [
            {
                "arxivId": "2304.12245",
                "title": "Inferring comparative advantage via entropy maximization",
                "abstract": "We revise the procedure proposed by Balassa to infer comparative advantage, which is a standard tool in Economics to analyze specialization (of countries, regions, etc). Balassa\u2019s approach compares a country\u2019s export of a given product with what would be expected from a benchmark based on the total volumes of countries and product flows. Based on results in the literature, we show that implementing Balassa\u2019s idea leads to conditions for estimating parameters conflicting with the information content of the model itself. Moreover, Balassa\u2019s approach does not implement any statistical validation. Hence, we propose an alternative procedure to overcome such a limitation, based upon the framework of entropy maximization and implementing a proper test of hypothesis: the \u2018key products\u2019 of a country are, now, the ones whose production is significantly larger than expected, under a null-model constraining the same amount of information defining Balassa\u2019s approach. What we found is that country diversification is always observed, regardless of the strictness of the validation procedure. Besides, the ranking of countries\u2019 fitnesses is only partially affected by the details of the validation scheme employed for the analysis while large differences are found to affect the rankings of product complexities. The routine for implementing the entropy-based filtering procedures employed here is freely available through the official Python Package Index PyPI."
            },
            {
                "arxivId": "2209.08382",
                "title": "Multidimensional economic complexity and inclusive green growth",
                "abstract": null
            },
            {
                "arxivId": "2206.00368",
                "title": "Capability accumulation patterns across economic, innovation, and knowledge-production activities",
                "abstract": null
            },
            {
                "arxivId": "2202.01804",
                "title": "The different structure of economic ecosystems at the scales of companies and countries",
                "abstract": "A key element to understand complex systems is the relationship between the spatial scale of investigation and the structure of the interrelation among its elements. When it comes to economic systems, it is now well-known that the country-product bipartite network exhibits a nested structure, which is the foundation of different algorithms that have been used to scientifically investigate countries\u2019 development and forecast national economic growth. Changing the subject from countries to companies, a significantly different scenario emerges. Through the analysis of a unique dataset of Italian firms\u2019 exports and a worldwide dataset comprising countries\u2019 exports, here we find that, while a globally nested structure is observed at the country level, a local, in-block nested structure emerges at the level of firms. This in-block nestedness is statistically significant with respect to suitable null models and the algorithmic partitions of products into blocks correspond well with the UN-COMTRADE product classification. These findings lay a solid foundation for developing a scientific approach based on the physics of complex systems to the analysis of companies, which has been lacking until now."
            },
            {
                "arxivId": "2103.06017",
                "title": "Relatedness in the era of machine learning",
                "abstract": null
            },
            {
                "arxivId": "1909.05604",
                "title": "The Emergence of Innovation Complexity at Different Geographical and Technological Scales",
                "abstract": "We define a novel quantitative strategy inspired by the ecological notion of nestedness to single out the scale at which innovation complexity emerges from the aggregation of specialized building blocks. Our analysis not only suggests that the innovation space can be interpreted as a natural system in which advantageous capabilities are selected by evolutionary pressure, but also that the emerging structure of capabilities is not independent of the scale of observation at which they are observed. Expanding on this insight allows us to understand whether the capabilities characterizing the innovation space at a given scale are compatible with a complex evolutionary dynamics or, rather, a set of essentially independent activities allowing to reduce the system at that scale to a set of disjoint non interacting sub-systems. This yields a measure of the innovation complexity of the system, i.e. of the degree of interdependence between the sets of capabilities underlying the system's building blocks."
            },
            {
                "arxivId": "1808.10428",
                "title": "The Role of Complex Analysis in Modelling Economic Growth",
                "abstract": "Development and growth are complex and tumultuous processes. Modern economic growth theories identify some key determinants of economic growth. However, the relative importance of the determinants remains unknown, and additional variables may help clarify the directions and dimensions of the interactions. The novel stream of literature on economic complexity goes beyond aggregate measures of productive inputs and considers instead a more granular and structural view of the productive possibilities of countries, i.e., their capabilities. Different endowments of capabilities are crucial ingredients in explaining differences in economic performances. In this paper we employ economic fitness, a measure of productive capabilities obtained through complex network techniques. Focusing on the combined roles of fitness and some more traditional drivers of growth\u2014GDP per capita, capital intensity, employment ratio, life expectancy, human capital and total factor productivity\u2014we build a bridge between economic growth theories and the economic complexity literature. Our findings show that fitness plays a crucial role in fostering economic growth and, when it is included in the analysis, can be either complementary to traditional drivers of growth or can completely overshadow them. Notably, for the most complex countries, which have the most diversified export baskets and the largest endowments of capabilities, fitness is complementary to the chosen growth determinants in enhancing economic growth. The empirical findings are in agreement with neoclassical and endogenous growth theories. By contrast, for countries with intermediate and low capability levels, fitness emerges as the key growth driver. This suggests that economic models should account for capabilities; in fact, describing the technological possibilities of countries solely in terms of their production functions may lead to a misinterpretation of the roles of factors."
            },
            {
                "arxivId": "1709.05272",
                "title": "Economic Complexity: \"Buttarla in caciara\" vs a constructive approach",
                "abstract": "This note is a contribution to the debate about the optimal algorithm for Economic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2] eventually agree that the ECI+ algorithm [1] consists just in a renaming of the Fitness algorithm we introduced in 2012, as we explicitly showed in [3]. However, they omit any comment on the fact that their extensive numerical tests claimed to demonstrate that the same algorithm works well if they name it ECI+, but not if its name is Fitness. They should realize that this eliminates any credibility to their numerical methods and therefore also to their new analysis, in which they consider many algorithms [2]. Since by their own admission the best algorithm is the Fitness one, their new claim became that the search for the best algorithm is pointless and all algorithms are alike. This is exactly the opposite of what they claimed a few days ago and it does not deserve much comments. After these clarifications we also present a constructive analysis of the status of Economic Complexity, its algorithms, its successes and its perspectives. For us the discussion closes here, we will not reply to further comments."
            },
            {
                "arxivId": "1708.03511",
                "title": "Technology networks: the autocatalytic origins of innovation",
                "abstract": "We analyse the autocatalytic structure of technological networks and evaluate its significance for the dynamics of innovation patenting. To this aim, we define a directed network of technological fields based on the International Patents Classification, in which a source node is connected to a receiver node via a link if patenting activity in the source field anticipates patents in the receiver field in the same region more frequently than we would expect at random. We show that the evolution of the technology network is compatible with the presence of a growing autocatalytic structure, i.e. a portion of the network in which technological fields mutually benefit from being connected to one another. We further show that technological fields in the core of the autocatalytic set display greater fitness, i.e. they tend to appear in a greater number of patents, thus suggesting the presence of positive spillovers as well as positive reinforcement. Finally, we observe that core shifts take place whereby different groups of technology fields alternate within the autocatalytic structure; this points to the importance of recombinant innovation taking place between close as well as distant fields of the hierarchical classification of technological fields."
            },
            {
                "arxivId": "1511.08622",
                "title": "Complex Economies Have a Lateral Escape from the Poverty Trap",
                "abstract": "We analyze the decisive role played by the complexity of economic systems at the onset of the industrialization process of countries over the past 50 years. Our analysis of the input growth dynamics, considering a further dimension through a recently introduced measure of economic complexity, reveals that more differentiated and more complex economies face a lower barrier (in terms of GDP per capita) when starting the transition towards industrialization. As a consequence, we can extend the classical concept of a one-dimensional poverty trap, by introducing a two-dimensional poverty trap: a country will start the industrialization process if it is rich enough (as in neo-classical economic theories), complex enough (using this new dimension and laterally escaping from the poverty trap), or a linear combination of the two. This naturally leads to the proposal of a Complex Index of Relative Development (CIRD) which shows, when analyzed as a function of the growth due to input, a shape of an upside down parabola similar to that expected from the standard economic theories when considering only the GDP per capita dimension."
            },
            {
                "arxivId": "1507.02099",
                "title": "A review of the literature on citation impact indicators",
                "abstract": null
            },
            {
                "arxivId": "1505.07907",
                "title": "Linking Economic Complexity, Institutions and Income Inequality",
                "abstract": null
            },
            {
                "arxivId": "1503.05098",
                "title": "Randomizing bipartite networks: the case of the World Trade Web",
                "abstract": null
            },
            {
                "arxivId": "1408.2138",
                "title": "How the Taxonomy of Products Drives the Economic Development of Countries",
                "abstract": "We introduce an algorithm able to reconstruct the relevant network structure on which the time evolution of country-product bipartite networks takes place. The significant links are obtained by selecting the largest values of the projected matrix. We first perform a number of tests of this filtering procedure on synthetic cases and a toy model. Then we analyze the bipartite network constituted by countries and exported products, using two databases for a total of almost 50 years. It is then possible to build a hierarchically directed network, in which the taxonomy of products emerges in a natural way. We study the influence of the structure of this taxonomy network on countries' development; in particular, guided by an example taken from the industrialization of South Korea, we link the structure of the taxonomy network to the empirical temporal connections between product activations, finding that the most relevant edges for countries' development are the ones suggested by our network. These results suggest paths in the product space which are easier to achieve, and so can drive countries' policies in the industrialization process."
            },
            {
                "arxivId": "1108.2590",
                "title": "A Network Analysis of Countries\u2019 Export Flows: Firm Grounds for the Building Blocks of the Economy",
                "abstract": "In this paper we analyze the bipartite network of countries and products from UN data on country production. We define the country-country and product-product projected networks and introduce a novel method of filtering information based on elements\u2019 similarity. As a result we find that country clustering reveals unexpected socio-geographic links among the most competing countries. On the same footings the products clustering can be efficiently used for a bottom-up classification of produced goods. Furthermore we mathematically reformulate the \u201creflections method\u201d introduced by Hidalgo and Hausmann as a fixpoint problem; such formulation highlights some conceptual weaknesses of the approach. To overcome such an issue, we introduce an alternative methodology (based on biased Markov chains) that allows to rank countries in a conceptually consistent way. Our analysis uncovers a strong non-linear interaction between the diversification of a country and the ubiquity of its products, thus suggesting the possible need of moving towards more efficient and direct non-linear fixpoint algorithms to rank countries and products in the global market."
            },
            {
                "arxivId": "1101.1707",
                "title": "The network structure of economic output",
                "abstract": null
            },
            {
                "arxivId": "0708.2090",
                "title": "The Product Space Conditions the Development of Nations",
                "abstract": "Economies grow by upgrading the products they produce and export. The technology, capital, institutions, and skills needed to make newer products are more easily adapted from some products than from others. Here, we study this network of relatedness between products, or \u201cproduct space,\u201d finding that more-sophisticated products are located in a densely connected core whereas less-sophisticated products occupy a less-connected periphery. Empirically, countries move through the product space by developing goods close to those they currently produce. Most countries can reach the core only by traversing empirically infrequent distances, which may help explain why poor countries have trouble developing more competitive exports and fail to converge to the income levels of rich countries."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2401.05549",
        "category": "q-fin",
        "title": "Boundary conditions at infinity for Black-Scholes equations",
        "abstract": "We propose numerical procedures for computing the prices of forward contracts, in which the underlying asset price is a Markovian local martingale. If the underlying process is a strict local martingale, multiple solutions exist for the corresponding Black-Scholes equations, and the derivative prices are characterized as the minimal solutions. Our prices are upper and lower bounds obtained using numerical methods on a finite grid under the respective boundary conditions. These bounds and the boundary values converge to the exact value as the underlying price approaches infinity. The proposed procedures are demonstrated through numerical tests.",
        "references": [
            {
                "arxivId": "2303.13956",
                "title": "Pitman's Theorem, Black-Scholes Equation, and Derivative Pricing for Fundraisers",
                "abstract": "We propose a financial market model that comprises a savings account and a stock, where the stock price process is modeled as a one-dimensional diffusion, wherein two types of agents exist: an ordinary investor and a fundraiser who buys or sells stocks as funding activities. Although the investor information is the natural filtration of the diffusion, the fundraiser possesses extra information regarding the funding, as well as additional cash flows as a result of the funding. This concept is modeled using Pitman's theorem for the three-dimensional Bessel process. Two contributions are presented: First, the prices of European options for the fundraiser are derived. Second, a numerical scheme is proposed for call option prices in a market with a bubble, where multiple solutions exist for the Black-Scholes equation and the derivative prices are characterized as the smallest nonnegative supersolution. More precisely, the call option price in such a market is approximated from below by the prices for the fundraiser. This scheme overcomes the difficulty that stems from the discrepancy that the payoff shows linear growth, whereas the price function shows strictly sublinear growth."
            },
            {
                "arxivId": "1303.5899",
                "title": "Distribution of the time to explosion for one-dimensional diffusions",
                "abstract": null
            },
            {
                "arxivId": "1202.6188",
                "title": "On the hedging of options on exploding exchange rates",
                "abstract": null
            },
            {
                "arxivId": "1108.4177",
                "title": "Strict local martingales and bubbles",
                "abstract": "This paper deals with asset price bubbles modeled by strict local martingales. With any strict local martingale, one can associate a new measure, which is studied in detail in the first part of the paper. In the second part, we determine the \u201cdefault term\u201d apparent in risk-neutral option prices if the underlying stock exhibits a bubble modeled by a strict local martingale. Results for certain path dependent options and last passage time formulas are given."
            },
            {
                "arxivId": "0908.1086",
                "title": "On the uniqueness of classical solutions of Cauchy problems",
                "abstract": "Given that the terminal condition is of at most linear growth, it is well known that a Cauchy problem admits a unique classical solution when the coefficient multiplying the second derivative (i.e., the volatility) is also a function of at most linear growth. In this note, we give a condition on the volatility that is necessary and sufficient for a Cauchy problem to admit a unique solution."
            },
            {
                "arxivId": "0808.3402",
                "title": "Call Option Prices Based on Bessel Processes",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.05541",
        "category": "q-fin",
        "title": "AI in ESG for Financial Institutions: An Industrial Survey",
        "abstract": "The burgeoning integration of Artificial Intelligence (AI) into Environmental, Social, and Governance (ESG) initiatives within the financial sector represents a paradigm shift towards more sus-tainable and equitable financial practices. This paper surveys the industrial landscape to delineate the necessity and impact of AI in bolstering ESG frameworks. With the advent of stringent regulatory requirements and heightened stakeholder awareness, financial institutions (FIs) are increasingly compelled to adopt ESG criteria. AI emerges as a pivotal tool in navigating the complex in-terplay of financial activities and sustainability goals. Our survey categorizes AI applications across three main pillars of ESG, illustrating how AI enhances analytical capabilities, risk assessment, customer engagement, reporting accuracy and more. Further, we delve into the critical con-siderations surrounding the use of data and the development of models, underscoring the importance of data quality, privacy, and model robustness. The paper also addresses the imperative of responsible and sustainable AI, emphasizing the ethical dimensions of AI deployment in ESG-related banking processes. Conclusively, our findings suggest that while AI offers transformative potential for ESG in banking, it also poses significant challenges that necessitate careful consideration. The final part of the paper synthesizes the survey's insights, proposing a forward-looking stance on the adoption of AI in ESG practices. We conclude with recommendations with a reference architecture for future research and development, advocating for a balanced approach that leverages AI's strengths while mitigating its risks within the ESG domain.",
        "references": [
            {
                "arxivId": "2401.11641",
                "title": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights",
                "abstract": "In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry."
            },
            {
                "arxivId": "2311.10723",
                "title": "Large Language Models in Finance: A Survey",
                "abstract": "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.05830",
        "category": "q-fin",
        "title": "The impact of social status on the formation of collaborative ties and effort provision: An experimental study",
        "abstract": "We study whether competition for social status induces higher effort provision and efficiency when individuals collaborate with their network neighbors. We consider a laboratory experiment in which individuals choose a costly collaborative effort and their network neighbors. They benefit from their neighbors' effort and effort choices of direct neighbors are strategic complements. We introduce two types of social status in a 2x2 factorial design: 1) individuals receive monetary benefits for incoming links representing popularity; 2) they receive feedback on their relative payoff ranking within the group. We find that link benefits induce higher effort provision and strengthen the collaborative ties relative to the Baseline treatment without social status. In contrast, the ranking information induces lower effort as individuals start competing for higher ranking. Overall, we find that social status has no significant impact on the number of links in the network and the efficiency of collaboration in the group.",
        "references": [
            {
                "arxivId": "0911.1438",
                "title": "International collaboration in science and the formation of a core group",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.05913",
        "category": "q-fin",
        "title": "Network formation and efficiency in linear-quadratic games: An experimental study",
        "abstract": "We experimentally study effort provision and network formation in the linear-quadratic game characterized by positive externality and complementarity of effort choices among network neighbors. We compare experimental outcomes to the equilibrium and efficient allocations and study the impact of group size and linking costs. We find that individuals overprovide effort relative to the equilibrium level on the network they form. However, their payoffs are lower than the equilibrium payoffs because they create fewer links than it is optimal which limits the beneficial spillovers of effort provision. Reducing the linking costs does not significantly increase the connectedness of the network and the welfare loss is higher in larger groups. Individuals connect to the highest effort providers in the group and ignore links to relative low effort providers, even if those links would be beneficial to form. This effect explains the lack of links in the network.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06035",
        "category": "q-fin",
        "title": "Capital Structure Adjustment Speed and Expected Returns: Examination of Information Asymmetry as a Moderating Role",
        "abstract": "Shareholders' expectations of stock returns and fluctuations are constantly changing due to restrictions in financial status and undesirable capital structure, which constrain managers to limit the changes in price trends in order to cover the risk instigated and infused by the unfavorable situation. The present research examines the moderating impact of information asymmetry on the relationship between capital structure adjustment and expected returns. The data from 120 companies approved in the Tehran Stock Exchange were extracted, and a hybrid data regression model was used to test the research hypotheses. Findings indicate that the capital structure adjustment speed correlates with the expected returns. Moreover, the information asymmetry positively affects the relationship between capital structure adjustment speed and expected returns.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06150",
        "category": "q-fin",
        "title": "Algorithmic Collusion and Price Discrimination: The Over-Usage of Data",
        "abstract": "As firms' pricing strategies increasingly rely on algorithms, two concerns have received much attention: algorithmic tacit collusion and price discrimination. This paper investigates the interaction between these two issues through simulations. In each period, a new buyer arrives with independently and identically distributed willingness to pay (WTP), and each firm, observing private signals about WTP, adopts Q-learning algorithms to set prices. We document two novel mechanisms that lead to collusive outcomes. Under asymmetric information, the algorithm with information advantage adopts a Bait-and-Restrained-Exploit strategy, surrendering profits on some signals by setting higher prices, while exploiting limited profits on the remaining signals by setting much lower prices. Under a symmetric information structure, competition on some signals facilitates convergence to supra-competitive prices on the remaining signals. Algorithms tend to collude more on signals with higher expected WTP. Both uncertainty and the lack of correlated signals exacerbate the degree of collusion, thereby reducing both consumer surplus and social welfare. A key implication is that the over-usage of data, both payoff-relevant and non-relevant, by AIs in competitive contexts will reduce the degree of collusion and consequently lead to a decline in industry profits.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06188",
        "category": "q-fin",
        "title": "On Geometrically Convex Risk Measures",
        "abstract": "Geometrically convex functions constitute an interesting class of functions obtained by replacing the arithmetic mean with the geometric mean in the definition of convexity. As recently suggested, geometric convexity may be a sensible property for financial risk measures ([7,13,4]). We introduce a notion of GG-convex conjugate, parallel to the classical notion of convex conjugate introduced by Fenchel, and we discuss its properties. We show how GG-convex conjugation can be axiomatized in the spirit of the notion of general duality transforms introduced in [2,3]. We then move to the study of GG-convex risk measures, which are defined as GG-convex functionals defined on suitable spaces of random variables. We derive a general dual representation that extends analogous expressions presented in [4] under the additional assumptions of monotonicity and positive homogeneity. As a prominent example, we study the family of Orlicz risk measures. Finally, we introduce multiplicative versions of the convex and of the increasing convex order and discuss related consistency properties of law-invariant GG-convex risk measures.",
        "references": [
            {
                "arxivId": "2302.13070",
                "title": "Elicitability of Return Risk Measures",
                "abstract": "Informally, a risk measure is said to be elicitable if there exists a suitable scoring function such that minimizing its expected value recovers the risk measure. In this paper, we analyze the elicitability properties of the class of return risk measures (i.e., normalized, monotone and positively homogeneous risk measures). First, we provide dual representation results for convex and geometrically convex return risk measures. Next, we establish new axiomatic characterizations of Orlicz premia (i.e., Luxemburg norms). More specifically, we prove, under different sets of conditions, that Orlicz premia naturally arise as the only elicitable return risk measures. Finally, we provide a general family of strictly consistent scoring functions for Orlicz premia, a myriad of specific examples and a mixture representation suitable for constructing Murphy diagrams."
            },
            {
                "arxivId": "2208.07694",
                "title": "Quasi-Logconvex Measures of Risk",
                "abstract": "This paper introduces and fully characterizes the novel class of quasi-logconvex measures of risk, to stand on equal footing with the rich class of quasi-convex measures of risk. Quasi-logconvex risk measures naturally generalize logconvex return risk measures, just like quasi-convex risk measures generalize convex monetary risk measures. We establish their dual representation and analyze their taxonomy in a few (sub)classi\ufb01cation results. Furthermore, we characterize quasi-logconvex risk measures in terms of properties of families of acceptance sets and provide their law-invariant representation. Examples and applications to portfolio choice and capital allocation are also discussed."
            },
            {
                "arxivId": "1812.04074",
                "title": "Disciplined geometric programming",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06303",
        "category": "q-fin",
        "title": "A Unifying Approach for the Pricing of Debt Securities",
        "abstract": "We propose a unifying framework for the pricing of debt securities under general time-inhomogeneous short-rate diffusion processes. The pricing of bonds, bond options, callable/putable bonds, and convertible bonds (CBs) are covered. Using continuous-time Markov chain (CTMC) approximation, we obtain closed-form matrix expressions to approximate the price of bonds and bond options under general one-dimensional short-rate processes. A simple and efficient algorithm is also developed to price callable/putable debts. The availability of a closed-form expression for the price of zero-coupon bonds allows for the perfect fit of the approximated model to the current market term structure of interest rates, regardless of the complexity of the underlying diffusion process selected. We further consider the pricing of CBs under general bi-dimensional time-inhomogeneous diffusion processes to model equity and short-rate dynamics. Credit risk is also incorporated into the model using the approach of Tsiveriotis and Fernandes (1998). Based on a two-layer CTMC method, an efficient algorithm is developed to approximate the price of convertible bonds. When conversion is only allowed at maturity, a closed-form matrix expression is obtained. Numerical experiments show the accuracy and efficiency of the method across a wide range of model parameters and short-rate models.",
        "references": [
            {
                "arxivId": "2207.14793",
                "title": "Analysis of VIX-linked fee incentives in variable annuities via continuous-time Markov chain approximation",
                "abstract": "We consider the pricing of variable annuities (VAs) with general fee structures under a class of stochastic volatility models which includes the Heston, Hull-White, Scott, \u03b1-Hypergeometric, 3/2, and 4/2 models. In particular, we analyze the impact of different VIX-linked fee structures on the optimal surrender strategy of a VA contract with guaranteed minimum maturity benefit (GMMB). Under the assumption that the VA contract can be surrendered before maturity, the pricing of a VA contract corresponds to an optimal stopping problem with an unbounded, time-dependent, and discontinuous payoff function. We develop efficient algorithms for the pricing of VA contracts using a two-layer continuous-time Markov chain approximation for the fund value process. When the contract is kept until maturity and under a general fee structure, we show that the value of the contract can be approximated by a closed-form matrix expression. We also provide a quick and simple way to determine the value of early surrenders via a recursive algorithm and give an easy procedure to approximate the optimal surrender surface. We show numerically that the optimal surrender strategy is more robust to changes in the volatility of the account value when the fee is linked to the VIX index."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06368",
        "category": "q-fin",
        "title": "A New Testing Method for Justification Bias Using High-Frequency Data of Health and Employment",
        "abstract": "Justification bias, wherein retirees may report poorer health to rationalize their retirement, poses a major concern to the widely-used measure of self-assessed health in retirement studies. This paper introduces a novel method for testing the presence of this bias in the spirit of regression discontinuity. The underlying idea is that any sudden shift in self-assessed health immediately following retirement is more likely attributable to the bias. Our strategy is facilitated by a unique high-frequency data that offers monthly, in contrast to the typical biennial, information on employment, self-assessed health, and objective health conditions. Across a wider post-retirement time frame, we observe a decline in self-assessed health, potentially stemming from both justification bias and changes in actual health. However, this adverse effect diminishes with shorter intervals, indicating no evidence of such bias. Our method also validates a widely-used indirect testing approach.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06482",
        "category": "q-fin",
        "title": "Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning",
        "abstract": "User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higher-order structures from multi-view motif-based graphs for financial default prediction. Specifically, to solve the problem of weak connectivity in motif-based graphs, we design the motif-based gating mechanism. It utilizes the information learned from the original graph with good connectivity to strengthen the learning of the higher-order structure. And considering that the motif patterns of different samples are highly unbalanced, we propose a curriculum learning mechanism on the whole learning process to more focus on the samples with uncommon motif distributions. Extensive experiments on one public dataset and two industrial datasets all demonstrate the effectiveness of our proposed method.",
        "references": [
            {
                "arxivId": "2101.00797",
                "title": "Beyond Low-frequency Information in Graph Convolutional Networks",
                "abstract": "Graph neural networks (GNNs) have been proven to be effective in various network-related tasks. Most existing GNNs usually exploit the low-frequency signals of node features, which gives rise to one fundamental question: is the low-frequency information all we need in the real world applications? In this paper, we first present an experimental investigation assessing the roles of low-frequency and high-frequency signals, where the results clearly show that exploring low-frequency signal only is distant from learning an effective node representation in different scenarios. How can we adaptively learn more information beyond low-frequency information in GNNs? A well-informed answer can help GNNs enhance the adaptability. We tackle this challenge and propose a novel Frequency Adaptation Graph Convolutional Networks (FAGCN) with a self-gating mechanism, which can adaptively integrate different signals in the process of message passing. For a deeper understanding, we theoretically analyze the roles of low-frequency signals and high-frequency signals on learning node representations, which further explains why FAGCN can perform well on different types of networks. Extensive experiments on six real-world networks validate that FAGCN not only alleviates the over-smoothing problem, but also has advantages over the state-of-the-arts."
            },
            {
                "arxivId": "2003.01171",
                "title": "A Semi-Supervised Graph Attentive Network for Financial Fraud Detection",
                "abstract": "With the rapid growth of financial services, fraud detection has been a very important problem to guarantee a healthy environment for both users and providers. Conventional solutions for fraud detection mainly use some rule-based methods or distract some features manually to perform prediction. However, in financial services, users have rich interactions and they themselves always show multifaceted information. These data form a large multiview network, which is not fully exploited by conventional methods. Additionally, among the network, only very few of the users are labelled, which also poses a great challenge for only utilizing labeled data to achieve a satisfied performance on fraud detection. To address the problem, we expand the labeled data through their social relations to get the unlabeled data and propose a semi-supervised attentive graph neural network, named SemiGNN to utilize the multi-view labeled and unlabeled data for fraud detection. Moreover, we propose a hierarchical attention mechanism to better correlate different neighbors and different views. Simultaneously, the attention mechanism can make the model interpretable and tell what are the important factors for the fraud and why the users are predicted as fraud. Experimentally, we conduct the prediction task on the users of Alipay, one of the largest third-party online and offline cashless payment platform serving more than 4 hundreds of million users in China. By utilizing the social relations and the user attributes, our method can achieve a better accuracy compared with the state-of-the-art methods on two tasks. Moreover, the interpretable results also give interesting intuitions regarding the tasks."
            },
            {
                "arxivId": "1908.08227",
                "title": "motif2vec: Motif Aware Node Representation Learning for Heterogeneous Networks",
                "abstract": "Recent years have witnessed a surge of interest in machine learning on graphs and networks with applications ranging from IoT traffic management to social network recommendations. Supervised machine learning tasks in networks such as node classification and link prediction require us to perform feature engineering that is known and agreed to be the key to success in applied machine learning. Research efforts dedicated to representation learning, especially representation learning using deep learning, has shown us ways to automatically learn relevant features from vast amounts of potentially noisy, raw data. However, most of the methods are inadequate to handle heterogeneous information networks which pretty much represents most real world data today. The methods cannot preserve the structure and semantic of multiple types of nodes and links well enough, capture higher-order heterogeneous connectivity patterns, and ensure coverage of nodes for which representations are generated. In this paper, we propose a novel efficient algorithm, motif2vec that learns node representations or embeddings for heterogeneous networks. Specifically, we leverage higher-order, recurring, and statistically significant network connectivity patterns in the form of motifs to transform the original graph to motif graph(s), conduct biased random walk to efficiently explore higher order neighborhoods, and then employ heterogeneous skip-gram model to generate the embeddings. We evaluate the proposed algorithm on multiple real-world networks from diverse domains and against existing state-of-the-art methods on multi-class node classification and link prediction tasks, and demonstrate its consistent superiority over prior work."
            },
            {
                "arxivId": "1903.07293",
                "title": "Heterogeneous Graph Attention Network",
                "abstract": "Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis."
            },
            {
                "arxivId": "1901.11213",
                "title": "Multi-GCN: Graph Convolutional Networks for Multi-View Networks, with Applications to Global Poverty",
                "abstract": "With the rapid expansion of mobile phone networks in developing countries, large-scale graph machine learning has gained sudden relevance in the study of global poverty. Recent applications range from humanitarian response and poverty estimation to urban planning and epidemic containment. Yet the vast majority of computational tools and algorithms used in these applications do not account for the multi-view nature of social networks: people are related in myriad ways, but most graph learning models treat relations as binary. In this paper, we develop a graph-based convolutional network for learning on multi-view networks. We show that this method outperforms state-of-the-art semi-supervised learning algorithms on three different prediction tasks using mobile phone datasets from three different developing countries. We also show that, while designed specifically for use in poverty research, the algorithm also outperforms existing benchmarks on a broader set of learning tasks on multi-view networks, including node labelling in citation networks."
            },
            {
                "arxivId": "1812.04202",
                "title": "Deep Learning on Graphs: A Survey",
                "abstract": "Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions."
            },
            {
                "arxivId": "2002.12307",
                "title": "Heterogeneous Graph Neural Networks for Malicious Account Detection",
                "abstract": "We present, GEM, the first heterogeneous graph neural network approach for detecting malicious accounts at Alipay, one of the world's leading mobile cashless payment platform. Our approach, inspired from a connected subgraph approach, adaptively learns discriminative embeddings from heterogeneous account-device graphs based on two fundamental weaknesses of attackers, i.e. device aggregation and activity aggregation. For the heterogeneous graph consists of various types of nodes, we propose an attention mechanism to learn the importance of different types of nodes, while using the sum operator for modeling the aggregation patterns of nodes in each type. Experiments show that our approaches consistently perform promising results compared with competitive methods over time."
            },
            {
                "arxivId": "1810.02959",
                "title": "Higher-order Spectral Clustering for Heterogeneous Graphs",
                "abstract": "Higher-order connectivity patterns such as small induced sub-graphs called graphlets (network motifs) are vital to understand the important components (modules/functional units) governing the configuration and behavior of complex networks. Existing work in higher-order clustering has focused on simple homogeneous graphs with a single node/edge type. However, heterogeneous graphs consisting of nodes and edges of different types are seemingly ubiquitous in the real-world. In this work, we introduce the notion of typed-graphlet that explicitly captures the rich (typed) connectivity patterns in heterogeneous networks. Using typed-graphlets as a basis, we develop a general principled framework for higher-order clustering in heterogeneous networks. The framework provides mathematical guarantees on the optimality of the higher-order clustering obtained. The experiments demonstrate the effectiveness of the framework quantitatively for three important applications including (i) clustering, (ii) link prediction, and (iii) graph compression. In particular, the approach achieves a mean improvement of 43x over all methods and graphs for clustering while achieving a 18.7% and 20.8% improvement for link prediction and graph compression, respectively."
            },
            {
                "arxivId": "1805.04234",
                "title": "Distributed Deep Forest and its Application to Automatic Detection of Cash-Out Fraud",
                "abstract": "Internet companies are facing the need for handling large-scale machine learning applications on a daily basis and distributed implementation of machine learning algorithms which can handle extra-large-scale tasks with great performance is widely needed. Deep forest is a recently proposed deep learning framework which uses tree ensembles as its building blocks and it has achieved highly competitive results on various domains of tasks. However, it has not been tested on extremely large-scale tasks. In this work, based on our parameter server system, we developed the distributed version of deep forest. To meet the need for real-world tasks, many improvements are introduced to the original deep forest model, including MART (Multiple Additive Regression Tree) as base learners for efficiency and effectiveness consideration, the cost-based method for handling prevalent class-imbalanced data, MART based feature selection for high dimension data, and different evaluation metrics for automatically determining the cascade level. We tested the deep forest model on an extra-large-scale task, i.e., automatic detection of cash-out fraud, with more than 100 million training samples. Experimental results showed that the deep forest model has the best performance according to the evaluation metrics from different perspectives even with very little effort for parameter tuning. This model can block fraud transactions in a large amount of money each day. Even compared with the best-deployed model, the deep forest model can additionally bring a significant decrease in economic loss each day."
            },
            {
                "arxivId": "1801.10247",
                "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
                "abstract": "The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate."
            },
            {
                "arxivId": "1710.10903",
                "title": "Graph Attention Networks",
                "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
            },
            {
                "arxivId": "1709.04875",
                "title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "abstract": "Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets."
            },
            {
                "arxivId": "1706.02216",
                "title": "Inductive Representation Learning on Large Graphs",
                "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
            },
            {
                "arxivId": "1702.07390",
                "title": "Detecting Strong Ties Using Network Motifs",
                "abstract": "Detecting strong ties among users in social and information networks is a fundamental operation that can improve performance on a multitude of personalization and ranking tasks. There are a variety of ways a tie can be deemed ``strong'', and in this work we use a data-driven (or supervised) approach by assuming that we are provided a sample set of edges labeled as strong ties in the network. Such labeled edges are often readily obtained from the social network as users often participate in multiple overlapping networks via features such as following and messaging. These networks may vary greatly in size, density and the information they carry --- for instance, a heavily-used dense network (such as the network of followers) commonly overlaps with a secondary sparser network composed of strong ties (such as a network of email or phone contacts). This setting leads to a natural strong tie detection task: given a small set of labeled strong tie edges, how well can one detect unlabeled strong ties in the remainder of the network? This task becomes particularly daunting for the Twitter network due to scant availability of pairwise relationship attribute data, and sparsity of strong tie networks such as phone contacts. Given these challenges, a natural approach is to instead use structural network features for the task, produced by combining the strong and ``weak'' edges. In this work, we demonstrate via experiments on Twitter data that using only such structural network features is sufficient for detecting strong ties with high precision. These structural network features are obtained from the presence and frequency of small network motifs on combined strong and weak ties. We observe that using motifs larger than triads alleviate sparsity problems that arise for smaller motifs, both due to increased combinatorial possibilities as well as benefiting strongly from searching beyond the ego network. Empirically, we observe that not all motifs are equally useful, and need to be carefully constructed from the combined edges in order to be effective for strong tie detection. Finally, we reinforce our experimental findings with providing theoretical justification that suggests why incorporating these larger sized motifs as features could lead to increased performance in planted graph models."
            },
            {
                "arxivId": "1609.02907",
                "title": "Semi-Supervised Classification with Graph Convolutional Networks",
                "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
            },
            {
                "arxivId": "1612.08447",
                "title": "Higher-order organization of complex networks",
                "abstract": "Resolving a network of hubs Graphs are a pervasive tool for modeling and analyzing network data throughout the sciences. Benson et al. developed an algorithmic framework for studying how complex networks are organized by higher-order connectivity patterns (see the Perspective by Pr\u017eulj and Malod-Dognin). Motifs in transportation networks reveal hubs and geographical elements not readily achievable by other methods. A motif previously suggested as important for neuronal networks is part of a \u201crich club\u201d of subnetworks. Science, this issue p. 163; see also p. 123 A mathematical framework for clustering reveals organizational features of a variety of networks. Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networks\u2014at the level of small network subgraphs\u2014remains largely unknown. Here, we develop a generalized framework for clustering networks on the basis of higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks, including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns."
            },
            {
                "arxivId": "1607.00653",
                "title": "node2vec: Scalable Feature Learning for Networks",
                "abstract": "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks."
            },
            {
                "arxivId": "0710.0059",
                "title": "Motif-based communities in complex networks",
                "abstract": "Community definitions usually focus on edges, inside and between the communities. However, the high density of edges within a community determines correlations between nodes going beyond nearest neighbors, and which are indicated by the presence of motifs. We show how motifs can be used to define general classes of nodes, including communities, by extending the mathematical expression of Newman\u2013Girvan modularity. We construct then a general framework and apply it to some synthetic and real networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-12.json",
        "arxivId": "2403.06779",
        "category": "q-fin",
        "title": "From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing",
        "abstract": "This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing. It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets. It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization. These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images. In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models. This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance.",
        "references": [
            {
                "arxivId": "2309.00073",
                "title": "Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction",
                "abstract": "Multi-step stock price prediction over a long-term horizon is crucial for forecasting its volatility, allowing financial institutions to price and hedge derivatives, and banks to quantify the risk in their trading books. Additionally, most financial regulators also require a liquidity horizon of several days for institutional investors to exit their risky assets, in order to not materially affect market prices. However, the task of multi-step stock price prediction is challenging, given the highly stochastic nature of stock data. Current solutions to tackle this problem are mostly designed for single-step, classification-based predictions, and are limited to low representation expressiveness. The problem also gets progressively harder with the introduction of the target price sequence, which also contains stochastic noise and reduces generalizability at test-time. To tackle these issues, we combine a deep hierarchical variational-autoencoder (VAE) and diffusion probabilistic techniques to do seq2seq stock prediction through a stochastic generative process. The hierarchical VAE allows us to learn the complex and low-level latent variables for stock prediction, while the diffusion probabilistic model trains the predictor to handle stock price stochasticity by progressively adding random noise to the stock data. To deal with the additional stochasticity in the target price sequence, we also augment the target series with noise via a coupled diffusion process. We then perform a denoising process to \"clean\" the prediction outputs that were trained on the stochastic target sequence data, which increases the generalizability of the model at test-time. Our Diffusion-VAE (D-Va) model is shown to outperform state-of-the-art solutions in terms of its prediction accuracy and variance. Through an ablation study, we also show how each of the components introduced helps to improve overall prediction accuracy by reducing the data noise. Most importantly, the multi-step outputs can also allow us to form a stock portfolio over the prediction length. We demonstrate the effectiveness of our model outputs in the portfolio investment task through the Sharpe ratio metric and highlight the importance of dealing with different types of prediction uncertainties. Our code can be accessed through https://github.com/koa-fin/dva."
            },
            {
                "arxivId": "2211.03107",
                "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning",
                "abstract": "Finance is a particularly difficult playground for deep reinforcement learning. However, establishing high-quality market environments and benchmarks for financial reinforcement learning is challenging due to three major factors, namely, low signal-to-noise ratio of financial data, survivorship bias of historical data, and model overfitting in the backtesting stage. In this paper, we present an openly accessible FinRL-Meta library that has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we will provide hundreds of market environments through an automatic pipeline that collects dynamic datasets from real-world markets and processes them into gym-style market environments. Second, we reproduce popular papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, FinRL-Meta provides tens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. FinRL-Meta is available at: https://github.com/AI4Finance-Foundation/FinRL-Meta"
            },
            {
                "arxivId": "2210.01774",
                "title": "MetaTrader: An Reinforcement Learning Approach Integrating Diverse Policies for Portfolio Optimization",
                "abstract": "Portfolio management is a fundamental problem in finance. It involves periodic reallocations of assets to maximize the expected returns within an appropriate level of risk exposure. Deep reinforcement learning (RL) has been considered a promising approach to solving this problem owing to its strong capability in sequential decision making. However, due to the non-stationary nature of financial markets, applying RL techniques to portfolio optimization remains a challenging problem. Extracting trading knowledge from various expert strategies could be helpful for agents to accommodate the changing markets. In this paper, we propose MetaTrader, a novel two-stage RL-based approach for portfolio management, which learns to integrate diverse trading policies to adapt to various market conditions. In the first stage, MetaTrader incorporates an imitation learning objective into the reinforcement learning framework. Through imitating different expert demonstrations, MetaTrader acquires a set of trading policies with great diversity. In the second stage, MetaTrader learns a meta-policy to recognize the market conditions and decide on the most proper learned policy to follow. We evaluate the proposed approach on three real-world index datasets and compare it to state-of-the-art baselines. The empirical results demonstrate that MetaTrader significantly outperforms those baselines in balancing profits and risks. Furthermore, thorough ablation studies validate the effectiveness of the components in the proposed approach."
            },
            {
                "arxivId": "2106.12950",
                "title": "Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport",
                "abstract": "Successful quantitative investment usually relies on precise predictions of the future movement of the stock price. Recently, machine learning based solutions have shown their capacity to give more accurate stock prediction and become indispensable components in modern quantitative investment systems. However, the i.i.d. assumption behind existing methods is inconsistent with the existence of diverse trading patterns in the stock market, which inevitably limits their ability to achieve better stock prediction performance. In this paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to empower existing stock prediction models with the ability to model multiple stock trading patterns. Essentially, TRA is a lightweight module that consists of a set of independent predictors for learning multiple patterns as well as a router to dispatch samples to different predictors. Nevertheless, the lack of explicit pattern identifiers makes it quite challenging to train an effective TRA-based model. To tackle this challenge, we further design a learning algorithm based on Optimal Transport (OT) to obtain the optimal sample to predictor assignment and effectively optimize the router with such assignment through an auxiliary loss term. Experiments on the real-world stock ranking task show that compared to the state-of-the-art baselines, e.g., Attention LSTM and Transformer, the proposed method can improve information coefficient (IC) from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used in this work are publicly available2: https://github.com/microsoft/qlib."
            },
            {
                "arxivId": "2102.12061",
                "title": "Deep video prediction for time series forecasting",
                "abstract": "Time series forecasting is essential for decision making in many domains. In this work, we address the challenge of predicting prices evolution among multiple potentially interacting financial assets. A solution to this problem has obvious importance for governments, banks, and investors. Statistical methods such as Auto Regressive Integrated Moving Average (ARIMA) are widely applied to these problems. In this paper, we propose to approach economic time series forecasting of multiple financial assets in a novel way via video prediction. Given past prices of multiple potentially interacting financial assets, we aim to predict the prices evolution in the future. Instead of treating the snapshot of prices at each time point as a vector, we spatially layout these prices in 2D as an image similar to market change visualization, and we can harness the power of CNNs in learning a latent representation for these financial assets. Thus, the history of these prices becomes a sequence of images, and our goal becomes predicting future images. We build on advances from computer vision for video prediction. Our experiments involve the prediction task of the price evolution of nine financial assets traded in U.S. stock markets. The proposed method outperforms baselines including ARIMA, Prophet and variations of the proposed method, demonstrating the benefits of harnessing the power of CNNs in the problem of economic time series forecasting."
            },
            {
                "arxivId": "1908.07999",
                "title": "HATS: A Hierarchical Graph Attention Network for Stock Movement Prediction",
                "abstract": "Many researchers both in academia and industry have long been interested in the stock market. Numerous approaches were developed to accurately predict future trends in stock prices. Recently, there has been a growing interest in utilizing graph-structured data in computer science research communities. Methods that use relational data for stock market prediction have been recently proposed, but they are still in their infancy. First, the quality of collected information from different types of relations can vary considerably. No existing work has focused on the effect of using different types of relations on stock market prediction or finding an effective way to selectively aggregate information on different relation types. Furthermore, existing works have focused on only individual stock prediction which is similar to the node classification task. To address this, we propose a hierarchical attention network for stock prediction (HATS) which uses relational data for stock market prediction. Our HATS method selectively aggregates information on different relation types and adds the information to the representations of each company. Specifically, node representations are initialized with features extracted from a feature extraction module. HATS is used as a relational modeling module with initialized node representations. Then, node representations with the added information are fed into a task-specific layer. Our method is used for predicting not only individual stock prices but also market index movements, which is similar to the graph classification task. The experimental results show that performance can change depending on the relational data used. HATS which can automatically select information outperformed all the existing methods."
            },
            {
                "arxivId": "1908.02646",
                "title": "AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks",
                "abstract": "Recent years have witnessed the successful marriage of finance innovations and AI techniques in various finance applications including quantitative trading (QT). Despite great research efforts devoted to leveraging deep learning (DL) methods for building better QT strategies, existing studies still face serious challenges especially from the side of finance, such as the balance of risk and return, the resistance to extreme loss, and the interpretability of strategies, which limit the application of DL-based strategies in real-life financial markets. In this work, we propose AlphaStock, a novel reinforcement learning (RL) based investment strategy enhanced by interpretable deep attention networks, to address the above challenges. Our main contributions are summarized as follows: i) We integrate deep attention networks with a Sharpe ratio-oriented reinforcement learning framework to achieve a risk-return balanced investment strategy; ii) We suggest modeling interrelationships among assets to avoid selection bias and develop a cross-asset attention mechanism; iii) To our best knowledge, this work is among the first to offer an interpretable investment strategy using deep reinforcement learning models. The experiments on long-periodic U.S. and Chinese markets demonstrate the effectiveness and robustness of AlphaStock over diverse market states. It turns out that AlphaStock tends to select the stocks as winners with high long-term growth, low volatility, high intrinsic value, and being undervalued recently."
            },
            {
                "arxivId": "1907.10046",
                "title": "Trading via image classification",
                "abstract": "The art of systematic financial trading evolved with an array of approaches, ranging from simple strategies to complex algorithms, all relying primarily on aspects of time-series analysis (e.g., Murphy, 1999; De Prado, 2018; Tsay, 2005). After visiting the trading floor of a leading financial institution, we noticed that traders always execute their trade orders while observing images of financial time-series on their screens. In this work, we build upon image recognition's success (e.g., Krizhevsky et al., 2012; Szegedy et al., 2015; Zeiler and Fergus, 2014; Wang et al., 2017; Koch et al., 2015; LeCun et al., 2015) and examine the value of transforming the traditional time-series analysis to that of image classification. We create a large sample of financial time-series images encoded as candlestick (Box and Whisker) charts and label the samples following three algebraically-defined binary trade strategies (Murphy, 1999). Using the images, we train over a dozen machine-learning classification models and find that the algorithms efficiently recover the complicated, multiscale label-generating rules when the data is visually represented. We suggest that the transformation of continuous numeric time-series classification problem to a vision problem is useful for recovering signals typical of technical analysis."
            },
            {
                "arxivId": "1905.10437",
                "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting",
                "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy."
            },
            {
                "arxivId": "1905.09474",
                "title": "Machine learning for pricing American options in high-dimensional Markovian and non-Markovian models",
                "abstract": "In this paper we propose two efficient techniques which allow one to compute the price of American basket options. In particular, we consider a basket of assets that follow a multi-dimensional Black\u2013Scholes dynamics. The proposed techniques, called GPR Tree (GRP-Tree) and GPR Exact Integration (GPR-EI), are both based on Machine Learning, exploited together with binomial trees or with a closed form formula for integration. Moreover, these two methods solve the backward dynamic programing problem considering a Bermudan approximation of the American option. On the exercise dates, the value of the option is first computed as the maximum between the exercise value and the continuation value and then approximated by means of Gaussian Process Regression. The two methods mainly differ in the approach used to compute the continuation value: a single step of the binomial tree or integration according to the probability density of the process. Numerical results show that these two methods are accurate and reliable in handling American options on very large baskets of assets. Moreover we also consider the rough Bergomi model, which provides stochastic volatility with memory. Despite that this model is only bidimensional, the whole history of the process impacts on the price, and how to handle all this information is not obvious at all. To this aim, we present how to adapt the GPR-Tree and GPR-EI methods and we focus on pricing American options in this non-Markovian framework."
            },
            {
                "arxivId": "1904.00745",
                "title": "Deep Learning in Asset Pricing",
                "abstract": "We use deep neural networks to estimate an asset pricing model for individual stock returns that takes advantage of the vast amount of conditioning information, keeps a fully flexible form, and accounts for time variation. The key innovations are to use the fundamental no-arbitrage condition as criterion function to construct the most informative test assets with an adversarial approach and to extract the states of the economy from many macroeconomic time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in terms of Sharpe ratio, explained variation, and pricing errors and identifies the key factors that drive asset prices. This paper was accepted by Agostino Capponi, finance. Supplemental Material: The online appendix and data are available at https://doi.org/10.1287/mnsc.2023.4695 ."
            },
            {
                "arxivId": "1706.10059",
                "title": "A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem",
                "abstract": "Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2207.02379",
        "category": "q-fin",
        "title": "Some Tradeoffs of Competition in Grant Contests",
        "abstract": "When funding public goods, resources are often allocated via mechanisms that resemble contests, especially in the case of research grants. A common critique of these contests is that they induce ``too much'' effort from participants. This need not be true if the effort in the contest is itself directed towards the public good. This papers analyzes survey data on scientists' time use and finds that scientists allocate their time in a way that is consistent with fundraising effort (e.g., grant writing) having inherent scientific value -- scientists who spend more time fundraising do not spent significantly less time on research even after conditioning on confounding factors. Theoretical models of contests are used to show that the presence of such a positive effort externality, where scientists generate social value when pursuing grants, changes the relationship between competition and the aggregate productivity of a grant contest. Ensuring that scientists exert socially valuable effort to obtain grants is increasingly important as grant contests become more competitive.",
        "references": [
            {
                "arxivId": "2312.01442",
                "title": "New Facts and Data about Professors and their Research",
                "abstract": "We introduce a new survey of professors at roughly 150 of the most research-intensive institutions of higher education in the US. We document seven new features of how research-active professors are compensated, how they spend their time, and how they perceive their research pursuits: (1) there is more inequality in earnings within fields than there is across fields; (2) institutions, ranks, tasks, and sources of earnings can account for roughly half of the total variation in earnings; (3) there is significant variation across fields in the correlations between earnings and different kinds of research output, but these account for a small amount of earnings variation; (4) measuring professors' productivity in terms of output-per-year versus output-per-research-hour can yield substantial differences; (5) professors' beliefs about the riskiness of their research are best predicted by their fundraising intensity, their risk-aversion in their personal lives, and the degree to which their research involves generating new hypotheses; (6) older and younger professors have very different research outputs and time allocations, but their intended audiences are quite similar; (7) personal risk-taking is highly predictive of professors' orientation towards applied, commercially-relevant research."
            },
            {
                "arxivId": "1903.06958",
                "title": "Early-career setback and future career impact",
                "abstract": null
            },
            {
                "arxivId": "1804.03732",
                "title": "Contest models highlight inherent inefficiencies of scientific funding competitions",
                "abstract": "Scientific research funding is allocated largely through a system of soliciting and ranking competitive grant proposals. In these competitions, the proposals themselves are not the deliverables that the funder seeks, but instead are used by the funder to screen for the most promising research ideas. Consequently, some of the funding program's impact on science is squandered because applying researchers must spend time writing proposals instead of doing science. To what extent does the community's aggregate investment in proposal preparation negate the scientific impact of the funding program? Are there alternative mechanisms for awarding funds that advance science more efficiently? We use the economic theory of contests to analyze how efficiently grant proposal competitions advance science, and compare them with recently proposed, partially randomized alternatives such as lotteries. We find that the effort researchers waste in writing proposals may be comparable to the total scientific value of the research that the funding supports, especially when only a few proposals can be funded. Moreover, when professional pressures motivate investigators to seek funding for reasons that extend beyond the value of the proposed science (e.g., promotion, prestige), the entire program can actually hamper scientific progress when the number of awards is small. We suggest that lost efficiency may be restored either by partial lotteries for funding or by funding researchers based on past scientific success instead of proposals for future work."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2312.17061",
        "category": "q-fin",
        "title": "Bayesian Analysis of High Dimensional Vector Error Correction Model",
        "abstract": "Vector Error Correction Model (VECM) is a classic method to analyse cointegration relationships amongst multivariate non-stationary time series. In this paper, we focus on high dimensional setting and seek for sample-size-efficient methodology to determine the level of cointegration. Our investigation centres at a Bayesian approach to analyse the cointegration matrix, henceforth determining the cointegration rank. We design two algorithms and implement them on simulated examples, yielding promising results particularly when dealing with high number of variables and relatively low number of observations. Furthermore, we extend this methodology to empirically investigate the constituents of the S&P 500 index, where low-volatility portfolios can be found during both in-sample training and out-of-sample testing periods.",
        "references": [
            {
                "arxivId": "2010.06451",
                "title": "Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO",
                "abstract": "High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable election and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of univariate linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which forms a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2403.07019",
        "category": "q-fin",
        "title": "Reasons behind the Water Crisis and its Potential Health Outcomes",
        "abstract": "Globally, the water crisis has become a significant problem that affects developing and industrialized nations. Water shortage can harm public health by increasing the chance of contracting water-borne diseases, dehydration, and malnutrition. This study aims to examine the causes of the water problem and its likely effects on human health. The study scrutinizes the reasons behind the water crisis, including population increase, climate change, and inefficient water management techniques. The results of a lack of water on human health, such as the spread of infectious diseases, a higher risk of starvation and dehydration, and psychological stress, are also concealed in the study. The research further suggests several ways to deal with the water situation and lessen its potential outcomes on human health. These remedies include enhanced sanitation and hygiene procedures, water management, and conservation techniques like rainwater gathering and wastewater recycling.",
        "references": [
            {
                "arxivId": "2301.00122",
                "title": "Hair and Scalp Disease Detection using Machine Learning and Image Processing",
                "abstract": "Almost 80 million Americans suffer from hair loss due to aging, stress, medication, or genetic makeup. Hair and scalp-related diseases often go unnoticed in the beginning. Sometimes, a patient cannot differentiate between hair loss and regular hair fall. Diagnosing hair-related diseases is time-consuming as it requires professional dermatologists to perform visual and medical tests. Because of that, the overall diagnosis gets delayed, which worsens the severity of the illness. Due to the image-processing ability, neural network-based applications are used in various sectors, especially healthcare and health informatics, to predict deadly diseases like cancers and tumors. These applications assist clinicians and patients and provide an initial insight into early-stage symptoms. In this study, we used a deep learning approach that successfully predicts three main types of hair loss and scalp-related diseases: alopecia, psoriasis, and folliculitis. However, limited study in this area, unavailability of a proper dataset, and degree of variety among the images scattered over the internet made the task challenging. 150 images were obtained from various sources and then preprocessed by denoising, image equalization, enhancement, and data balancing, thereby minimizing the error rate. After feeding the processed data into the 2D convolutional neural network (CNN) model, we obtained overall training accuracy of 96.2%, with a validation accuracy of 91.1%. The precision and recall score of alopecia, psoriasis, and folliculitis are 0.895, 0.846, and 1.0, respectively. We also created a dataset of the scalp images for future prospective researchers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2403.07166",
        "category": "q-fin",
        "title": "Small Price Changes, Sales Volume, and Menu Cost",
        "abstract": "The finding of small price changes in many retail price datasets is often viewed as a puzzle. We show that a possible explanation for the presence of small price changes is related to sales volume, an observation that has been overlooked in the existing literature. Analyzing a large retail scanner price dataset that contains information on both prices and sales volume, we find that small price changes are more frequent when products sales volume is high. This finding holds across product categories, within product categories, and for individual products. It is also robust to various sensitivity analyses such as measurement errors, the definition of small price changes, the inclusion of measures of price synchronization, the size of producers, the time horizon used to compute the average sales volume, the revenues, the competition, shoppers characteristics, etc.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2403.07180",
        "category": "q-fin",
        "title": "Study of the Impact of the Big Data Era on Accounting and Auditing",
        "abstract": "\u00a0In the wake of rapid advancements in science and technology, big data has emerged as a pivotal facet of contemporary society, revolutionizing the way we collect, analyze, and utilize information. Its integration into accounting and auditing practices has witnessed a steady and expansive trajectory, fundamentally reshaping traditional approaches to financial analysis, risk assessment, and compliance monitoring. This paper aims to delve into the profound influence of the big data era on the realms of accounting and auditing, meticulously dissecting the challenges and opportunities that accompany this evolution. As organizations grapple with unprecedented volumes of data generated from diverse sources such as social media, IoT devices, and transactional records, the traditional methods of financial reporting and auditing are being challenged. The sheer volume, velocity, and variety of data require innovative approaches to extract meaningful insights while ensuring data integrity and reliability. Moreover, the emergence of artificial intelligence and machine learning algorithms offers promising avenues for automating routine tasks, enhancing analytical capabilities, and detecting anomalies with greater accuracy and efficiency. However, the proliferation of big data also presents formidable challenges, ranging from data privacy and security concerns to the complexities of data integration and interoperability. Accounting and auditing professionals must navigate this complex landscape while upholding ethical standards, regulatory compliance, and professional skepticism. Moreover, the rapid pace of technological change necessitates continuous learning and adaptation to stay abreast of emerging trends and best practices. In light of these challenges and opportunities, this paper endeavors to proffer strategic countermeasures tailored to navigate this transformative landscape effectively. From leveraging advanced analytics tools and cloud computing platforms to investing in cybersecurity measures and talent development initiatives, organizations can enhance their capacity to harness the potential of big data while mitigating associated risks. Moreover, collaboration between stakeholders, including regulators, industry associations, and academia, is essential to foster innovation, knowledge sharing, and industry-wide standards. By embracing the opportunities afforded by the big data era while addressing its inherent challenges, accounting and auditing professionals can position themselves as trusted advisors, equipped to provide actionable insights and strategic guidance in an increasingly data-driven world. Through continuous collaboration, innovation, and adaptation, the profession can seize the transformative potential of big data to drive sustainable growth, enhance transparency, and uphold the integrity of financial markets.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2403.07530",
        "category": "q-fin",
        "title": "Carbon Economics of Different Agricultural Practices for Farming Soil",
        "abstract": "The loss of soil organic carbon (SOC) poses a severe danger to agricultural sustainability around the World. This review examines various farming practices and their impact on soil organic carbon storage. After a careful review of the literature, most of the research indicated that different farming practices, such as organic farming, cover crops, conservation tillage, and agroforestry, play vital roles in increasing the SOC content of the soil sustainably. Root exudation from cover crops increases microbial activity and helps break down complex organic compounds into organic carbon. Conservation tillage enhances the soil structure and maintains carbon storage without disturbing the soil. Agroforestry systems boost organic carbon input and fasten nutrient cycling because the trees and crops have symbiotic relationships. Intercropping and crop rotations have a role in changing the composition of plant residues and promoting carbon storage. There were many understanding on the complex interactions between soil organic carbon dynamics and agricultural practices. Based on the study, the paper reveals, the role of different agricultural practices like Carbon storage through cover crops, crop rotation, mulching Conservation tillage, conventional tillage, zero tillage and organic amendments in organic carbon storage in the soil for maximum crop yield to improve the economic condition of the cultivators.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-13.json",
        "arxivId": "2403.07617",
        "category": "q-fin",
        "title": "Price Gouging or Market Forces? Fairness Perceptions of Price Hikes in the Pandemic",
        "abstract": "We report the results of surveys we conducted in the US and Israel in 2020, a time when many prices increased following the spread of the pandemic. To assess respondents perceptions of price increases, we focus on goods whose prices have increased during the pandemic, including some essential goods. Consistent with the principle of dual entitlement, we find that respondents perceive price increases as more acceptable if they are due to cost shocks than if they are due to demand shocks. However, we also find large differences across the two populations, as well as across goods.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-14.json",
        "arxivId": "2307.14218",
        "category": "q-fin",
        "title": "Interest rate convexity in a Gaussian framework",
        "abstract": "The contributions of this paper are twofold: we define and investigate the properties of a short rate model driven by a general Gaussian Volterra process and, after defining precisely a notion of convexity adjustment, derive explicit formulae for it.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-14.json",
        "arxivId": "2311.08617",
        "category": "q-fin",
        "title": "Bank Performance Determinants: State of the Art and Future Research Avenues",
        "abstract": "Banks' performance is an important topic for both professionals and researchers. Given the important literature on this subject, this paper aims to bring an up-to-date and organized review of literature on the determinants of banks performance. This paper discusses the main approaches that molded the debate on banks\u2019 performance and their main determinants. An in-depth understanding of these latter may allow on the one hand, bank managers and regulators to improve the sectors\u2019 efficiency and to deal with the new trends shaping the future of their industry and on the other hand, academicians to enrich research and knowledge on this field. Through the analysis of 54 studies published in 42 peer-reviewed journals, we show that despite the importance of the existent literature, the subject of bank performance factors did not reveal all its secrets and still constitute a fertile field for critical debates, especially since the COVID-19 and the increasingly pressing rise in power of digital transformation and artificial intelligence in general and FinTechs in particular. The study concludes by suggesting new promising research avenues.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-14.json",
        "arxivId": "2403.07998",
        "category": "q-fin",
        "title": "Pairs Trading Using a Novel Graphical Matching Approach",
        "abstract": "Pairs trading, a strategy that capitalizes on price movements of asset pairs driven by similar factors, has gained significant popularity among traders. Common practice involves selecting highly cointegrated pairs to form a portfolio, which often leads to the inclusion of multiple pairs sharing common assets. This approach, while intuitive, inadvertently elevates portfolio variance and diminishes risk-adjusted returns by concentrating on a small number of highly cointegrated assets. Our study introduces an innovative pair selection method employing graphical matchings designed to tackle this challenge. We model all assets and their cointegration levels with a weighted graph, where edges signify pairs and their weights indicate the extent of cointegration. A portfolio of pairs is a subgraph of this graph. We construct a portfolio which is a maximum weighted matching of this graph to select pairs which have strong cointegration while simultaneously ensuring that there are no shared assets within any pair of pairs. This approach ensures each asset is included in just one pair, leading to a significantly lower variance in the matching-based portfolio compared to a baseline approach that selects pairs purely based on cointegration. Theoretical analysis and empirical testing using data from the S\\&P 500 between 2017 and 2023, affirm the efficacy of our method. Notably, our matching-based strategy showcases a marked improvement in risk-adjusted performance, evidenced by a gross Sharpe ratio of 1.23, a significant enhancement over the baseline value of 0.48 and market value of 0.59. Additionally, our approach demonstrates reduced trading costs attributable to lower turnover, alongside minimized single asset risk due to a more diversified asset base.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-14.json",
        "arxivId": "2403.08202",
        "category": "q-fin",
        "title": "Trading Large Orders in the Presence of Multiple High-Frequency Anticipatory Traders",
        "abstract": "We investigate a market with a normal-speed informed trader (IT) who may employ mixed strategy and multiple anticipatory high-frequency traders (HFTs) who are under different inventory pressures, in a three-period Kyle's model. The pure- and mixed-strategy equilibria are considered and the results provide recommendations for IT's randomization strategy with different numbers of HFTs. Some surprising results about investors' profits arise: the improvement of anticipatory traders' speed or a more precise prediction may harm themselves but help IT.",
        "references": [
            {
                "arxivId": "2304.13985",
                "title": "The Effects of High-Frequency Anticipatory Trading: Small Informed Trader vs. Round-Tripper",
                "abstract": "In an extended Kyle's model, the interactions between a large informed trader and a high-frequency trader (HFT) who can anticipate the former's incoming order are studied. We find that, in equilibrium, HFT may play the role of Small-IT or Round-Tripper: both of them trade in the same direction as IT in advance, but when IT's order arrives, Small-IT continues to take liquidity away, while Round-Tripper supplies liquidity back. So Small-IT always harms IT, while Round-Tripper may benefit her. What's more, with an anticipatory HFT, normal-speed small uninformed traders suffer less and price discovery is accelerated."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-14.json",
        "arxivId": "2403.08230",
        "category": "q-fin",
        "title": "A vicious cycle along busy bus corridors and how to abate it",
        "abstract": "We unveil that a previously-unreported vicious cycle can be created when bus queues form at curbside stops along a corridor. Buses caught in this cycle exhibit growing variation in headways as they travel from stop to stop. Bus (and patron) delays accumulate in like fashion and can grow large on long, busy corridors. We show that this damaging cycle can be abated in simple ways. Present solutions entail holding buses at a corridor entrance and releasing them as per various strategies proposed in the literature. We introduce a modest variant to the simplest of these strategies. It releases buses at headways that are slightly less than, or equal to, the scheduled values. It turns out that periodically releasing buses at slightly smaller headways can substantially reduce bus delays caused by holding so that benefits can more readily outweigh costs in corridors that contain a sufficient number of serial bus stops. The simple variant is shown to perform about as well as, or better than, other bus-holding strategies in terms of saving delays, and is more effective than other strategies in regularizing bus headways. We also show that grouping buses from across multiple lines and holding them by group can be effective when patrons have the flexibility to choose buses from across all lines in a group. Findings come by formulating select models of bus-corridor dynamics and using these to simulate part of the Bus Rapid Transit corridor in Guangzhou, China.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-15.json",
        "arxivId": "2403.09045",
        "category": "q-fin",
        "title": "Entangled vs. Separable Choice",
        "abstract": "We study joint probabilistic choice rules that describe the behavior of two decision makers, each facing a possibly different menu. These choice rules are separable when they can be factored into autonomous choices from each individual solely correlated through their individual probabilistic choice rules. Despite recent interest in studying such rules, a complete characterization of the restrictions on them remains an open question. A reasonable conjecture is that such restrictions on separable joint choice can be factored into individual choice restrictions. We name these restrictions separable and show that this conjecture is true if and only if the probabilistic choice rule of at least one decision maker uniquely identifies the distribution over deterministic choice rules. Otherwise, entangled choice rules exist that satisfy separable restrictions yet are not separable. The possibility of entangled choice complicates the characterization of separable choice since one needs to augment the separable restrictions with the new emerging ones.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-18.json",
        "arxivId": "2205.08104",
        "category": "q-fin",
        "title": "Restricting Entries to All-Pay Contests",
        "abstract": "We study an all-pay contest where players with low abilities are filtered prior to the round of competing for prizes. These are often practiced due to limited resources or to enhance the competitiveness of the contest. We consider a setting where the designer admits a certain number of top players into the contest. The players admitted into the contest update their beliefs about their opponents based on the signal that their abilities are among the top. We find that their posterior beliefs, even with IID priors, are correlated and depend on players' private abilities, representing a unique feature of this game. We explicitly characterize the symmetric and unique Bayesian equilibrium strategy. We find that each admitted player's equilibrium effort is in general not monotone with the number of admitted players. Despite this non-monotonicity, surprisingly, all players exert their highest efforts when all players are admitted. This result holds generally -- it is true under any ranking-based prize structure, ability distribution, and cost function. We also discuss a two-stage extension where players with top first-stage efforts can proceed to the second stage competing for prizes.",
        "references": [
            {
                "arxivId": "1111.2893",
                "title": "Optimal Crowdsourcing Contests",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-18.json",
        "arxivId": "2309.06393",
        "category": "q-fin",
        "title": "Real-time VaR Calculations for Crypto Derivatives in KDB /Q",
        "abstract": "Cryptocurrency market is known for exhibiting significantly higher volatility than traditional asset classes. Efficient and adequate risk calculation is vital for managing risk exposures in such market environments where extreme price fluctuations occur in short timeframes. The objective of this thesis is to build a real-time computation workflow that provides VaR estimates for non-linear portfolios of cryptocurrency derivatives. Many researchers have examined the predictive capabilities of time-series models within the context of cryptocurrencies. In this work, we applied three commonly used models - EMWA, GARCH and HAR - to capture and forecast volatility dynamics, in conjunction with delta-gamma-theta approach and Cornish-Fisher expansion to crypto derivatives, examining their performance from the perspectives of calculation efficiency and accuracy. We present a calculation workflow which harnesses the information embedded in high-frequency market data and the computation simplicity inherent in analytical estimation procedures. This workflow yields reasonably robust VaR estimates with calculation latencies on the order of milliseconds.",
        "references": [
            {
                "arxivId": "2012.02395",
                "title": "A New Parametrization of Correlation Matrices",
                "abstract": "We introduce a novel parametrization of the correlation matrix. The reparametrization facilitates modeling of correlation and covariance matrices by an unrestricted vector, where positive definiteness is an innate property. This parametrization can be viewed as a generalization of Fisther's Z-transformation to higher dimensions and has a wide range of potential applications. An algorithm for reconstructing the unique n x n correlation matrix from any d-dimensional vector (with d = n(n-1)/2) is provided, and we derive its numerical complexity."
            },
            {
                "arxivId": "0909.0827",
                "title": "Estimation of Volatility Functionals in the Simultaneous Presence of Microstructure Noise and Jumps",
                "abstract": "We propose a new concept of modulated bipower variation for diffusion models with microstructure noise. We show that this method provides simple estimates for such important quantities as integrated volatility or integrated quarticity. Under mild conditions the consistency of modulated bipower variation is proven. Under further assumptions we prove stable convergence of our estimates with the optimal rate n^(-1/4). Moreover, we construct estimates which are robust to finite activity jumps."
            },
            {
                "arxivId": "0708.1627",
                "title": "Rearranging Edgeworth\u2013Cornish\u2013Fisher expansions",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-18.json",
        "arxivId": "2311.10685",
        "category": "q-fin",
        "title": "High-Throughput Asset Pricing",
        "abstract": "We use empirical Bayes (EB) to mine data on 140,000 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. This\"high-throughput asset pricing\"produces out-of-sample performance comparable to strategies in top finance journals. But unlike the published strategies, the data-mined strategies are free of look-ahead bias. EB predicts that high returns are concentrated in accounting strategies, small stocks, and pre-2004 samples, consistent with limited attention theories. The intuition is seen in the cross-sectional distribution of t-stats, which is far from the null for equal-weighted accounting strategies. High-throughput methods provide a rigorous, unbiased method for documenting asset pricing facts.",
        "references": [
            {
                "arxivId": "2006.04269",
                "title": "False (and Missed) Discoveries in Financial Economics",
                "abstract": "Multiple testing plagues many important questions in finance such as fund and factor selection. We propose a new way to calibrate both Type I and Type II errors. Next, using a double-bootstrap method, we establish a t-statistic hurdle that is associated with a specific false discovery rate (e.g., 5%). We also establish a hurdle that is associated with a certain acceptable ratio of misses to false discoveries (Type II error scaled by Type I error), which effectively allows for differential costs of the two types of mistakes. Evaluating current methods, we find that they lack power to detect outperforming managers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-18.json",
        "arxivId": "2403.10441",
        "category": "q-fin",
        "title": "A Mean-Field Game of Market Entry: Portfolio Liquidation with Trading Constraints",
        "abstract": "We consider both $N$-player and mean-field games of optimal portfolio liquidation in which the players are not allowed to change the direction of trading. Players with an initially short position of stocks are only allowed to buy while players with an initially long position are only allowed to sell the stock. Under suitable conditions on the model parameters we show that the games are equivalent to games of timing where the players need to determine the optimal times of market entry and exit. We identify the equilibrium entry and exit times and prove that equilibrium mean-trading rates can be characterized in terms of the solutions to a highly non-linear higher-order integral equation with endogenous terminal condition. We prove the existence of a unique solution to the integral equation from which we obtain the existence of a unique equilibrium both in the mean-field and the $N$-player game.",
        "references": [
            {
                "arxivId": "2011.11533",
                "title": "Control and optimal stopping Mean Field Games: a linear programming approach",
                "abstract": "We develop the linear programming approach to mean-field games in a general setting. This relaxed control approach allows to prove existence results under weak assumptions, and lends itself well to numerical implementation. We consider mean-field game problems where the representative agent chooses both the optimal control and the optimal time to exit the game, where the instantaneous reward function and the coefficients of the state process may depend on the distribution of the other agents. Furthermore, we establish the equivalence between mean-field games equilibria obtained by the linear programming approach and the ones obtained via the controlled/stopped martingale approach, another relaxation method used in a few previous papers in the case when there is only control."
            },
            {
                "arxivId": "2004.14057",
                "title": "The entry and exit game in the electricity markets: A mean-field game approach",
                "abstract": "We develop a model for the industry dynamics in the electricity market, based on mean-field games of optimal stopping. In our model, there are two types of agents: the renewable producers and the conventional producers. The renewable producers choose the optimal moment to build new renewable plants, and the conventional producers choose the optimal moment to exit the market. The agents interact through the market price, determined by matching the aggregate supply of the two types of producers with an exogenous demand function. Using a relaxed formulation of optimal stopping mean-field games, we prove the existence of a Nash equilibrium and the uniqueness of the equilibrium price process. An empirical example, inspired by the UK electricity market is presented. The example shows that while renewable subsidies clearly lead to higher renewable penetration, this may entail a cost to the consumer in terms of higher peakload prices. In order to avoid rising prices, the renewable subsidies must be combined with mechanisms ensuring that sufficient conventional capacity remains in place to meet the energy demand during peak periods."
            },
            {
                "arxivId": "1812.06196",
                "title": "Mean-Field Games of Optimal Stopping: A Relaxed Solution Approach",
                "abstract": "We consider the mean-field game where each agent determines the optimal time to exit the game by solving an optimal stopping problem with reward function depending on the density of the state processes of agents still present in the game. We place ourselves in the framework of relaxed optimal stopping, which amounts to looking for the optimal occupation measure of the stopper rather than the optimal stopping time. This framework allows us to prove the existence of the relaxed Nash equilibrium and the uniqueness of the associated value of the representative agent under mild assumptions. Further, we prove a rigorous relation between relaxed Nash equilibria and the notion of mixed solutions introduced in earlier works on the subject, and provide a criterion, under which the optimal strategies are pure strategies, that is, behave in a similar way to stopping times. Finally, we present a numerical method for computing the equilibrium in the case of potential games and show its convergence."
            },
            {
                "arxivId": "1810.06101",
                "title": "Mean\u2010field games with differing beliefs for algorithmic trading",
                "abstract": "Even when confronted with the same data, agents often disagree on a model of the real world. Here, we address the question of how interacting heterogeneous agents, who disagree on what model the real world follows, optimize their trading actions. The market has latent factors that drive prices, and agents account for the permanent impact they have on prices. This leads to a large stochastic game, where each agents performance criteria are computed under a different probability measure. We analyze the mean\u2010field game (MFG) limit of the stochastic game and show that the Nash equilibrium is given by the solution to a nonstandard vector\u2010valued forward\u2013backward stochastic differential equation. Under some mild assumptions, we construct the solution in terms of expectations of the filtered states. Furthermore, we prove that the MFG strategy forms an \u03b5\u2010Nash equilibrium for the finite player game. Finally, we present a least square Monte Carlo based algorithm for computing the equilibria and show through simulations that increasing disagreement may increase price volatility and trading activity."
            },
            {
                "arxivId": "1804.04911",
                "title": "A Mean Field Game of Optimal Portfolio Liquidation",
                "abstract": "We consider a mean field game (MFG) of optimal portfolio liquidation under asymmetric information. We prove that the solution to the MFG can be characterized in terms of a forward-backward stochastic differential equation (FBSDE) with a possibly singular terminal condition on the backward component or, equivalently, in terms of an FBSDE with a finite terminal value yet a singular driver. Extending the method of continuation to linear-quadratic FBSDEs with a singular driver, we prove that the MFG has a unique solution. Our existence and uniqueness result allows proving that the MFG with a possibly singular terminal condition can be approximated by a sequence of MFGs with finite terminal values."
            },
            {
                "arxivId": "1803.04094",
                "title": "Mean Field Games with Partial Information for Algorithmic Trading",
                "abstract": "Financial markets are often driven by latent factors which traders cannot observe. Here, we address an algorithmic trading problem with collections of heterogeneous agents who aim to perform optimal execution or statistical arbitrage, where all agents filter the latent states of the world, and their trading actions have permanent and temporary price impact. This leads to a large stochastic game with heterogeneous agents. We solve the stochastic game by investigating its mean-field game (MFG) limit, with sub-populations of heterogeneous agents, and, using a convex analysis approach, we show that the solution is characterized by a vector-valued forward-backward stochastic differential equation (FBSDE). We demonstrate that the FBSDE admits a unique solution, obtain it in closed-form, and characterize the optimal behaviour of the agents in the MFG equilibrium. Moreover, we prove the MFG equilibrium provides an $\\epsilon$-Nash equilibrium for the finite player game. We conclude by illustrating the behaviour of agents using the optimal MFG strategy through simulated examples."
            },
            {
                "arxivId": "1712.10253",
                "title": "Second-order BSDE under monotonicity condition and liquidation problem under uncertainty",
                "abstract": "In this work we investigate an optimal closure problem under Knightian uncertainty. We obtain the value function and an optimal control as the minimal (super-)solution of a second order BSDE with monotone generator and with a singular terminal condition."
            },
            {
                "arxivId": "1612.03816",
                "title": "$N$-player games and mean-field games with absorption",
                "abstract": "We introduce a simple class of mean field games with absorbing boundary over a finite time horizon. In the corresponding N-player games, the evolution of players\u2019 states is described by a system of weakly interacting Ito equations with absorption on first exit from a bounded open set. Once a player exits, her/his contribution is removed from the empirical measure of the system. Players thus interact through a renormalized empirical measure. In the definition of solution to the mean field game, the renormalization appears in form of a conditional law. We justify our definition of solution in the usual way, that is, by showing that a solution of the mean field game induces approximate Nash equilibria for the N-player games with approximation error tending to zero as N tends to infinity. This convergence is established provided the diffusion coefficient is non-degenerate. The degenerate case is more delicate and gives rise to counter-examples."
            },
            {
                "arxivId": "1611.03435",
                "title": "Optimal Trade Execution with Instantaneous Price Impact and Stochastic Resilience",
                "abstract": "We study an optimal execution problem in illiquid markets with both instantaneous and persistent price impact and stochastic resilience when only absolutely continuous trading strategies are admissible. In our model the value function can be described by a three-dimensional system of backward stochastic differential equations (BSDE) with a singular terminal condition in one component. We prove existence and uniqueness of a solution to the BSDE system and characterize both the value function and the optimal strategy in terms of the unique solution to the BSDE system. Our existence proof is based on an asymptotic expansion of the BSDE system at the terminal time that allows us to express the system in terms of a equivalent system with finite terminal value but singular driver."
            },
            {
                "arxivId": "1610.09904",
                "title": "Mean field game of controls and an application to trade crowding",
                "abstract": null
            },
            {
                "arxivId": "1609.00599",
                "title": "Optimal Execution in a Multiplayer Model of Transient Price Impact",
                "abstract": "Trading algorithms that execute large orders are susceptible to exploitation by order anticipation strategies. This paper studies the influence of order anticipation strategies in a multi-investor model of optimal execution under transient price impact. Existence and uniqueness of a Nash equilibrium is established under the assumption that trading incurs quadratic transaction costs. A closed-form representation of the Nash equilibrium is derived for exponential decay kernels. With this representation, it is shown that while order anticipation strategies raise the execution costs of a large order significantly, they typically do not cause price overshooting in the sense of Brunnermeier and Pedersen."
            },
            {
                "arxivId": "1606.03709",
                "title": "Mean Field Games of Timing and Models for Bank Runs",
                "abstract": null
            },
            {
                "arxivId": "1509.08281",
                "title": "High-Frequency Limit of Nash Equilibria in a Market Impact Game with Transient Price Impact",
                "abstract": "We study the high-frequency limits of strategies and costs in a Nash equilibrium for two agents that are competing to minimize liquidation costs in a discrete-time market impact model with exponentially decaying price impact and quadratic transaction costs of size $\\theta\\ge0$. We show that, for $\\theta=0$, equilibrium strategies and costs will oscillate indefinitely between two accumulation points. For $\\theta>0$, however, strategies, costs, and total transaction costs will converge towards limits that are independent of $\\theta$. We then show that the limiting strategies form a Nash equilibrium for a continuous-time version of the model with $\\theta$ equal to a certain critical value $\\theta^*>0$, and that the corresponding expected costs coincide with the high-frequency limits of the discrete-time equilibrium costs. For $\\theta\\neq\\theta^*$, however, continuous-time Nash equilibria will typically not exist. Our results permit us to give mathematically rigorous proofs of numerical observations made in [..."
            },
            {
                "arxivId": "1504.01150",
                "title": "Minimal supersolutions for BSDEs with singular terminal condition and application to optimal position targeting",
                "abstract": null
            },
            {
                "arxivId": "1305.6541",
                "title": "BSDEs with Singular Terminal Condition and a Control Problem with Constraints",
                "abstract": "We provide a probabilistic solution of a not necessarily Markovian control problem with a state constraint by means of a backward stochastic differential equation (BSDE). The novelty of our solution approach is that the BSDE possesses a singular terminal condition. We prove that a solution of the BSDE exists, thus partly generalizing existence results obtained by Popier in [Stochastic Process. Appl., 116 (2006), pp. 2014--2056] and [Ann. Probab., 35 (2007), pp. 1071--1117]. We perform a verification and discuss special cases for which the control problem has explicit solutions."
            },
            {
                "arxivId": "1109.2631",
                "title": "OPTIMAL TRADE EXECUTION AND PRICE MANIPULATION IN ORDER BOOKS WITH TIME\u2010VARYING LIQUIDITY",
                "abstract": "In financial markets, liquidity is not constant over time but exhibits strong seasonal patterns. In this paper, we consider a limit order book model that allows for time\u2010dependent, deterministic depth and resilience of the book and determine optimal portfolio liquidation strategies. In a first model variant, we propose a trading\u2010dependent spread that increases when market orders are matched against the order book. In this model, no price manipulation occurs and the optimal strategy is of the wait region/buy region type often encountered in singular control problems. In a second model, we assume that there is no spread in the order book. Under this assumption, we find that price manipulation can occur, depending on the model parameters. Even in the absence of classical price manipulation, there may be transaction triggered price manipulation. In specific cases, we can state the optimal strategy in closed form."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "1411.6250",
        "category": "q-fin",
        "title": "Identification and Estimation of Multidimensional Screening",
        "abstract": "We study the identification and estimation of a multidimensional screening model, where a monopolist sells a product with multiple qualities to consumers with private information about their multidimensional preferences. Under optimal screening, the seller designs product and payment rules that exclude\"low-type\"consumers, bunches the\"medium types\"at\"medium-quality\"products, and perfectly screens the\"high types.\"We determine sufficient conditions to identify the joint distribution of preferences and the marginal costs from data on optimal individual choices and payments. Then, we propose estimators for these objects, establish their asymptotic properties, and assess their small-sample performance using Monte Carlo experiments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "1805.05606",
        "category": "q-fin",
        "title": "Nonparametric Bayesian volatility learning under microstructure noise",
        "abstract": null,
        "references": [
            {
                "arxivId": "cond-mat/0405293",
                "title": "A Theory for the Term Structure of Interest Rates",
                "abstract": "The Convolution and Master equations governing the time behavior of the term structure of Interest Rates are set up both for continuous variables and for their discretised forms. The notion of Seed is introduced. The discretised theoretical distributions matching the empirical data from the Federal Reserve System (FRS) are deduced from a discretised seed which enjoys remarkable scaling laws. In particular the tails of the distributions are very well reproduced. These results may be used to develop new methods for the computation of the value-at-risk and fixed-income derivative pricing."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "1905.13660",
        "category": "q-fin",
        "title": "On Policy Evaluation with Aggregate Time-Series Shocks",
        "abstract": "We propose a general strategy for estimating treatment effects, in contexts where the only source of exogenous variation is a sequence of aggregate time-series shocks. We start by arguing that commonly used estimation procedures tend to ignore the crucial time-series aspects of the data. Next, we develop a graphical tool and a novel test to illustrate the issues of the design using data from influential studies in development economics [Nunn and Qian, 2014] and macroeconomics [Nakamura and Steinsson, 2014]. Motivated by these studies, we construct a new estimator, which is based on the time-series model for the aggregate shock. We analyze the statistical properties of our estimator in the practically relevant case, where both cross-sectional and time-series dimensions are of similar size. Finally, to provide causal interpretation for our estimator, we analyze a new causal model that allows taking into account both rich unobserved heterogeneity in potential outcomes and unobserved aggregate shocks.",
        "references": [
            {
                "arxivId": "2306.14311",
                "title": "Simple estimation of semiparametric models with measurement errors",
                "abstract": "We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a\"corrected\"set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates, and settings with multivariate or non-classical EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models."
            },
            {
                "arxivId": "2104.08931",
                "title": "Least Squares with Error in Variables",
                "abstract": "Error-in-variables regression is a common ingredient in treatment effect estimators using panel data. This includes synthetic control estimators, counterfactual time series forecasting estimators, and combinations. We study high-dimensional least squares with correlated error-in-variables with a focus on these uses. We use our results to derive conditions under which the synthetic control estimator is asymptotically unbiased and normal with estimable variance, permitting inference without assuming time-stationarity, unit-exchangeability, or the absence of weak factors. These results hold in an asymptotic regime in which the number of pre-treatment periods goes to infinity and the number of control units can be much larger $(p \\gg n)$."
            },
            {
                "arxivId": "1909.09412",
                "title": "Double-Robust Identification for Causal Panel Data Models",
                "abstract": "\n We study identification and estimation of causal effects in settings with panel data. Traditionally researchers follow model-based identification strategies relying on assumptions governing the relation between the potential outcomes and the observed and unobserved confounders. We focus on a different, complementary approach to identification where assumptions are made about the connection between the treatment assignment and the unobserved confounders. Such strategies are common in cross-section\u00a0settings but rarely used with panel data. We introduce different sets of assumptions that follow the two paths to identification and develop a double robust approach. We propose estimation methods that build on these identification strategies."
            },
            {
                "arxivId": "1812.09970",
                "title": "Synthetic Difference in Differences",
                "abstract": "We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \u201csynthetic difference-in-differences\u201d estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality. (JEL C23, H25, H71, I18, L66)"
            },
            {
                "arxivId": "1811.04170",
                "title": "The Augmented Synthetic Control Method",
                "abstract": "Abstract The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The \u201csynthetic control\u201d is a weighted average of control units that balances the treated unit\u2019s pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package."
            },
            {
                "arxivId": "1808.05293",
                "title": "Design-Based Analysis in Difference-in-Differences Settings with Staggered Adoption",
                "abstract": "In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is is an unbiased estimator of a particular weighted average causal effect. We characterize the proeperties of this estimand, and show that the standard variance estimator is conservative."
            },
            {
                "arxivId": "1806.07928",
                "title": "Shift-Share Designs: Theory and Inference",
                "abstract": "\n We study inference in shift-share regression designs, such as when a regional outcome is regressed on a weighted average of sectoral shocks, using regional sector shares as weights. We conduct a placebo exercise in which we estimate the effect of a shift-share regressor constructed with randomly generated sectoral shocks on actual labor market outcomes across U.S. commuting zones. Tests based on commonly used standard errors with 5% nominal significance level reject the null of no effect in up to 55% of the placebo samples. We use a stylized economic model to show that this overrejection problem arises because regression residuals are correlated across regions with similar sectoral shares, independent of their geographic location. We derive novel inference methods that are valid under arbitrary cross-regional correlation in the regression residuals. We show using popular applications of shift-share designs that our methods may lead to substantially wider confidence intervals in practice."
            },
            {
                "arxivId": "1806.01888",
                "title": "High-dimensional econometrics and regularized GMM",
                "abstract": "This chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. High-dimensional models are characterized by having a number of unknown parameters that is not vanishingly small relative to the sample size. We first present results in a framework where estimators of parameters of interest may be represented directly as approximate means. Within this context, we review fundamental results including high-dimensional central limit theorems, bootstrap approximation of high-dimensional limit distributions, and moderate deviation theory. We also review key concepts underlying inference when many parameters are of interest such as multiple testing with family-wise error rate or false discovery rate control. We then turn to a general high-dimensional minimum distance framework with a special focus on generalized method of moments problems where we present results for estimation and inference about model parameters. The presented results cover a wide array of econometric applications, and we discuss several leading special cases including high-dimensional linear regression and linear instrumental variables models to illustrate the general results."
            },
            {
                "arxivId": "1806.01221",
                "title": "Quasi-Experimental Shift-Share Research Designs",
                "abstract": "Many studies use shift-share (or \"Bartik\") instruments, which average a set of shocks with exposure share weights. We provide a new econometric framework for shift-share instrumental variable (SSIV) regressions in which identification follows from the quasi-random assignment of shocks, while exposure shares are allowed to be endogenous. The framework is motivated by an equivalence result: the orthogonality between a shift-share instrument and an unobserved residual can be represented as the orthogonality between the underlying shocks and a shock-level unobservable. SSIV regression coefficients can similarly be obtained from an equivalent shock-level regression, motivating shock-level conditions for their consistency. We discuss and illustrate several practical insights of this framework in the setting of Autor et al. (2013), estimating the effect of Chinese import competition on manufacturing employment across U.S. commuting zones."
            },
            {
                "arxivId": "1607.06801",
                "title": "High-dimensional regression adjustments in randomized experiments",
                "abstract": "Significance As datasets get larger and more complex, there is a growing interest in using machine-learning methods to enhance scientific analysis. In many settings, considerable work is required to make standard machine-learning methods useful for specific scientific applications. We find, however, that in the case of treatment effect estimation with randomized experiments, regression adjustments via machine-learning methods designed to minimize test set error directly induce efficient estimates of the average treatment effect. Thus, machine-learning methods can be used out of the box for this task, without any special-case adjustments. We study the problem of treatment effect estimation in randomized experiments with high-dimensional covariate information and show that essentially any risk-consistent regression adjustment can be used to obtain efficient estimates of the average treatment effect. Our results considerably extend the range of settings where high-dimensional regression adjustments are guaranteed to provide valid inference about the population average treatment effect. We then propose cross-estimation, a simple method for obtaining finite-sample\u2013unbiased treatment effect estimates that leverages high-dimensional regression adjustments. Our method can be used when the regression model is estimated using the lasso, the elastic net, subset selection, etc. Finally, we extend our analysis to allow for adaptive specification search via cross-validation and flexible nonparametric regression adjustments with machine-learning methods such as random forests or neural networks."
            },
            {
                "arxivId": "1306.2872",
                "title": "Hanson-Wright inequality and sub-gaussian concentration",
                "abstract": "In this expository note, we give a modern proof of Hanson-Wright inequality for quadratic forms in sub-gaussian random variables.We deduce a useful concentration inequality for sub-gaussian random vectors.Two examples are given to illustrate these results: a concentration of distances between random vectors and subspaces, and a bound on the norms of products of random and deterministic matrices."
            },
            {
                "arxivId": "1208.2301",
                "title": "Agnostic notes on regression adjustments to experimental data: Reexamining Freedman's critique",
                "abstract": "Freedman [Adv. in Appl. Math. 40 (2008) 180-193; Ann. Appl. Stat. 2 (2008) 176-196] critiqued ordinary least squares regression adjustment of estimated treatment effects in randomized experiments, using Neyman's model for randomization inference. Contrary to conventional wisdom, he argued that adjustment can lead to worsened asymptotic precision, invalid measures of precision, and small-sample bias. This paper shows that in sufficiently large samples, those problems are either minor or easily fixed. OLS adjustment cannot hurt asymptotic precision when a full set of treatment-covariate interactions is included. Asymptotically valid confidence intervals can be constructed with the Huber-White sandwich standard error estimator. Checks on the asymptotic approximations are illustrated with data from Angrist, Lang, and Oreopoulos's [Am. Econ. J.: Appl. Econ. 1:1 (2009) 136--163] evaluation of strategies to improve college students' achievement. The strongest reasons to support Freedman's preference for unadjusted estimates are transparency and the dangers of specification search."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2005.04312",
        "category": "q-fin",
        "title": "Utility maximization under endogenous pricing",
        "abstract": "We study the expected utility maximization problem of a large investor who is allowed to make transactions on tradable assets in an incomplete financial market with endogenous permanent market impacts. The asset prices are assumed to follow a nonlinear price curve quoted in the market as the utility indifference curve of a representative liquidity supplier. We show that optimality can be fully characterized via a system of coupled forward-backward stochastic differential equations (FBSDEs) which corresponds to a non-linear backward stochastic partial differential equation (BSPDE). We show existence of solutions to the optimal investment problem and the FBSDEs in the case where the driver function of the representative market maker grows at least quadratically or the utility function of the large investor falls faster than quadratically or is exponential. Furthermore, we derive smoothness results for the existence of solutions of BSPDEs. Examples are provided when the market is complete or the utility function is exponential.",
        "references": [
            {
                "arxivId": "2111.07246",
                "title": "Existence of global solutions for multi-dimensional coupled FBSDEs with diagonally quadratic generators",
                "abstract": null
            },
            {
                "arxivId": "2001.00622",
                "title": "An FBSDE approach to market impact games with stochastic parameters",
                "abstract": "In this study, we have analyzed a market impact game between n risk-averse agents who compete for liquidity in a market impact model with a permanent price impact and additional slippage. Most market parameters, including volatility and drift, are allowed to vary stochastically. Our first main result characterizes the Nash equilibrium in terms of a fully coupled system of forward-backward stochastic differential equations (FBSDEs). Our second main result provides conditions under which this system of FBSDEs has a unique solution, resulting in a unique Nash equilibrium."
            },
            {
                "arxivId": "1909.12678",
                "title": "Numerical Resolution of McKean-Vlasov FBSDEs Using Neural Networks",
                "abstract": null
            },
            {
                "arxivId": "1901.10989",
                "title": "Equilibrium asset pricing with transaction costs",
                "abstract": null
            },
            {
                "arxivId": "1702.01385",
                "title": "Perfect hedging under endogenous permanent market impacts",
                "abstract": null
            },
            {
                "arxivId": "1612.01302",
                "title": "A Primer on Portfolio Choice with Small Transaction Costs",
                "abstract": "This review is an introduction to asymptotic methods for portfolio choice problems with small transaction costs. We outline how to derive the corresponding dynamic programming equations and how to simplify them in the small-cost limit. This allows one to obtain explicit solutions in a wide range of settings, which we illustrate for a model with mean-reverting expected returns and proportional transaction costs. For more complex models, we present a policy iteration scheme that allows one to numerically compute the solution."
            },
            {
                "arxivId": "0708.0948",
                "title": "Pricing, Hedging and Optimally Designing Derivatives via Minimization of Risk Measures",
                "abstract": "The question of pricing and hedging a given contingent claim has a unique solution in a complete market framework. When some incompleteness is introduced, the problem becomes however more difficult. Several approaches have been adopted in the literature to provide a satisfactory answer to this problem, for a particular choice criterion. In this paper, in order to price and hedge a non-tradable contingent claim, we first start with a (standard) utility maximization problem and end up with an equivalent risk measure minimization. This hedging problem can be seen as a particular case of a more general situation of risk transfer between different agents, one of them consisting of the financial market. In order to provide constructive answers to this general optimal risk transfer problem, both static and dynamic approaches are considered. When considering a dynamic framework, our main purpose is to find a trade-off between static and very abstract risk measures as we are more interested in tractability issues and interpretations of the dynamic risk measures we obtain rather than the ultimate general results. Therefore, after introducing a general axiomatic approach to dynamic risk measures, we relate the dynamic version of convex risk measures to BSDEs."
            },
            {
                "arxivId": "2101.11413",
                "title": "Quadratic BSDEs with convex generators and unbounded terminal conditions",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2107.12862",
        "category": "q-fin",
        "title": "Quasi-sure essential supremum and applications to finance",
        "abstract": "When uncertainty is modelled by a set of non-dominated and non-compact probability measures, a notion of essential supremum for a family of real-valued functions is developed in terms of upper semi-analytic functions. We show how the properties postulated on the initial functions carry over to their quasi-sure essential supremum. We propose various applications to financial problems with frictions. We analyse super-replication and prove a bi-dual characterization of the super-hedging cost. We also study a weak no-arbitrage condition called Absence of Instantaneous Profit (AIP) under which prices are finite. This requires new results on the aggregation of quasi-sure statements.",
        "references": [
            {
                "arxivId": "2104.02688",
                "title": "Pricing without no-arbitrage condition in discrete time",
                "abstract": null
            },
            {
                "arxivId": "1904.08780",
                "title": "No-arbitrage with multiple-priors in discrete time",
                "abstract": null
            },
            {
                "arxivId": "1812.11201",
                "title": "The Robust Superreplication Problem: A Dynamic Approach",
                "abstract": "In the frictionless discrete time financial market of Bouchard et al.(2015) we consider a trader who, due to regulatory requirements or internal risk management reasons, is required to hedge a claim $\\xi$ in a risk-conservative way relative to a family of probability measures $\\mathcal{P}$. We first describe the evolution of $\\pi_t(\\xi)$ - the superhedging price at time $t$ of the liability $\\xi$ at maturity $T$ - via a dynamic programming principle and show that $\\pi_t(\\xi)$ can be seen as a concave envelope of $\\pi_{t+1}(\\xi)$ evaluated at today's prices. Then we consider an optimal investment problem for a trader who is rolling over her robust superhedge and phrase this as a robust maximisation problem, where the expected utility of inter-temporal consumption is optimised subject to a robust superhedging constraint. This utility maximisation is carrried out under a new family of measures $\\mathcal{P}^u$, which no longer have to capture regulatory or institutional risk views but rather represent trader's subjective views on market dynamics. Under suitable assumptions on the trader's utility functions, we show that optimal investment and consumption strategies exist and further specify when, and in what sense, these may be unique."
            },
            {
                "arxivId": "1612.09103",
                "title": "Conditional nonlinear expectations",
                "abstract": null
            },
            {
                "arxivId": "1610.00999",
                "title": "Exponential utility maximization under model uncertainty for unbounded endowments",
                "abstract": "We consider the robust exponential utility maximization problem in discrete time: An investor maximizes the worst case expected exponential utility with respect to a family of non-dominated probabilistic models of her endowment by dynamically investing in a financial market. We show that, for any measurable random endowment (regardless of whether the problem is finite or not) an optimal strategy exists, a dual representation in terms of martingale measures holds true, and that the problem satisfies the dynamic programming principle."
            },
            {
                "arxivId": "1305.6008",
                "title": "Arbitrage and duality in nondominated discrete-time models",
                "abstract": "We consider a nondominated model of a discrete-time financial market where stocks are traded dynamically and options are available for static hedging. In a general measure-theoretic setting, we show that absence of arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of martingale measures. In the arbitrage-free case, we show that optimal superhedging strategies exist for general contingent claims, and that the minimal superhedging price is given by the supremum over the martingale measures. Moreover, we obtain a nondominated version of the Optional Decomposition Theorem."
            },
            {
                "arxivId": "1003.4431",
                "title": "Quasi-sure Stochastic Analysis through Aggregation",
                "abstract": "This paper is on developing stochastic analysis simultaneously under a general family of probability measures that are not dominated by a single probability measure. The interest in this question originates from the probabilistic representations of fully nonlinear partial differential equations and applications to mathematical finance. The existing literature relies either on the capacity theory (Denis and Martini), or on the underlying nonlinear partial differential equation (Peng). In both approaches, the resulting theory requires certain smoothness, the so-called quasi-sure continuity, of the corresponding processes and random variables in terms of the underlying canonical process. In this paper, we investigate this question for a larger class of ``non-smooth\" processes, but with a restricted family of non-dominated probability measures. For smooth processes, our approach leads to similar results as in previous literature, provided the restricted family satisfies an additional density property."
            },
            {
                "arxivId": "math/0607111",
                "title": "A THEORETICAL FRAMEWORK FOR THE PRICING OF CONTINGENT CLAIMS IN THE PRESENCE OF MODEL UNCERTAINTY",
                "abstract": "The aim of this work is to evaluate the cheapest superreplication price of a general (possibly path-dependent) European contingent claim in a context where the model is uncertain. This setting is a generalization of the uncertain volatility model (UVM) introduced in by Avellaneda, Levy and Paras. The uncertainty is specified by a family of martingale probability measures which may not be dominated. We obtain a partial characterization result and a full characterization which extends Avellaneda, Levy and Paras results in the UVM case."
            },
            {
                "arxivId": "math/0601699",
                "title": "Multi-dimensional G-Brownian motion and related stochastic calculus under G-expectation",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2112.14514",
        "category": "q-fin",
        "title": "Technology, Institution, and Regional Growth: Evidence from Mineral Mining Industry in Industrializing Japan",
        "abstract": "Coal extraction was an influential economic activity in interwar Japan. In the initial stage, coal mines used not only males but also females as the core workforce in the pits. However, the innovation of labor-saving technologies and the renewal of traditional extraction methodology induced institutional change through the revision of labor regulations on female miners in the early 1930s. This dramatically changed the mines as the place where skilled males were the principal miners engaged in the underground works. I investigate the impact of coal mining on regional growth and assess how the institutional change induced by the labor regulations affected its process. By linking the location information of mines with registration- and census-based statistics, I found that coal mines led to remarkable population growth. The labor regulations did not stagnate but accelerated the local population growth as they forced female miners to exit from the labor market and form their families. The regulations prohibited risky underground works by female miners. This reduction in occupational hazards also improved early-life mortality via the mortality selection mechanism in utero.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2210.02736",
        "category": "q-fin",
        "title": "Toward environmental sustainability: an empirical study on airports efficiency",
        "abstract": "The transition to more environmentally sustainable production processes and managerial practices is an increasingly important topic. Many industries need to undergo radical change to meet environmental sustainability requirements; the tourism industry is no exception. In this respect, a particular aspect that needs further attention is the relationship between airport performances and investments in environmental sustainability policies. This work represents a first attempt to provide empirical evidences about this relationship. Through the application of a non-parametrical method, we first assess the efficiency of the Italian airports industry. Secondly, we investigated the relationship between airports performance and management commitment toward the ecological transition using a Tobit regression model. The results show that airports adherence to formal multi-year ecological transition programs has a positive and consistent impact on their performance.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2211.15573",
        "category": "q-fin",
        "title": "Dynamic Equilibrium with Insider Information and General Uninformed Agent Utility",
        "abstract": "We study a continuous time economy where agents have asymmetric information. The informed agent (``$I$''), at time zero, receives a private signal about the risky assets' terminal payoff $\\Psi(X_T)$, while the uninformed agent (``$U$'') has no private signal. $\\Psi$ is an arbitrary payoff function, and $X$ follows a time-homogeneous diffusion. Crucially, we allow $U$ to have von Neumann-Morgenstern preferences with a general utility function on $(0,\\infty)$ satisfying the standard conditions. This extends previous constructions of equilibria with asymmetric information used when all agents have exponential utilities and enables us to study the impact of $U$'s initial share endowment on equilibrium. To allow for $U$ to have general preferences, we introduce a new method to prove existence of a partial communication equilibrium (PCE), where at time $0$, $U$ receives a less-informative signal than $I$. In the single asset case, this signal is recoverable by viewing the equilibrium price process over an arbitrarily short period of time, and hence the PCE is a dynamic noisy rational expectations equilibrium. Lastly, when $U$ has power (constant relative risk aversion) utility, we identify the equilibrium price in the small and large risk aversion limits.",
        "references": [
            {
                "arxivId": "1711.09567",
                "title": "Backward uniqueness for general parabolic operators in the whole space",
                "abstract": null
            },
            {
                "arxivId": "1508.03282",
                "title": "The strong predictable representation property in initially enlarged filtrations",
                "abstract": null
            },
            {
                "arxivId": "1505.05046",
                "title": "American Options with Asymmetric Information and Reflected BSDE",
                "abstract": "We consider an American contingent claim on a financial market where the buyer has additional information. Both agents (seller and buyer) observe the same prices, while the information available to them may differ due to some extra exogenous knowledge the buyer has. The buyer's information flow is modeled by an initial enlargement of the reference filtration. It seems natural to investigate the value of the American contingent claim with asymmetric information. We provide a representation for the cost of the additional information relying on some results on reflected backward stochastic differential equations (RBSDE). This is done by using an interpretation of prices of American contingent claims with extra information for the buyer by solutions of appropriate RBSDE."
            },
            {
                "arxivId": "1110.3248",
                "title": "Integral representation of martingales motivated by the problem of endogenous completeness in financial economics",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2309.10152",
        "category": "q-fin",
        "title": "Sparse Index Tracking: Simultaneous Asset Selection and Capital Allocation via \ud835\udcc10-Constrained Portfolio",
        "abstract": "Sparse index tracking is a prominent passive portfolio management strategy that constructs a sparse portfolio to track a financial index. A sparse portfolio is preferable to a full portfolio in terms of reducing transaction costs and avoiding illiquid assets. To achieve portfolio sparsity, conventional studies have utilized $\\ell_p$-norm regularizations as a continuous surrogate of the $\\ell_0$-norm regularization. Although these formulations can construct sparse portfolios, their practical application is challenging due to the intricate and time-consuming process of tuning parameters to define the precise upper limit of assets in the portfolio. In this paper, we propose a new problem formulation of sparse index tracking using an $\\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. Moreover, our approach offers a choice between constraints on portfolio and turnover sparsity, further reducing transaction costs by limiting asset updates at each rebalancing interval. Furthermore, we develop an efficient algorithm for solving this problem based on a primal-dual splitting method. Finally, we illustrate the effectiveness of the proposed method through experiments on the S&P500 and Russell3000 index datasets.",
        "references": [
            {
                "arxivId": "2207.11050",
                "title": "Graph Spatio-Spectral Total Variation Model for Hyperspectral Image Denoising",
                "abstract": "The spatio-spectral total variation (SSTV) model has been widely used as an effective regularization of hyperspectral images (HSIs) for various applications such as mixed noise removal. However, since SSTV computes local spatial differences uniformly, it is difficult to remove noise while preserving complex spatial structures with fine edges and textures, especially in situations of high noise intensity. To solve this problem, we propose a new TV-type regularization called graph-SSTV (GSSTV), which generates a graph explicitly reflecting the spatial structure of the target HSI from noisy HSIs and incorporates a weighted spatial difference operator designed based on this graph. Furthermore, we formulate the mixed noise removal problem as a convex optimization problem involving GSSTV and develop an efficient algorithm based on the primal-dual splitting method to solve this problem. Finally, we demonstrate the effectiveness of GSSTV compared with existing HSI regularization models through experiments on mixed noise removal. The source code will be available at https://www.mdi.c.titech.ac.jp/publications/gsstv."
            },
            {
                "arxivId": "2104.02845",
                "title": "A General Destriping Framework for Remote Sensing Images Using Flatness Constraint",
                "abstract": "Removing stripe noise, i.e., destriping, from remote sensing images is an essential task in terms of visual quality and subsequent processing. Most existing destriping methods are designed by combining a particular image regularization with a stripe noise characterization that cooperates with the regularization, which precludes us to examine and activate different regularizations to adapt to various target images. To resolve this, two requirements need to be considered: a general framework that can handle a variety of image regularizations in destriping, and a strong stripe noise characterization that can consistently capture the nature of stripe noise, regardless of the choice of image regularization. To this end, this article proposes a general destriping framework using a newly introduced stripe noise characterization, named flatness constraint (FC), where we can handle various regularization functions in a unified manner. Specifically, we formulate the destriping problem as a nonsmooth convex optimization problem involving a general form of image regularization and the FC. The constraint mathematically models that the intensity of each stripe is constant along one direction, resulting in a strong characterization of stripe noise. For solving the optimization problem, we also develop an efficient algorithm based on a diagonally preconditioned primal-dual splitting algorithm (DP-PDS), which can automatically adjust the step sizes. The effectiveness of our framework is demonstrated through destriping experiments, where we comprehensively compare combinations of a variety of image regularizations and stripe noise characterizations using hyperspectral images (HSIs) and infrared (IR) videos."
            },
            {
                "arxivId": "2008.04565",
                "title": "Epigraphical Relaxation for Minimizing Layered Mixed Norms",
                "abstract": "This paper proposes an epigraphical relaxation (ERx) technique for non-proximable mixed norm minimization. Mixed norm regularization methods play a central role in signal reconstruction and processing, where their optimization relies on the fact that the proximity operators of the mixed norms can be computed efficiently. To bring out the power of regularization, sophisticated layered modeling of mixed norms that can capture inherent signal structure is a key ingredient, but the proximity operator of such a mixed norm is often unavailable (non-proximable). Our ERx decouples a layered non-proximable mixed norm into a norm and multiple epigraphical constraints. This enables us to handle a wide range of non-proximable mixed norms in optimization, as long as both the proximal operator of the outermost norm and the projection onto each epigraphical constraint are efficiently computable. Moreover, under mild conditions, we prove that ERx does not change the minimizer of the original problem despite relaxing equality constraints into inequality ones. We also develop new regularizers based on ERx:one is decorrelated structure-tensor total variation for color image restoration, and the other is amplitude-spectrum nuclear norm for low-rank amplitude recovery. We examine the power of these regularizers through experiments, which illustrates the utility of ERx."
            },
            {
                "arxivId": "1410.1390",
                "title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems",
                "abstract": "In this paper, we analyze the behavior of the alternating direction method of multipliers (ADMM), for solving a family of nonconvex problems. Our focus is given to the well-known consensus and sharing problems, both of which have wide applications in signal processing. We show that in the presence of nonconvex objective function, classical ADMM is able to reach the set of stationary solutions for these problems, if the stepsize is chosen large enough. An interesting consequence of our analysis is that the ADMM is convergent for a family of sharing problems, regardless of the number of blocks or the convexity of the objective function. Our analysis is broadly applicable to many ADMM variants involving proximal update rules and various flexible block selection rules."
            },
            {
                "arxivId": "1406.5429",
                "title": "Playing with Duality: An overview of recent primal?dual approaches for solving large-scale optimization problems",
                "abstract": "Optimization methods are at the core of many problems in signal/image processing, computer vision, and machine learning. For a long time, it has been recognized that looking at the dual of an optimization problem may drastically simplify its solution. However, deriving efficient strategies that jointly bring into play the primal and dual problems is a more recent idea that has generated many important new contributions in recent years. These novel developments are grounded in the recent advances in convex analysis, discrete optimization, parallel processing, and nonsmooth optimization with an emphasis on sparsity issues. In this article, we aim to present the principles of primal-dual approaches while providing an overview of the numerical methods that have been proposed in different contexts. Last but not least, primal-dual methods lead to algorithms that are easily parallelizable. Today, such parallel algorithms are becoming increasingly important for efficiently handling high-dimensional problems."
            },
            {
                "arxivId": "1110.1697",
                "title": "A splitting algorithm for dual monotone inclusions involving cocoercive operators",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2311.13802",
        "category": "q-fin",
        "title": "On the Relevance and Appropriateness of Name Concentration Risk Adjustments for Portfolios of Multilateral Development Banks",
        "abstract": "Sovereign loan portfolios of Multilateral Development Banks (MDBs) typically consist of only a small number of borrowers and hence are heavily exposed to single name concentration risk. Based on realistic MDB portfolios constructed from publicly available data, this paper quantifies the magnitude of the exposure to name concentration risk using exact Monte Carlo simulations. In comparing the exact adjustment for name concentration risk to its analytic approximation as currently applied by the major rating agency Standard&Poor's, we further investigate whether current capital adequacy frameworks for MDBs are overly conservative. Finally, we discuss the choice of appropriate model parameters and their impact on measures of name concentration risk.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2312.08927",
        "category": "q-fin",
        "title": "Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process",
        "abstract": "Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock's LOB in the NASDAQ exchange and compare them against several baselines. Finally, we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated.",
        "references": [
            {
                "arxivId": "2402.17359",
                "title": "Limit Order Book Simulations: A Review",
                "abstract": "Limit Order Books (LOBs) serve as a mechanism for buyers and sellers to interact with each other in the financial markets. Modelling and simulating LOBs is quite often necessary for calibrating and fine-tuning the automated trading strategies developed in algorithmic trading research. The recent AI revolution and availability of faster and cheaper compute power has enabled the modelling and simulations to grow richer and even use modern AI techniques. In this review we examine the various kinds of LOB simulation models present in the current state of the art. We provide a classification of the models on the basis of their methodology and provide an aggregate view of the popular stylized facts used in the literature to test the models. We additionally provide a focused study of price impact's presence in the models since it is one of the more crucial phenomena to model in algorithmic trading. Finally, we conduct a comparative analysis of various qualities of fits of these models and how they perform when tested against empirical data."
            },
            {
                "arxivId": "2307.09077",
                "title": "Estimation of an Order Book Dependent Hawkes Process for Large Datasets",
                "abstract": "\n A point process for event arrivals in high-frequency trading is presented. The intensity is the product of a Hawkes process and high-dimensional functions of covariates derived from the order book. Conditions for stationarity of the process are stated. An algorithm is presented to estimate the model even in the presence of billions of data points, possibly mapping covariates into a high-dimensional space. Large sample sizes can be common for high-frequency data applications using multiple instruments. Consistency results under weak conditions are established. A test statistic to assess out of sample performance of different model specifications is suggested. The methodology is applied to the study of four stocks that trade on the New York Stock Exchange. The out of sample testing procedure suggests that capturing the nonlinearity of the order book information adds value to the self-exciting nature of high-frequency trading events."
            },
            {
                "arxivId": "2201.10173",
                "title": "Modeling bid and ask price dynamics with an extended Hawkes process and its empirical applications for high-frequency stock market data",
                "abstract": "This study proposes a versatile model for the dynamics of the best bid and ask prices using an extended Hawkes process. The model incorporates the zero intensities of the spreadnarrowing processes at the minimum bid-ask spread, spread-dependent intensities, possible negative excitement, and nonnegative intensities. We apply the model to high-frequency best bid and ask price data from US stock markets. The empirical findings demonstrate a spread-narrowing tendency, excitations of the intensities caused by previous events, the impact of flash crashes, characteristic trends in fast trading over time, and the different features of market participants in the various exchanges."
            },
            {
                "arxivId": "2109.15110",
                "title": "Deep Hawkes process for high-frequency market making",
                "abstract": null
            },
            {
                "arxivId": "1906.05420",
                "title": "From asymptotic properties of general point processes to the ranking of financial agents",
                "abstract": "We propose a general non-linear order book model that is built from the individual behaviours of the agents. Our framework encompasses Markovian and Hawkes based models. Under mild assumptions, we prove original results on the ergodicity and diffusivity of such system. Then we provide closed form formulas for various quantities of interest: stationary distribution of the best bid and ask quantities, spread, liquidity fluctuations and price volatility. These formulas are expressed in terms of individual order flows of market participants. Our approach enables us to establish a ranking methodology for the market makers with respect to the quality of their trading."
            },
            {
                "arxivId": "1901.08938",
                "title": "Queue-reactive Hawkes models for the order flow",
                "abstract": "In this work we introduce two variants of multivariate Hawkes models with an explicit dependency on various queue sizes aimed at modeling the stochastic time evolution of a limit order book. The models we propose thus integrate the influence of both the current book state and the past order flow. The first variant considers the flow of order arrivals at a specific price level as independent from the other one and describes this flow by adding a Hawkes component to the arrival rates provided by the continuous time Markov \"Queue Reactive\" model of Huang et al. Empirical calibration using Level-I order book data from Eurex future assets (Bund and DAX) show that the Hawkes term dramatically improves the pure \"Queue-Reactive\" model not only for the description of the order flow properties (as e.g. the statistics of inter-event times) but also with respect to the shape of the queue distributions. The second variant we introduce describes the joint dynamics of all events occurring at best bid and ask sides of some order book during a trading day. This model can be considered as a queue dependent extension of the multivariate Hawkes order-book model of Bacry et al. We provide an explicit way to calibrate this model either with a Maximum-Likelihood method or with a Least-Square approach. Empirical estimation from Bund and DAX level-I order book data allow us to recover the main features of Hawkes interactions uncovered in Bacry et al. but also to unveil their joint dependence on bid and ask queue sizes. We notably find that while the market order or mid-price changes rates can mainly be functions on the volume imbalance this is not the case for the arrival rate of limit or cancel orders. Our findings also allows us to clearly bring to light various features that distinguish small and large tick assets."
            },
            {
                "arxivId": "1809.08060",
                "title": "State-dependent Hawkes processes and their application to limit order book modelling",
                "abstract": "We study statistical aspects of state-dependent Hawkes processes, which are an extension of Hawkes processes where a self- and cross-exciting counting process and a state process are fully coupled, interacting with each other. The excitation kernel of the counting process depends on the state process that, reciprocally, switches state when there is an event in the counting process. We first establish the existence and uniqueness of state-dependent Hawkes processes and explain how they can be simulated. Then we develop maximum likelihood estimation methodology for parametric specifications of the process. We apply state-dependent Hawkes processes to high-frequency limit order book data, allowing us to build a novel model that captures the feedback loop between the order flow and the shape of the limit order book. We estimate two specifications of the model, using the bid\u2013ask spread and the queue imbalance as state variables, and find that excitation effects in the order flow are strongly state-dependent. Additionally, we find that the endogeneity of the order flow, measured by the magnitude of excitation, is also state-dependent, being more pronounced in disequilibrium states of the limit order book."
            },
            {
                "arxivId": "1806.05101",
                "title": "Order-book modelling and market making strategies",
                "abstract": "Market making is one of the most important aspects of algorithmic trading, and it has been studied quite extensively from a theoretical point of view. The practical implementation of so-called \"optimal strategies\" however suffers from the failure of most order book models to faithfully reproduce the behaviour of real market participants. \nThis paper is twofold. First, some important statistical properties of order driven markets are identified, advocating against the use of purely Markovian order book models. Then, market making strategies are designed and their performances are compared, based on simulation as well as backtesting. We find that incorporating some simple non-Markovian features in the limit order book greatly improves the performances of market making strategies in a realistic context."
            },
            {
                "arxivId": "1710.03506",
                "title": "A buffer Hawkes process for limit order books",
                "abstract": "We introduce a Markovian single point process model, with random intensity regulated through a buffer mechanism and a self-exciting effect controlling the arrival stream to the buffer. The model applies the principle of the Hawkes process in which point process jumps generate a shot-noise intensity field. Unlike the Hawkes case, the intensity field is fed into a separate buffer, the size of which is the driving intensity of new jumps. In this manner, the intensity loop portrays mutual-excitation of point process events and buffer size dynamics. This scenario is directly applicable to the market evolution of limit order books, with buffer size being the current number of limit orders and the jumps representing the execution of market orders. We give a branching process representation of the point process and prove that the scaling limit is Brownian motion with explicit volatility."
            },
            {
                "arxivId": "1709.01292",
                "title": "A Scaling Limit for Limit Order Books Driven by Hawkes Processes",
                "abstract": "In this paper we derive a scaling limit for an infinite dimensional limit order book model driven by Hawkes random measures. The dynamics of the incoming order flow is allowed to depend on the current market price as well as on a volume indicator. With our choice of scaling the dynamics converges to a coupled SDE-ODE system where limiting best bid and ask price processes follows a diffusion dynamics, the limiting volume density functions follows an ODE in a Hilbert space and the limiting order arrival and cancellation intensities follow a Volterra-Fredholm integral equation."
            },
            {
                "arxivId": "1707.03003",
                "title": "Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling",
                "abstract": "Tick is a statistical learning library for Python~3, with a particular emphasis on time-dependent models, such as point processes, and tools for generalized linear models and survival analysis. The core of the library is an optimization module providing model computational classes, solvers and proximal operators for regularization. tick relies on a C++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from this https URL"
            },
            {
                "arxivId": "1607.06333",
                "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants",
                "abstract": "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each nodes of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the third-order integrated cumulants of the process. We show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the MemeTracker database."
            },
            {
                "arxivId": "1602.07663",
                "title": "The role of volume in order book dynamics: a multivariate Hawkes process analysis",
                "abstract": "Abstract We show that multivariate Hawkes processes coupled with the nonparametric estimation procedure first proposed in Bacry and Muzy [IEEE Trans. Inform. Theory, 2016, 62, 2184\u20132202] can be successfully used to study complex interactions between the time of arrival of orders and their size observed in a limit order book market. We apply this methodology to high-frequency order book data of futures traded at EUREX. Specifically, we demonstrate how this approach is amenable not only to analyse interplay between different order types (market orders, limit orders, cancellations) but also to include other relevant quantities, such as the order size, into the analysis, showing also that simple models assuming the independence between volume and time are not suitable to describe the data."
            },
            {
                "arxivId": "1509.02017",
                "title": "An estimation procedure for the Hawkes process",
                "abstract": "In this paper, we present a nonparametric estimation procedure for the multivariate Hawkes point process. The timeline is cut into bins and\u2014for each component process\u2014the number of points in each bin is counted. As a consequence of earlier results in Kirchner [Stoch. Process. Appl., 2016, 162, 2494\u20132525], the distribution of the resulting \u2018bin-count sequences\u2019 can be approximated by an integer-valued autoregressive model known as the (multivariate) INAR(p) model. We represent the INAR(p) model as a standard vector-valued linear autoregressive time series with white-noise innovations (VAR(p)). We establish consistency and asymptotic normality for conditional least-squares estimation of the VAR(p), respectively, the INAR(p) model. After appropriate scaling, these time-series estimates yield estimates for the underlying multivariate Hawkes process as well as corresponding variance estimates. The estimates depend on a bin-size and a support s. We discuss the impact and the choice of these parameters. All results are presented in such a way that computer implementation, e.g. in R, is straightforward. Simulation studies confirm the effectiveness of our estimation procedure. In the second part of the paper, we present a data example where the method is applied to bivariate event-streams in financial limit-order-book data. We fit a bivariate Hawkes model on the joint process of limit and market order arrivals. The analysis exhibits a remarkably asymmetric relation between the two component processes: incoming market orders excite the limit-order flow heavily whereas the market-order flow is hardly affected by incoming limit orders. For the estimated excitement functions, we observe power-law shapes, inhibitory effects for lags under 0.003 s, second periodicities and local maxima at 0.01, 0.1 and 0.5 s."
            },
            {
                "arxivId": "1502.04592",
                "title": "Hawkes Processes in Finance",
                "abstract": "In this paper we propose an overview of the recent academic literature devoted to the applications of Hawkes processes in finance. Hawkes processes constitute a particular class of multivariate point processes that has become very popular in empirical high-frequency finance this last decade. After a reminder of the main definitions and properties that characterize Hawkes processes, we review their main empirical applications to address many different problems in high-frequency finance. Because of their great flexibility and versatility, we show that they have been successfully involved in issues as diverse as estimating the volatility at the level of transaction data, estimating the market stability, accounting for systemic risk contagion, devising optimal execution strategies or capturing the dynamics of the full order book."
            },
            {
                "arxivId": "1301.5007",
                "title": "Ergodicity and Scaling Limit of a Constrained Multivariate Hawkes Process",
                "abstract": "We introduce a multivariate Hawkes process with constraints on its conditional density. It is a multivariate point process with conditional intensity similar to that of a multivariate Hawkes process but certain events are forbidden with respect to boundary conditions on a multidimensional constraint variable, whose evolution is driven by the point process. We study this process in the special case where the fertility function is exponential so that the process is entirely described by an underlying Markov chain, which includes the constraint variable. Some conditions on the parameters are established to ensure the ergodicity of the chain. Moreover, scaling limits are derived for the integrated point process. This study is primarily motivated by the stochastic modelling of a limit order book for high frequency financial data analysis."
            },
            {
                "arxivId": "1012.0349",
                "title": "Limit order books",
                "abstract": "Abstract Limit order books (LOBs) match buyers and sellers in more than half of the world\u2019s financial markets. This survey highlights the insights that have emerged from the wealth of empirical and theoretical studies of LOBs. We examine the findings reported by statistical analyses of historical LOB data and discuss how several LOB models provide insight into certain aspects of the mechanism. We also illustrate that many such models poorly resemble real LOBs and that several well-established empirical facts have yet to be reproduced satisfactorily. Finally, we identify several key unresolved questions about LOBs."
            },
            {
                "arxivId": "1003.3796",
                "title": "\"Market making\" behaviour in an order book model and its impact on the bid-ask spread",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2402.17684",
        "category": "q-fin",
        "title": "Stochastic expansion for the pricing of Asian and basket options",
        "abstract": "We present closed analytical approximations for the pricing of basket options, also applicable to Asian options with discrete averaging under the Black-Scholes model with time-dependent parameters. The formulae are obtained by using a stochastic Taylor expansion around a log-normal proxy model and are found to be highly accurate for Asian options in practice as well as for vanilla options with discrete dividends.",
        "references": [
            {
                "arxivId": "2106.12971",
                "title": "The Pricing of Vanilla Options with Cash Dividends as a Classic Vanilla Basket Option Problem",
                "abstract": "In the standard Black-Scholes-Merton framework, dividends are represented as a continuous dividend yield and the pricing of Vanilla options on a stock is achieved through the well-known Black-Scholes formula. In reality however, stocks pay a discrete fixed cash dividend at each dividend ex-date. This leads to the so-called piecewise lognormal model, where the asset jumps from a fixed known amount at each dividend date. There is however no exact closed-form formula for the pricing of Vanilla options under this model. Approximations must be used. While there exists many approximations taylored to this specific problem in the litterature, this paper explores the use of existing well-known basket option formulas for the pricing of European options on a single asset with cash dividends in the piecewise lognormal model."
            },
            {
                "arxivId": "2106.12051",
                "title": "More Stochastic Expansions for the Pricing of Vanilla Options with Cash Dividends",
                "abstract": "There is no exact closed form formula for pricing of European options with discrete cash dividends under the model where the underlying asset price follows a piecewise lognormal process with jumps at dividend ex-dates. This paper presents alternative expansions based on the technique of Etore and Gobet, leading to more robust first, second and third order expansions accross the range of strikes and the range of dividend dates."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.06253",
        "category": "q-fin",
        "title": "Entropy corrected geometric Brownian motion",
        "abstract": "The geometric Brownian motion (GBM) is widely employed for modeling stochastic processes, yet its solutions are characterized by the log-normal distribution. This comprises predictive capabilities of GBM mainly in terms of forecasting applications. Here, entropy corrections to GBM are proposed to go beyond log-normality restrictions and better account for intricacies of real systems. It is shown that GBM solutions can be effectively refined by arguing that entropy is reduced when deterministic content of considered data increases. Notable improvements over conventional GBM are observed for several cases of non-log-normal distributions, ranging from a dice roll experiment to real world data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.10631",
        "category": "q-fin",
        "title": "Default Resilience and Worst-Case Effects in Financial Networks",
        "abstract": "In this paper we analyze the resilience of a network of banks to joint price fluctuations of the external assets in which they have shared exposures, and evaluate the worst-case effects of the possible default contagion. Indeed, when the prices of certain external assets either decrease or increase, all banks exposed to them experience varying degrees of simultaneous shocks to their balance sheets. These coordinated and structured shocks have the potential to exacerbate the likelihood of defaults. In this context, we introduce first a concept of {default resilience margin}, $\\epsilon^*$, i.e., the maximum amplitude of asset prices fluctuations that the network can tolerate without generating defaults. Such threshold value is computed by considering two different measures of price fluctuations, one based on the maximum individual variation of each asset, and the other based on the sum of all the asset's absolute variations. For any price perturbation having amplitude no larger than $\\epsilon^*$, the network absorbs the shocks remaining default free. When the perturbation amplitude goes beyond $\\epsilon^*$, however, defaults may occur. In this case we find the worst-case systemic loss, that is, the total unpaid debt under the most severe price variation of given magnitude. Computation of both the threshold level $\\epsilon^*$ and of the worst-case loss and of a corresponding worst-case asset price scenario, amounts to solving suitable linear programming problems.}",
        "references": [
            {
                "arxivId": "2103.10872",
                "title": "Optimal Clearing Payments in a Financial Contagion Model",
                "abstract": "Financial networks are characterized by complex structures of mutual obligations. These obligations are fulfilled entirely or in part (when defaults occur) via a mechanism called clearing, which determines a set of payments that settle the claims by respecting rules such as limited liability, absolute priority, and proportionality (pro-rated payments). In the presence of shocks on the financial system, however, the clearing mechanism may lead to cascaded defaults and eventually to financial disaster. In this paper, we first study the clearing model under pro-rated payments of Eisenberg and Noe, and we derive novel necessary and sufficient conditions for the uniqueness of the clearing payments, valid for an arbitrary topology of the financial network. Then, we argue that the proportionality rule is one of the factors responsible for cascaded defaults, and that the overall system loss can be reduced if this rule is lifted. The proposed approach thus shifts the focus from the individual interest to the overall system's interest to control and contain adverse effects of cascaded failures, and we show that clearing payments in this setting can be computed by solving suitable convex optimization problems."
            },
            {
                "arxivId": "1912.04815",
                "title": "Equilibria and Systemic Risk in Saturated Networks",
                "abstract": "We undertake a fundamental study of network equilibria modeled as solutions of fixed-point equations for monotone linear functions with saturation nonlinearities. The considered model extends one originally proposed to study systemic risk in networks of financial institutions interconnected by mutual obligations. It is one of the simplest continuous models accounting for shock propagation phenomena and cascading failure effects. This model also characterizes Nash equilibria of constrained quadratic network games with strategic complementarities. We first derive explicit expressions for network equilibria and prove necessary and sufficient conditions for their uniqueness, encompassing and generalizing results available in the literature. Then, we study jump discontinuities of the network equilibria when the exogenous flows cross certain regions of measure 0 representable as graphs of continuous functions. Finally, we discuss some implications of our results in the two main motivating applications. In financial networks, this bifurcation phenomenon is responsible for how small shocks in the assets of a few nodes can trigger major aggregate losses to the system and cause the default of several agents. In constrained quadratic network games, it induces a blow-up behavior of the sensitivity of Nash equilibria with respect to the individual benefits."
            },
            {
                "arxivId": "1909.09239",
                "title": "Systemic cascades on inhomogeneous random financial networks",
                "abstract": null
            },
            {
                "arxivId": "1903.08367",
                "title": "Computation of Systemic Risk Measures: A Mixed-Integer Programming Approach",
                "abstract": "Mixed-Integer Programming Formulations for Systemic Risk Measures Measurement and allocation of systemic risk have become important tasks in interconnected financial networks. In \u201cComputation of Systemic Risk Measures: A Mixed-Integer Programming Approach,\u201d Ararat and Meimanjan consider a clearing model that considers external assets and business costs of a bank simultaneously by extending the Rogers\u2013Veraart network model with default costs. They prove that clearing payment vectors can be calculated by solving mixed-integer programming problems in which the existence of business costs and default costs are represented by binary variables. The systemic risk measure corresponding to this model yields nonconvex sets of capital allocation vectors. The authors compute these nonconvex sets by solving a multiobjective optimization problem whose scalarizations are two-stage mixed-integer programming problems with an expectation constraint. The computational procedure is versatile enough to accommodate other applications of systemic risk where the risk factors are aggregated by solving a general mixed-integer programming problem."
            },
            {
                "arxivId": "1810.01372",
                "title": "Pricing of Debt and Equity in a Financial Network with Comonotonic Endowments",
                "abstract": "Valuing Debt and Equity in Interbank Networks Valuation adjustments to account for, for example, counterparty risk, have become an important part of derivative valuation by any bank since the 2007\u20132009 financial crisis. As evidenced by that crisis, considering the risk of a single firm alone can cause gross misspecification of firm health. In \u201cPricing of Debt and Equity in a Financial Network with Comonotonic Endowments,\u201d Banerjee and Feinstein construct an analytical formulation for a valuation adjustment that takes the entire network of counterparty obligations into account when all institutions have comonotonic endowments. From the perspective of stress testing, such a setting corresponds to a systematic shock to all firms. This comonotonic setting is then shown to provide computationally tractable upper and lower bounds to the general network valuation problem. The authors demonstrate that the comonotonic endowment setting arises endogenously in a system of equity maximizing firms."
            },
            {
                "arxivId": "1805.08544",
                "title": "Impact of contingent payments on systemic risk in financial networks",
                "abstract": null
            },
            {
                "arxivId": "1708.01561",
                "title": "Sensitivity of the Eisenberg-Noe Clearing Vector to Individual Interbank Liabilities",
                "abstract": "We quantify the sensitivity of the Eisenberg-Noe clearing vector to estimation errors in the bilateral liabilities of a financial system. The interbank liabilities matrix is a crucial input to the computation of the clearing vector. However, in practice central bankers and regulators must often estimate this matrix because complete information on bilateral liabilities is rarely available. As a result, the clearing vector may suffer from estimation errors in the liabilities matrix. We quantify the clearing vector's sensitivity to such estimation errors and show that its directional derivatives are, like the clearing vector itself, solutions of fixed point equations. We describe estimation errors utilizing a basis for the space of matrices representing permissible perturbations and derive analytical solutions to the maximal deviations of the Eisenberg-Noe clearing vector. This allows us to compute upper bounds for the worst case perturbations of the clearing vector. Moreover, we quantify the probability of observing clearing vector deviations of a certain magnitude, for uniformly or normally distributed errors in the relative liability matrix. Applying our methodology to a dataset of European banks, we find that perturbations to the relative liabilities can result in economically sizeable differences that could lead to an underestimation of the risk of contagion. Importantly, our results allow regulators to bound the error of their simulations."
            },
            {
                "arxivId": "1602.05883",
                "title": "Pathways towards instability in financial networks",
                "abstract": null
            },
            {
                "arxivId": "1506.00937",
                "title": "Financial contagion and asset liquidation strategies",
                "abstract": null
            },
            {
                "arxivId": "2106.01946",
                "title": "Convex optimization",
                "abstract": "This textbook is based on lectures given by the authors at MIPT (Moscow), HSE (Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all, the authors focused on the program of a two-semester course of lectures on convex optimization, which is given to students of MIPT. The first chapter of this book contains the materials of the first semester (\"Fundamentals of convex analysis and optimization\"), the second and third chapters contain the materials of the second semester (\"Numerical methods of convex optimization\"). The textbook has a number of features. First, in contrast to the classic manuals, this book does not provide proofs of all the theorems mentioned. This allowed, on one side, to describe more themes, but on the other side, made the presentation less self-sufficient. The second important point is that part of the material is advanced and is published in the Russian educational literature, apparently for the first time. Third, the accents that are given do not always coincide with the generally accepted accents in the textbooks that are now popular. First of all, we talk about a sufficiently advanced presentation of conic optimization, including robust optimization, as a vivid demonstration of the capabilities of modern convex analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.10652",
        "category": "q-fin",
        "title": "Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization",
        "abstract": "In an effort to improve the accuracy of credit lending decisions, many financial intuitions are now using predictions from machine learning models. While such predictions enjoy many advantages, recent research has shown that the predictions have the potential to be biased and unfair towards certain subgroups of the population. To combat this, several techniques have been introduced to help remove the bias and improve the overall fairness of the predictions. We introduce a new fairness technique, called \\textit{Subgroup Threshold Optimizer} (\\textit{STO}), that does not require any alternations to the input training data nor does it require any changes to the underlying machine learning algorithm, and thus can be used with any existing machine learning pipeline. STO works by optimizing the classification thresholds for individual subgroups in order to minimize the overall discrimination score between them. Our experiments on a real-world credit lending dataset show that STO can reduce gender discrimination by over 90\\%.",
        "references": [
            {
                "arxivId": "1908.09635",
                "title": "A Survey on Bias and Fairness in Machine Learning",
                "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields."
            },
            {
                "arxivId": "1905.10546",
                "title": "Protecting the Protected Group: Circumventing Harmful Fairness",
                "abstract": "The recent literature on fair Machine Learning manifests that the choice of fairness constraints must be driven by the utilities of the population. However, virtually all previous work makes the unrealistic assumption that the exact underlying utilities of the population (representing private tastes of individuals) are known to the regulator that imposes the fairness constraint. In this paper we initiate the discussion of the \\emph{mismatch}, the unavoidable difference between the underlying utilities of the population and the utilities assumed by the regulator. We demonstrate that the mismatch can make the disadvantaged protected group worse off after imposing the fairness constraint and provide tools to design fairness constraints that help the disadvantaged group despite the mismatch."
            },
            {
                "arxivId": "1803.03239",
                "title": "Fairness Through Computationally-Bounded Awareness",
                "abstract": "We study the problem of fair classification within the versatile framework of Dwork et al. [ITCS 2012], which assumes the existence of a metric that measures similarity between pairs of individuals. Unlike previous works on metric-based fairness, we do not assume that the entire metric is known to the learning algorithm. Instead, we study the setting where a learning algorithm can query this metric a bounded number of times to ascertain similarities between particular pairs of individuals. For example, the queries might be answered by a panel of specialists spanning social scientists, statisticians, demographers, and ethicists. \nWe propose \"metric multifairness,\" a new definition of fairness that is parameterized by a similarity metric $\\delta$ on pairs of individuals and a collection ${\\cal C}$ of\"comparison sets\" over pairs of individuals. One way to view this collection is as the family of comparisons that can be expressed within some computational bound. With this interpretation, metric multifairnesss loosely guarantees that similar subpopulations are treated similarly, as long as these subpopulations can be identified within this bound. In particular, metric multifairness implies that a rich class of subpopulations are protected from a multitude of discriminatory behaviors. \nWe provide a general-purpose framework for learning a metric multifair hypothesis that achieves near-optimal loss from a small number of random samples from the metric $\\delta$. We study the sample complexity and time complexity of learning a metric multifair hypothesis (providing rather tight upper and lower bounds) by connecting it to the task of learning the class ${\\cal C}$. In particular, if the class ${\\cal C}$ admits an efficient agnostic learner, then we can learn such a metric multifair hypothesis efficiently."
            },
            {
                "arxivId": "1711.05144",
                "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
                "abstract": "The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent \"fairness gerrymandering\", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. \nWe then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets."
            },
            {
                "arxivId": "1711.00399",
                "title": "Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR",
                "abstract": "There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system."
            },
            {
                "arxivId": "1610.02413",
                "title": "Equality of Opportunity in Supervised Learning",
                "abstract": "We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv."
            },
            {
                "arxivId": "1607.06520",
                "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
            },
            {
                "arxivId": "1104.3913",
                "title": "Fairness through awareness",
                "abstract": "We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.11622",
        "category": "q-fin",
        "title": "Asset management with an ESG mandate",
        "abstract": "We investigate the portfolio frontier and risk premia in equilibrium when an institutional investor aims to minimize the tracking error variance and to attain an ESG score higher than the benchmark's one (ESG mandate). Provided that a negative ESG premium for stocks is priced by the market, we show that an ESG mandate can reduce the mean-variance inefficiency of the portfolio frontier when the asset manager targets a limited over-performance with respect to the benchmark. Instead, for a high over-performance target, an ESG mandate leads to a higher variance. The mean-variance improvement is due to the fact that the ESG mandate induces the asset manager to over-invest in assets with a high mean-standard deviation ratio. In equilibrium, with asset managers and mean-variance investors, a negative ESG premium arises if the ESG mandate is binding for asset managers. A result that is borne out by the data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.11680",
        "category": "q-fin",
        "title": "Multiscale Orientation Values for Biodiversity, Climate and Water: A Scientific Input for Science- Based Targets",
        "abstract": "In this study, we explore a range of options and outcomes associated with using different allocation approaches to operationalise the Planetary Boundaries (PB) framework at the country, sector, and city scales. We demonstrate: (i) how to translate the PB framework into various sub-global scales (countries, cities, industries); and (ii) how to take global/local aspects (e.g., water use at the watershed level) into account. Finally, we apply the proposed methodology to derive country, city, and sector-specific budgets consistent with the PB concept for Switzerland. We then benchmark the translated PBs for climate, biodiversity, and freshwater use against actual environmental pressures in Switzerland from both production- and consumption-based perspectives. This effectively enables us to provide a comprehensive assessment of whether Switzerland is living within its safe operating space.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.11738",
        "category": "q-fin",
        "title": "A path-dependent PDE solver based on signature kernels",
        "abstract": "We develop a provably convergent kernel-based solver for path-dependent PDEs (PPDEs). Our numerical scheme leverages signature kernels, a recently introduced class of kernels on path-space. Specifically, we solve an optimal recovery problem by approximating the solution of a PPDE with an element of minimal norm in the signature reproducing kernel Hilbert space (RKHS) constrained to satisfy the PPDE at a finite collection of collocation paths. In the linear case, we show that the optimisation has a unique closed-form solution expressed in terms of signature kernel evaluations at the collocation paths. We prove consistency of the proposed scheme, guaranteeing convergence to the PPDE solution as the number of collocation points increases. Finally, several numerical examples are presented, in particular in the context of option pricing under rough volatility. Our numerical scheme constitutes a valid alternative to the ubiquitous Monte Carlo methods.",
        "references": [
            {
                "arxivId": "2302.04586",
                "title": "New directions in the applications of rough path theory",
                "abstract": "This article provides a concise overview of some of the recent advances in the application of rough path theory to machine learning. Controlled differential equations (CDEs) are discussed as the key mathematical model to describe the interaction of a stream with a physical control system. A collection of iterated integrals known as the signature naturally arises in the description of the response produced by such interactions. The signature comes equipped with a variety of powerful properties rendering it an ideal feature map for streamed data. We summarise recent advances in the symbiosis between deep learning and CDEs, studying the link with RNNs and culminating with the Neural CDE model. We concluded with a discussion on signature kernel methods."
            },
            {
                "arxivId": "2110.10249",
                "title": "Neural Stochastic PDEs: Resolution-Invariant Learning of Continuous Spatiotemporal Dynamics",
                "abstract": "Stochastic partial differential equations (SPDEs) are the mathematical tool of choice for modelling spatiotemporal PDE-dynamics under the influence of randomness. Based on the notion of mild solution of an SPDE, we introduce a novel neural architecture to learn solution operators of PDEs with (possibly stochastic) forcing from partially observed data. The proposed Neural SPDE model provides an extension to two popular classes of physics-inspired architectures. On the one hand, it extends Neural CDEs and variants -- continuous-time analogues of RNNs -- in that it is capable of processing incoming sequential information arriving at arbitrary spatial resolutions. On the other hand, it extends Neural Operators -- generalizations of neural networks to model mappings between spaces of functions -- in that it can parameterize solution operators of SPDEs depending simultaneously on the initial condition and a realization of the driving noise. By performing operations in the spectral domain, we show how a Neural SPDE can be evaluated in two ways, either by calling an ODE solver (emulating a spectral Galerkin scheme), or by solving a fixed point problem. Experiments on various semilinear SPDEs, including the stochastic Navier-Stokes equations, demonstrate how the Neural SPDE model is capable of learning complex spatiotemporal dynamics in a resolution-invariant way, with better accuracy and lighter training data requirements compared to alternative models, and up to 3 orders of magnitude faster than traditional solvers."
            },
            {
                "arxivId": "2107.00447",
                "title": "Weighted signature kernels",
                "abstract": "Suppose that $\\gamma$ and $\\sigma$ are two continuous bounded variation paths which take values in a finite-dimensional inner product space $V$. Recent papers have introduced the truncated and the untruncated signature kernel of $\\gamma$ and $\\sigma$, and showed how these concepts can be used in classification and prediction tasks involving multivariate time series. In this paper, we introduce general signature kernels and show how they can be interpreted, in many examples, as an average of PDE solutions, and hence how they can be computed using suitable quadrature rules. We extend this analysis to derive closed-form formulae for expressions involving the expected (Stratonovich) signature of Brownian motion. In doing so, we articulate a novel connection between signature kernels and the hyperbolic development map, the latter of which has been a broadly useful tool in the analysis of the signature. As an application we evaluate the use of different general signature kernels as the basis for non-parametric goodness-of-fit tests to Wiener measure on path space."
            },
            {
                "arxivId": "2102.07904",
                "title": "SK-Tree: a systematic malware detection algorithm on streaming trees via the signature kernel",
                "abstract": "The development of machine learning algorithms in the cyber security domain has been impeded by the complex, hierarchical, sequential and multimodal nature of the data involved. In this paper we introduce the notion of a streaming tree as a generic data structure encompassing a large portion of real-world cyber security data. Starting from host-based event logs we represent computer processes as streaming trees that evolve in continuous time. Leveraging the properties of the signature kernel, a machine learning tool that recently emerged as a leading technology for learning with complex sequences of data, we develop the SK-Tree algorithm. SK-Tree is a supervised learning method for systematic malware detection on streaming trees that is robust to irregular sampling and high dimensionality of the underlying streams. We demonstrate the effectiveness of SK-Tree to detect malicious events on a portion of the publicly available DARPA OpTC dataset, achieving an AUROC score of 98%."
            },
            {
                "arxivId": "2012.01910",
                "title": "Slow-fast systems with fractional environment and dynamics",
                "abstract": "We prove an averaging principle for interacting slow-fast systems driven by independent fractional Brownian motions. The mode of convergence is in H\\\"older norm in probability. We also establish geometric ergodicity for a class of fractional-driven stochastic differential equations, partially improving a recent result of Panloup and Richard."
            },
            {
                "arxivId": "2006.14794",
                "title": "The Signature Kernel Is the Solution of a Goursat PDE",
                "abstract": "Recently, there has been an increased interest in the development of kernel methods for learning with sequential data. The signature kernel is a learning tool with potential to handle irregularly sampled, multivariate time series. In\"Kernels for sequentially ordered data\"the authors introduced a kernel trick for the truncated version of this kernel avoiding the exponential complexity that would have been involved in a direct computation. Here we show that for continuously differentiable paths, the signature kernel solves a hyperbolic PDE and recognize the connection with a well known class of differential equations known in the literature as Goursat problems. This Goursat PDE only depends on the increments of the input sequences, does not require the explicit computation of signatures and can be solved efficiently using state-of-the-arthyperbolic PDE numerical solvers, giving a kernel trick for the untruncated signature kernel, with the same raw complexity as the method from\"Kernels for sequentially ordered data\", but with the advantage that the PDE numerical scheme is well suited for GPU parallelization, which effectively reduces the complexity by a full order of magnitude in the length of the input sequences. In addition, we extend the previous analysis to the space of geometric rough paths and establish, using classical results from rough path theory, that the rough version of the signature kernel solves a rough integral equation analogous to the aforementioned Goursat PDE. Finally, we empirically demonstrate the effectiveness of our PDE kernel as a machine learning tool in various machine learning applications dealing with sequential data. We release the library sigkernel publicly available at https://github.com/crispitagorico/sigkernel."
            },
            {
                "arxivId": "1601.08169",
                "title": "Kernels for sequentially ordered data",
                "abstract": "We present a novel framework for kernel learning with sequential data of any kind, such as time series, sequences of graphs, or strings. Our approach is based on signature features which can be seen as an ordered variant of sample (cross-)moments; it allows to obtain a \"sequentialized\" version of any static kernel. The sequential kernels are efficiently computable for discrete sequences and are shown to approximate a continuous moment form in a sampling sense. \nA number of known kernels for sequences arise as \"sequentializations\" of suitable static kernels: string kernels may be obtained as a special case, and alignment kernels are closely related up to a modification that resolves their open non-definiteness issue. Our experiments indicate that our signature-based sequential kernel framework may be a promising approach to learning with sequential data, such as time series, that allows to avoid extensive manual pre-processing."
            },
            {
                "arxivId": "1507.03004",
                "title": "Hybrid scheme for Brownian semistationary processes",
                "abstract": null
            },
            {
                "arxivId": "0709.0626",
                "title": "Consistency and robustness of kernel-based regression in convex risk minimization",
                "abstract": "We investigate statistical properties for a broad class of modern kernel-based regression (KBR) methods. These kernel methods were developed during the last decade and are inspired by convex risk minimization in infinite-dimensional Hilbert spaces. One leading example is support vector regression. We first describe the relationship between the loss function $L$ of the KBR method and the tail of the response variable. We then establish the $L$-risk consistency for KBR which gives the mathematical justification for the statement that these methods are able to ``learn''. Then we consider robustness properties of such kernel methods. In particular, our results allow us to choose the loss function and the kernel to obtain computationally tractable and consistent KBR methods that have bounded influence functions. Furthermore, bounds for the bias and for the sensitivity curve, which is a finite sample version of the influence function, are developed, and the relationship between KBR and classical $M$ estimators is discussed."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.11824",
        "category": "q-fin",
        "title": "Nonconcave Robust Utility Maximization under Projective Determinacy",
        "abstract": "We study a robust utility maximization problem in a general discrete-time frictionless market. The investor is assumed to have a random, nonconcave and nondecreasing utility function, which may or may not be finite on the whole real-line. She also faces model ambiguity on her beliefs about the market, which is modeled through a set of priors. We prove, using only primal methods, the existence of an optimal investment strategy when the utility function is also upper-semicontinuous. For that, we introduce the new notion of projectively measurable functions. We show basic properties of these functions as stability under sums, differences, products, suprema, infima and compositions but also assuming the set-theoretical axiom of Projective Determinacy (PD) stability under integration and existence of $\\epsilon$-optimal selectors. We consider projectively measurable random utility function and price process and assume that the graphs of the sets of local priors are projective sets. Our other assumptions are stated on a prior-by-prior basis and correspond to generally accepted assumptions in the literature on markets without ambiguity.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-19.json",
        "arxivId": "2403.11897",
        "category": "q-fin",
        "title": "Risk premium and rough volatility",
        "abstract": "One the one hand, rough volatility has been shown to provide a consistent framework to capture the properties of stock price dynamics both under the historical measure and for pricing purposes. On the other hand, market price of volatility risk is a well-studied object in Financial Economics, and empirical estimates show it to be stochastic rather than deterministic. Starting from a rough volatility model under the historical measure, we take up this challenge and provide an analysis of the impact of such a non-deterministic risk for pricing purposes.",
        "references": [
            {
                "arxivId": "2212.10917",
                "title": "The Quintic Ornstein-Uhlenbeck Volatility Model that Jointly Calibrates SPX & VIX Smiles",
                "abstract": "The quintic Ornstein-Uhlenbeck volatility model is a stochastic volatility model where the volatility process is a polynomial function of degree five of a single Ornstein-Uhlenbeck process with fast mean reversion and large vol-of-vol. The model is able to achieve remarkable joint fits of the SPX-VIX smiles with only 6 effective parameters and an input curve that allows to match certain term structures. We provide several practical specifications of the input curve, study their impact on the joint calibration problem and consider additionally time-dependent parameters to help achieve better fits for longer maturities going beyond 1 year. Even better, the model remains very simple and tractable for pricing and calibration: the VIX squared is again polynomial in the Ornstein-Uhlenbeck process, leading to efficient VIX derivative pricing by a simple integration against a Gaussian density; simulation of the volatility process is exact; and pricing SPX products derivatives can be done efficiently and accurately by standard Monte Carlo techniques with suitable antithetic and control variates."
            },
            {
                "arxivId": "1811.10935",
                "title": "On the martingale property in the rough Bergomi model",
                "abstract": "We consider a class of fractional stochastic volatility models (including the so-called rough Bergomi model), where the volatility is a superlinear function of a fractional Gaussian process. We show that the stock price is a true martingale if and only if the correlation $\\rho$ between the driving Brownian motions of the stock and the volatility is nonpositive. We also show that for each $\\rho \\frac{1}{{1-\\rho^2}}$, the $m$-th moment of the stock price is infinite at each positive time."
            },
            {
                "arxivId": "1610.00332",
                "title": "Decoupling the Short- and Long-Term Behavior of Stochastic Volatility",
                "abstract": "\n We introduce a new class of continuous-time models of the stochastic volatility of asset prices. The models can simultaneously incorporate roughness and slowly decaying autocorrelations, including proper long memory, which are two stylized facts often found in volatility data. Our prime model is based on the so-called Brownian semistationary process and we derive a number of theoretical properties of this process, relevant to volatility modeling. Applying the models to realized volatility measures covering a vast panel of assets, we find evidence consistent with the hypothesis that time series of realized measures of volatility are both rough and very persistent. Lastly, we illustrate the utility of the models in an extensive forecasting study; we find that the models proposed in this article outperform a wide array of benchmarks considerably, indicating that it pays off to exploit both roughness and persistence in volatility forecasting."
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-20.json",
        "arxivId": "2312.09353",
        "category": "q-fin",
        "title": "Residual U-net with Self-Attention to Solve Multi-Agent Time-Consistent Optimal Trade Execution",
        "abstract": "In this paper, we explore the use of a deep residual U-net with self-attention to solve the the continuous time time-consistent mean variance optimal trade execution problem for multiple agents and assets. Given a finite horizon we formulate the time-consistent mean-variance optimal trade execution problem following the Almgren-Chriss model as a Hamilton-Jacobi-Bellman (HJB) equation. The HJB formulation is known to have a viscosity solution to the unknown value function. We reformulate the HJB to a backward stochastic differential equation (BSDE) to extend the problem to multiple agents and assets. We utilize a residual U-net with self-attention to numerically approximate the value function for multiple agents and assets which can be used to determine the time-consistent optimal control. In this paper, we show that the proposed neural network approach overcomes the limitations of finite difference methods. We validate our results and study parameter sensitivity. With our framework we study how an agent with significant price impact interacts with an agent without any price impact and the optimal strategies used by both types of agents. We also study the performance of multiple sellers and buyers and how they compare to a holding strategy under different economic conditions.",
        "references": [
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1512.03385",
                "title": "Deep Residual Learning for Image Recognition",
                "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            {
                "arxivId": "1403.2229",
                "title": "A reinforcement learning extension to the Almgren-Chriss framework for optimal trade execution",
                "abstract": "Reinforcement learning is explored as a candidate machine learning technique to enhance existing analytical solutions for optimal trade execution with elements from the market microstructure. Given a volume-to-trade, fixed time horizon and discrete trading periods, the aim is to adapt a given volume trajectory such that it is dynamic with respect to favourable/unfavourable conditions during realtime execution, thereby improving overall cost of trading. We consider the standard Almgren-Chriss model with linear price impact as a candidate base model. This model is popular amongst sell-side institutions as a basis for arrival price benchmark execution algorithms. By training a learning agent to modify a volume trajectory based on the market's prevailing spread and volume dynamics, we are able to improve post-trade implementation shortfall by up to 10.3% on average compared to the base model, based on a sample of stocks and trade sizes in the South African equity market."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-20.json",
        "arxivId": "2403.12078",
        "category": "q-fin",
        "title": "Student t-L\\'evy regression model in YUIMA",
        "abstract": "The aim of this paper is to discuss an estimation and a simulation method in the \\textsf{R} package YUIMA for a linear regression model driven by a Student-$t$ L\\'evy process with constant scale and arbitrary degrees of freedom. This process finds applications in several fields, for example finance, physic, biology, etc. The model presents two main issues. The first is related to the simulation of a sample path at high-frequency level. Indeed, only the $t$-L\\'evy increments defined on an unitary time interval are Student-$t$ distributed. In YUIMA, we solve this problem by means of the inverse Fourier transform for simulating the increments of a Student-$t$ L\\'{e}vy defined on a interval with any length. A second problem is due to the fact that joint estimation of trend, scale, and degrees of freedom does not seem to have been investigated as yet. In YUIMA, we develop a two-step estimation procedure that efficiently deals with this issue. Numerical examples are given in order to explain methods and classes used in the YUIMA package.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-20.json",
        "arxivId": "2403.12285",
        "category": "q-fin",
        "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications",
        "abstract": "There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment analysis data, so as to jointly handle the complexities of financial lexicon and context, and further equipping it with a neural network based decision mechanism. Such a generator-classifier scheme, referred to as FinLlama, is trained not only to classify the sentiment valence but also quantify its strength, thus offering traders a nuanced insight into financial news articles. Complementing this, the implementation of parameter-efficient fine-tuning through LoRA optimises trainable parameters, thus minimising computational and memory requirements, without sacrificing accuracy. Simulation results demonstrate the ability of the proposed FinLlama to provide a framework for enhanced portfolio management decisions and increased market returns. These results underpin the ability of FinLlama to construct high-return portfolios which exhibit enhanced resilience, even during volatile periods and unpredictable market events.",
        "references": [
            {
                "arxivId": "2307.10485",
                "title": "FinGPT: Democratizing Internet-scale Data for Financial Large Language Models",
                "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available, and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, Financial Generative Pre-trained Transformer (FinGPT), that automates the collection and curation of real-time financial data from 34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes have been open-sourced."
            },
            {
                "arxivId": "2307.09288",
                "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
            },
            {
                "arxivId": "2306.12659",
                "title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models",
                "abstract": "Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital."
            },
            {
                "arxivId": "2303.17564",
                "title": "BloombergGPT: A Large Language Model for Finance",
                "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT."
            },
            {
                "arxivId": "2106.09685",
                "title": "LoRA: Low-Rank Adaptation of Large Language Models",
                "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
            },
            {
                "arxivId": "1908.10063",
                "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models",
                "abstract": "Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods."
            },
            {
                "arxivId": "1711.05101",
                "title": "Fixing Weight Decay Regularization in Adam",
                "abstract": "We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process."
            },
            {
                "arxivId": "1810.04805",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-21.json",
        "arxivId": "2212.12356",
        "category": "q-fin",
        "title": "Equivalence between the Fitness-Complexity and the Sinkhorn-Knopp algorithms",
        "abstract": "\n We uncover the connection between the Fitness-Complexity algorithm, developed in the economic complexity field, and the Sinkhorn-Knopp algorithm, widely used in diverse domains ranging from computer science and mathematics to economics. Despite minor formal differences between the two methods, both converge to the same fixed-point solution up to normalization. The discovered connection allows us to derive a rigorous interpretation of the Fitness and the Complexity metrics as the potentials of a suitable energy function. Under this interpretation, high-energy products are unfeasible for low-fitness countries, which explains why the algorithm is effective at displaying nested patterns in bipartite networks. We also show that the proposed interpretation reveals the scale invariance of the Fitness-Complexity algorithm, which has practical implications for the algorithm's implementation in different datasets. Further, analysis of empirical trade data under the new perspective reveals three categories of countries that might benefit from different development strategies.",
        "references": [
            {
                "arxivId": "1808.00062",
                "title": "PopRank: Ranking pages\u2019 impact and users\u2019 engagement on Facebook",
                "abstract": "The advent of social networks revolutionized the way people access to information sources. Understanding the complex relationship between these sources and users is crucial. We introduce an algorithm, that we call PopRank, to assess both the Impact of Facebook pages as well as users\u2019 Engagement on the basis of their mutual interactions. The ideas behind the PopRank are that i) high impact pages attract many users with a low engagement, which means that they receive comments from users that rarely comment, and ii) high engagement users interact with high impact pages, that is they mostly comment pages with a high popularity. The resulting ranking of pages can predict the number of comments a page will receive and the number of its future posts. Pages\u2019 impact turns out to be slightly dependent on the quality of pages\u2019 informative content (e.g., science vs conspiracy) but independent of users\u2019 polarization."
            },
            {
                "arxivId": "1807.10276",
                "title": "A New and Stable Estimation Method of Country Economic Fitness and Product Complexity",
                "abstract": "We present a new metric estimating fitness of countries and complexity of products by exploiting a non-linear non-homogeneous map applied to the publicly available information on the goods exported by a country. The non homogeneous terms guarantee both convergence and stability. After a suitable rescaling of the relevant quantities, the non homogeneous terms are eventually set to zero so that this new metric is parameter free. This new map almost reproduces the results of the original homogeneous metrics already defined in literature and allows for an approximate analytic solution in case of actual binarized matrices based on the Revealed Comparative Advantage (RCA) indicator. This solution is connected with a new quantity describing the neighborhood of nodes in bipartite graphs, representing in this work the relations between countries and exported products. Moreover, we define the new indicator of country net-efficiency quantifying how a country efficiently invests in capabilities able to generate innovative complex high quality products. Eventually, we demonstrate analytically the local convergence of the algorithm involved."
            },
            {
                "arxivId": "1803.00567",
                "title": "Computational Optimal Transport",
                "abstract": "Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a \"global\" cost to every such transport, using the \"local\" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications."
            },
            {
                "arxivId": "1711.01851",
                "title": "Overrelaxed Sinkhorn-Knopp Algorithm for Regularized Optimal Transport",
                "abstract": "This article describes a set of methods for quickly computing the solution to the regularized optimal transport problem. It generalizes and improves upon the widely used iterative Bregman projections algorithm (or Sinkhorn\u2013Knopp algorithm). We first proposed to rely on regularized nonlinear acceleration schemes. In practice, such approaches lead to fast algorithms, but their global convergence is not ensured. Hence, we next proposed a new algorithm with convergence guarantees. The idea is to overrelax the Bregman projection operators, allowing for faster convergence. We proposed a simple method for establishing global convergence by ensuring the decrease of a Lyapunov function at each step. An adaptive choice of the overrelaxation parameter based on the Lyapunov function was constructed. We also suggested a heuristic to choose a suitable asymptotic overrelaxation parameter, based on a local convergence analysis. Our numerical experiments showed a gain in convergence speed by an order of magnitude in certain regimes."
            },
            {
                "arxivId": "1707.05146",
                "title": "Unfolding the innovation system for the development of countries: coevolution of Science, Technology and Production",
                "abstract": null
            },
            {
                "arxivId": "1704.08027",
                "title": "Ranking in evolving complex networks",
                "abstract": null
            },
            {
                "arxivId": "1603.06407",
                "title": "The mathematics of non-linear metrics for nested networks",
                "abstract": null
            },
            {
                "arxivId": "1509.01482",
                "title": "Measuring economic complexity of countries and products: which metric to use?",
                "abstract": null
            },
            {
                "arxivId": "1502.05378",
                "title": "Ranking species in mutualistic networks",
                "abstract": null
            },
            {
                "arxivId": "1306.0895",
                "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
                "abstract": "Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-21.json",
        "arxivId": "2304.01272",
        "category": "q-fin",
        "title": "Equilibrium with Heterogeneous Information Flows",
        "abstract": "We study a continuous time economy where throughout time, insiders receive private signals regarding the risky assets' terminal payoff. We prove existence of a partial communication equilibrium where, at each private signal time, the public receives a signal of the same form as the associated insider, but of lower quality. This causes a jump in both the public information flow and equilibrium asset price. The resultant markets, while complete between each jump time, are incomplete over each jump. After establishing equilibrium for a finite number of private signal times, we consider the limit as the private signals become more and more frequent. Under appropriate scaling we prove convergence of the public filtration to the natural filtration generated by both the fundamental factor process $X$ and a continuous time process $J$ taking the form $J_t = X_1 + Y_t$ where $X_1$ is the terminal payoff and $Y$ an independent Gaussian process. This coincides with the filtration considered in 'Additional Utility of Insiders with Imperfect Dynamical Information' (Corcuera, et al. Finance&Stochastics 2004). However, while therein the filtration was exogenously assumed to be that of an insider who observes a private signal flow, here it arises endogenously as the public filtration when there are a large number of insiders receiving signals throughout time.",
        "references": [
            {
                "arxivId": "1506.00188",
                "title": "Market completion with derivative securities",
                "abstract": null
            },
            {
                "arxivId": "0901.3318",
                "title": "Partial equilibria with convex capital requirements: existence, uniqueness and stability",
                "abstract": null
            },
            {
                "arxivId": "0706.0478",
                "title": "OPTIMAL INVESTMENT WITH AN UNBOUNDED RANDOM ENDOWMENT AND UTILITY\u2010BASED PRICING",
                "abstract": "This paper studies the problem of maximizing the expected utility of terminal wealth for a financial agent with an unbounded random endowment, and with a utility function which supports both positive and negative wealth. We prove the existence of an optimal trading strategy within a class of permissible strategies\u2014those strategies whose wealth process is a super\u2010martingale under all pricing measures with finite relative entropy. We give necessary and sufficient conditions for the absence of utility\u2010based arbitrage, and for the existence of a solution to the primal problem. We consider two utility\u2010based methods which can be used to price contingent claims. Firstly we investigate marginal utility\u2010based price processes (MUBPP's). We show that such processes can be characterized as local martingales under the normalized optimal dual measure for the utility maximizing investor. Finally, we present some new results on utility indifference prices, including continuity properties and volume asymptotics for the case of a general utility function, unbounded endowment and unbounded contingent claims."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-21.json",
        "arxivId": "2403.13192",
        "category": "q-fin",
        "title": "Modeling stock price dynamics on the Ghana Stock Exchange: A Geometric Brownian Motion approach",
        "abstract": "Modeling financial data often relies on assumptions that may prove insufficient or unrealistic in practice. The Geometric Brownian Motion (GBM) model is frequently employed to represent stock price processes. This study investigates whether the behavior of weekly and monthly returns of selected equities listed on the Ghana Stock Exchange conforms to the GBM model. Parameters of the GBM model were estimated for five equities, and forecasts were generated for three months. Evaluation of estimation accuracy was conducted using mean square error (MSE). Results indicate that the expected prices from the modeled equities closely align with actual stock prices observed on the Exchange. Furthermore, while some deviations were observed, the actual prices consistently fell within the estimated confidence intervals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-25.json",
        "arxivId": "2210.10858",
        "category": "q-fin",
        "title": "Beyond Capacity: Contractual Form In\u00a0Electricity Reliability Obligations",
        "abstract": null,
        "references": [
            {
                "arxivId": "2106.14351",
                "title": "On the Design of an Insurance Mechanism for Reliability Differentiation in Electricity Markets",
                "abstract": null
            },
            {
                "arxivId": "1706.08398",
                "title": "On risk averse competitive equilibrium",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-25.json",
        "arxivId": "2403.14841",
        "category": "q-fin",
        "title": "On the Hull-White model with volatility smile for Valuation Adjustments",
        "abstract": "Affine Diffusion dynamics are frequently used for Valuation Adjustments (xVA) calculations due to their analytic tractability. However, these models cannot capture the market-implied skew and smile, which are relevant when computing xVA metrics. Hence, additional degrees of freedom are required to capture these market features. In this paper, we address this through an SDE with state-dependent coefficients. The SDE is consistent with the convex combination of a finite number of different AD dynamics. We combine Hull-White one-factor models where one model parameter is varied. We use the Randomized AD (RAnD) technique to parameterize the combination of dynamics. We refer to our SDE with state-dependent coefficients and the RAnD parametrization of the original models as the rHW model. The rHW model allows for efficient semi-analytic calibration to European swaptions through the analytic tractability of the Hull-White dynamics. We use a regression-based Monte-Carlo simulation to calculate exposures. In this setting, we demonstrate the significant effect of skew and smile on exposures and xVAs of linear and early-exercise interest rate derivatives.",
        "references": [
            {
                "arxivId": "2211.17026",
                "title": "Accelerated Computations of Sensitivities for xVA",
                "abstract": "Exposure simulations are fundamental to many xVA calculations and are a nested expectation problem where repeated portfolio valuations create a significant computational expense. Sensitivity calculations which require shocked and unshocked valuations in bump-and-revalue schemes exacerbate the computational load. A known reduction of the portfolio valuation cost is understood to be found in polynomial approximations, which we apply in this article to interest rate sensitivities of expected exposures. We consider a method based on the approximation of the shocked and unshocked valuation functions, as well as a novel approach in which the difference between these functions is approximated. Convergence results are shown, and we study the choice of interpolation nodes. Numerical experiments with interest rate derivatives are conducted to demonstrate the high accuracy and remarkable computational cost reduction. We further illustrate how the method can be extended to more general xVA models using the example of CVA with wrong-way risk."
            },
            {
                "arxivId": "2211.05014",
                "title": "Randomization of Short-Rate Models, Analytic Pricing and Flexibility in Controlling Implied Volatilities",
                "abstract": "We focus on extending existing short-rate models, enabling control of the generated implied volatility while preserving analyticity. We achieve this goal by applying the Randomized A\ufb03ne Di\ufb00usion (RAnD) method [11] to the class of short-rate processes under the Heath-Jarrow-Morton framework. Under arbitrage-free conditions, the model parameters can be exogenously stochastic, thus facilitating additional degrees of freedom that enhance the calibration procedure. We show that with the randomized short-rate models, the shapes of implied volatility can be controlled and signi\ufb01cantly improve the quality of the model calibration, even for standard 1D variants. In particular, we illustrate that randomization applied to the Hull-White model leads to dynamics of the local volatility type, with the prices for standard volatility-sensitive derivatives explicitly available. The randomized Hull-White (rHW) model o\ufb00ers an almost perfect calibration \ufb01t to the swaption implied volatilities."
            },
            {
                "arxivId": "2208.12518",
                "title": "On Randomization of Affine Diffusion Processes with Application to Pricing of Options on VIX and S&P 500",
                "abstract": "The class of A\ufb03ne (Jump) Di\ufb00usion [8] (AD) has, due to its closed form characteristic function (ChF), gained tremendous popularity among practitioners and researchers. However, there is clear evidence that a linearity constraint is insu\ufb03cient for precise and consistent option pricing. Any non-a\ufb03ne model must pass the strict requirement of quick calibration- which is often challenging. We focus here on Randomized AD (RAnD) models, i.e., we allow for exogenous stochasticity of the model parameters. Randomization of a pricing model occurs outside the a\ufb03ne model and, therefore, forms a generalization that relaxes the a\ufb03nity constraints. The method is generic and can apply to any model parameter. It relies on the existence of moments of the so-called randomizer- a random variable for the stochastic parameter. The RAnD model allows \ufb02exibility while bene\ufb01ting from fast calibration and well-established, large-step Monte Carlo simulation, often available for AD processes. The article will discuss theoretical and practical aspects of the RAnD method, like derivations of the corresponding ChF, simulation, and computations of sensitivities. We will also illustrate the advantages of the randomized stochastic volatility models in the consistent pricing of options on the S&P 500 and VIX."
            },
            {
                "arxivId": "2105.07061",
                "title": "Efficient Least Squares Monte-Carlo Technique for PFE/EE Calculations",
                "abstract": "We describe a regression-based method, generally referred to as the Least Squares Monte Carlo (LSMC) method, to speed up exposure calculations of a portfolio. We assume that the portfolio contains several exotic derivatives that are priced using Monte-Carlo on each real world scenario and time step. Such a setting is often referred to as a Monte Carlo over a Monte Carlo or a Nested Monte Carlo method."
            },
            {
                "arxivId": "2009.03202",
                "title": "The Seven-League Scheme: Deep learning for large time step Monte Carlo simulations of stochastic differential equations",
                "abstract": "We propose an accurate data-driven numerical scheme to solve stochastic differential equations (SDEs), by taking large time steps. The SDE discretization is built up by means of the polynomial chaos expansion method, on the basis of accurately determined stochastic collocation (SC) points. By employing an artificial neural network to learn these SC points, we can perform Monte Carlo simulations with large time steps. Basic error analysis indicates that this data-driven scheme results in accurate SDE solutions in the sense of strong convergence, provided the learning methodology is robust and accurate. With a method variant called the compression\u2013decompression collocation and interpolation technique, we can drastically reduce the number of neural network functions that have to be learned, so that computational speed is enhanced. As a proof of concept, 1D numerical experiments confirm a high-quality strong convergence error when using large time steps, and the novel scheme outperforms some classical numerical SDE discretizations. Some applications, here in financial option valuation, are also presented."
            },
            {
                "arxivId": "1912.01280",
                "title": "Speed-up credit exposure calculations for pricing and risk management",
                "abstract": "We introduce a new method to calculate the credit exposure of European and path-dependent options. The proposed method is able to calculate accurate expected exposure and potential future exposure profiles under the risk-neutral and the real-world measure. A key advantage is that it delivers an accuracy comparable to a full re-evaluation and at the same time it is faster than a regression-based method. The core of the approach is solving a dynamic programming problem by function approximation. This yields a closed-form approximation along the paths together with the option's delta and gamma. The simple structure allows for highly efficient evaluation of the exposures, even for a large number of simulated paths. The approach is flexible in the model choice, payoff profiles and asset classes. We validate the accuracy of the method numerically for three different equity products and a Bermudan interest rate swaption. Benchmarking against the popular least-squares Monte Carlo approach shows that our method is able to deliver a higher accuracy in a faster runtime."
            },
            {
                "arxivId": "0812.4052",
                "title": "The General Mixture Diffusion Sde and its Relationship with an Uncertain-Volatility Option Model with Volatility-Asset Decorrelation",
                "abstract": "In the present paper, given an evolving mixture of probability densities, we define a candidate diffusion process whose marginal law follows the same evolution. We derive as a particular case a stochastic differential equation (SDE) admitting a unique strong solution and whose density evolves as a mixture of Gaussian densities. We present an interesting result on the comparison between the instantaneous and the terminal correlation between the obtained process and its squared diffusion coefficient. As an application to mathematical finance, we construct diffusion processes whose marginal densities are mixtures of lognormal densities. We explain how such processes can be used to model the market smile phenomenon. We show that the lognormal mixture dynamics is the one-dimensional diffusion version of a suitable uncertain volatility model, and suitably reinterpret the earlier correlation result. We explore numerically the relationship between the future smile structures of both the diffusion and the uncertain volatility versions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-25.json",
        "arxivId": "2403.14862",
        "category": "q-fin",
        "title": "The Power of Linear Programming in Sponsored Listings Ranking: Evidence from Field Experiments",
        "abstract": "Sponsored listing is one of the major revenue sources for many prominent online marketplaces, such as Amazon, Walmart, and Alibaba. When consumers visit a marketplace's webpage for a specific item, in addition to that item, the marketplace might also display a ranked listing of sponsored items from various third-party sellers. These sellers are charged an advertisement fee if a user purchases any of the sponsored items from this listing. Determining how to rank these sponsored items for each incoming visit is a crucial challenge for online marketplaces, a problem known as sponsored listings ranking (SLR). The major difficulty of SLR lies in balancing the trade-off between maximizing the overall revenue and recommending high-quality and relevant ranked listings. While a more relevant ranking may result in more purchases and consumer engagement, the marketplace also needs to take account of the potential revenue when making ranking decisions. Due to the latency requirement and historical reasons, many online marketplaces use score-based ranking algorithms for SLR optimization. Alternatively, recent research also discusses obtaining the ranking by solving linear programming (LP). In this paper, we collaborate with a leading online global marketplace and conduct a series of field experiments to compare the performance of the score-based ranking algorithms and the LP-based algorithms. The field experiment lasted for $19$ days, which included $329.3$ million visits in total. We observed that the LP-based approach improved all major metrics by $1.80\\%$ of revenue, $1.55\\%$ of purchase, and $1.39\\%$ of the gross merchandise value (GMV), compared to an extremely-tuned score-based algorithm that was previously used in production by the marketplace.",
        "references": [
            {
                "arxivId": "2106.04756",
                "title": "Practical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient",
                "abstract": "We present PDLP, a practical first-order method for linear programming (LP) that can solve to the high levels of accuracy that are expected in traditional LP applications. In addition, it can scale to very large problems because its core operation is matrix-vector multiplications. PDLP is derived by applying the primal-dual hybrid gradient (PDHG) method, popularized by Chambolle and Pock (2011), to a saddle-point formulation of LP. PDLP enhances PDHG for LP by combining several new techniques with older tricks from the literature; the enhancements include diagonal preconditioning, presolving, adaptive step sizes, and adaptive restarting. PDLP improves the state of the art for first-order methods applied to LP. We compare PDLP with SCS, an ADMM-based solver, on a set of 383 LP instances derived from MIPLIB 2017. With a target of $10^{-8}$ relative accuracy and 1 hour time limit, PDLP achieves a 6.3x reduction in the geometric mean of solve times and a 4.6x reduction in the number of instances unsolved (from 227 to 49). Furthermore, we highlight standard benchmark instances and a large-scale application (PageRank) where our open-source prototype of PDLP, written in Julia, outperforms a commercial LP solver."
            },
            {
                "arxivId": "2105.12715",
                "title": "Faster first-order primal-dual methods for linear programming using restarts and sharpness",
                "abstract": null
            },
            {
                "arxivId": "2102.04592",
                "title": "Infeasibility Detection with Primal-Dual Hybrid Gradient for Large-Scale Linear Programming",
                "abstract": "We study the problem of detecting infeasibility of large-scale linear programming problems using the primal-dual hybrid gradient method (PDHG) of Chambolle and Pock (2011). The literature on PDHG has mostly focused on settings where the problem at hand is assumed to be feasible. When the problem is not feasible, the iterates of the algorithm do not converge. In this scenario, we show that the iterates diverge at a controlled rate towards a well-defined ray. The direction of this ray is known as the infimal displacement vector $v$. The first contribution of our work is to prove that this vector recovers certificates of primal and dual infeasibility whenever they exist. Based on this fact, we propose a simple way to extract approximate infeasibility certificates from the iterates of PDHG. We study three different sequences that converge to the infimal displacement vector: the difference of iterates, the normalized iterates, and the normalized average. All of them are easy to compute, and thus the approach is suitable for large-scale problems. Our second contribution is to establish tight convergence rates for these sequences. We demonstrate that the normalized iterates and the normalized average achieve a convergence rate of $O(1/k)$, improving over the known rate of $O(1/\\sqrt{k})$. This rate is general and applies to any fixed-point iteration of a nonexpansive operator. Thus, it is a result of independent interest since it covers a broad family of algorithms, including, for example, ADMM, and can be applied settings beyond linear programming, such as quadratic and semidefinite programming. Further, in the case of linear programming we show that, under nondegeneracy assumptions, the iterates of PDHG identify the active set of an auxiliary feasible problem in finite time, which ensures that the difference of iterates exhibits eventual linear convergence to the infimal displacement vector."
            },
            {
                "arxivId": "2002.09458",
                "title": "Sequential Submodular Maximization and Applications to Ranking an Assortment of Products",
                "abstract": "We introduce and study a variation of the submodular maximization problem motivated by applications in online retail. A platform displays a list of products to a user in response to a search query. The user inspects the first k items in the list for a k chosen at random from a given distribution, and decides whether to purchase an item from that set based on a choice model. The goal of the platform is to maximize the engagement of the shopper defined as the probability of purchase. This problem gives rise to a less-studied variation of submodular maximization in which we are asked to choose an ordering of a set of elements to maximize a linear combination of different submodular functions. First, using a reduction to maximizing submodular functions over matroids, we give an optimal (1-1/e)-approximation for this problem. We then consider a variant in which the platform cares not only about user engagement, but also about diversification across various groups of users, that is, guaranteeing a certain probability of purchase in each group. We characterize the polytope of feasible solutions and give a bi-criteria ((1-1/e)2,(1-1/e)2)-approximation for this problem by rounding an approximate solution of a linear programming relaxation. For rounding, we rely on our reduction and the particular rounding techniques for matroid polytopes. For the special case in which underlying submodular functions are coverage functions -- which is practically relevant in online retail -- we propose an alternative LP relaxation and a simpler randomized rounding for the problem. This approach yields to an optimal bi-criteria (1-1/e,1-1/e)-approximation algorithm for the special case of the problem with coverage functions."
            },
            {
                "arxivId": "1905.01989",
                "title": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search",
                "abstract": "We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members."
            },
            {
                "arxivId": "1803.00710",
                "title": "Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application",
                "abstract": "In E-commerce platforms such as Amazon and TaoBao , ranking items in a search session is a typical multi-step decision-making problem. Learning to rank (LTR) methods have been widely applied to ranking problems. However, such methods often consider different ranking steps in a session to be independent, which conversely may be highly correlated to each other. For better utilizing the correlation between different ranking steps, in this paper, we propose to use reinforcement learning (RL) to learn an optimal ranking policy which maximizes the expected accumulative rewards in a search session. Firstly, we formally define the concept of search session Markov decision process (SSMDP) to formulate the multi-step ranking problem. Secondly, we analyze the property of SSMDP and theoretically prove the necessity of maximizing accumulative rewards. Lastly, we propose a novel policy gradient algorithm for learning an optimal ranking policy, which is able to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments are conducted in simulation and TaoBao search engine. The results demonstrate that our algorithm performs much better than the state-of-the-art LTR methods, with more than 40% and 30% growth of total transaction amount in the simulation and the real application, respectively."
            },
            {
                "arxivId": "1706.06978",
                "title": "Deep Interest Network for Click-Through Rate Prediction",
                "abstract": "Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic."
            },
            {
                "arxivId": "1704.06840",
                "title": "Ranking with Fairness Constraints",
                "abstract": "Ranking algorithms are deployed widely to order a set of items in applications such as search engines, news feeds, and recommendation systems. Recent studies, however, have shown that, left unchecked, the output of ranking algorithms can result in decreased diversity in the type of content presented, promote stereotypes, and polarize opinions. In order to address such issues, we study the following variant of the traditional ranking problem when, in addition, there are fairness or diversity constraints. Given a collection of items along with 1) the value of placing an item in a particular position in the ranking, 2) the collection of sensitive attributes (such as gender, race, political opinion) of each item and 3) a collection of constraints that, for each k, bound the number of items with each attribute that are allowed to appear in the top k positions of the ranking, the goal is to output a ranking that maximizes the value with respect to the original rank quality metric while respecting the constraints. This problem encapsulates various well-studied problems related to bipartite and hypergraph matching as special cases and turns out to be hard to approximate even with simple constraints. Our main technical contributions are fast exact and approximation algorithms along with complementary hardness results that, together, come close to settling the approximability of this constrained ranking maximization problem. Unlike prior work on the constrained matching problems, our algorithm runs in linear time, even when the number of constraints is large, its approximation ratio does not depend on the number of constraints, and it produces solutions with small constraint violations. Our results rely on insights about the constrained matching problem when the objective satisfies properties that appear in common ranking metrics such as Discounted Cumulative Gain, Spearman's rho or Bradley-Terry."
            },
            {
                "arxivId": "1606.07792",
                "title": "Wide & Deep Learning for Recommender Systems",
                "abstract": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-25.json",
        "arxivId": "2403.14868",
        "category": "q-fin",
        "title": "A Markov approach to credit rating migration conditional on economic states",
        "abstract": "We develop a model for credit rating migration that accounts for the impact of economic state fluctuations on default probabilities. The joint process for the economic state and the rating is modelled as a time-homogeneous Markov chain. While the rating process itself possesses the Markov property only under restrictive conditions, methods from Markov theory can be used to derive the rating process' asymptotic behaviour. We use the mathematical framework to formalise and analyse different rating philosophies, such as point-in-time (PIT) and through-the-cycle (TTC) ratings. Furthermore, we introduce stochastic orders on the bivariate process' transition matrix to establish a consistent notion of\"better\"and\"worse\"ratings. Finally, the construction of PIT and TTC ratings is illustrated on a Merton-type firm-value process.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-25.json",
        "arxivId": "2403.15074",
        "category": "q-fin",
        "title": "A Taxmans guide to taxation of crypto assets",
        "abstract": "The Financial system has witnessed rapid technological changes. The rise of Bitcoin and other crypto assets based on Distributed Ledger Technology mark a fundamental change in the way people transact and transmit value over a decentralized network, spread across geographies. This has created regulatory and tax policy blind spots, as governments and tax administrations take time to understand and provide policy responses to this innovative, revolutionary, and fast-paced technology. Due to the breakneck speed of innovation in blockchain technology and advent of Decentralized Finance, Decentralized Autonomous Organizations and the Metaverse, it is unlikely that the policy interventions and guidance by regulatory authorities or tax administrations would be ahead or in sync with the pace of innovation. This paper tries to explain the principles on which crypto assets function, their underlying technology and relates them to the tax issues and taxable events which arise within this ecosystem. It also provides instances of tax and regulatory policy responses already in effect in various jurisdictions, including the recent changes in reporting standards by the FATF and the OECD. This paper tries to explain the rationale behind existing laws and policies and the challenges in their implementation. It also attempts to present a ballpark estimate of tax potential of this asset class and suggests creation of global public digital infrastructure that can address issues related to pseudonymity and extra-territoriality. The paper analyses both direct and indirect taxation issues related to crypto assets and discusses more recent aspects like proof-of-stake and maximal extractable value in greater detail.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-25.json",
        "arxivId": "2403.15200",
        "category": "q-fin",
        "title": "Teamwork and Spillover Effects in Performance Evaluations",
        "abstract": "This article shows how coworker performance affects individual performance evaluation in a teamwork setting at the workplace. We use high-quality data on football matches to measure an important component of individual performance, shooting performance, isolated from collaborative effects. Employing causal machine learning methods, we address the assortative matching of workers and estimate both average and heterogeneous effects. There is substantial evidence for spillover effects in performance evaluations. Coworker shooting performance, meaningfully impacts both, manager decisions and third-party expert evaluations of individual performance. Our results underscore the significant role coworkers play in shaping career advancements and highlight a complementary channel, to productivity gains and learning effects, how coworkers impact career advancement. We characterize the groups of workers that are most and least affected by spillover effects and show that spillover effects are reference point dependent. While positive deviations from a reference point create positive spillover effects, negative deviations are not harmful for coworkers.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-26.json",
        "arxivId": "2308.10568",
        "category": "q-fin",
        "title": "Analytical valuation of vulnerable derivative claims with bilateral cash flows under credit, funding and wrong-way risk",
        "abstract": "We study the problem of valuing and hedging a vulnerable derivative claim with bilateral cash flows between two counterparties in the presence of asymmetric funding costs, defaults and wrong way risk (WWR). We characterize the pre-default claim value as the solution to a non-linear Cauchy problem. We show an explicit stochastic representation of the solution exists under a funding policy which linearises the Cauchy PDE. We apply this framework to the valuation of a vulnerable equity forward and show it can be represented as a portfolio of European options. Despite the complexity of the model, we prove the forward's value admits an analytical formula involving only elementary functions and Gaussian integrals. Based on this explicit formula, numerical analysis demonstrates WWR has a significant impact even under benign assumptions: with a parameter configuration less punitive than that representative of Archegos AM default, we find WWR can shift values for vulnerable forwards by 100bps of notional, while peak exposures increase by 25% of notional. This framework is the first to apply to contracts with bilateral cash flows in the presence of credit, funding and WWR, resulting in a non-linear valuation formula which admits a closed-form solution under a suitable funding policy.",
        "references": [
            {
                "arxivId": "1602.05998",
                "title": "Funding, repo and credit inclusive valuation as modified option pricing",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-26.json",
        "arxivId": "2403.15980",
        "category": "q-fin",
        "title": "Markovian projections for It\\^o semimartingales with jumps",
        "abstract": "Given a general It\\^o semimartingale, its Markovian projection is an It\\^o process, with Markovian differential characteristics, that matches the one-dimensional marginal laws of the original process. We construct Markovian projections for It\\^o semimartingales with jumps, whose flows of one-dimensional marginal laws are solutions to non-local Fokker--Planck--Kolmogorov equations (FPKEs). As an application, we show how Markovian projections appear in building calibrated diffusion/jump models with both local and stochastic features.",
        "references": [
            {
                "arxivId": "1607.00077",
                "title": "Existence of a calibrated regime switching local volatility model",
                "abstract": "By Gy\u00f6ngy's theorem, a local and stochastic volatility model is calibrated to the market prices of all European call options with positive maturities and strikes if its local volatility (LV) function is equal to the ratio of the Dupire LV function over the root conditional mean square of the stochastic volatility factor given the spot value. This leads to a stochastic differential equation (SDE) nonlinear in the sense of McKean. Particle methods based on a kernel approximation of the conditional expectation, as presented in Guyon and Henry\u2010Labord\u00e8re [Risk Magazine, 25, 92\u201397], provide an efficient calibration procedure even if some calibration errors may appear when the range of the stochastic volatility factor is very large. But so far, no global existence result is available for the SDE nonlinear in the sense of McKean. When the stochastic volatility factor is a jump process taking finitely many values and with jump intensities depending on the spot level, we prove existence of a solution to the associated Fokker\u2013Planck equation under the condition that the range of the squared stochastic volatility factor is not too large. We then deduce existence to the calibrated model by extending the results in Figalli [Journal of Functional Analysis, 254(1), 109\u2013153]."
            },
            {
                "arxivId": "1302.2009",
                "title": "STOCHASTIC LOCAL INTENSITY LOSS MODELS WITH INTERACTING PARTICLE SYSTEMS",
                "abstract": "It is well known from the work of Sch\u00f6nbucher that the marginal laws of a loss process can be matched by a unit increasing time inhomogeneous Markov process, whose deterministic jump intensity is called local intensity. The stochastic local intensity (SLI) models such as the one proposed by Arnsdorf and Halperin allow to get a stochastic jump intensity while keeping the same marginal laws. These models involve a nonlinear stochastic differential equation (SDE) with jumps. The first contribution of this paper is to prove the existence and uniqueness of such processes. This is made by means of an interacting particle system, whose convergence rate toward the nonlinear SDE is analyzed. Second, this approach provides a powerful way to compute pathwise expectations with the SLI model: we show that the computational cost is roughly the same as a crude Monte Carlo algorithm for standard SDEs."
            },
            {
                "arxivId": "1011.0111",
                "title": "Mimicking an It\u00f4 process by a solution of a stochastic differential equation",
                "abstract": "Given a multi-dimensional It\\^{o} process whose drift and diffusion terms are adapted processes, we construct a weak solution to a stochastic differential equation that matches the distribution of the It\\^{o} process at each fixed time. Moreover, we show how to match the distributions at each fixed time of functionals of the It\\^{o} process, including the running maximum and running average of one of the components of the process. A consequence of this result is that a wide variety of exotic derivative securities have the same prices when the underlying asset price is modeled by the original It\\^{o} process or the mimicking process that solves the stochastic differential equation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-26.json",
        "arxivId": "2403.16006",
        "category": "q-fin",
        "title": "Crypto Inverse-Power Options and Fractional Stochastic Volatility",
        "abstract": "Recent empirical evidence has highlighted the crucial role of jumps in both price and volatility within the cryptocurrency market. In this paper, we introduce an analytical model framework featuring fractional stochastic volatility, accommodating price--volatility co-jumps and volatility short-term dependency concurrently. We particularly focus on inverse options, including the emerging Quanto inverse options and their power-type generalizations, aimed at mitigating cryptocurrency exchange rate risk and adjusting inherent risk exposure. Characteristic function-based pricing--hedging formulas are derived for these inverse options. The general model framework is then applied to asymmetric Laplace jump-diffusions and Gaussian-mixed tempered stable-type processes, employing three types of fractional kernels, for an extensive empirical analysis involving model calibration on two independent Bitcoin options data sets, during and after the COVID-19 pandemic. Key insights from our theoretical analysis and empirical findings include: (1) the superior performance of fractional stochastic-volatility models compared to various benchmark models, including those incorporating jumps and stochastic volatility, (2) the practical necessity of jumps in both price and volatility, along with their co-jumps and rough volatility, in the cryptocurrency market, (3) stability of calibrated parameter values in line with stylized facts, and (4) the suggestion that a piecewise kernel offers much higher computational efficiency relative to the commonly used Riemann--Liouville kernel in constructing fractional models, yet maintaining the same accuracy level, thanks to its potential for obtaining explicit model characteristic functions.",
        "references": [
            {
                "arxivId": "2009.02583",
                "title": "Average\u2010tempered stable subordinators with applications",
                "abstract": "In this paper the running average of a subordinator with a tempered stable distribution is considered. We investigate a family of previously unexplored infinite-activity subordinators induced by the probability distribution of the running average process and determine their jump intensity measures. Special cases including gamma processes and inverse Gaussian processes are discussed. Then we derive easily implementable formulas for the distribution functions, cumulants, and moments, as well as provide explicit estimates for their asymptotic behaviors. Numerical experiments are conducted for illustrating the applicability and efficiency of the proposed formulas. Two important extensions of the running average process and its equi-distributed subordinator are examined with concrete applications to structural degradation modeling and financial derivatives pricing, where their advantages relative to several existing models are highlighted together with the mention of Euler discretization and compound Poisson approximation techniques."
            },
            {
                "arxivId": "1708.08796",
                "title": "Affine Volterra processes",
                "abstract": "We introduce affine Volterra processes, defined as solutions of certain stochastic convolution equations with affine coefficients. Classical affine diffusions constitute a special case, but affine Volterra processes are neither semimartingales, nor Markov processes in general. We provide explicit exponential-affine representations of the Fourier-Laplace functional in terms of the solution of an associated system of deterministic integral equations of convolution type, extending well-known formulas for classical affine diffusions. For specific state spaces, we prove existence, uniqueness, and invariance properties of solutions of the corresponding stochastic convolution equations. Our arguments avoid infinite-dimensional stochastic analysis as well as stochastic integration with respect to non-semimartingales, relying instead on tools from the theory of finite-dimensional deterministic convolution equations. Our findings generalize and clarify recent results in the literature on rough volatility models in finance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2005.11500",
        "category": "q-fin",
        "title": "Quickest Detection of Ecological Regimes for Natural Resource Management",
        "abstract": null,
        "references": [
            {
                "arxivId": "1506.07134",
                "title": "Catastrophic Regime Shift in Water Reservoirs and S\u00e3o Paulo Water Supply Crisis",
                "abstract": "The relation between rainfall and water accumulated in reservoirs comprises nonlinear feedbacks. Here we show that they may generate alternative equilibrium regimes, one of high water-volume, the other of low water-volume. Reservoirs can be seen as socio-environmental systems at risk of regime shifts, characteristic of tipping point transitions. We analyze data from stored water, rainfall, and water inflow and outflow in the main reservoir serving the metropolitan area of S\u00e3o Paulo, Brazil, by means of indicators of critical regime shifts, and find a strong signal of a transition. We furthermore build a mathematical model that gives a mechanistic view of the dynamics and demonstrates that alternative stable states are an expected property of water reservoirs. We also build a stochastic version of this model that fits well to the data. These results highlight the broader aspect that reservoir management must account for their intrinsic bistability, and should benefit from dynamical systems theory. Our case study illustrates the catastrophic consequences of failing to do so."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2112.11808",
        "category": "q-fin",
        "title": "Mild to Classical Solutions for XVA Equations Under Stochastic Volatility",
        "abstract": "We extend the valuation of contingent claims in presence of default, collateral and funding to a random functional setting and characterise pre-default value processes by martingales. Pre-default value semimartingales can also be described by BSDEs with random path-dependent coefficients and martingales as drivers. En route, we generalise previous settings by relaxing conditions on the available market information, allowing for an arbitrary default-free filtration and constructing a broad class of default times. Moreover, under stochastic volatility, we characterise pre-default value processes via mild solutions to parabolic semilinear PDEs and give sufficient conditions for mild solutions to exist uniquely and to be classical.",
        "references": [
            {
                "arxivId": "1905.11328",
                "title": "A unified approach to xVA with CSA discounting and initial margin",
                "abstract": "In this paper we extend the existing literature on xVA along three directions. First, we enhance current BSDE-based xVA frameworks to include initial margin in presence of defaults. Next, we solve ..."
            },
            {
                "arxivId": "1806.08988",
                "title": "On the support of solutions to stochastic differential equations with path-dependent coefficients",
                "abstract": null
            },
            {
                "arxivId": "1802.10228",
                "title": "Risk-Neutral Valuation Under Differential Funding Costs, Defaults and Collateralization",
                "abstract": "We develop a unified valuation theory that incorporates credit risk (defaults), collateralization and funding costs, by expanding the replication approach to a generality that has not yet been studied previously and reaching valuation when replication is not assumed. This unifying theoretical framework clarifies the relationship between the two valuation approaches: the adjusted cash flows approach pioneered for example by Brigo, Pallavicini and co-authors ([12, 13, 34]) and the classic replication approach illustrated for example by Bielecki and Rutkowski and co-authors ([3, 8]). In particular, results of this work cover most previous papers where the authors studied specific replication models."
            },
            {
                "arxivId": "1701.03272",
                "title": "Markovian integral equations",
                "abstract": "We analyze multidimensional Markovian integral equations that are formulated with a time-inhomogeneous progressive Markov process that has Borel measurable transition probabilities. In the case of a path-dependent diffusion process, the solutions to these integral equations lead to the concept of mild solutions to semilinear parabolic path-dependent partial differential equations (PPDEs). Our goal is to establish uniqueness, stability, existence, and non-extendibility of solutions among a certain class of maps. By requiring the Feller property of the Markov process, we give weak conditions under which solutions become continuous. Moreover, we provide a multidimensional Feynman-Kac formula and a one-dimensional global existence- and uniqueness result."
            },
            {
                "arxivId": "1611.08318",
                "title": "Mild and viscosity solutions to semilinear parabolic path-dependent PDEs",
                "abstract": "We study and compare two concepts for weak solutions to semilinear parabolic path-dependent partial differential equations (PPDEs). The first is that of mild solutions as it appears, e.g., in the log-Laplace functionals of historical superprocesses. The aim of this paper is to show that mild solutions are also solutions in a viscosity sense. This result is motivated by the fact that mild solutions can provide value functions and optimal strategies for problems of stochastic optimal control. Since unique mild solutions exist under weak conditions, we obtain as a corollary a general existence result for viscosity solutions to semiilinear parabolic PPDEs."
            },
            {
                "arxivId": "1608.02690",
                "title": "Arbitrage\u2010free XVA",
                "abstract": "We develop a framework for computing the total valuation adjustment (XVA) of a European claim accounting for funding costs, counterparty credit risk, and collateralization. Based on no\u2010arbitrage arguments, we derive backward stochastic differential equations associated with the replicating portfolios of long and short positions in the claim. This leads to the definition of buyer's and seller's XVA, which in turn identify a no\u2010arbitrage interval. In the case that borrowing and lending rates coincide, we provide a fully explicit expression for the unique XVA, expressed as a percentage of the price of the traded claim, and for the corresponding replication strategies. In the general case of asymmetric funding, repo, and collateral rates, we study the semilinear partial differential equations characterizing buyer's and seller's XVA and show the existence of a unique classical solution to it. To illustrate our results, we conduct a numerical study demonstrating how funding costs, repo rates, and counterparty risk contribute to determine the total valuation adjustment."
            },
            {
                "arxivId": "1502.05648",
                "title": "Path-dependent equations and viscosity solutions in infinite dimension",
                "abstract": "Path Dependent PDE's (PPDE's) are natural objects to study when one deals with non Markovian models. Recently, after the introduction (see [12]) of the so-called pathwise (or functional or Dupire) calculus, various papers have been devoted to study the well-posedness of such kind of equations, both from the point of view of regular solutions (see e.g. [18]) and viscosity solutions (see e.g. [13]), in the case of finite dimensional underlying space. In this paper, motivated by the study of models driven by path dependent stochastic PDE's, we give a first well-posedness result for viscosity solutions of PPDE's when the underlying space is an infinite dimensional Hilbert space. The proof requires a substantial modification of the approach followed in the finite dimensional case. We also observe that, differently from the finite dimensional case, our well-posedness result, even in the Markovian case, apply to equations which cannot be treated, up to now, with the known theory of viscosity solutions."
            },
            {
                "arxivId": "1310.0150",
                "title": "Stability",
                "abstract": "Reproducibility is imperative for any scientific discovery. More often than not, modern scientific findings rely on statistical analysis of high-dimensional data. At a minimum, reproducibility manifests itself in stability of statistical results relative to\"reasonable\"perturbations to data and to the model used. Jacknife, bootstrap, and cross-validation are based on perturbations to data, while robust statistics methods deal with perturbations to models. In this article, a case is made for the importance of stability in statistics. Firstly, we motivate the necessity of stability for interpretable and reliable encoding models from brain fMRI signals. Secondly, we find strong evidence in the literature to demonstrate the central role of stability in statistical inference, such as sensitivity analysis and effect detection. Thirdly, a smoothing parameter selector based on estimation stability (ES), ES-CV, is proposed for Lasso, in order to bring stability to bear on cross-validation (CV). ES-CV is then utilized in the encoding models to reduce the number of predictors by 60% with almost no loss (1.3%) of prediction performance across over 2,000 voxels. Last, a novel\"stability\"argument is seen to drive new results that shed light on the intriguing interactions between sample to sample variability and heavier tail error distribution (e.g., double-exponential) in high-dimensional regression models with $p$ predictors and $n$ independent samples. In particular, when $p/n\\rightarrow\\kappa\\in(0.3,1)$ and the error distribution is double-exponential, the Ordinary Least Squares (OLS) is a better estimator than the Least Absolute Deviation (LAD) estimator."
            },
            {
                "arxivId": "1210.3811",
                "title": "Funding, Collateral and Hedging: Uncovering the Mechanics and the Subtleties of Funding Valuation Adjustments",
                "abstract": "The main result of this paper is a collateralized counterparty valuation adjusted pricing equation, which allows to price a deal while taking into account credit and debit valuation adjustments (CVA, DVA) along with margining and funding costs, all in a consistent way. Funding risk breaks the bilateral nature of the valuation formula. We find that the equation has a recursive form, making the introduction of a purely additive funding valuation adjustment (FVA) difficult. Yet, we can cast the pricing equation into a set of iterative relationships which can be solved by means of standard least-square Monte Carlo techniques. As a consequence, we find that identifying funding costs and debit valuation adjustments is not tenable in general, contrary to what has been suggested in the literature in simple cases. The assumptions under which funding costs vanish are a very special case of the more general theory. We define a comprehensive framework that allows us to derive earlier results on funding or counterparty risk as a special case, although our framework is more than the sum of such special cases. We derive the general pricing equation by resorting to a risk-neutral approach where the new types of risks are included by modifying the payout cash flows. We consider realistic settings and include in our models the common market practices suggested by ISDA documentation, without assuming restrictive constraints on margining procedures and close-out netting rules. In particular, we allow for asymmetric collateral and funding rates, and exogenous liquidity policies and hedging strategies. Re-hypothecation liquidity risk and close-out amount evaluation issues are also covered. Finally, relevant examples of non-trivial settings illustrate how to derive known facts about discounting curves from a robust general framework and without resorting to ad hoc hypotheses."
            },
            {
                "arxivId": "1205.6542",
                "title": "COLLATERALIZED CVA VALUATION WITH RATING TRIGGERS AND CREDIT MIGRATIONS",
                "abstract": "In this paper we discuss the issue of computation of the bilateral credit valuation adjustment (CVA) under rating triggers, and in presence of ratings-linked margin agreements. Specifically, we consider collateralized OTC contracts, that are subject to rating triggers, between two parties \u2014 an investor and a counterparty. Moreover, we model the margin process as a functional of the credit ratings of the counterparty and the investor. We employ a Markovian approach for modeling of the rating transitions of the two parties to the contract. In this framework, we derive the representation for bilateral CVA. We also introduce a new component in the decomposition of the counterparty risky price: namely the rating valuation adjustment (RVA) that accounts for the rating triggers. We give two examples of dynamic collateralization schemes where the margin thresholds are linked to the credit ratings of the parties. Our results are illustrated via computation of various counterparty risk adjustments for a CDS contract and for an IRS contract."
            },
            {
                "arxivId": "1112.1521",
                "title": "Funding Valuation Adjustment: A Consistent Framework Including CVA, DVA, Collateral, Netting Rules and Re-Hypothecation",
                "abstract": "In this paper we describe how to include funding and margining costs into a risk-neutral pricing framework for counterparty credit risk. We consider realistic settings and we include in our models the common market practices suggested by the ISDA documentation without assuming restrictive constraints on margining procedures and close-out netting rules. In particular, we allow for asymmetric collateral and funding rates, and exogenous liquidity policies and hedging strategies. Re-hypothecation liquidity risk and close-out amount evaluation issues are also covered. We define a comprehensive pricing framework which allows us to derive earlier results on funding or counterparty risk. Some relevant examples illustrate the non trivial settings needed to derive known facts about discounting curves by starting from a general framework and without resorting to ad hoc hypotheses. Our main result is a bilateral collateralized counterparty valuation adjusted pricing equation, which allows to price a deal while taking into account credit and debt valuation adjustments along with margining and funding costs in a coherent way. We find that the equation has a recursive form, making the introduction of an additive funding valuation adjustment difficult. Yet, we can cast the pricing equation into a set of iterative relationships which can be solved by means of standard least-square Monte Carlo techniques."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2305.16915",
        "category": "q-fin",
        "title": "When Is Cross Impact Relevant?",
        "abstract": "Trading pressure from one asset can move the price of another, a phenomenon referred to as cross impact. Using tick-by-tick data spanning 5 years for 500 assets listed in the United States, we identify the features that make cross-impact relevant to explain the variance of price returns. We show that price formation occurs endogenously within highly liquid assets. Then, trades in these assets influence the prices of less liquid correlated products, with an impact velocity constrained by their minimum trading frequency. We investigate the implications of such a multidimensional price formation mechanism on interest rate markets. We find that the 10-year bond future serves as the primary liquidity reservoir, influencing the prices of cash bonds and futures contracts within the interest rate curve. Such behaviour challenges the validity of the theory in Financial Economics that regards long-term rates as agents anticipations of future short term rates.",
        "references": [
            {
                "arxivId": "1612.07742",
                "title": "Cross-impact and no-dynamic-arbitrage",
                "abstract": "We extend the \u2018No-dynamic-arbitrage and market impact\u2019-framework of Gatheral [Quant. Finance, 2010, 10(7), 749\u2013759] to the multi-dimensional case where trading in one asset has a cross-impact on the price of other assets. From the condition of absence of dynamical arbitrage we derive theoretical limits for the size and form of cross-impact that can be directly verified on data. For bounded decay kernels we find that cross-impact must be an odd and linear function of trading intensity and cross-impact from asset i to asset j must be equal to the one from j to i. To test these constraints we estimate cross-impact among sovereign bonds traded on the electronic platform MOT. While we find significant violations of the above symmetry condition of cross-impact, we show that these are not arbitrageable with simple strategies because of the presence of the bid-ask spread."
            },
            {
                "arxivId": "1609.02395",
                "title": "Dissecting cross-impact on stock markets: an empirical analysis",
                "abstract": "The vast majority of market impact studies assess each product individually, and the interactions between the different order flows are disregarded. This strong approximation may lead to an underestimation of trading costs and possible contagion effects. Transactions in fact mediate a significant part of the correlation between different instruments. In turn, liquidity shares the sectorial structure of market correlations, which can be encoded as a set of eigenvalues and eigenvectors. We introduce a multivariate linear propagator model that successfully describes such a structure, and accounts for a significant fraction of the covariance of stock returns. We dissect the various dynamical mechanisms that contribute to the joint dynamics of assets. We also define two simplified models with substantially less parameters in order to reduce overfitting, and show that they have superior out-of-sample performance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2306.02987",
        "category": "q-fin",
        "title": "Frequency Regulation with Storage: On Losses and Profits",
        "abstract": null,
        "references": [
            {
                "arxivId": "1909.09110",
                "title": "Stochastic properties of the frequency dynamics in real and synthetic power grids",
                "abstract": "The frequency constitutes a key state variable of electrical power grids. However, as the frequency is subject to several sources of fluctuations, ranging from renewable volatility to demand fluctuations and dispatch, it is strongly dynamic. Yet, the statistical and stochastic properties of the frequency fluctuation dynamics are far from fully understood. Here, we analyse properties of power grid frequency trajectories recorded from different synchronous regions. We highlight the non-Gaussian and still approximately Markovian nature of the frequency statistics. Further, we find that the frequency displays significant fluctuations exactly at the time intervals of regulation and trading, confirming the need of having a regulatory and market design that respects the technical and dynamical constraints in future highly renewable power grids. Finally, employing a recently proposed synthetic model for the frequency dynamics, we combine our statistical and stochastic analysis and analyse in how far dynamically modelled frequency properties match the ones of real trajectories."
            },
            {
                "arxivId": "1102.5750",
                "title": "Neyman-Pearson Classification, Convexity and Stochastic Constraints",
                "abstract": "Motivated by problems of anomaly detection, this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classification with a convex loss. Given a finite collection of classifiers, we combine them and obtain a new classifier that satisfies simultaneously the two following properties with high probability: (i) its probability of type I error is below a pre-specified level and (ii), it has probability of type II error close to the minimum possible. The proposed classifier is obtained by solving an optimization problem with an empirical objective and an empirical constraint. New techniques to handle such problems are developed and have consequences on chance constrained programming."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2311.16570",
        "category": "q-fin",
        "title": "Epistemic Limits of Empirical Finance: Causal Reductionism and Self-Reference",
        "abstract": "The clarion call for causal reduction in the study of capital markets is intensifying. However, in self-referencing and open systems such as capital markets, the idea of unidirectional causation (if applicable) may be limiting at best, and unstable or fallacious at worst. In this research, we critically assess the use of scientific deduction and causal inference within the study of empirical finance and econometrics. We then demonstrate the idea of competing causal chains using a toy model adapted from ecological predator/prey relationships. From this, we develop the alternative view that the study of empirical finance, and the risks contained therein, may be better appreciated once we admit that our current arsenal of quantitative finance tools may be limited to ex post causal inference under popular assumptions. Where these assumptions are challenged, for example in a recognizable reflexive context, the prescription of unidirectional causation proves deeply problematic.",
        "references": [
            {
                "arxivId": "1310.4067",
                "title": "On pricing kernels, information and risk",
                "abstract": "This paper compares out-of-sample, ex-ante risk and returns of arbitrage pricing theory (APT) risk-factor based, zero-cost portfolios with characteristic-based, zero-cost portfolios. In particular the Haugen and Baker characteristic-based model framework is used in a comparison with the capital asset pricing model (CAPM) (Haugen, R. A., & Baker, L. N. (1996). Commonality in the determinants of expected stocks returns. Journal of Financial Economics, 41, 401\u2013439), and three-factor Fama and French APT model portfolios to analyse returns of stocks listed on the Johannesburg Stock Exchange (Fama, E., & French, K. (1993). Common risk factors in the returns on stocks and bonds. Journal of Financial Economics, 33, 3\u201356). The finding that cross-sectional characteristic-based models have yielded portfolios with higher excess monthly returns but lower risk than their arbitrage pricing theory counterparts is discussed. Under the assumption of general no arbitrage conditions, it is argued that evidence in favour of characteristic-based pricing provides insight into the non-linear nature in which information is assimilated into pricing kernels for the market considered."
            },
            {
                "arxivId": "1209.0453",
                "title": "Crises and Collective Socio-Economic Phenomena: Simple Models and Challenges",
                "abstract": null
            },
            {
                "arxivId": "1109.4250",
                "title": "Complex dynamics in learning complicated games",
                "abstract": "Game theory is the standard tool used to model strategic interactions in evolutionary biology and social science. Traditionally, game theory studies the equilibria of simple games. However, is this useful if the game is complicated, and if not, what is? We define a complicated game as one with many possible moves, and therefore many possible payoffs conditional on those moves. We investigate two-person games in which the players learn based on a type of reinforcement learning called experience-weighted attraction (EWA). By generating games at random, we characterize the learning dynamics under EWA and show that there are three clearly separated regimes: (i) convergence to a unique fixed point, (ii) a huge multiplicity of stable fixed points, and (iii) chaotic behavior. In case (iii), the dimension of the chaotic attractors can be very high, implying that the learning dynamics are effectively random. In the chaotic regime, the total payoffs fluctuate intermittently, showing bursts of rapid change punctuated by periods of quiescence, with heavy tails similar to what is observed in fluid turbulence and financial markets. Our results suggest that, at least for some learning algorithms, there is a large parameter regime for which complicated strategic interactions generate inherently unpredictable behavior that is best described in the language of dynamical systems theory."
            },
            {
                "arxivId": "0810.5306",
                "title": "Economics needs a scientific revolution",
                "abstract": null
            },
            {
                "arxivId": "0710.4235",
                "title": "Top-down causation by information control: from a philosophical problem to a scientific research programme",
                "abstract": "It has been claimed that different types of causes must be considered in biological systems, including top-down as well as same-level and bottom-up causation, thus enabling the top levels to be causally efficacious in their own right. To clarify this issue, the important distinctions between information and signs are introduced here and the concepts of information control and functional equivalence classes in those systems are rigorously defined and used to characterize when top-down causation by feedback control happens, in a way that is testable. The causally significant elements we consider are equivalence classes of lower level processes, realized in biological systems through different operations having the same outcome within the context of information control and networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2402.17919",
        "category": "q-fin",
        "title": "Quanto Option Pricing on a Multivariate Levy Process Model with a Generative Artificial Intelligence",
        "abstract": "In this study, we discuss a machine learning technique to price exotic options with two underlying assets based on a non-Gaussian Levy process model. We introduce a new multivariate Levy process model named the generalized normal tempered stable (gNTS) process, which is defined by time-changed multivariate Brownian motion. Since the gNTS process does not provide a simple analytic formula for the probability density function (PDF), we use the conditional real-valued non-volume preserving (CRealNVP) model, which is a type of flow-based generative network. Then, we discuss the no-arbitrage pricing on the gNTS model for pricing the quanto option, whose underlying assets consist of a foreign index and foreign exchange rate. We present the training of the CRealNVP model to learn the PDF of the gNTS process using a training set generated by Monte Carlo simulation. Next, we estimate the parameters of the gNTS model with the trained CRealNVP model using the empirical data observed in the market. Finally, we provide a method to find an equivalent martingale measure on the gNTS model and to price the quanto option using the CRealNVP model with the risk-neutral parameters of the gNTS model.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17095",
        "category": "q-fin",
        "title": "Revisiting Boehmer et al. (2021): Recent Period, Alternative Method, Different Conclusions",
        "abstract": "We reassess Boehmer et al. (2021, BJZZ)'s seminal work on the predictive power of retail order imbalance (ROI) for future stock returns. First, we replicate their 2010-2015 analysis in the more recent 2016-2021 period. We find that the ROI's predictive power weakens significantly. Specifically, past ROI can no longer predict weekly returns on large-cap stocks, and the long-short strategy based on past ROI is no longer profitable. Second, we analyze the effect of using the alternative quote midpoint (QMP) method to identify and sign retail trades on their main conclusions. While the results based on the QMP method align with BJZZ's findings in 2010-2015, the two methods provide different conclusions in 2016-2021. Our study shows that BJZZ's original findings are sensitive to the sample period and the approach to identify ROIs.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17112",
        "category": "q-fin",
        "title": "The Impact of Pradhan Mantri Ujjwala Yojana on Indian Households",
        "abstract": "This study critically evaluates the impact of the Pradhan Mantri Ujjwala Yojana (PMUY) on LPG accessibility among poor households in India. Using Propensity Score Matching and Difference-in-Differences estimators and the National Family Health Survey (NFHS) dataset, the Average Treatment Effect on the interdedly Treated is a modest 2.1 percentage point increase in LPG consumption due to PMUY, with a parallel decrease in firewood consumption. Regional analysis reveals differential impacts, with significant progress in the North, West, and South but less pronounced effects in the East and North East. The study also underscores variance across social groups, with Schedule Caste households showing the most substantial benefits, while Scheduled Tribes households are hardly affected. Despite the PMUY's initial success in facilitating LPG access, sustaining its usage remains challenging. Policy should emphasise targeted interventions, income support, and address regional and community-specific disparities for the sustained usage of LPG.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17127",
        "category": "q-fin",
        "title": "High-Dimensional Mean-Variance Spanning Tests",
        "abstract": "We introduce a new framework for the mean-variance spanning (MVS) hypothesis testing. The procedure can be applied to any test-asset dimension and only requires stationary asset returns and the number of benchmark assets to be smaller than the number of time periods. It involves individually testing moment conditions using a robust Student-t statistic based on the batch-mean method and combining the p-values using the Cauchy combination test. Simulations demonstrate the superior performance of the test compared to state-of-the-art approaches. For the empirical application, we look at the problem of domestic versus international diversification in equities. We find that the advantages of diversification are influenced by economic conditions and exhibit cross-country variation. We also highlight that the rejection of the MVS hypothesis originates from the potential to reduce variance within the domestic global minimum-variance portfolio.",
        "references": [
            {
                "arxivId": "2107.06040",
                "title": "The Cauchy Combination Test under Arbitrary Dependence Structures",
                "abstract": "Abstract Combining individual p-values to perform an overall test is often encountered in statistical applications. The Cauchy combination test (CCT) (Journal of the American Statistical Association, 2020, 115, 393\u2013402) is a powerful and computationally efficient approach to integrate individual p-values under arbitrary dependence structures for sparse signals. We revisit this test to additionally show that (i) the tail probability of the CCT can be approximated just as well when more relaxed assumptions are imposed on individual p-values compared to those of the original test statistics; (ii) such assumptions are satisfied by six popular copula distributions; and (iii) the power of the CCT is no less than that of the minimum p-value test when the number of p-values goes to infinity under some regularity conditions. These findings are confirmed by both simulations and applications in two real datasets, thus, further broadening the theory and applications of the CCT."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17162",
        "category": "q-fin",
        "title": "Design Insights for Industrial CO2 Capture, Transport, and Storage Systems",
        "abstract": "We present design methods and insights for CO2 capture, transport, and storage systems for clusters of industrial facilities, with a case-study focus on the state of Louisiana. Our analytical framework includes: (1) evaluating the scale and concentration of capturable CO2 emissions at individual facilities for the purpose of estimating the cost of CO2 capture retrofits, (2) a screening method to identify potential CO2 storage sites and estimate their storage capacities, injectivities, and costs; and (3) an approach for cost-minimized design of pipeline infrastructure connecting CO2 capture plants with storage sites that considers land use patterns, existing rights-of-way, demographics, and a variety of social and environmental justice factors. In applying our framework to Louisiana, we estimate up to 50 million tCO2/y of industrial emissions (out of today's total emissions of 130 MtCO2/y) can be captured at under 100 USD/tCO2, and up to 100 MtCO2/y at under 120 USD/tCO2. We identified 98 potential storage sites with estimated aggregate total injectivity between 330 and 730 MtCO2/yr and storage costs ranging from 8 to 17 USD/tCO2. We find dramatic reductions in the aggregate pipeline length and CO2 transport cost per tonne when groups of capture plants share pipeline infrastructure rather than build dedicated single-user pipelines. Smaller facilities (emitting less than 1 MtCO2/y), which account for a quarter of Louisiana's industrial emissions, see the largest transport cost benefits from sharing of infrastructure. Pipeline routes designed to avoid disadvantaged communities (social and environmental justice) so as not to reinforce historical practices of disenfranchisement involve only modestly higher pipeline lengths and costs.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17187",
        "category": "q-fin",
        "title": "Alternatives to classical option pricing",
        "abstract": "We develop two alternate approaches to arbitrage-free, market-complete, option pricing. The first approach requires no riskless asset. We develop the general framework for this approach and illustrate it with two specific examples. The second approach does use a riskless asset. However, by ensuring equality between real-world and risk-neutral price-change probabilities, the second approach enables the computation of risk-neutral option prices utilizing expectations under the natural world probability P. This produces the same option prices as the classical approach in which prices are computed under the risk neutral measure Q. The second approach and the two specific examples of the first approach require the introduction of new, marketable asset types, specifically perpetual derivatives of a stock, and a stock whose cumulative return (rather than price) is deflated.",
        "references": [
            {
                "arxivId": "2303.17014",
                "title": "Option pricing using a skew random walk pricing tree",
                "abstract": "Motivated by the Corns-Satchell, continuous time, option pricing model, we develop a binary tree pricing model with underlying asset price dynamics following It\\^o-Mckean skew Brownian motion. While the Corns-Satchell market model is incomplete, our discrete time market model is defined in the natural world; extended to the risk neutral world under the no-arbitrage condition where derivatives are priced under uniquely determined risk-neutral probabilities; and is complete. The skewness introduced in the natural world is preserved in the risk neutral world. Furthermore, we show that the model preserves skewness under the continuous-time limit. We provide numerical applications of our model to the valuation of European put and call options on exchange-traded funds tracking the S&P Global 1200 index."
            },
            {
                "arxivId": "0906.2318",
                "title": "No Arbitrage Without Semimartingales",
                "abstract": "We show that with suitable restrictions on allowable trading strategies, one has no arbitrage in settings where the traditional theory would admit arbitrage possibilities. In particular, price processes that are not semimartingales are possible in our setting, for example fractional Brownian motion."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17546",
        "category": "q-fin",
        "title": "Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining",
        "abstract": "The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools of text mining and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, text mining techniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances. Finally, topic modeling is used to examine key components and their relative significance in job descriptions. By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals. This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-27.json",
        "arxivId": "2403.17798",
        "category": "q-fin",
        "title": "Ethical considerations when planning, implementing and releasing health economic model software: a new proposal",
        "abstract": "Most health economic analyses are undertaken with the aid of computers. However, the research ethics of implementing health economic models as software (or computational health economic models (CHEMs)) are poorly understood. We propose that developers and funders of CHEMs should adhere to research ethics principles and pursue the goals of: (i) socially acceptable user requirements and design specifications; (ii) fit for purpose implementations; and (iii) socially beneficial post-release use. We further propose that a transparent (T), reusable (R) and updatable (U) CHEM is suggestive of a project team that has largely met these goals. We propose six criteria for assessing TRU CHEMs: (T1) software files are publicly available; (T2) developer contributions and judgments on appropriate use are easily identified; (R1) programming practices facilitate independent reuse of model components; (R2) licenses permit reuse and derivative works; (U1) maintenance infrastructure is in place; and (U2) releases are systematically retested and deprecated. Few existing CHEMs would meet all TRU criteria. Addressing these limitations will require the development of new and updated good practice guidelines and investments by governments and other research funders in enabling infrastructure and human capital.",
        "references": [
            {
                "arxivId": "2005.01469",
                "title": "An environment for sustainable research software in Germany and beyond: current state, open challenges, and call for action",
                "abstract": "Research software has become a central asset in academic research. It optimizes existing and enables new research methods, implements and embeds research knowledge, and constitutes an essential research product in itself. Research software must be sustainable in order to understand, replicate, reproduce, and build upon existing research or conduct new research effectively. In other words, software must be available, discoverable, usable, and adaptable to new needs, both now and in the future. Research software therefore requires an environment that supports sustainability. Hence, a change is needed in the way research software development and maintenance are currently motivated, incentivized, funded, structurally and infrastructurally supported, and legally treated. Failing to do so will threaten the quality and validity of research. In this paper, we identify challenges for research software sustainability in Germany and beyond, in terms of motivation, selection, research software engineering personnel, funding, infrastructure, and legal aspects. Besides researchers, we specifically address political and academic decision-makers to increase awareness of the importance and needs of sustainable research software practices. In particular, we recommend strategies and measures to create an environment for sustainable research software, with the ultimate goal to ensure that software-driven research is valid, reproducible and sustainable, and that software is recognized as a first class citizen in research. This paper is the outcome of two workshops run in Germany in 2019, at deRSE19 - the first International Conference of Research Software Engineers in Germany - and a dedicated DFG-supported follow-up workshop in Berlin."
            },
            {
                "arxivId": "1703.07019",
                "title": "Continuous Integration, Delivery and Deployment: A Systematic Review on Approaches, Tools, Challenges and Practices",
                "abstract": "Continuous practices, i.e., continuous integration, delivery, and deployment, are the software development industry practices that enable organizations to frequently and reliably release new features and products. With the increasing interest in the literature on continuous practices, it is important to systematically review and synthesize the approaches, tools, challenges, and practices reported for adopting and implementing continuous practices. This paper aimed at systematically reviewing the state of the art of continuous practices to classify approaches and tools, identify challenges and practices in this regard, and identify the gaps for future research. We used the systematic literature review method for reviewing the peer-reviewed papers on continuous practices published between 2004 and June 1, 2016. We applied the thematic analysis method for analyzing the data extracted from reviewing 69 papers selected using predefined criteria. We have identified 30 approaches and associated tools, which facilitate the implementation of continuous practices in the following ways: 1) reducing build and test time in continuous integration (CI); 2) increasing visibility and awareness on build and test results in CI; 3) supporting (semi-) automated continuous testing; 4) detecting violations, flaws, and faults in CI; 5) addressing security and scalability issues in deployment pipeline; and 6) improving dependability and reliability of deployment process. We have also determined a list of critical factors, such as testing (effort and time), team awareness and transparency, good design principles, customer, highly skilled and motivated team, application domain, and appropriate infrastructure that should be carefully considered when introducing continuous practices in a given organization. The majority of the reviewed papers were validation (34.7%) and evaluation (36.2%) research types. This paper also reveals that continuous practices have been successfully applied to both greenfield and maintenance projects. Continuous practices have become an important area of software engineering research and practice. While the reported approaches, tools, and practices are addressing a wide range of challenges, there are several challenges and gaps, which require future research work for improving the capturing and reporting of contextual information in the studies reporting different aspects of continuous practices; gaining a deep understanding of how software-intensive systems should be (re-) architected to support continuous practices; and addressing the lack of knowledge and tools for engineering processes of designing and running secure deployment pipelines."
            },
            {
                "arxivId": "1609.00037",
                "title": "Good enough practices in scientific computing",
                "abstract": "Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2206.01114",
        "category": "q-fin",
        "title": "Coarse Wage-Setting and Behavioral Firms",
        "abstract": "This paper shows that the bunching of wages at round numbers is partly driven by firm coarse wage-setting. Using data on 280 million new hires from Brazil, I first establish that salaries tend to cluster at round numbers. Then, I show that firms that tend to hire workers at round-numbered salaries are less sophisticated and have worse market outcomes. Next, I develop a wage-posting model in which optimization costs lead to the adoption of coarse rounded wages and provide evidence supporting three model predictions using two research designs. Finally, I examine some consequences of coarse wage-setting for relevant economic outcomes.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2306.11376",
        "category": "q-fin",
        "title": "Coevolution of cognition and cooperation in structured populations under reinforcement learning",
        "abstract": "We study the evolution of behavior under reinforcement learning in a Prisoner's Dilemma where agents interact in a regular network and can learn about whether they play one-shot or repeatedly by incurring a cost of deliberation. With respect to other behavioral rules used in the literature, (i) we confirm the existence of a threshold value of the probability of repeated interaction, switching the emergent behavior from intuitive defector to dual-process cooperator; (ii) we find a different role of the node degree, with smaller degrees reducing the evolutionary success of dual-process cooperators; (iii) we observe a higher frequency of deliberation.",
        "references": [
            {
                "arxivId": "1907.12990",
                "title": "The evolution of lying in well-mixed populations",
                "abstract": "Lies can have profoundly negative consequences for individuals, groups and even for societies. Understanding how lying evolves and when it proliferates is therefore of significant importance for our personal and societal well-being. To that effect, we here study the sender\u2013receiver game in well-mixed populations with methods of statistical physics. We use the Monte Carlo method to determine the stationary frequencies of liars and believers for four different lie types. We consider altruistic white lies that favour the receiver at a cost to the sender, black lies that favour the sender at a cost to the receiver, spiteful lies that harm both the sender and the receiver, and Pareto white lies that favour both the sender and the receiver. We find that spiteful lies give rise to trivial behaviour, where senders quickly learn that their best strategy is to send a truthful message, while receivers likewise quickly learn that their best strategy is to believe the sender\u2019s message. For altruistic white lies and black lies, we find that most senders lie while most receivers do not believe the sender\u2019s message, but the exact frequencies of liars and non-believers depend significantly on the payoffs, and they also evolve non-monotonically before reaching the stationary state. Lastly, for Pareto white lies we observe the most complex dynamics, with the possibility of both lying and believing evolving with all frequencies between 0 and 1 in dependence on the payoffs. We discuss the implications of these results for moral behaviour in human experiments."
            },
            {
                "arxivId": "1106.6107",
                "title": "Evolution of cooperation facilitated by reinforcement learning with adaptive aspiration levels.",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2308.11138",
        "category": "q-fin",
        "title": "NLP-based detection of systematic anomalies among the narratives of consumer complaints",
        "abstract": "We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.",
        "references": [
            {
                "arxivId": "2212.01285",
                "title": "A Text Analysis for Operational Risk Loss Descriptions",
                "abstract": "Financial institutions manage operational risk (OpRisk) by carrying out activities required by regulation, such as collecting loss data, calculating capital requirements, and reporting. For this purpose, for each OpRisk event, loss amounts, dates, organizational units involved, event types, and descriptions are recorded in the OpRisk databases. In recent years, operational risk functions have been required to go beyond their regulatory tasks to proactively manage operational risk, preventing or mitigating its impact. As OpRisk databases also contain event descriptions, an area of opportunity is to extract information from such texts. The present work introduces for the first time a structured workflow for the application of text analysis techniques (one of the main Natural Language Processing tasks) to the OpRisk event descriptions to identify managerial clusters (more granular than regulatory categories) representing the root-causes of the underlying risks. We have complemented and enriched the established framework of statistical methods based on quantitative data. Specifically, after delicate tasks like data cleaning, text vectorization, and semantic adjustment, we have applied methods of dimensionality reduction and several clustering models with algorithms to compare their performances and weaknesses. Our results improve retrospective knowledge of loss events and enable to mitigate future risks."
            },
            {
                "arxivId": "1904.05790",
                "title": "Absorbing random walks interpolating between centrality measures on complex networks.",
                "abstract": "Centrality, which quantifies the importance of individual nodes, is among the most essential concepts in modern network theory. As there are many ways in which a node can be important, many different centrality measures are in use. Here, we concentrate on versions of the common betweenness and closeness centralities. The former measures the fraction of paths between pairs of nodes that go through a given node, while the latter measures an average inverse distance between a particular node and all other nodes. Both centralities only consider shortest paths (i.e., geodesics) between pairs of nodes. Here we develop a method, based on absorbing Markov chains, that enables us to continuously interpolate both of these centrality measures away from the geodesic limit and toward a limit where no restriction is placed on the length of the paths the walkers can explore. At this second limit, the interpolated betweenness and closeness centralities reduce, respectively, to the well-known current-betweenness and resistance-closeness (information) centralities. The method is tested numerically on four real networks, revealing complex changes in node centrality rankings with respect to the value of the interpolation parameter. Nonmonotonic betweenness behaviors are found to characterize nodes that lie close to intercommunity boundaries in the studied networks."
            },
            {
                "arxivId": "1806.09736",
                "title": "Computational Analysis of Insurance Complaints: GEICO Case Study",
                "abstract": "The online environment has provided a great opportunity for insurance policyholders to share their complaints with respect to different services. These complaints can reveal valuable information for insurance companies who seek to improve their services; however, analyzing a huge number of online complaints is a complicated task for human and must involve computational methods to create an efficient process. This research proposes a computational approach to characterize the major topics of a large number of online complaints. Our approach is based on using the topic modeling approach to disclose the latent semantic of complaints. The proposed approach deployed on thousands of GEICO negative reviews. Analyzing 1,371 GEICO complaints indicates that there are 30 major complains in four categories: (1) customer service, (2) insurance coverage, paperwork, policy, and reports, (3) legal issues, and (4) costs, estimates, and payments. This research approach can be used in other applications to explore a large number of reviews."
            },
            {
                "arxivId": "1806.06295",
                "title": "Detecting intrusions in control systems : A rule of thumb, its justification and illustrations",
                "abstract": "Abstract Control systems are exposed to unintentional errors, deliberate intrusions, false data injection attacks, and various other disruptions. In this paper we propose, justify, and illustrate a rule of thumb for detecting, or confirming the absence of, such disruptions. To facilitate the use of the rule, we rigorously discuss underlying results that delineate the boundaries of the rule\u2019s applicability. We also discuss ways to further widen the applicability of the proposed intrusion-detection methodology."
            },
            {
                "arxivId": "1805.10633",
                "title": "Assessing Transfer Functions in Control Systems",
                "abstract": null
            },
            {
                "arxivId": "1712.09661",
                "title": "Estimating the Index of Increase via Balancing Deterministic and Random Data",
                "abstract": null
            },
            {
                "arxivId": "1705.02742",
                "title": "Quantifying non-monotonicity of functions and the lack of positivity in signed measures",
                "abstract": "In various research areas related to decision making, problems and their solutions frequently rely on certain functions being monotonic. In the case of non-monotonic functions, one would then wish to quantify their lack of monotonicity. In this paper we develop a method designed specifically for this task, including quantification of the lack of positivity, negativity, or sign-constancy in signed measures. We note relevant applications in Insurance, Finance, and Economics, and discuss some of them in detail."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2402.06731",
        "category": "q-fin",
        "title": "Closed-form solutions for generic N-token AMM arbitrage",
        "abstract": "Convex optimisation has provided a mechanism to determine arbitrage trades on automated market markets (AMMs) since almost their inception. Here we outline generic closed-form solutions for $N$-token geometric mean market maker pool arbitrage, that in simulation (with synthetic and historic data) provide better arbitrage opportunities than convex optimisers and is able to capitalise on those opportunities sooner. Furthermore, the intrinsic parallelism of the proposed approach (unlike convex optimisation) offers the ability to scale on GPUs, opening up a new approach to AMM modelling by offering an alternative to numerical-solver-based methods. The lower computational cost of running this new mechanism can also enable on-chain arbitrage bots for multi-asset pools.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2402.19380",
        "category": "q-fin",
        "title": "Not flexible enough? Impacts of electric carsharing on a power sector with variable renewables",
        "abstract": "Electrifying the car fleet is a major strategy for mitigating greenhouse gas emissions in the transport sector. However, electrification alone will not solve all the negative externalities associated with cars. In light of other problems such as street space as well as concerns about the use of mineral resources for battery electric cars, reducing the car fleet size would be beneficial, particularly in cities. Carsharing could offer a way to reconcile current car usage habits with a reduction in the car fleet size. However, it could also reduce the potential of electric cars to align their grid interactions with variable renewable electricity generation. We investigate how electric carsharing may impact the power sector in the future. We combine three open-source quantitative methods, including sequence clustering of car travel diaries, a probabilistic tool to generate synthetic electric vehicle time series, and an optimization model of the power sector. For 2030 scenarios of Germany with a renewable share of at least 80%, we show that switching to electric carsharing only moderately increases power sector costs. In our main setting, carsharing increases yearly power sector costs by less than 100 euros per substituted private electric car. This cost effect is largest under the assumption of bidirectional charging. It is mitigated when other sources of flexibility for the power sector are considered. Carsharing further causes a shift from wind power to solar PV in the optimal capacity mix, and may also trigger additional investments in stationary electricity storage. Overall, we find that shared electric cars still have the potential to be operated largely in line with variable renewable electricity generation. We conclude that electric carsharing is unlikely to cause much damage to the power sector, but could bring various other benefits, which may outweigh power sector cost increases.",
        "references": [
            {
                "arxivId": "1801.05290",
                "title": "Synergies of sector coupling and transmission reinforcement in a cost-optimised, highly renewable European energy system",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2403.09267",
        "category": "q-fin",
        "title": "Deep Limit Order Book Forecasting",
        "abstract": "We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that evaluates predictions' practicality by focusing on the probability of accurately forecasting complete transactions. This work offers academics and practitioners an avenue to make informed and robust decisions on the application of deep learning techniques, their scope and limitations, effectively exploiting emergent statistical properties of the Limit Order Book.",
        "references": [
            {
                "arxivId": "2308.01915",
                "title": "LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study",
                "abstract": "The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation, and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions."
            },
            {
                "arxivId": "2306.15337",
                "title": "Homological Neural Networks: A Sparse Architecture for Multivariate Complexity",
                "abstract": "The rapid progress of Artificial Intelligence research came with the development of increasingly complex deep learning models, leading to growing challenges in terms of computational complexity, energy efficiency and interpretability. In this study, we apply advanced network-based information filtering techniques to design a novel deep neural network unit characterized by a sparse higher-order graphical architecture built over the homological structure of underlying data. We demonstrate its effectiveness in two application domains which are traditionally challenging for deep learning: tabular data and time series regression problems. Results demonstrate the advantages of this novel design which can tie or overcome the results of state-of-the-art machine learning and deep learning models using only a fraction of parameters."
            },
            {
                "arxivId": "2302.11371",
                "title": "Ftx's Downfall and Binance's Consolidation: The Fragility of Centralized Digital Finance",
                "abstract": null
            },
            {
                "arxivId": "2301.08688",
                "title": "Asynchronous Deep Double Dueling Q-learning for trading-signal execution in limit order book markets",
                "abstract": "We employ deep reinforcement learning (RL) to train an agent to successfully translate a high-frequency trading signal into a trading strategy that places individual limit orders. Based on the ABIDES limit order book simulator, we build a reinforcement learning OpenAI gym environment and utilize it to simulate a realistic trading environment for NASDAQ equities based on historic order book messages. To train a trading agent that learns to maximize its trading return in this environment, we use Deep Dueling Double Q-learning with the APEX (asynchronous prioritized experience replay) architecture. The agent observes the current limit order book state, its recent history, and a short-term directional forecast. To investigate the performance of RL for adaptive trading independently from a concrete forecasting algorithm, we study the performance of our approach utilizing synthetic alpha signals obtained by perturbing forward-looking returns with varying levels of noise. Here, we find that the RL agent learns an effective trading strategy for inventory management and order placing that outperforms a heuristic benchmark trading strategy having access to the same signal."
            },
            {
                "arxivId": "2211.13777",
                "title": "The Short-Term Predictability of Returns in Order Book Markets: A Deep Learning Perspective",
                "abstract": null
            },
            {
                "arxivId": "2211.03107",
                "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning",
                "abstract": "Finance is a particularly difficult playground for deep reinforcement learning. However, establishing high-quality market environments and benchmarks for financial reinforcement learning is challenging due to three major factors, namely, low signal-to-noise ratio of financial data, survivorship bias of historical data, and model overfitting in the backtesting stage. In this paper, we present an openly accessible FinRL-Meta library that has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we will provide hundreds of market environments through an automatic pipeline that collects dynamic datasets from real-world markets and processes them into gym-style market environments. Second, we reproduce popular papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, FinRL-Meta provides tens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. FinRL-Meta is available at: https://github.com/AI4Finance-Foundation/FinRL-Meta"
            },
            {
                "arxivId": "2207.13914",
                "title": "Anatomy of a Stablecoin's failure: the Terra-Luna case",
                "abstract": null
            },
            {
                "arxivId": "2205.13504",
                "title": "Are Transformers Effective for Time Series Forecasting?",
                "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. \nTo validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future."
            },
            {
                "arxivId": "2203.03991",
                "title": "Sparsification and Filtering for Spatial-temporal GNN in Multivariate Time-series",
                "abstract": "We propose an end-to-end architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a matrix filtering module. This module generates filtered (inverse) correlation graphs from multivariate time series before inputting them into a GNN. In contrast with existing sparsification methods adopted in graph neural network, our model explicitly leverage time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales from a synthetic time-series sales dataset. The proposed spatial-temporal graph neural network displays superior performances with respect to baseline approaches, with no graphical information, and with fully connected, disconnected graphs and unfiltered graphs."
            },
            {
                "arxivId": "2202.07125",
                "title": "Transformers in Time Series: A Survey",
                "abstract": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance."
            },
            {
                "arxivId": "2112.08534",
                "title": "Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture",
                "abstract": "We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model."
            },
            {
                "arxivId": "2110.13701",
                "title": "Heterogeneous Criticality in High Frequency Finance: A Phase Transition in Flash Crashes",
                "abstract": "Flash crashes in financial markets have become increasingly important, attracting attention from financial regulators, market makers as well as from the media and the broader audience. Systemic risk and the propagation of shocks in financial markets is also a topic of great relevance that has attracted increasing attention in recent years. In the present work, we bridge the gap between these two topics with an in-depth investigation of the systemic risk structure of co-crashes in high frequency trading. We find that large co-crashes are systemic in their nature and differ from small ones. We demonstrate that there is a phase transition between co-crashes of small and large sizes, where the former involves mostly illiquid stocks, while large and liquid stocks are the most represented and central in the latter. This suggests that systemic effects and shock propagation might be triggered by simultaneous withdrawals or movement of liquidity by HFTs, arbitrageurs and market makers with cross-asset exposures."
            },
            {
                "arxivId": "2110.13718",
                "title": "Self-organised criticality in high frequency finance: the case of flash crashes",
                "abstract": "High frequency trading and the dynamics of market microstructure have seen growing interest in both academia and industry, with the rise of electronic trading and the emergence of major HFT market makers in recent years. Electronic trading has led to an increase in trading volumes and frequencies particularly in the last decade, which sparked an investigation into how market dynamics have evolved with market players. The literature and practitioners alike have noted that price efficiency has improved as a result, but also that exchanges, assets and even markets now carry more systemic risk as a result of large players covering the whole space with similar methods and risk constraints shared across assets and exchanges. The growth in systemic risk due to electronic trading has given rise to a growing number of increasingly large flash crashes (1) of which the one of May 6th 2010 was the first notorious example. The report on the May 6th crash by Nanex (2) begins to suggest how high frequency order placement and saturation might have worsened the extent of the crash. The authors in (3) delve deeper into this idea and investigate flash crashes between 2006 and 2011 to show a system-wide phase transition around \u223c 500ms to an all-electronic trading market characterised by black swan events. Further insight into the impact of HFT players in flash crashes was provided by the simulations in (4), where the authors show how lowering the number of HFT players in the simulation reduces the extent of the crash, even when the size of the large sell order which triggered it is kept constant. The authors explain the relation between the number of HFT players and the extent of the crash as due to the \u201chot potato\u201d phenomenon described in (5). This phenomenon can be explained as follows. When an unusually large sell (buy) order hits the market it gets absorbed by liquidity providers (often HFT market makers) which, as a result, accumulate a short position in their inventory. The unusually large order though impacts the price and potentially triggers risk limits by those market makers holding the inventory. Meanwhile as the price drops (rises) dramatically ordinary market players withdraw from the market. As a result of the risk limits HFTs try to reduce their inventory with aggressive market orders. As everyone else has withdrawn, they trade with each other at extremely high frequency, thereby creating high trading volumes. This particular phase characterises the \u201chot potato\u201d phenomenon, i.e. the inventory exposure being passed around like a \u201chot potato\u201d. The positive feedback loop continues as follows. When trading volume is used as a proxy for liquidity the \u201chot potato\u201d phenomenon creates apparent liquidity which triggers execution by lower frequency players which are also trying to reduce exposure as the market drops. The authors in (5) make two further points on HFTs and modern market dynamics during these extreme events. Another cause of apparent liquidity and market depth is fleeing liquidity by HFT market makers. The fast cancellation of limit orders has been the extensive topic of discussion and was suggested to create the illusion of market depth with the consequences discussed above. As the number of trading venues and exchanges rises liquidity is fragmented. Sweep orders and arbitrageurs aim to move liquidity and remove arbitrage opportunities across exchanges, but can create dangerous effects. Sweep orders between markets operate at lower frequencies than these fast cancellations, thereby sweeping the book after liquidity has potentially already been removed. This worsens the systemic aspect of these events across the fragmented liquidity structure of exchanges. Extreme events of this kind, which are characterised by non-linear reactions to shocks in the system and positive feedback loops, exist in man-made and natural systems and are instances of self-organised criticality. In this type of systemic events the system reaches a critical state where a small release in energy or imbalance triggers highly non-linear reactions in size. This is the case of avalanches and more, as described in (6). Events with such underlying dynamics are characterised by heavy-tailed and in particular power law distributions. The tail\u2019s decay exponent in these distributions is crucial for their modeling as it indicates which moments of the distribution are defined and which diverge. This can be of great practical relevance for both regulators and practitioners, in particular market makers. These are market players with an obligation to provide liquidity at all times, such as banks, which cannot simply withdraw from the market under extreme conditions, but must continue to provide prices for their clients to trade at. This constraint brings the need to model trade volume flow"
            },
            {
                "arxivId": "2106.07040",
                "title": "Exogenous and endogenous price jumps belong to different dynamical classes",
                "abstract": "Synchronising a database of stock specific news with 5 years worth of order book data on 300 stocks, we show that abnormal price movements following news releases (exogenous) exhibit markedly different dynamical features from those arising spontaneously (endogenous). On average, large volatility fluctuations induced by exogenous events occur abruptly and are followed by a decaying power-law relaxation, while endogenous price jumps are characterized by progressively accelerating growth of volatility, also followed by a power-law relaxation, but slower than for exogenous jumps. Remarkably, our results are reminiscent of what is observed in different contexts, namely Amazon book sales and YouTube views. Finally, we show that fitting power-laws to individual volatility profiles allows one to classify large events into endogenous and exogenous dynamical classes, without relying on the news feed."
            },
            {
                "arxivId": "2105.10430",
                "title": "Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing Units",
                "abstract": "We design multi-horizon forecasting models for limit order book (LOB) data by using deep learning techniques. Unlike standard structures where a single prediction is made, we adopt encoder-decoder models with sequence-to-sequence and Attention mechanisms to generate a forecasting path. Our methods achieve comparable performance to state-of-art algorithms at short prediction horizons. Importantly, they outperform when generating predictions over long horizons by leveraging the multi-horizon setup. Given that encoder-decoder models rely on recurrent neural layers, they generally suffer from slow training processes. To remedy this, we experiment with utilising novel hardware, so-called Intelligent Processing Units (IPUs) produced by Graphcore. IPUs are specifically designed for machine intelligence workload with the aim to speed up the computation process. We show that in our setup this leads to significantly faster training times when compared to training models with GPUs."
            },
            {
                "arxivId": "2105.00521",
                "title": "Order flow and price formation",
                "abstract": "I present an overview of some recent advancements on the empirical analysis and theoretical modeling of the process of price formation in financial markets as the result of the arrival of orders in a limit order book exchange. After discussing critically the possible modeling approaches and the observed stylized facts of order flow, I consider in detail market impact and transaction cost of trades executed incrementally over an extended period of time, by comparing model predictions and recent extensive empirical results. I also discuss how the simultaneous presence of many algorithmic trading executions affects the quality and cost of trading."
            },
            {
                "arxivId": "2102.09672",
                "title": "Improved Denoising Diffusion Probabilistic Models",
                "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion"
            },
            {
                "arxivId": "2101.07107",
                "title": "Deep Reinforcement Learning for Active High Frequency Trading",
                "abstract": "We introduce the first end-to-end Deep Reinforcement Learning (DRL) based framework for active high frequency trading. We train DRL agents to trade one unit of Intel Corporation stock by employing the Proximal Policy Optimization algorithm. The training is performed on three contiguous months of high frequency Limit Order Book data, of which the last month constitutes the validation data. In order to maximise the signal to noise ratio in the training data, we compose the latter by only selecting training samples with largest price changes. The test is then carried out on the following month of data. Hyperparameters are tuned using the Sequential Model Based Optimization technique. We consider three different state characterizations, which differ in their LOB-based meta-features. Analysing the agents' performances on test data, we argue that the agents are able to create a dynamic representation of the underlying environment. They identify occasional regularities present in the data and exploit them to create long-term profitable trading strategies. Indeed, agents learn trading strategies able to produce stable positive returns in spite of the highly stochastic and non-stationary environment."
            },
            {
                "arxivId": "1912.01703",
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            {
                "arxivId": "1907.05600",
                "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
                "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments."
            },
            {
                "arxivId": "1901.08280",
                "title": "Temporal Logistic Neural Bag-of-Features for Financial Time series Forecasting leveraging Limit Order Book Data",
                "abstract": null
            },
            {
                "arxivId": "1707.05642",
                "title": "Sequence Classification of the Limit Order Book Using Recurrent Neural Networks",
                "abstract": "Recurrent neural networks (RNNs) are types of artificial neural networks (ANNs) that are well suited to forecasting and sequence classification. They have been applied extensively to forecasting univariate financial time series, however their application to high frequency trading has not been previously considered. This paper solves a sequence classification problem in which a short sequence of observations of limit order book depths and market orders is used to predict a next event price-flip. The capability to adjust quotes according to this prediction reduces the likelihood of adverse price selection. Our results demonstrate the ability of the RNN to capture the non-linear relationship between the near-term price-flips and a spatio-temporal representation of the limit order book. The RNN compares favorably with other classifiers, including a linear Kalman filter, using S&P500 E-mini futures level II data over the month of August 2016. Further results assess the effect of retraining the RNN daily and the sensitivity of the performance to trade latency."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1705.03233",
                "title": "Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods",
                "abstract": "Managing the prediction of metrics in high-frequency financial markets is a challenging task. An efficient way is by monitoring the dynamics of a limit order book to identify the information edge. This paper describes the first publicly available benchmark dataset of high-frequency limit order markets for mid-price prediction. We extracted normalized data representations of time series data for five stocks from the NASDAQ Nordic stock market for a time period of ten consecutive days, leading to a dataset of ~4,000,000 time series samples in total. A day-based anchored cross-validation experimental protocol is also provided that can be used as a benchmark for comparing the performance of state-of-the-art methodologies. Performance of baseline approaches are also provided to facilitate experimental comparisons. We expect that such a large-scale dataset can serve as a testbed for devising novel solutions of expert systems for high-frequency limit order book data analysis."
            },
            {
                "arxivId": "1610.00261",
                "title": "Limit Order Strategic Placement with Adverse Selection Risk and the Role of Latency",
                "abstract": "This paper is split in three parts: first we use labelled trade data to exhibit how market participants accept or not transactions via limit orders as a function of liquidity imbalance; then we develop a theoretical stochastic control framework to provide details on how one can exploit his knowledge on liquidity imbalance to control a limit order. We emphasis the exposure to adverse selection, of paramount importance for limit orders. For a participant buying using a limit order: if the price has chances to go down the probability to be filled is high but it is better to wait a little more before the trade to obtain a better price. In a third part we show how the added value of exploiting a knowledge on liquidity imbalance is eroded by latency: being able to predict future liquidity consuming flows is of less use if you have not enough time to cancel and reinsert your limit orders. There is thus a rational for market makers to be as fast as possible as a protection to adverse selection. Thanks to our optimal framework we can measure the added value of latency to limit orders placement. \nTo authors' knowledge this paper is the first to make the connection between empirical evidences, a stochastic framework for limit orders including adverse selection, and the cost of latency. Our work is a first stone to shed light on the roles of latency and adverse selection for limit order placement, within an accurate stochastic control framework."
            },
            {
                "arxivId": "1608.00756",
                "title": "A Continuous and Efficient Fundamental Price on the Discrete Order Book Grid",
                "abstract": "This paper develops a model of liquidity provision in financial markets by adapting the Madhavan, Richardson, and Roomans (1997) price formation model to realistic order books with quote discretization and liquidity rebates. We postulate that liquidity providers observe a fundamental price which is continuous, efficient, and can assume values outside the interval spanned by the best quotes. We confirm the predictions of our price formation model with extensive empirical tests on large high-frequency datasets of 100 liquid Nasdaq stocks. Finally we use the model to propose an estimator of the fundamental price based on the rebate adjusted volume imbalance at the best quotes and we empirically show that it outperforms other simpler estimators."
            },
            {
                "arxivId": "1601.01987",
                "title": "Deep learning for limit order books",
                "abstract": "This paper develops a new neural network architecture for modeling spatial distributions (i.e. distributions on ) which is more computationally efficient than a traditional fully-connected feedforward architecture. The design of the architecture takes advantage of the specific structure of limit order books. The new architecture, which we refer to as a \u2018spatial neural network\u2019, yields a low-dimensional model of price movements deep into the limit order book, allowing more effective use of information from deep in the limit order book (i.e. many levels beyond the best bid and best ask). The spatial neural network models the joint distribution of the state of the limit order book at a future time conditional on the current state of the limit order book. The spatial neural network outperforms status quo models such as the naive empirical model, logistic regression (with nonlinear features), and a standard neural network architecture. Both neural networks strongly outperform the logistic regression model. Due to its more effective use of information deep in the limit order book, the spatial neural network especially outperforms the standard neural network in the tail of the distribution, which is important for risk management applications. The models are trained and tested on nearly 500 U.S. stocks. Techniques from deep learning such as dropout are employed to improve performance. Due to the significant computational challenges associated with the large amount of data, models are trained with a cluster of 50\u2009GPUs."
            },
            {
                "arxivId": "1511.08458",
                "title": "An Introduction to Convolutional Neural Networks",
                "abstract": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. \nThis document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning."
            },
            {
                "arxivId": "1508.04348",
                "title": "Designating Market Maker Behaviour in Limit Order Book Markets",
                "abstract": "Financial exchanges provide incentives for limit order book (LOB) liquidity provision to certain market participants, termed designated market makers or designated sponsors. While quoting requirements typically enforce the activity of these participants for a certain portion of the day, an argument that liquidity demand throughout the trading day is far from uniformly distributed is made, and thus this liquidity provision may not be calibrated to the demand. Furthermore, it is propose that quoting obligations also include requirements about the speed of liquidity replenishment, and then a recommendation that use of the Threshold Exceedance Duration (TED) for this purpose be considered. To support this argument a comprehensive regression modelling approach using GLM and GAMLSS models to relate the TED to the state of the LOB and identify the regression structures that are best suited to modelling the TED is presented. Such an approach can be used by exchanges to set target levels of liquidity replenishment for designated market makers."
            },
            {
                "arxivId": "1503.03585",
                "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
                "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm."
            },
            {
                "arxivId": "1311.6262",
                "title": "Agent-based models for latent liquidity and concave price impact.",
                "abstract": "We revisit the \"\u025b-intelligence\" model of T\u00f3th et\u00a0al. [Phys. Rev. X 1, 021006 (2011)], which was proposed as a minimal framework to understand the square-root dependence of the impact of meta-orders on volume in financial markets. The basic idea is that most of the daily liquidity is \"latent\" and furthermore vanishes linearly around the current price, as a consequence of the diffusion of the price itself. However, the numerical implementation of T\u00f3th et\u00a0al. (2011) was criticized as being unrealistic, in particular because all the \"intelligence\" was conferred to market orders, while limit orders were passive and random. In this work, we study various alternative specifications of the model, for example, allowing limit orders to react to the order flow or changing the execution protocols. By and large, our study lends strong support to the idea that the square-root impact law is a very generic and robust property that requires very few ingredients to be valid. We also show that the transition from superdiffusion to subdiffusion reported in T\u00f3th et\u00a0al. (2011) is in fact a crossover but that the original model can be slightly altered in order to give rise to a genuine phase transition, which is of interest on its own. We finally propose a general theoretical framework to understand how a nonlinear impact may appear even in the limit where the bias in the order flow is vanishingly small."
            },
            {
                "arxivId": "2010.16061",
                "title": "Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation",
                "abstract": "Commonly used evaluation measures including Recall, Precision, F-Factor and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case. ."
            },
            {
                "arxivId": "1204.1381",
                "title": "Price Jump Prediction in Limit Order Book",
                "abstract": "A limit order book provides information on available limit order prices and their volumes. Based on these quantities, we give an empirical result on the relationship between the bid-ask liquidity balance and trade sign and we show that liquidity balance on best bid/best ask is quite informative for predicting the future market order's direction. Moreover, we de ne price jump as a sell (buy) market order arrival which is executed at a price which is smaller (larger) than the best bid (best ask) price at the moment just after the precedent market order arrival. Features are then extracted related to limit order volumes, limit order price gaps, market order information and limit order event information. Logistic regression is applied to predict the price jump from the limit order book's feature. LASSO logistic regression is introduced to help us make variable selection from which we are capable to highlight the importance of di erent features in predicting the future price jump. In order to get rid of the intraday data seasonality, the analysis is based on two separated datasets: morning dataset and afternoon dataset. Based on an analysis on forty largest French stocks of CAC40, we nd that trade sign and market order size as well as the liquidity on the best bid (best ask) are consistently informative for predicting the incoming price jump."
            },
            {
                "arxivId": "1104.4596",
                "title": "Price Dynamics in a Markovian Limit Order Market",
                "abstract": "We propose and study a simple stochastic model for the dynamics of a limit order book, in which arrivals of market order, limit orders and order cancellations are described in terms of a Markovian queueing system. Through its analytical tractability, the model allows to obtain analytical expressions for various quantities of interest such as the distribution of the duration between price changes, the distribution and autocorrelation of price changes, and the probability of an upward move in the price, conditional on the state of the order book. We study the diffusion limit of the price process and express the volatility of price changes in terms of parameters describing the arrival rates of buy and sell orders and cancelations. These analytical results provide some insight into the relation between order flow and price dynamics in order-driven markets."
            },
            {
                "arxivId": "1012.0349",
                "title": "Limit order books",
                "abstract": "Abstract Limit order books (LOBs) match buyers and sellers in more than half of the world\u2019s financial markets. This survey highlights the insights that have emerged from the wealth of empirical and theoretical studies of LOBs. We examine the findings reported by statistical analyses of historical LOB data and discuss how several LOB models provide insight into certain aspects of the mechanism. We also illustrate that many such models poorly resemble real LOBs and that several well-established empirical facts have yet to be reproduced satisfactorily. Finally, we identify several key unresolved questions about LOBs."
            },
            {
                "arxivId": "0904.0900",
                "title": "The price impact of order book events: market orders, limit orders and cancellations",
                "abstract": "While the long-ranged correlation of market orders and their impact on prices has been relatively well studied in the literature, the corresponding studies of limit orders and cancellations are scarce. We provide here an empirical study of the cross-correlation between all these different events, and their respective impact on future price changes. We define and extract from the data the \u2018bare\u2019 impact these events would have if they were to happen in isolation. For large tick stocks, we show that a model where the bare impact of all events is permanent and non-fluctuating is in good agreement with the data. For small tick stocks, however, bare impacts must contain a history-dependent part, reflecting the internal fluctuations of the order book. We show that this effect can be accurately described by an autoregressive model of the past order flow. This framework allows us to decompose the impact of an event into three parts: an instantaneous jump component, the modification of the future rates of the different events, and the modification of the jump sizes of future events. We compare in detail the present formalism with the temporary impact model that was proposed earlier to describe the impact of market orders when other types of events are not observed. Finally, we extend the model to describe the dynamics of the bid\u2013ask spread."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2403.18126",
        "category": "q-fin",
        "title": "Revisiting Elastic String Models of Forward Interest Rates",
        "abstract": "Twenty five years ago, several authors proposed to model the forward interest rate curve (FRC) as an elastic string along which idiosyncratic shocks propagate, accounting for the peculiar structure of the return correlation across different maturities. In this paper, we revisit the specific\"stiff'' elastic string field theory of Baaquie and Bouchaud (2004) in a way that makes its micro-foundation more transparent. Our model can be interpreted as capturing the effect of market forces that set the rates of nearby tenors in a self-referential fashion. The model is parsimonious and accurately reproduces the whole correlation structure of the FRC over the time period 1994-2023, with an error below 2%. We need only two parameters, the values of which being very stable except perhaps during the Quantitative Easing period 2009-2014. The dependence of correlation on time resolution (also called the Epps effect) is also faithfully reproduced within the model and leads to a cross-tenor information propagation time of 10 minutes. Finally, we confirm that the perceived time in interest rate markets is a strongly sub-linear function of real time, as surmised by Baaquie and Bouchaud (2004). In fact, our results are fully compatible with hyperbolic discounting, in line with the recent behavioural literature (Farmer and Geanakoplos, 2009).",
        "references": [
            {
                "arxivId": "1907.06151",
                "title": "From quadratic Hawkes processes to super-Heston rough volatility models with Zumbach effect",
                "abstract": "Building log-normal-like rough volatility models with proper Zumbach effect using a microstructural approach"
            },
            {
                "arxivId": "0704.1099",
                "title": "The Epps effect revisited",
                "abstract": "We analyse the dependence of stock return cross-correlations on the data sampling frequency, known as the Epps effect: for high-resolution data the cross-correlations are significantly smaller than their asymptotic value as observed for daily data. The former description implies that a changing trading frequency should alter the characteristic time of the phenomenon. This is not true for empirical data: the Epps curves do not scale with market activity. The latter result indicates that the time scale of the phenomenon is related to the reaction time of market participants (this we denote as the human time scale), independent of market activity. In this paper we give a new description of the Epps effect through the decomposition of cross-correlations. After testing our method on a model of generated random walk price changes we justify our analytical results by fitting the Epps curves of real-world data."
            },
            {
                "arxivId": "q-bio/0701039",
                "title": "Universally Sloppy Parameter Sensitivities in Systems Biology Models",
                "abstract": "Quantitative computational models play an increasingly important role in modern biology. Such models typically involve many free parameters, and assigning their values is often a substantial obstacle to model development. Directly measuring in vivo biochemical parameters is difficult, and collectively fitting them to other experimental data often yields large parameter uncertainties. Nevertheless, in earlier work we showed in a growth-factor-signaling model that collective fitting could yield well-constrained predictions, even when it left individual parameters very poorly constrained. We also showed that the model had a \u201csloppy\u201d spectrum of parameter sensitivities, with eigenvalues roughly evenly distributed over many decades. Here we use a collection of models from the literature to test whether such sloppy spectra are common in systems biology. Strikingly, we find that every model we examine has a sloppy spectrum of sensitivities. We also test several consequences of this sloppiness for building predictive models. In particular, sloppiness suggests that collective fits to even large amounts of ideal time-series data will often leave many parameters poorly constrained. Tests over our model collection are consistent with this suggestion. This difficulty with collective fits may seem to argue for direct parameter measurements, but sloppiness also implies that such measurements must be formidably precise and complete to usefully constrain many model predictions. We confirm this implication in our growth-factor-signaling model. Our results suggest that sloppy sensitivity spectra are universal in systems biology models. The prevalence of sloppiness highlights the power of collective fits and suggests that modelers should focus on predictions rather than on parameters."
            },
            {
                "arxivId": "cond-mat/0501699",
                "title": "Volatility conditional on price trends",
                "abstract": "The influence of the past price behaviour on the realized volatility is investigated, showing that trending (driftless) prices lead to increased (decreased) realized volatility. This \u2018volatility induced by trend\u2019 constitutes a new stylized fact. The past price behaviour is measured by the product of two non-overlapping returns (of the form r \u00d7 L[r] where L is the lag operator), and is different from the usual heteroskedasticity. The effect is studied empirically using USD/CHF foreign exchange data, in a large range of time horizons. On the modelling side, a set of ARCH based processes are modified in order to include the \u2018volatility induced by trend\u2019 effect, and their forecasting performances are compared. The aim is to understand the role and importance of the various terms that can be included in such a model. For a better forecast, it is shown that the main factor is the shape of the memory kernel (i.e. power law), and the next most important factor is the trend effect. The subtle role of mean reversion is also discussed."
            },
            {
                "arxivId": "cond-mat/9801321",
                "title": "The Dynamics of the Forward Interest Rate Curve with Stochastic String Shocks",
                "abstract": "This paper offers a new class of models of the term structure of interest rates. We allow each instantaneous forward rate to be driven by a different stochastic shock, constrained in such a way as to keep the forward rate curve continuous. We term the process followed by the shocks to the forward curve ``stochastic strings'', and construct them as the solution to stochastic partial differential equations, that allow us to offer a variety of interesting parametrizations. The models can produce, with parsimony, any sort of correlation pattern among forward rates of different maturities. This feature makes the models consistent with any panel dataset of bond prices, not requiring the addition of error terms in econometric models. Interest rate options can easily be priced by simulation. However, options can only be perfectly hedged by trading in bonds of all maturities available."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2403.18177",
        "category": "q-fin",
        "title": "Growth rate of liquidity provider's wealth in G3Ms",
        "abstract": "Geometric mean market makers (G3Ms), such as Uniswap and Balancer, represent a widely used class of automated market makers (AMMs). These G3Ms are characterized by the following rule: the reserves of the AMM must maintain the same (weighted) geometric mean before and after each trade. This paper investigates the effects of trading fees on liquidity providers' (LP) profitability in a G3M, as well as the adverse selection faced by LPs due to arbitrage activities involving a reference market. Our work expands the model described in previous studies for G3Ms, integrating transaction fees and continuous-time arbitrage into the analysis. Within this context, we analyze G3M dynamics, characterized by stochastic storage processes, and calculate the growth rate of LP wealth. In particular, our results align with and extend the results concerning the constant product market maker, commonly referred to as Uniswap v2.",
        "references": [
            {
                "arxivId": "2401.01526",
                "title": "An arbitrage driven price dynamics of Automated Market Makers in the presence of fees",
                "abstract": "We present a model for price dynamics in the Automated Market Makers (AMM) setting. Within this framework, we propose a reference market price following a geometric Brownian motion. The AMM price is constrained by upper and lower bounds, determined by constant multiplications of the reference price. Through the utilization of local times and excursion-theoretic approaches, we derive several analytical results, including its time-changed representation and limiting behavior."
            },
            {
                "arxivId": "2305.14604",
                "title": "Automated Market Making and Arbitrage Profits in the Presence of Fees",
                "abstract": "We consider the impact of trading fees on the profits of arbitrageurs trading against an automated marker marker (AMM) or, equivalently, on the adverse selection incurred by liquidity providers due to arbitrage. We extend the model of Milionis et al. [2022] for a general class of two asset AMMs to both introduce fees and discrete Poisson block generation times. In our setting, we are able to compute the expected instantaneous rate of arbitrage profit in closed form. When the fees are low, in the fast block asymptotic regime, the impact of fees takes a particularly simple form: fees simply scale down arbitrage profits by the fraction of time that an arriving arbitrageur finds a profitable trade."
            },
            {
                "arxivId": "2208.06046",
                "title": "Automated Market Making and Loss-Versus-Rebalancing",
                "abstract": "Automated market making (AMM) protocols such as Uniswap have recently emerged as an alternative to the most common market structure for electronic trading, the limit order book. Relative to limit order books, AMMs are both more computationally efficient and do not require the participation of active market making intermediaries. As such, AMMs have emerged as the dominant market mechanism for trust-less decentralized exchanges (DEXs). We develop a model the underlying economics of AMMs from the perspective of their passive liquidity providers (LPs). Our central contribution is a\"Black-Scholes formula for AMMs\". Like the Black-Scholes formula, we consider the return to LPs once market risk has been hedged. We identify the main adverse selection cost incurred by LPs, which we call\"loss-versus-rebalancing\"(LVR, pronounced\"lever\"). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. In a continuous time Black-Scholes setting, we are able to derive closed-form expressions for this adverse selection cost, for all automated market makers, including constant function market makers and those featuring concentrated liquidity (e.g., Uniswap v3). Qualitatively, we highlight the main forces that drive AMM LP returns, including asset characteristics (volatility) and AMM characteristics (curvature/marginal liquidity). Quantitatively, we illustrate how our model's expressions for LP returns match actual LP returns for the Uniswap v2 WETH-USDC trading pair. Our model provides tradable insight into both the ex ante and ex post assessment of AMM LP investment decisions. LVR can also inform the design of the next generation of DEX market mechanisms -- in fact, in the short time since our work has been released,\"LVR mitigation\"has already emerged as the dominant challenge among practitioners in the AMM protocol designer community."
            },
            {
                "arxivId": "2006.08806",
                "title": "Liquidity Provider Returns in Geometric Mean Markets",
                "abstract": "Geometric mean market makers (G3Ms), such as Uniswap and Balancer, comprise a popular class of automated market makers (AMMs) defined by the following rule: the reserves of the AMM before and after each trade must have the same (weighted) geometric mean. This paper extends several results known for constant-weight G3Ms to the general case of G3Ms with time-varying and potentially stochastic weights. These results include the returns and no-arbitrage prices of liquidity pool (LP) shares that investors receive for supplying liquidity to G3Ms. Using these expressions, we show how to create G3Ms whose LP shares replicate the payoffs of financial derivatives. The resulting hedges are model-independent and exact for derivative contracts whose payoff functions satisfy an elasticity constraint. These strategies allow LP shares to replicate various trading strategies and financial contracts, including standard options. G3Ms are thus shown to be capable of recreating a variety of active trading strategies through passive positions in LP shares."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2403.18528",
        "category": "q-fin",
        "title": "Limited Attention Allocation in a Stochastic Linear Quadratic System with Multiplicative Noise",
        "abstract": "This study addresses limited attention allocation in a stochastic linear quadratic system with multiplicative noise. Our approach enables strategic resource allocation to enhance noise estimation and improve control decisions. We provide analytical optimal control and propose a numerical method for optimal attention allocation. Additionally, we apply our ffndings to dynamic mean-variance portfolio selection, showing effective resource allocation across time periods and factors, providing valuable insights for investors.",
        "references": [
            {
                "arxivId": "1303.1064",
                "title": "Unified Framework of Mean-Field Formulations for Optimal Multi-Period Mean-Variance Portfolio Selection",
                "abstract": "When a dynamic optimization problem is not decomposable by a stage-wise backward recursion, it is nonseparable in the sense of dynamic programming. The classical dynamic programming-based optimal stochastic control methods would fail in such nonseparable situations as the principle of optimality no longer applies. Among these notorious nonseparable problems, the dynamic mean-variance portfolio selection formulation had posed a great challenge to our research community until recently. Different from the existing literature that invokes embedding schemes and auxiliary parametric formulations to solve the dynamic mean-variance portfolio selection formulation, we propose in this paper a novel mean-field framework that offers a more efficient modeling tool and a more accurate solution scheme in tackling directly the issue of nonseparability and deriving the optimal policies analytically for the multi-period mean-variance-type portfolio selection problems."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-28.json",
        "arxivId": "2403.18737",
        "category": "q-fin",
        "title": "Optimal Rebalancing in Dynamic AMMs",
        "abstract": "Dynamic AMM pools, as found in Temporal Function Market Making, rebalance their holdings to a new desired ratio (e.g. moving from being 50-50 between two assets to being 90-10 in favour of one of them) by introducing an arbitrage opportunity that disappears when their holdings are in line with their target. Structuring this arbitrage opportunity reduces to the problem of choosing the sequence of portfolio weights the pool exposes to the market via its trading function. Linear interpolation from start weights to end weights has been used to reduce the cost paid by pools to arbitrageurs to rebalance. Here we obtain the $\\textit{optimal}$ interpolation in the limit of small weight changes (which has the downside of requiring a call to a transcendental function) and then obtain a cheap-to-compute approximation to that optimal approach that gives almost the same performance improvement. We then demonstrate this method on a range of market backtests, including simulating pool performance when trading fees are present, finding that the new approximately-optimal method of changing weights gives robust increases in pool performance. For a BTC-ETH-DAI pool from July 2022 to June 2023, the increases of pool P\\&L from approximately-optimal weight changes is $\\sim25\\%$ for a range of different strategies and trading fees.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "1807.05126",
        "category": "q-fin",
        "title": "At the mercy of the common noise: blow-ups in a conditional McKean\u2013Vlasov Problem",
        "abstract": "We extend a model of positive feedback and contagion in large mean-field systems by introducing a common source of noise driven by Brownian motion. Although the dynamics in the model are continuous, the feedback effect can lead to jump discontinuities in the solutions, which we refer to as `blow-ups'. We prove existence of solutions to the corresponding conditional McKean--Vlasov equation, by realising them as suitable limit points of the finite-dimensional particle system, and we show that the pathwise realisation of the common noise can both trigger and prevent blow-ups.",
        "references": [
            {
                "arxivId": "1902.05174",
                "title": "Global solutions to the supercooled Stefan problem with blow-ups: regularity and uniqueness",
                "abstract": "We consider the supercooled Stefan problem, which captures the freezing of a supercooled liquid, in one space dimension. A probabilistic reformulation of the problem allows to define global solutions, even in the presence of blow-ups of the freezing rate. We provide a complete description of such solutions, by relating the temperature distribution in the liquid to the regularity of the ice growth process. The latter is shown to transition between (i) continuous differentiability, (ii) H\u007folder continuity, and (iii) discontinuity. In particular, in the second regime we rediscover the square root behavior of the growth process pointed out by Stefan in his seminal paper [Ste89] from 1889 for the ordinary Stefan problem. In our second main theorem, we establish the uniqueness of the global solutions, a first result of this kind in the context of growth processes with singular self-excitation when blow-ups are present."
            },
            {
                "arxivId": "1811.12356",
                "title": "Uniqueness for contagious McKean\u2013Vlasov systems in the weak feedback regime",
                "abstract": "We present a simple uniqueness argument for a collection of McKean\u2013Vlasov problems that have seen recent interest. Our first result shows that, in the weak feedback regime, there is global uniqueness for a very general class of random drivers. By weak feedback we mean the case where the contagion parameters are small enough to prevent blow\u2010ups in solutions. Next, we specialise to a Brownian driver and show how the same techniques can be extended to give short\u2010time uniqueness after blow\u2010ups, regardless of the feedback strength, by exploiting a localised form of weak feedback. The heart of our approach is a surprisingly simple probabilistic comparison argument that is robust in the sense that it does not ask for any regularity of the solutions."
            },
            {
                "arxivId": "1805.11678",
                "title": "Simulation of particle systems interacting through hitting times",
                "abstract": "We develop an Euler-type particle method for the simulation of a McKean--Vlasov equation arising from a mean-field model with positive feedback from hitting a boundary. Under assumptions on the parameters which ensure differentiable solutions, we establish convergence of order $1/2$ in the time step. Moreover, we give a modification of the scheme using Brownian bridges and local mesh refinement, which improves the order to $1$. We confirm our theoretical results with numerical tests and empirically investigate cases with blow-up."
            },
            {
                "arxivId": "1801.10088",
                "title": "An SPDE model for systemic risk with endogenous contagion",
                "abstract": null
            },
            {
                "arxivId": "1711.01096",
                "title": "Ubiquity of collective irregular dynamics in balanced networks of spiking neurons",
                "abstract": "We revisit the dynamics of a prototypical model of balanced activity in networks of spiking neurons. A detailed investigation of the thermodynamic limit for fixed density of connections (massive coupling) shows that, when inhibition prevails, the asymptotic regime is not asynchronous but rather characterized by a self-sustained irregular, macroscopic (collective) dynamics. So long as the connectivity is massive, this regime is found in many different setups: leaky as well as quadratic integrate-and-fire neurons; large and small coupling strength; weak and strong external currents."
            },
            {
                "arxivId": "1705.00691",
                "title": "Particle systems with singular interaction through hitting times: Application in systemic risk modeling",
                "abstract": "We propose an interacting particle system to model the evolution of a system of banks with mutual exposures. In this model, a bank defaults when its normalized asset value hits a lower threshold, and its default causes instantaneous losses to other banks, possibly triggering a cascade of defaults. The strength of this interaction is determined by the level of the so-called non-core exposure. We show that, when the size of the system becomes large, the cumulative loss process of a bank resulting from the defaults of other banks exhibits discontinuities. These discontinuities are naturally interpreted as systemic events, and we characterize them explicitly in terms of the level of non-core exposure and the fraction of banks that are \"about to default\". The main mathematical challenges of our work stem from the very singular nature of the interaction between the particles, which is inherited by the limiting system. A similar particle system is analyzed in [DIRT15a] and [DIRT15b], and we build on and extend their results. In particular, we characterize the large-population limit of the system and analyze the jump times, the regularity between jumps, and the local uniqueness of the limiting process."
            },
            {
                "arxivId": "1605.00669",
                "title": "A stochastic McKean--Vlasov equation for absorbing diffusions on the half-line",
                "abstract": "We study a finite system of diffusions on the half-line, absorbed when they hit zero, with a correlation effect that is controlled by the proportion of the processes that have been absorbed. As the number of processes in the system becomes large, the empirical measure of the population converges to the solution of a non-linear stochastic heat equation with Dirichlet boundary condition. The diffusion coefficients are allowed to have finitely many discontinuities (piecewise Lipschitz) and we prove pathwise uniqueness of solutions to the limiting stochastic PDE. As a corollary we obtain a representation of the limit as the unique solution to a stochastic McKean--Vlasov problem. Our techniques involve energy estimation in the dual of the first Sobolev space, which connects the regularity of solutions to their boundary behaviour, and tightness calculations in the Skorokhod M1 topology defined for distribution-valued processes, which exploits the monotonicity of the loss process $L$. The motivation for this model comes from the analysis of large portfolio credit problems in finance."
            },
            {
                "arxivId": "1509.02855",
                "title": "Skorokhod's M1 topology for distribution-valued processes",
                "abstract": "Skorokhod\u2019s M1 topology is defined for cadlag paths taking values in the space of tempered distributions (more generally, in the dual of a countably Hilbertian nuclear space). Compactness and tightness characterisations are derived which allow us to study a collection of stochastic processes through their projections on the familiar space of real-valued cadlag processes. It is shown how this topological space can be used in analysing the convergence of empirical process approximations to distributionvalued evolution equations with Dirichlet boundary conditions."
            },
            {
                "arxivId": "1409.3296",
                "title": "Endogenous Crisis Waves: A Stochastic Model with Synchronized Collective Behavior",
                "abstract": "We propose a simple framework to understand commonly observed crisis waves in macroeconomic agent-based models, which is also relevant to a variety of other physical or biological situations where synchronization occurs. We compute exactly the phase diagram of the model and the location of the synchronization transition in parameter space. Many modifications and extensions can be studied, confirming that the synchronization transition is extremely robust against various sources of noise or imperfections."
            },
            {
                "arxivId": "1407.6181",
                "title": "Mean field games with common noise",
                "abstract": "A theory of existence and uniqueness is developed for general stochastic differential mean field games with common noise. The concepts of strong and weak solutions are introduced in analogy with the theory of stochastic differential equations, and existence of weak solutions for mean field games is shown to hold under very general assumptions. Examples and counter-examples are provided to enlighten the underpinnings of the existence theory. Finally, an analog of the famous result of Yamada and Watanabe is derived, and it is used to prove existence and uniqueness of a strong solution under additional assumptions."
            },
            {
                "arxivId": "1406.1151",
                "title": "Particle systems with a singular mean-field self-excitation. Application to neuronal networks",
                "abstract": null
            },
            {
                "arxivId": "1211.0299",
                "title": "Global solvability of a networked integrate-and-fire model of McKean\u2013Vlasov type",
                "abstract": "We here investigate the well-posedness of a networked integrate-and-fire model describing an infinite population of neurons which interact with one another through their common statistical distribution. The interaction is of the self-excitatory type as, at any time, the potential of a neuron increases when some of the others fire: precisely, the kick it receives is proportional to the instantaneous proportion of firing neurons at the same time. From a mathematical point of view, the coefficient of proportionality is of great importance as the resulting system is known to blow-up as this becomes large. In the current paper, we focus on the complementary regime and prove that existence and uniqueness hold for all time when the coefficient of proportionality is small enough."
            },
            {
                "arxivId": "1109.1298",
                "title": "Classical Solutions for a Nonlinear Fokker-Planck Equation Arising in Computational Neuroscience",
                "abstract": "In this paper we analyze the global existence of classical solutions to the initial boundary-value problem for a nonlinear parabolic equation describing the collective behavior of an ensemble of neurons. These equations were obtained as a diffusive approximation of the mean-field limit of a stochastic differential equation system. The resulting nonlocal Fokker-Planck equation presents a nonlinearity in the coefficients depending on the probability flux through the boundary. We show by an appropriate change of variables that this parabolic equation with nonlinear boundary conditions can be transformed into a non standard Stefan-like free boundary problem with a Dirac-delta source term. We prove that there are global classical solutions for inhibitory neural networks, while for excitatory networks we give local well-posedness of classical solutions together with a blow up criterium. Surprisingly, we will show that the spectrum for the operator in the linear case, that corresponding to a system of uncoupled networks, does not give any information about the large time asymptotic behavior."
            },
            {
                "arxivId": "1010.4683",
                "title": "Analysis of nonlinear noisy integrate & fire neuron models: blow-up and steady states",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9904278",
                "title": "Fast Global Oscillations in Networks of Integrate-and-Fire Neurons with Low Firing Rates",
                "abstract": "We study analytically the dynamics of a network of sparsely connected inhibitory integrate-and-fire neurons in a regime where individual neurons emit spikes irregularly and at a low rate. In the limit when the number of neurons N , the network exhibits a sharp transition between a stationary and an oscillatory global activity regime where neurons are weakly synchronized. The activity becomes oscillatory when the inhibitory feedback is strong enough. The period of the global oscillation is found to be mainly controlled by synaptic times but depends also on the characteristics of the external input. In large but finite networks, the analysis shows that global oscillations of finite coherence time generically exist both above and below the critical inhibition threshold. Their characteristics are determined as functions of systems parameters in these two different regimes. The results are found to be in good agreement with numerical simulations."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2206.15365",
        "category": "q-fin",
        "title": "Most Claimed Statistical Findings in Cross-Sectional Return Predictability Are Likely True",
        "abstract": "Harvey, Liu, and Zhu (2016) \u201cargue that most claimed research findings in financial economics are likely false.\u201d Surprisingly, their false discovery rate (FDR) estimates suggest most are true. I revisit their results by developing non- and semi-parametric FDR estimators that account for publication bias and empirical correlations. These estimators provide simple closed-form expressions and reliably produce an upper bound on the FDR in simulations that cluster-bootstrap from empirical predictor returns. Applying these estimators to the Chen-Zimmermann dataset of 205 predictors, I find that most claimed statistical findings in the cross-sectional predictability literature are likely true.",
        "references": [
            {
                "arxivId": "2006.04269",
                "title": "False (and Missed) Discoveries in Financial Economics",
                "abstract": "Multiple testing plagues many important questions in finance such as fund and factor selection. We propose a new way to calibrate both Type I and Type II errors. Next, using a double-bootstrap method, we establish a t-statistic hurdle that is associated with a specific false discovery rate (e.g., 5%). We also establish a hurdle that is associated with a certain acceptable ratio of misses to false discoveries (Type II error scaled by Type I error), which effectively allows for differential costs of the two types of mistakes. Evaluating current methods, we find that they lack power to detect outperforming managers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2306.03822",
        "category": "q-fin",
        "title": "Swing contract pricing: With and without neural networks",
        "abstract": "We propose two parametric approaches to evaluate swing contracts with firm constraints. Our objective is to define approximations for the optimal control, which represents the amounts of energy purchased throughout the contract. The first approach involves approximating the optimal control by means of an explicit parametric function, where the parameters are determined using stochastic gradient descent based algorithms. The second approach builds on the first one, where we replace parameters in the first approach by the output of a neural network. Our numerical experiments demonstrate that by using Langevin based algorithms, both parameterizations provide, in a short computation time, better prices compared to state-of-the-art methods.",
        "references": [
            {
                "arxivId": "2212.14718",
                "title": "Langevin algorithms for very deep Neural Networks with application to image classification",
                "abstract": "Training a very deep neural network is a challenging task, as the deeper a neural network is, the more non-linear it is. We compare the performances of various preconditioned Langevin algorithms with their non-Langevin counterparts for the training of neural networks of increasing depth. For shallow neural networks, Langevin algorithms do not lead to any improvement, however the deeper the network is and the greater are the gains provided by Langevin algorithms. Adding noise to the gradient descent allows to escape from local traps, which are more frequent for very deep neural networks. Following this heuristic we introduce a new Langevin algorithm called Layer Langevin, which consists in adding Langevin noise only to the weights associated to the deepest layers. We then prove the benefits of Langevin and Layer Langevin algorithms for the training of popular deep residual architectures for image classification."
            },
            {
                "arxivId": "2109.11669",
                "title": "Convergence of Langevin-simulated annealing algorithms with multiplicative noise",
                "abstract": "<p>We study the convergence of Langevin-Simulated Annealing type algorithms with multiplicative noise, i.e. for <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"upper V colon double-struck upper R Superscript d Baseline right-arrow double-struck upper R\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>V</mml:mi>\n      <mml:mo>:</mml:mo>\n      <mml:msup>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi mathvariant=\"double-struck\">R</mml:mi>\n        </mml:mrow>\n        <mml:mi>d</mml:mi>\n      </mml:msup>\n      <mml:mo stretchy=\"false\">\u2192<!-- \u2192 --></mml:mo>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi mathvariant=\"double-struck\">R</mml:mi>\n      </mml:mrow>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">V : \\mathbb {R}^d \\to \\mathbb {R}</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> a potential function to minimize, we consider the stochastic differential equation <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"d upper Y Subscript t Baseline equals minus sigma sigma Superscript down-tack Baseline nabla upper V left-parenthesis upper Y Subscript t Baseline right-parenthesis\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>d</mml:mi>\n      <mml:msub>\n        <mml:mi>Y</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo>=</mml:mo>\n      <mml:mo>\u2212<!-- \u2212 --></mml:mo>\n      <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n      <mml:msup>\n        <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n        <mml:mi mathvariant=\"normal\">\u22a4<!-- \u22a4 --></mml:mi>\n      </mml:msup>\n      <mml:mi mathvariant=\"normal\">\u2207<!-- \u2207 --></mml:mi>\n      <mml:mi>V</mml:mi>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:msub>\n        <mml:mi>Y</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">dY_t = - \\sigma \\sigma ^\\top \\nabla V(Y_t)</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"d t plus a left-parenthesis t right-parenthesis sigma left-parenthesis upper Y Subscript t Baseline right-parenthesis d upper W Subscript t plus a left-parenthesis t right-parenthesis squared normal upper Upsilon left-parenthesis upper Y Subscript t Baseline right-parenthesis d t\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>d</mml:mi>\n      <mml:mi>t</mml:mi>\n      <mml:mo>+</mml:mo>\n      <mml:mi>a</mml:mi>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:mi>t</mml:mi>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n      <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:msub>\n        <mml:mi>Y</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n      <mml:mi>d</mml:mi>\n      <mml:msub>\n        <mml:mi>W</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo>+</mml:mo>\n      <mml:mi>a</mml:mi>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:mi>t</mml:mi>\n      <mml:msup>\n        <mml:mo stretchy=\"false\">)</mml:mo>\n        <mml:mn>2</mml:mn>\n      </mml:msup>\n      <mml:mi mathvariant=\"normal\">\u03a5<!-- \u03a5 --></mml:mi>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:msub>\n        <mml:mi>Y</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n      <mml:mi>d</mml:mi>\n      <mml:mi>t</mml:mi>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">dt + a(t)\\sigma (Y_t)dW_t + a(t)^2\\Upsilon (Y_t)dt</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>, where <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"left-parenthesis upper W Subscript t Baseline right-parenthesis\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:msub>\n        <mml:mi>W</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">(W_t)</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> is a Brownian motion, where <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"sigma colon double-struck upper R Superscript d Baseline right-arrow script upper M Subscript d Baseline left-parenthesis double-struck upper R right-parenthesis\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n      <mml:mo>:</mml:mo>\n      <mml:msup>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi mathvariant=\"double-struck\">R</mml:mi>\n        </mml:mrow>\n        <mml:mi>d</mml:mi>\n      </mml:msup>\n      <mml:mo stretchy=\"false\">\u2192<!-- \u2192 --></mml:mo>\n      <mml:msub>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">M</mml:mi>\n        </mml:mrow>\n        <mml:mi>d</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:mrow class=\"MJX-TeXAtom-ORD\">\n        <mml:mi mathvariant=\"double-struck\">R</mml:mi>\n      </mml:mrow>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">\\sigma : \\mathbb {R}^d \\to \\mathcal {M}_d(\\mathbb {R})</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> is an adaptive (multiplicative) noise, where <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"a colon double-struck upper R Superscript plus Baseline right-arrow double-struck upper R Superscript plus\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>a</mml:mi>\n      <mml:mo>:</mml:mo>\n      <mml:msup>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi mathvariant=\"double-struck\">R</mml:mi>\n        </mml:mrow>\n        <mml:mo>+</mml:mo>\n      </mml:msup>\n      <mml:mo stretchy=\"false\">\u2192<!-- \u2192 --></mml:mo>\n      <mml:msup>\n        <mml:mrow class=\"MJX-TeXAtom-ORD\">\n          <mml:mi mathvariant=\"double-struck\">R</mml:mi>\n        </mml:mrow>\n        <mml:mo>+</mml:mo>\n      </mml:msup>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">a : \\mathbb {R}^+ \\to \\mathbb {R}^+</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> is a function decreasing to <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"0\">\n  <mml:semantics>\n    <mml:mn>0</mml:mn>\n    <mml:annotation encoding=\"application/x-tex\">0</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> and where <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"normal upper Upsilon\">\n  <mml:semantics>\n    <mml:mi mathvariant=\"normal\">\u03a5<!-- \u03a5 --></mml:mi>\n    <mml:annotation encoding=\"application/x-tex\">\\Upsilon</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> is a correction term. This setting can be applied to optimization problems arising in Machine Learning; allowing <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"sigma\">\n  <mml:semantics>\n    <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n    <mml:annotation encoding=\"application/x-tex\">\\sigma</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> to depend on the position brings faster convergence in comparison with the classical Langevin equation <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"d upper Y Subscript t Baseline equals minus nabla upper V left-parenthesis upper Y Subscript t Baseline right-parenthesis d t plus sigma d upper W Subscript t\">\n  <mml:semantics>\n    <mml:mrow>\n      <mml:mi>d</mml:mi>\n      <mml:msub>\n        <mml:mi>Y</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo>=</mml:mo>\n      <mml:mo>\u2212<!-- \u2212 --></mml:mo>\n      <mml:mi mathvariant=\"normal\">\u2207<!-- \u2207 --></mml:mi>\n      <mml:mi>V</mml:mi>\n      <mml:mo stretchy=\"false\">(</mml:mo>\n      <mml:msub>\n        <mml:mi>Y</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n      <mml:mo stretchy=\"false\">)</mml:mo>\n      <mml:mi>d</mml:mi>\n      <mml:mi>t</mml:mi>\n      <mml:mo>+</mml:mo>\n      <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n      <mml:mi>d</mml:mi>\n      <mml:msub>\n        <mml:mi>W</mml:mi>\n        <mml:mi>t</mml:mi>\n      </mml:msub>\n    </mml:mrow>\n    <mml:annotation encoding=\"application/x-tex\">dY_t = -\\nabla V(Y_t)dt + \\sigma dW_t</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>. The case where <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"sigma\">\n  <mml:semantics>\n    <mml:mi>\u03c3<!-- \u03c3 --></mml:mi>\n    <mml:annotation encoding=\"application/x-tex\">\\sigma</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> is a constant matrix has been extensively studied; however little attention has been paid to the general case. We prove the convergence for the <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"upper L Superscript 1\">\n  <mml:semantics>\n    <mml:msup>\n      <mml:mi>L</mml:mi>\n      <mml:mn>1</mml:mn>\n    </mml:msup>\n    <mml:annotation encoding=\"application/x-tex\">L^1</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula>-Wasserstein distance of <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"upper Y Subscript t\">\n  <mml:semantics>\n    <mml:msub>\n      <mml:mi>Y</mml:mi>\n      <mml:mi>t</mml:mi>\n    </mml:msub>\n    <mml:annotation encoding=\"application/x-tex\">Y_t</mml:annotation>\n  </mml:semantics>\n</mml:math>\n</inline-formula> and of the associated Euler scheme <inline-formula content-type=\"math/mathml\">\n<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" alttext=\"upper Y overbar Subsc"
            },
            {
                "arxivId": "2106.03780",
                "title": "Learning Stochastic Optimal Policies via Gradient Descent",
                "abstract": "We systematically develop a learning-based treatment of stochastic optimal control (SOC), relying on direct optimization of parametric control policies. We propose a derivation of adjoint sensitivity results for stochastic differential equations through direct application of variational calculus. Then, given an objective function for a predetermined task specifying the desiderata for the controller, we optimize their parameters via iterative gradient descent methods. In doing so, we extend the range of applicability of classical SOC techniques, often requiring strict assumptions on the functional form of system and control. We verify the performance of the proposed approach on a continuous\u2013time, finite horizon portfolio optimization with proportional transaction costs."
            },
            {
                "arxivId": "2101.11557",
                "title": "Convergence rates of Gibbs measures with degenerate minimum",
                "abstract": "We study convergence rates for Gibbs measures, with density proportional to $e^{-f(x)/t}$, as $t \\rightarrow 0$ where $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ admits a unique global minimum at $x^\\star$. We focus on the case where the Hessian is not definite at $x^\\star$. We assume instead that the minimum is strictly polynomial and give a higher order nested expansion of $f$ at $x^\\star$, which depends on every coordinate. We give an algorithm yielding such a decomposition if the polynomial order of $x^\\star$ is no more than $8$, in connection with Hilbert's $17^{\\text{th}}$ problem. However, we prove that the case where the order is $10$ or higher is fundamentally different and that further assumptions are needed. We then give the rate of convergence of Gibbs measures using this expansion. Finally we adapt our results to the multiple well case."
            },
            {
                "arxivId": "2012.14310",
                "title": "Unadjusted Langevin algorithm with multiplicative noise: Total variation and Wasserstein bounds",
                "abstract": "In this paper, we focus on non-asymptotic bounds related to the Euler scheme of an ergodic diffusion with a possibly multiplicative diffusion term (non-constant diffusion coefficient). More precisely, the objective of this paper is to control the distance of the standard Euler scheme with decreasing step ({usually called Unadjusted Langevin Algorithm in the Monte Carlo literature}) to the invariant distribution of such an ergodic diffusion. In an appropriate Lyapunov setting and under {uniform} ellipticity assumptions on the diffusion coefficient, we establish (or improve) such bounds for Total Variation and $L^1$-Wasserstein distances in both multiplicative and additive and frameworks. These bounds rely on weak error expansions using {Stochastic Analysis} adapted to decreasing step setting."
            },
            {
                "arxivId": "2012.14501",
                "title": "Neural network approximation",
                "abstract": "Neural networks (NNs) are the method of choice for building learning algorithms. They are now being investigated for other numerical tasks such as solving high-dimensional partial differential equations. Their popularity stems from their empirical success on several challenging learning problems (computer chess/Go, autonomous navigation, face recognition). However, most scholars agree that a convincing theoretical explanation for this success is still lacking. Since these applications revolve around approximating an unknown function from data observations, part of the answer must involve the ability of NNs to produce accurate approximations. This article surveys the known approximation properties of the outputs of NNs with the aim of uncovering the properties that are not present in the more traditional methods of approximation used in numerical analysis, such as approximations using polynomials, wavelets, rational functions and splines. Comparisons are made with traditional approximation methods from the viewpoint of rate distortion, i.e. error versus the number of parameters used to create the approximant. Another major component in the analysis of numerical approximation is the computational time needed to construct the approximation, and this in turn is intimately connected with the stability of the approximation algorithm. So the stability of numerical approximation using NNs is a large part of the analysis put forward. The survey, for the most part, is concerned with NNs using the popular ReLU activation function. In this case the outputs of the NNs are piecewise linear functions on rather complicated partitions of the domain of f into cells that are convex polytopes. When the architecture of the NN is fixed and the parameters are allowed to vary, the set of output functions of the NN is a parametrized nonlinear manifold. It is shown that this manifold has certain space-filling properties leading to an increased ability to approximate (better rate distortion) but at the expense of numerical stability. The space filling creates the challenge to the numerical method of finding best or good parameter choices when trying to approximate."
            },
            {
                "arxivId": "2003.02395",
                "title": "A Simple Convergence Proof of Adam and Adagrad",
                "abstract": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations $N$. This bound can be made arbitrarily small: Adam with a learning rate $\\alpha=1/\\sqrt{N}$ and a momentum parameter on squared gradients $\\beta_2=1-1/N$ achieves the same rate of convergence $O(\\ln(N)/\\sqrt{N})$ as Adagrad. Finally, we obtain the tightest dependency on the heavy ball momentum among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-\\beta_1)^{-3})$ to $O((1-\\beta_1)^{-1})$. Our technique also improves the best known dependency for standard SGD by a factor $1 - \\beta_1$."
            },
            {
                "arxivId": "1907.06474",
                "title": "Neural network regression for Bermudan option pricing",
                "abstract": "Abstract The pricing of Bermudan options amounts to solving a dynamic programming principle, in which the main difficulty, especially in high dimension, comes from the conditional expectation involved in the computation of the continuation value. These conditional expectations are classically computed by regression techniques on a finite-dimensional vector space. In this work, we study neural networks approximations of conditional expectations. We prove the convergence of the well-known Longstaff and Schwartz algorithm when the standard least-square regression is replaced by a neural network approximation, assuming an efficient algorithm to compute this approximation. We illustrate the numerical efficiency of neural networks as an alternative to standard regression methods for approximating conditional expectations on several numerical examples."
            },
            {
                "arxivId": "1904.01517",
                "title": "Convergence rates for the stochastic gradient descent method for non-convex objective functions",
                "abstract": "We prove the local convergence to minima and estimates on the rate of convergence for the stochastic gradient descent method in the case of not necessarily globally convex nor contracting objective functions. In particular, the results are applicable to simple objective functions arising in machine learning."
            },
            {
                "arxivId": "1902.00908",
                "title": "Stochastic Gradient Descent for Nonconvex Learning Without Bounded Gradient Assumptions",
                "abstract": "Stochastic gradient descent (SGD) is a popular and efficient method with wide applications in training deep neural nets and other nonconvex models. While the behavior of SGD is well understood in the convex learning setting, the existing theoretical results for SGD applied to nonconvex objective functions are far from mature. For example, existing results require to impose a nontrivial assumption on the uniform boundedness of gradients for all iterates encountered in the learning process, which is hard to verify in practical implementations. In this article, we establish a rigorous theoretical foundation for SGD in nonconvex learning by showing that this boundedness assumption can be removed without affecting convergence rates, and relaxing the standard smoothness assumption to H\u00f6lder continuity of gradients. In particular, we establish sufficient conditions for almost sure convergence as well as optimal convergence rates for SGD applied to both general nonconvex and gradient-dominated objective functions. A linear convergence is further derived in the case with zero variances."
            },
            {
                "arxivId": "1812.05916",
                "title": "Deep Neural Networks Algorithms for Stochastic Control Problems on Finite Horizon: Numerical Applications",
                "abstract": null
            },
            {
                "arxivId": "1611.07422",
                "title": "Deep Learning Approximation for Stochastic Control Problems",
                "abstract": "Many real world stochastic control problems suffer from the \"curse of dimensionality\". To overcome this difficulty, we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on Monte-Carlo sampling. We approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics. The objective function for the control problem plays the role of the loss function for the deep neural network. We test this approach using examples from the areas of optimal trading and energy storage. Our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time, can handle rather high dimensional problems."
            },
            {
                "arxivId": "1605.01559",
                "title": "High-dimensional Bayesian inference via the unadjusted Langevin algorithm",
                "abstract": "We consider in this paper the problem of sampling a high-dimensional probability distribution $\\pi$ having a density with respect to the Lebesgue measure on $\\mathbb{R}^d$, known up to a normalization constant $x \\mapsto \\pi(x)= \\mathrm{e}^{-U(x)}/\\int_{\\mathbb{R}^d} \\mathrm{e}^{-U(y)} \\mathrm{d} y$. Such problem naturally occurs for example in Bayesian inference and machine learning. Under the assumption that $U$ is continuously differentiable, $\\nabla U$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic bounds for the convergence to stationarity in Wasserstein distance of order $2$ and total variation distance of the sampling method based on the Euler discretization of the Langevin stochastic differential equation, for both constant and decreasing step sizes. The dependence on the dimension of the state space of these bounds is explicit. The convergence of an appropriately weighted empirical measure is also investigated and bounds for the mean square error and exponential deviation inequality are reported for functions which are measurable and bounded. An illustration to Bayesian inference for binary regression is presented to support our claims."
            },
            {
                "arxivId": "1512.07666",
                "title": "Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks",
                "abstract": "\n \n Effective training of deep neural networks suffers from two main issues. The first is that the parameter space of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models.\n \n"
            },
            {
                "arxivId": "1511.06807",
                "title": "Adding Gradient Noise Improves Learning for Very Deep Networks",
                "abstract": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures. Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures."
            },
            {
                "arxivId": "1507.05021",
                "title": "Non-asymptotic convergence analysis for the Unadjusted Langevin Algorithm",
                "abstract": "In this paper, we study a method to sample from a target distribution $\\pi$ over $\\mathbb{R}^d$ having a positive density with respect to the Lebesgue measure, known up to a normalisation factor. This method is based on the Euler discretization of the overdamped Langevin stochastic differential equation associated with $\\pi$. For both constant and decreasing step sizes in the Euler discretization, we obtain non-asymptotic bounds for the convergence to the target distribution $\\pi$ in total variation distance. A particular attention is paid to the dependency on the dimension $d$, to demonstrate the applicability of this method in the high dimensional setting. These bounds improve and extend the results of (Dalalyan 2014)."
            },
            {
                "arxivId": "1212.1824",
                "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes",
                "abstract": "Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the last SGD iterate scales as O(log(T)/\u221aT) for non-smooth convex objective functions, and O(log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some experimental illustrations."
            },
            {
                "arxivId": "0705.2110",
                "title": "Optimal Quantization for the Pricing of Swing Options",
                "abstract": "In this paper we investigate a numerical algorithm for the pricing of swing options, relying on the so\u2010called optimal quantization method. The numerical procedure is described in detail and numerous simulations are provided to assert its efficiency. In particular, we carry out a comparison with the Longstaff\u2013Schwartz algorithm."
            },
            {
                "arxivId": "0705.0466",
                "title": "When are Swing options bang-bang and how to use it",
                "abstract": "In this paper we investigate a class of swing options with firm constraints in view of the modeling of supply agreements. We show, for a fully general payoff process, that the premium, solution to a stochastic control problem, is concave and piecewise affine as a function of the global constraints of the contract. The existence of bang-bang optimal controls is established for a set of constraints which generates by affinity the whole premium function. When the payoff process is driven by an underlying Markov process, we propose a quantization based recursive backward procedure to price these contracts. A priori error bounds are established, uniformly with respect to the global constraints."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2310.10500",
        "category": "q-fin",
        "title": "Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies",
        "abstract": "Forecasting models for systematic trading strategies do not adapt quickly when financial market conditions change, as was seen in the advent of the COVID-19 pandemic in 2020, when market conditions changed dramatically causing many forecasting models to take loss-making positions. To deal with such situations, we propose a novel time-series trend-following forecaster that is able to quickly adapt to new market conditions, referred to as regimes. We leverage recent developments from the deep learning community and use few-shot learning. We propose the Cross Attentive Time-Series Trend Network - X-Trend - which takes positions attending over a context set of financial time-series regimes. X-Trend transfers trends from similar patterns in the context set to make predictions and take positions for a new distinct target regime. X-Trend is able to quickly adapt to new financial regimes with a Sharpe ratio increase of 18.9% over a neural forecaster and 10-fold over a conventional Time-series Momentum strategy during the turbulent market period from 2018 to 2023. Our strategy recovers twice as quickly from the COVID-19 drawdown compared to the neural-forecaster. X-Trend can also take zero-shot positions on novel unseen financial assets obtaining a 5-fold Sharpe ratio increase versus a neural time-series trend forecaster over the same period. X-Trend both forecasts next-day prices and outputs a trading signal. Furthermore, the cross-attention mechanism allows us to interpret the relationship between forecasts and patterns in the context set.",
        "references": [
            {
                "arxivId": "2308.11294",
                "title": "Network Momentum across Asset Classes",
                "abstract": "We investigate the concept of network momentum, a novel trading signal derived from momentum spillover across assets. Initially observed within the confines of pairwise economic and fundamental ties, such as the stock-bond connection of the same company and stocks linked through supply-demand chains, momentum spillover implies a propagation of momentum risk premium from one asset to another. The similarity of momentum risk premium, exemplified by co-movement patterns, has been spotted across multiple asset classes including commodities, equities, bonds and currencies. However, studying the network effect of momentum spillover across these classes has been challenging due to a lack of readily available common characteristics or economic ties beyond the company level. In this paper, we explore the interconnections of momentum features across a diverse range of 64 continuous future contracts spanning these four classes. We utilise a linear and interpretable graph learning model with minimal assumptions to reveal the intricacies of the momentum spillover network. By leveraging the learned networks, we construct a network momentum strategy that exhibits a Sharpe ratio of 1.5 and an annual return of 22%, after volatility scaling, from 2000 to 2022. This paper pioneers the examination of momentum spillover across multiple asset classes using only pricing data, presents a multi-asset investment strategy based on network momentum, and underscores the effectiveness of this strategy through robust empirical analysis."
            },
            {
                "arxivId": "2105.13727",
                "title": "Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection",
                "abstract": "Momentum strategies are an important part of alternative investments and are at the heart of the work of commodity trading advisors. These strategies have, however, been found to have difficulties adjusting to rapid changes in market conditions, such as during the 2020 market crash. In particular, immediately after momentum turning points, when a trend reverses from an uptrend (downtrend) to a downtrend (uptrend), time-series momentum strategies are prone to making bad bets. To improve the responsiveness to regime change, the authors introduce a novel approach, in which they insert an online changepoint detection (CPD) module into a deep momentum network pipeline, which uses a long short-term memory deep-learning architecture to simultaneously learn both trend estimation and position sizing. Furthermore, their model is able to optimize the way in which it balances (1) a slow momentum strategy that exploits persisting trends but does not overreact to localized price moves and (2) a fast mean-reversion strategy regime by quickly flipping its position and then swapping back again to exploit localized price moves. The CPD module outputs a changepoint location and severity score, allowing the model to learn to respond to varying degrees of disequilibrium, or smaller and more localized changepoints, in a data-driven manner. The authors back test their model over the period 1995\u20132020, and the addition of the CPD module leads to a 33% improvement in the Sharpe ratio. The module is especially beneficial in periods of significant nonstationarity; in particular, over the most recent years tested (2015\u20132020), the performance boost is approximately 66%. This is especially interesting because traditional momentum strategies underperformed in this period."
            },
            {
                "arxivId": "2007.11498",
                "title": "CrossTransformers: spatially-aware few-shot transfer",
                "abstract": "Given new tasks with very little data$-$such as new classes in a classification problem or a domain shift in the input$-$performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classifier that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets."
            },
            {
                "arxivId": "1912.09363",
                "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting",
                "abstract": null
            },
            {
                "arxivId": "1909.02729",
                "title": "A Baseline for Few-Shot Image Classification",
                "abstract": "Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the \"hardness\" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way."
            },
            {
                "arxivId": "1906.10264",
                "title": "Sequential Neural Processes",
                "abstract": "Neural Processes combine the strengths of neural networks and Gaussian processes to achieve both flexible learning and fast prediction in stochastic processes. However, a large class of problems comprises underlying temporal dependency structures in a sequence of stochastic processes that Neural Processes (NP) do not explicitly consider. In this paper, we propose Sequential Neural Processes (SNP) which incorporates a temporal state-transition model of stochastic processes and thus extends its modeling capabilities to dynamic stochastic processes. In applying SNP to dynamic 3D scene modeling, we introduce the Temporal Generative Query Networks. To our knowledge, this is the first 4D model that can deal with the temporal dynamics of 3D scenes. In experiments, we evaluate the proposed methods in dynamic (non-stationary) regression and 4D scene inference and rendering."
            },
            {
                "arxivId": "1808.03668",
                "title": "DeepLOB: Deep Convolutional Neural Networks for Limit Order Books",
                "abstract": "We develop a large-scale deep learning model to predict price movements from limit order book (LOB) data of cash equities. The architecture utilizes convolutional filters to capture the spatial structure of the LOBs as well as long short-term memory modules to capture longer time dependencies. The proposed network outperforms all existing state-of-the-art algorithms on the benchmark LOB dataset [A. Ntakaris, M. Magris, J. Kanniainen, M. Gabbouj, and A. Iosifidis, \u201cBenchmark dataset for mid-price prediction of limit order book data with machine learning methods,\u201d J. Forecasting, vol. 37, no. 8, 852\u2013866, 2018]. In a more realistic setting, we test our model by using one-year market quotes from the London Stock Exchange, and the model delivers a remarkably stable out-of-sample prediction accuracy for a variety of instruments. Importantly, our model translates well to instruments that were not part of the training set, indicating the model's ability to extract universal features. In order to better understand these features and to go beyond a \u201cblack box\u201d model, we perform a sensitivity analysis to understand the rationale behind the model predictions and reveal the components of LOBs that are most relevant. The ability to extract robust features that translate well to other instruments is an important property of our model, which has many other applications."
            },
            {
                "arxivId": "1703.03400",
                "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
                "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
            },
            {
                "arxivId": "1611.05763",
                "title": "Learning to reinforcement learn",
                "abstract": "In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience."
            },
            {
                "arxivId": "1611.02779",
                "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "abstract": "Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a \"fast\" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems."
            },
            {
                "arxivId": "1604.06737",
                "title": "Entity Embeddings of Categorical Variables",
                "abstract": "We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features. We further demonstrate in this paper that entity embedding helps the neural network to generalize better when the data is sparse and statistics is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering."
            },
            {
                "arxivId": "1511.07289",
                "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
                "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.18822",
        "category": "q-fin",
        "title": "Enhancing Financial Data Visualization for Investment Decision-Making",
        "abstract": "Navigating the intricate landscape of financial markets requires adept forecasting of stock price movements. This paper delves into the potential of Long Short-Term Memory (LSTM) networks for predicting stock dynamics, with a focus on discerning nuanced rise and fall patterns. Leveraging a dataset from the New York Stock Exchange (NYSE), the study incorporates multiple features to enhance LSTM's capacity in capturing complex patterns. Visualization of key attributes, such as opening, closing, low, and high prices, aids in unraveling subtle distinctions crucial for comprehensive market understanding. The meticulously crafted LSTM input structure, inspired by established guidelines, incorporates both price and volume attributes over a 25-day time step, enabling the model to capture temporal intricacies. A comprehensive methodology, including hyperparameter tuning with Grid Search, Early Stopping, and Callback mechanisms, leads to a remarkable 53% improvement in predictive accuracy. The study concludes with insights into model robustness, contributions to financial forecasting literature, and a roadmap for real-time stock market prediction. The amalgamation of LSTM networks, strategic hyperparameter tuning, and informed feature selection presents a potent framework for advancing the accuracy of stock price predictions, contributing substantively to financial time series forecasting discourse.",
        "references": [
            {
                "arxivId": "1201.0490",
                "title": "Scikit-learn: Machine Learning in Python",
                "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.18823",
        "category": "q-fin",
        "title": "Artificial Intelligence-based Analysis of Change in Public Finance between US and International Markets",
        "abstract": "Public finances are one of the fundamental mechanisms of economic governance that refer to the financial activities and decisions made by government entities to fund public services, projects, and operations through assets. In today\u2019s globalized landscape, even subtle shifts in one nation\u2019s public debt landscape can have significant impacts on that of international finances, necessitating a nuanced understanding of the correlations between international and national markets to help investors make informed investment decisions. Therefore, by leveraging the capabilities of artificial intelligence, this study utilizes neural networks to depict the correlations between US and International Public Finances and predict the changes in international public finances based on the changes in US public finances. With the neural network model achieving a commendable Mean Squared Error (MSE) value of 2.79, it is able to affirm a discernible correlation and also plot the effect of US market volatility on international markets. To further test the accuracy and significance of the model, an economic analysis was conducted that correlated the changes seen by the results of the model with historical stock market changes. This model also shows potential for investors to predict changes in international public finances based on signals from US markets, marking a development in comprehending the intricacies of global public finances and the role of artificial intelligence in decoding its multifaceted patterns for fincancial forecasting.",
        "references": [
            {
                "arxivId": "1909.09586",
                "title": "Understanding LSTM - a tutorial into Long Short-Term Memory Recurrent Neural Networks",
                "abstract": "Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are one of the most powerful dynamic classifiers publicly known. The network itself and the related learning algorithms are reasonably well documented to get an idea how it works. This paper will shed more light into understanding how LSTM-RNNs evolved and why they work impressively well, focusing on the early, ground-breaking publications. We significantly improved documentation and fixed a number of errors and inconsistencies that accumulated in previous publications. To support understanding we as well revised and unified the notation used."
            },
            {
                "arxivId": "1808.03314",
                "title": "Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.18831",
        "category": "q-fin",
        "title": "DeepTraderX: Challenging Conventional Trading Strategies with Deep Learning in Multi-Threaded Market Simulations",
        "abstract": "In this paper, we introduce DeepTraderX (DTX), a simple Deep Learning-based trader, and present results that demonstrate its performance in a multi-threaded market simulation. In a total of about 500 simulated market days, DTX has learned solely by watching the prices that other strategies produce. By doing this, it has successfully created a mapping from market data to quotes, either bid or ask orders, to place for an asset. Trained on historical Level-2 market data, i.e., the Limit Order Book (LOB) for specific tradable assets, DTX processes the market state $S$ at each timestep $T$ to determine a price $P$ for market orders. The market data used in both training and testing was generated from unique market schedules based on real historic stock market data. DTX was tested extensively against the best strategies in the literature, with its results validated by statistical analysis. Our findings underscore DTX's capability to rival, and in many instances, surpass, the performance of public-domain traders, including those that outclass human traders, emphasising the efficiency of simple models, as this is required to succeed in intricate multi-threaded simulations. This highlights the potential of leveraging\"black-box\"Deep Learning systems to create more efficient financial markets.",
        "references": [
            {
                "arxivId": "2012.00821",
                "title": "Automated Creation of a High-Performing Algorithmic Trader via Deep Learning on Level-2 Limit Order Book Data",
                "abstract": "We present results demonstrating that an appropriately configured deep learning neural network (DLNN) can automatically learn to be a high-performing algorithmic trading system, operating purely from training-data inputs generated by passive observation of an existing successful trader T. That is, we can point our black-box DLNN system at trader T and successfully have it learn from T\u2019s trading activity, such that it trades at least as well as T. Our system, called DeepTrader, takes inputs derived from Level-2 market data, i.e. the market\u2019s Limit Order Book (LOB) or Ladder for a tradeable asset. Unusually, DeepTrader makes no explicit prediction of future prices. Instead, we train it purely on input-output pairs where in each pair the input is a snapshot S of Level-2 LOB data taken at the time when T issued a quote Q (i.e. a bid or an ask order) to the market; and DeepTrader\u2019s desired output is to produce Q when it is shown S. That is, we train our DLNN by showing it the LOB data S that T saw at the time when T issued quote Q, and in doing so our system comes to behave like T, acting as an algorithmic trader issuing specific quotes in response to specific LOB conditions. We train DeepTrader on large numbers of these ${S/Q}$} snapshot/quote pairs, and then test it in a variety of market scenarios, evaluating it against other algorithmic trading systems in the public-domain literature, including two that have repeatedly been shown to outperform human traders. Our results demonstrate that DeepTrader learns to match or outperform such existing algorithmic trading systems. We analyse the successful DeepTrader network to identify what features it is relying on, and which features can be ignored. We propose that our methods can in principle create an explainable copy of an arbitrary trader T via \u201cblack-box\u201d deep learning methods."
            },
            {
                "arxivId": "2009.06905",
                "title": "Which Trading Agent is Best? Using a Threaded Parallel Simulation of a Financial Market Changes the Pecking-Order",
                "abstract": "This paper presents novel results generated from a new simulation model of a contemporary financial market, that cast serious doubt on the previously widely accepted view of the relative performance of various well-known public-domain automated-trading algorithms. Various public-domain trading algorithms have been proposed over the past 25 years in a kind of arms-race, where each new trading algorithm was compared to the previous best, thereby establishing a \"pecking order\", i.e. a partially-ordered dominance hierarchy from best to worst of the various trading algorithms. Many of these algorithms were developed and tested using simple minimal simulations of financial markets that only weakly approximated the fact that real markets involve many different trading systems operating asynchronously and in parallel. In this paper we use BSE, a public-domain market simulator, to run a set of experiments generating benchmark results from several well-known trading algorithms. BSE incorporates a very simple time-sliced approach to simulating parallelism, which has obvious known weaknesses. We then alter and extend BSE to make it threaded, so that different trader algorithms operate asynchronously and in parallel: we call this simulator Threaded-BSE (TBSE). We then re-run the trader experiments on TBSE and compare the TBSE results to our earlier benchmark results from BSE. Our comparison shows that the dominance hierarchy in our more realistic experiments is different from the one given by the original simple simulator. We conclude that simulated parallelism matters a lot, and that earlier results from simple simulations comparing different trader algorithms are no longer to be entirely trusted."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.18839",
        "category": "q-fin",
        "title": "Long Short-Term Memory Pattern Recognition in Currency Trading",
        "abstract": "This study delves into the analysis of financial markets through the lens of Wyckoff Phases, a framework devised by Richard D. Wyckoff in the early 20th century. Focusing on the accumulation pattern within the Wyckoff framework, the research explores the phases of trading range and secondary test, elucidating their significance in understanding market dynamics and identifying potential trading opportunities. By dissecting the intricacies of these phases, the study sheds light on the creation of liquidity through market structure, offering insights into how traders can leverage this knowledge to anticipate price movements and make informed decisions. The effective detection and analysis of Wyckoff patterns necessitate robust computational models capable of processing complex market data, with spatial data best analyzed using Convolutional Neural Networks (CNNs) and temporal data through Long Short-Term Memory (LSTM) models. The creation of training data involves the generation of swing points, representing significant market movements, and filler points, introducing noise and enhancing model generalization. Activation functions, such as the sigmoid function, play a crucial role in determining the output behavior of neural network models. The results of the study demonstrate the remarkable efficacy of deep learning models in detecting Wyckoff patterns within financial data, underscoring their potential for enhancing pattern recognition and analysis in financial markets. In conclusion, the study highlights the transformative potential of AI-driven approaches in financial analysis and trading strategies, with the integration of AI technologies shaping the future of trading and investment practices.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.19051",
        "category": "q-fin",
        "title": "TOWARDS STANDARDIZED REGULATIONS FOR BLOCK CHAIN SMART CONTRACTS: INSIGHTS FROM DELPHI AND SWARA ANALYSIS",
        "abstract": "The rise of digital currency and the public ledger Block Chain has led to the development of a new type of electronic contract known as \"smart contracts.\" For these contracts to be considered valid, they must adhere to traditional contract rules and be concluded without any impediments. Once written, encrypted, and signed, smart contracts are recorded in the Block Chain Ledger, providing transparent and secure record-keeping. Smart contracts offer several benefits, including their ability to execute automatically without requiring human intervention, their provision of public visibility of contract provisions on the Block Chain, their avoidance of financial crimes like Money Laundering, and their prevention of contract abuses. However, disputes arising from smart contracts still\nrequire human intervention, presenting unique challenges in enforcing these contracts, such as evidentiary issues, enforceability of waivers of defenses, and jurisdictional and choice-of-law considerations. Due to the novel nature of smart contracts, there are currently no standardized regulations that apply to them. Countries that have approved them have turned to customary law to legitimize their use. The Delphi method was used to identify critical success factors for applying blockchain transactions in a manufacturing company. Stepwise Weight Assessment Ratio Analysis (SWARA) was then utilized to determine the most influential factors. The proposed methodology was implemented, and results show that the most influential factors for the successful application of blockchain transactions as smart contracts in a manufacturing company are: turnover, the counter argument, vision, components for building, and system outcome quality. Conversely, connections with government entities and subcontractors, and the guarantee of quality have the least influence on successful implementation. These findings can contribute to the development of a legal framework for smart contracts in a manufacturing company.",
        "references": [
            {
                "arxivId": "2308.10090",
                "title": "Minimizing Turns in Watchman Robot Navigation: Strategies and Solutions",
                "abstract": "The Orthogonal Watchman Route Problem (OWRP) entails the search for the shortest path, known as the watchman route, that a robot must follow within a polygonal environment. The primary objective is to ensure that every point in the environment remains visible from at least one point on the route, allowing the robot to survey the entire area in a single, continuous sweep. This research places particular emphasis on reducing the number of turns in the route, as it is crucial for optimizing navigation in watchman routes within the field of robotics. The cost associated with changing direction is of significant importance, especially for specific types of robots. This paper introduces an efficient linear-time algorithm for solving the OWRP under the assumption that the environment is monotone. The findings of this study contribute to the progress of robotic systems by enabling the design of more streamlined patrol robots. These robots are capable of efficiently navigating complex environments while minimizing the number of turns. This advancement enhances their coverage and surveillance capabilities, making them highly effective in various real-world applications."
            },
            {
                "arxivId": "2304.02094",
                "title": "TM-vector: A Novel Forecasting Approach for Market stock movement with a Rich Representation of Twitter and Market data",
                "abstract": "Stock market forecasting has been a challenging part for many analysts and researchers. Trend analysis, statistical techniques, and movement indicators have traditionally been used to predict stock price movements, but text extraction has emerged as a promising method in recent years. The use of neural networks, especially recurrent neural networks, is abundant in the literature. In most studies, the impact of different users was considered equal or ignored, whereas users can have other effects. In the current study, we will introduce TM-vector and then use this vector to train an IndRNN and ultimately model the market users' behaviour. In the proposed model, TM-vector is simultaneously trained with both the extracted Twitter features and market information. Various factors have been used for the effectiveness of the proposed forecasting approach, including the characteristics of each individual user, their impact on each other, and their impact on the market, to predict market direction more accurately. Dow Jones 30 index has been used in current work. The accuracy obtained for predicting daily stock changes of Apple is based on various models, closed to over 95\\% and for the other stocks is significant. Our results indicate the effectiveness of TM-vector in predicting stock market direction."
            },
            {
                "arxivId": "2105.05192",
                "title": "Digital Building Twins and Blockchain for Performance-Based (Smart) Contracts",
                "abstract": null
            },
            {
                "arxivId": "1902.07986",
                "title": "Probabilistic Smart Contracts: Secure Randomness on the Blockchain",
                "abstract": "In today\u2019s programmable blockchains, smart contracts are limited to being deterministic and non-probabilistic. This lack of randomness is a consequential limitation, given that a wide variety of real-world financial contracts, such as casino games and lotteries, depend entirely on randomness. As a result, several ad-hoc random number generation approaches have been developed to be used in smart contracts. These include ideas such as using an oracle or relying on the block hash. However, these approaches are manipulatable, i.e. their output can be tampered with by parties who might not be neutral, such as the owner of the oracle or the miners.We propose a novel game-theoretic approach for generating provably unmanipulatable pseudorandom numbers on the blockchain. Our approach allows smart contracts to access a trustworthy source of randomness that does not rely on potentially compromised miners or oracles, hence enabling the creation of a new generation of smart contracts that are not limited to being non-probabilistic and can be drawn from the much more general class of probabilistic programs."
            },
            {
                "arxivId": "1901.07807",
                "title": "Interacting with the Internet of Things Using Smart Contracts and Blockchain Technologies",
                "abstract": null
            },
            {
                "arxivId": "1808.00093",
                "title": "Implementation of Smart Contracts Using Hybrid Architectures with On and Off\u2013Blockchain Components",
                "abstract": "Decentralised (on-blockchain) and centralised (off\u2013blockchain) platforms are available for the implementation of smart contracts. However, none of the two alternatives can individually provide the services and quality of services (QoS) imposed on smart contracts involved in a large class of applications. The reason is that blockchain platforms suffer from scalability, performance, transaction costs and other limitations. Likewise, off\u2013blockchain platforms are afflicted by drawbacks emerging from their dependence on single trusted third parties. We argue that in several applications, hybrid platforms composed from the integration of on and off\u2013blockchain platforms are more adequate. Developers that informatively choose between the three alternatives are likely to implement smart contracts that deliver the expected QoS. Hybrid architectures are largely unexplored. To help cover the gap and as a proof of concept, in this paper we discuss the implementation of smart contracts on hybrid architectures. We show how a smart contract can be split and executed partially on an off\u2013blockchain contract compliance checker and partially on the rinkeby ethereum network. To test the solution, we expose it to sequences of contractual operations generated mechanically by a contract validator tool."
            },
            {
                "arxivId": "1806.06185",
                "title": "EdgeChain: An Edge-IoT Framework and Prototype Based on Blockchain and Smart Contracts",
                "abstract": "The emerging Internet of Things (IoT) is facing significant scalability and security challenges. On one hand, IoT devices are \u201cweak\u201d and need external assistance. Edge computing provides a promising direction addressing the deficiency of centralized cloud computing in scaling massive number of devices. On the other hand, IoT devices are also relatively \u201cvulnerable\u201d facing malicious hackers due to resource constraints. The emerging blockchain and smart contracts technologies bring a series of new security features for IoT and edge computing. In this paper, to address the challenges, we design and prototype an edge-IoT framework named \u201cEdgeChain\u201d based on blockchain and smart contracts. The core idea is to integrate a permissioned blockchain and the internal currency or \u201ccoin\u201d system to link the edge cloud resource pool with each IoT device\u2019 account and resource usage, and hence behavior of the IoT devices. EdgeChain uses a credit-based resource management system to control how much resource IoT devices can obtain from edge servers, based on predefined rules on priority, application types, and past behaviors. Smart contracts are used to enforce the rules and policies to regulate the IoT device behavior in a nondeniable and automated manner. All the IoT activities and transactions are recorded into blockchain for secure data logging and auditing. We implement an EdgeChain prototype and conduct extensive experiments to evaluate the ideas. The results show that while gaining the security benefits of blockchain and smart contracts, the cost of integrating them into EdgeChain is within a reasonable and acceptable range."
            },
            {
                "arxivId": "1805.00626",
                "title": "On and Off-Blockchain Enforcement Of Smart Contracts",
                "abstract": null
            },
            {
                "arxivId": "1709.10000",
                "title": "Using Blockchain and smart contracts for secure data provenance management",
                "abstract": "Blockchain technology has evolved from being an immutable ledger of transactions for cryptocurrencies to a programmable interactive the environment for building distributed reliable applications. Although, blockchain technology has been used to address various challenges, to our knowledge none of the previous work focused on using blockchain to develop a secure and immutable scientific data provenance management framework that automatically verifies the provenance records. In this work, we leverage blockchain as a platform to facilitate trustworthy data provenance collection, verification, and management. The developed system utilizes smart contracts and open provenance model (OPM) to record immutable data trails. We show that our proposed framework can efficiently and securely capture and validate provenance data, and prevent any malicious modification to the captured data as long as the majority of the participants are honest."
            },
            {
                "arxivId": "1801.02029",
                "title": "A Perspective on Blockchain Smart Contracts: Reducing Uncertainty and Complexity in Value Exchange",
                "abstract": "The blockchain constitutes a technology-based, rather than social or regulation based, means to lower uncertainty about one another in order to exchange value. However, its use may very well also lead to increased complexity resulting from having to subsume work that displaced intermediary institutions had performed. We present our perspective that smart contracts may be used to mitigate this increased complexity. We further posit that smart contracts can be delineated according to complexity: Smart contracts that can be verified objectively without much uncertainty belong in an inter- organizational context; those that cannot be objectively verified belong in an intra- organizational context. We state that smart contracts that implement a formal (e.g. mathematical or simulation) model are especially beneficial for both contexts: They can be used to express and enforce inter-organizational agreements, and their basis in a common formalism may ensure effective evaluation and comparison between different intra-organizational contracts. Finally, we present a case study of our perspective by describing Intellichain, which implements formal, agent-based simulation model as a smart contract to provide epidemiological decision support."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.19077",
        "category": "q-fin",
        "title": "Blockchains, MEV and the knapsack problem: a primer",
        "abstract": "In this paper, we take a close look at a problem labeled maximal extractable value (MEV), which arises in a blockchain due to the ability of a block producer to manipulate the order of transactions within a block. Indeed, blockchains such as Ethereum have spent considerable resources addressing this issue and have redesigned the block production process to account for MEV. This paper provides an overview of the MEV problem and tracks how Ethereum has adapted to its presence. A vital aspect of the block building exercise is that it is a variant of the knapsack problem. Consequently, this paper highlights the role of designing auctions to fill a knapsack--or knapsack auctions--in alleviating the MEV problem. Overall, this paper presents a survey of the main issues and an accessible primer for researchers and students wishing to explore the economics of block building and MEV further.",
        "references": [
            {
                "arxivId": "2305.19037",
                "title": "Ethereum's Proposer-Builder Separation: Promises and Realities",
                "abstract": "With Ethereum's transition from Proof-of-Work to Proof-of-Stake in September 2022 came another paradigm shift, the Proposer-Builder Separation (PBS) scheme. PBS was introduced to decouple the roles of selecting and ordering transactions in a block (i.e., the builder), from those validating its contents and proposing the block to the network as the new head of the blockchain (i.e., the proposer). In this landscape, proposers are the validators in the Proof-of-Stake consensus protocol, while now relying on specialized block builders for creating blocks with the highest value for the proposer. Additionally, relays act as mediators between builders and proposers. We study PBS adoption and show that the current landscape exhibits significant centralization amongst the builders and relays. Further, we explore whether PBS effectively achieves its intended objectives of enabling hobbyist validators to maximize block profitability and preventing censorship. Our findings reveal that although PBS grants validators the opportunity to access optimized and competitive blocks, it tends to stimulate censorship rather than reduce it. Additionally, we demonstrate that relays do not consistently uphold their commitments and may prove unreliable. Specifically, proposers do not always receive the complete promised value, and the censorship or filtering capabilities pledged by relays exhibit significant gaps."
            },
            {
                "arxivId": "1904.05234",
                "title": "Flash Boys 2.0: Frontrunning, Transaction Reordering, and Consensus Instability in Decentralized Exchanges",
                "abstract": "Blockchains, and specifically smart contracts, have promised to create fair and transparent trading ecosystems. \nUnfortunately, we show that this promise has not been met. We document and quantify the widespread and rising deployment of arbitrage bots in blockchain systems, specifically in decentralized exchanges (or \"DEXes\"). Like high-frequency traders on Wall Street, these bots exploit inefficiencies in DEXes, paying high transaction fees and optimizing network latency to frontrun, i.e., anticipate and exploit, ordinary users' DEX trades. \nWe study the breadth of DEX arbitrage bots in a subset of transactions that yield quantifiable revenue to these bots. We also study bots' profit-making strategies, with a focus on blockchain-specific elements. We observe bots engage in what we call priority gas auctions (PGAs), competitively bidding up transaction fees in order to obtain priority ordering, i.e., early block position and execution, for their transactions. PGAs present an interesting and complex new continuous-time, partial-information, game-theoretic model that we formalize and study. We release an interactive web portal, this http URL, to provide the community with real-time data on PGAs. \nWe additionally show that high fees paid for priority transaction ordering poses a systemic risk to consensus-layer security. We explain that such fees are just one form of a general phenomenon in DEXes and beyond---what we call miner extractable value (MEV)---that poses concrete, measurable, consensus-layer security risks. We show empirically that MEV poses a realistic threat to Ethereum today. \nOur work highlights the large, complex risks created by transaction-ordering dependencies in smart contracts and the ways in which traditional forms of financial-market exploitation are adapting to and penetrating blockchain economies."
            },
            {
                "arxivId": "cs/0202017",
                "title": "Truth revelation in approximately efficient combinatorial auctions",
                "abstract": "Some important classical mechanisms considered in Microeconomics and Game Theory require the solution of a difficult optimization problem. This is true of mechanisms for combinatorial auctions, which have in recent years assumed practical importance, and in particular of the gold standard for combinatorial auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these mechanisms---in particular, their truth revelation properties---assumes that the optimization problems are solved precisely. In reality, these optimization problems can usually be solved only in an approximate fashion. We investigate the impact on such mechanisms of replacing exact solutions by approximate ones. Specifically, we look at a particular greedy optimization method. We show that the GVA payment scheme does not provide for a truth revealing mechanism. We introduce another scheme that does guarantee truthfulness for a restricted class of players. We demonstrate the latter property by identifying natural properties for combinatorial auctions and showing that, for our restricted class of players, they imply that truthful strategies are dominant. Those properties have applicability beyond the specific auction studied."
            },
            {
                "arxivId": "1110.0025",
                "title": "Computationally feasible VCG mechanisms",
                "abstract": "A major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When applying this method to complex problems such as combinatorial auctions, a difficulty arises: VCG mechanisms are required to compute optimal outcomes and are, therefore, computationally infeasible. However, if the optimal outcome is replaced by the results of a sub-optimal algorithm, the resulting mechanism (termed VCG-based) is no longer necessarily truthful. The first part of this paper studies this phenomenon in depth and shows that it is near universal. Specifically, we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield non-truthful VCG-based mechanisms. We generalize these results for affine maximizers. The second part of this paper proposes a general method for circumventing the above problem. We introduce a modification of VCG-based mechanisms in which the agents are given a chance to improve the output of the underlying algorithm. When the agents behave truthfully, the welfare obtained by the mechanism is at least as good as the one obtained by the algorithm\u2019s output. We provide a strong rationale for truth-telling behavior. Our method satisfies individual rationality as well."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.19502",
        "category": "q-fin",
        "title": "On the potential of quantum walks for modeling financial return distributions",
        "abstract": "Accurate modeling of the temporal evolution of asset prices is crucial for understanding financial markets. We explore the potential of discrete-time quantum walks to model the evolution of asset prices. Return distributions obtained from a model based on the quantum walk algorithm are compared with those obtained from classical methodologies. We focus on specific limitations of the classical models, and illustrate that the quantum walk model possesses great flexibility in overcoming these. This includes the potential to generate asymmetric return distributions with complex market tendencies and higher probabilities for extreme events than in some of the classical models. Furthermore, the temporal evolution in the quantum walk possesses the potential to provide asset price dynamics.",
        "references": [
            {
                "arxivId": "2303.16155",
                "title": "Entropy of Financial Time Series Due to the Shock of War",
                "abstract": "The concept of entropy is not uniquely relevant to the statistical mechanics but, among others, it can play pivotal role in the analysis of a time series, particularly the stock market data. In this area, sudden events are especially interesting as they describe abrupt data changes with potentially long-lasting effects. Here, we investigate the impact of such events on the entropy of financial time series. As a case study, we assume data of the Polish stock market, in the context of its main cumulative index, and discuss it for the finite time periods before and after outbreak of the 2022 Russian invasion of Ukraine. This analysis allows us to validate the entropy-based methodology in assessing changes in the market volatility, as driven by the extreme external factors. We show that some qualitative features of such market variations can be well captured in terms of the entropy. In particular, the discussed measure appears to highlight differences between data of the two considered timeframes in agreement with the character of their empirical distributions, which is not always the case in terms of the conventional standard deviation. Moreover, the entropy of cumulative index averages, qualitatively, the entropies of composing assets, suggesting capability for describing interdependencies between them. The entropy is also found to exhibit signatures of the upcoming extreme events. To this end, the role of recent war in shaping the current economic situation is briefly discussed."
            },
            {
                "arxivId": "2202.12067",
                "title": "A 2D L\u00e9vy-flight model for the complex dynamics of real-life financial markets.",
                "abstract": "We report on the emergence of scaling laws in the temporal evolution of the daily closing values of the S&P 500 index prices and its modeling based on the L\u00e9vy flights in two dimensions (2D). The efficacy of our proposed model is verified and validated by using the extreme value statistics in the random matrix theory. We find that the random evolution of each pair of stocks in a 2D price space is a scale-invariant complex trajectory whose tortuosity is governed by a 2/3 geometric law between the gyration radius Rg(t) and the total length \u2113(t) of the path, i.e., Rg(t)\u223c\u2113(t)2/3. We construct a Wishart matrix containing all stocks up to a specific variable period and look at its spectral properties for over 30 years. In contrast to the standard random matrix theory, we find that the distribution of eigenvalues has a power-law tail with a decreasing exponent over time-a quantitative indicator of the temporal correlations. We find that the time evolution of the distance of 2D L\u00e9vy flights with index \u03b1=3/2 from origin generates the same empirical spectral properties. The statistics of the largest eigenvalues of the model and the observations are in perfect agreement."
            },
            {
                "arxivId": "2201.02773",
                "title": "A Survey of Quantum Computing for Finance",
                "abstract": "Quantum computers are expected to surpass the computational capabilities of classical computers during this decade and have transformative impact on numerous industry sectors, particularly finance. In fact, finance is estimated to be the first industry sector to benefit from quantum computing, not only in the medium and long terms, but even in the short term. This survey paper presents a comprehensive summary of the state of the art of quantum computing for financial applications, with particular emphasis on stochastic modeling, optimization, and machine learning, describing how these solutions, adapted to work on a quantum computer, can potentially help to solve financial problems, such as derivative pricing, risk modeling, portfolio optimization, natural language processing, and fraud detection, more efficiently and accurately. We also discuss the feasibility of these algorithms on nearterm quantum computers with various hardware implementations and demonstrate how they relate to a wide range of use cases in finance. We hope this article will not only serve as a reference for academic researchers and industry practitioners but also inspire new ideas for future research. These authors contributed equally to this work. i ar X iv :2 20 1. 02 77 3v 4 [ qu an tph ] 2 7 Ju n 20 22"
            },
            {
                "arxivId": "2006.14510",
                "title": "Quantum Computing for Finance: State-of-the-Art and Future Prospects",
                "abstract": "This article outlines our point of view regarding the applicability, state-of-the-art, and potential of quantum computing for problems in finance. We provide an introduction to quantum computing as well as a survey on problem classes in finance that are computationally challenging classically and for which quantum computing algorithms are promising. In the main part, we describe in detail quantum algorithms for specific applications arising in financial services, such as those involving simulation, optimization, and machine learning problems. In addition, we include demonstrations of quantum algorithms on IBM Quantum back-ends and discuss the potential benefits of quantum algorithms for problems in financial services. We conclude with a summary of technical challenges and future prospects."
            },
            {
                "arxivId": "1910.02254",
                "title": "Quantum walks with sequential aperiodic jumps.",
                "abstract": "We analyze a set of discrete-time quantum walks for which the displacements on a chain follow binary aperiodic jumps according to three paradigmatic sequences: Fibonacci, Thue-Morse, and Rudin-Shapiro. We use a generalized Hadamard coin, C[over \u0302]_{H}, as well as a generalized Fourier coin, C[over \u0302]_{K}. We verify the QW experiences a slowdown of the wave packet spreading, \u03c3^{2}(t)\u223ct^{\u03b1}, by the aperiodic jumps whose exponent, \u03b1, depends on the type of aperiodicity. Additional aperiodicity-induced effects also emerge, namely, (1) while the superdiffusive regime (1<\u03b1<2) is predominant, \u03b1 displays an unusual sensibility with the type of coin operator where the more pronounced differences emerge for the Rudin-Shapiro and random protocols and (2) even though the angle \u03b8 of the coin operator is homogeneous in space and time, there is a nonmonotonic dependence of \u03b1 with \u03b8. Fingerprints of the aperiodicity in the hoppings are also found when distributional measures such as the Shannon and von Neumann entropies, the Inverse Participation Ratio, the Jensen-Shannon dissimilarity, and the kurtosis are computed, which allow assessing informational and delocalization features arising from these protocols and understanding the impact of linear and nonlinear correlations of the jump sequence in a quantum walk as well. Finally, we argue the spin-lattice entanglement is enhanced by aperiodic jumps."
            },
            {
                "arxivId": "1809.00505",
                "title": "Transfiguration of Quantum Walks on a line",
                "abstract": "We introduce an analytically treatable spin decoherence model for quantum walk on a line that yields the exact position probability distribution of an unbiased classical random walk at all-time scales. This spin decoherence model depicts a quantum channel in which simultaneous bit and phase flip operator is applied at random on the coin state. Based on this result we claim that there exist certain quantum channels that can produce exact classical statistical properties for a given one-dimensional quantum walk. Moreover, from the perspective of quantum computing, decoherence model introduced in this study may have useful algorithmic applications when it is applied on quantum walks with non-local initial states."
            },
            {
                "arxivId": "1807.03890",
                "title": "Quantum computing for finance: Overview and prospects",
                "abstract": null
            },
            {
                "arxivId": "1405.3512",
                "title": "Quantum Brownian motion model for the stock market",
                "abstract": null
            },
            {
                "arxivId": "1310.5527",
                "title": "Power laws and Self-Organized Criticality in Theory and Nature",
                "abstract": null
            },
            {
                "arxivId": "1207.7283",
                "title": "Quantum Walks",
                "abstract": "This tutorial article showcases the many varieties and uses of quantum walks. Discrete time quantum walks are introduced as counterparts of classical random walks. The emphasis is on the connections and differences between the two types of processes (with rather different underlying dynamics) for producing random distributions. We discuss algorithmic applications for graph-searching and compare the two approaches. Next, we look at quantization of Markov chains and show how it can lead to speedups for sampling schemes. Finally, we turn to continuous time quantum walks and their applications, which provide interesting (even exponential) speedups over classical approaches."
            },
            {
                "arxivId": "cond-mat/9905305",
                "title": "Scaling of the distribution of fluctuations of financial market indices.",
                "abstract": "We study the distribution of fluctuations of the S&P 500 index over a time scale deltat by analyzing three distinct databases. Database (i) contains approximately 1 200 000 records, sampled at 1-min intervals, for the 13-year period 1984-1996, database (ii) contains 8686 daily records for the 35-year period 1962-1996, and database (iii) contains 852 monthly records for the 71-year period 1926-1996. We compute the probability distributions of returns over a time scale deltat, where deltat varies approximately over a factor of 10(4)-from 1 min up to more than one month. We find that the distributions for deltat<or= 4 d (1560 min) are consistent with a power-law asymptotic behavior, characterized by an exponent alpha approximately 3, well outside the stable L\u00e9vy regime 0<alpha<2. To test the robustness of the S&P result, we perform a parallel analysis on two other financial market indices. Database (iv) contains 3560 daily records of the NIKKEI index for the 14-year period 1984-1997, and database (v) contains 4649 daily records of the Hang-Seng index for the 18-year period 1980-1997. We find estimates of alpha consistent with those describing the distribution of S&P 500 daily returns. One possible reason for the scaling of these distributions is the long persistence of the autocorrelation function of the volatility. For time scales longer than (deltat)x approximately 4 d, our results are consistent with a slow convergence to Gaussian behavior."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-03-29.json",
        "arxivId": "2403.19563",
        "category": "q-fin",
        "title": "Flexible Analysis of Individual Heterogeneity in Event Studies: Application to the Child Penalty",
        "abstract": "We provide a practical toolkit for analyzing effect heterogeneity in event studies. We develop an estimation algorithm and adapt existing econometric results to provide its theoretical justification. We apply these tools to Dutch administrative data to study individual heterogeneity in the child-penalty (CP) context in three ways. First, we document significant heterogeneity in the individual-level CP trajectories, emphasizing the importance of going beyond the average CP. Second, we use individual-level estimates to examine the impact of childcare supply expansion policies. Our approach uncovers nonlinear treatment effects, challenging the conventional policy evaluation methods constrained to less flexible specifications. Third, we use the individual-level estimates as a regressor on the right-hand side to study the intergenerational elasticity of the CP between mothers and daughters. After adjusting for the measurement error bias, we find the elasticity of 24\\%. Our methodological framework contributes to empirical practice by offering a flexible approach tailored to specific research questions and contexts. We provide an open-source package ('unitdid') to facilitate widespread adoption.",
        "references": [
            {
                "arxivId": "2311.15458",
                "title": "Causal Models for Longitudinal and Panel Data: A Survey",
                "abstract": "This survey discusses the recent causal panel data literature. This recent literature has focused on credibly estimating causal effects of binary interventions in settings with longitudinal data, with an emphasis on practical advice for empirical researchers. It pays particular attention to heterogeneity in the causal effects, often in situations where few units are treated and with particular structures on the assignment pattern. The literature has extended earlier work on difference-in-differences or two-way-fixed-effect estimators. It has more generally incorporated factor models or interactive fixed effects. It has also developed novel methods using synthetic control approaches."
            },
            {
                "arxivId": "2106.05024",
                "title": "Contamination Bias in Linear Regressions",
                "abstract": "We study regressions with multiple treatments and a set of controls that is flexible enough to purge omitted variable bias. We show that these regressions generally fail to estimate convex averages of heterogeneous treatment effects -- instead, estimates of each treatment's effect are contaminated by non-convex averages of the effects of other treatments. We discuss three estimation approaches that avoid such contamination bias, including the targeting of easiest-to-estimate weighted average effects. A re-analysis of nine empirical applications finds economically and statistically meaningful contamination bias in observational studies; contamination bias in experimental studies is more limited due to idiosyncratic effect heterogeneity."
            },
            {
                "arxivId": "2004.14497",
                "title": "Towards optimal doubly robust estimation of heterogeneous causal effects",
                "abstract": "Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations."
            },
            {
                "arxivId": "1804.05785",
                "title": "Estimating Dynamic Treatment Effects in Event Studies With Heterogeneous Treatment Effects",
                "abstract": "Event studies are frequently used to estimate average treatment effects on the treated (ATT). In estimating the ATT, researchers commonly use fixed effects models that implicitly assume constant treatment effects across cohorts. We show that this is not an innocuous assumption. In fixed effect models where the sole regressor is treatment status, the OLS coefficient is a non-convex average of the heterogeneous cohort-specific ATTs. When regressors containing lags and leads of treatment are added, the OLS coefficient corresponding to a given lead or lag picks up spurious terms consisting of treatment effects from other periods. Therefore, estimates from these commonly used models are not causally interpretable. We propose alternative estimators that identify certain convex averages of the cohort-specific ATTs, hence allowing for causal interpretation even under heterogeneous treatment effects. To illustrate the empirical content of our results, we show that the fixed effects estimators and our proposed estimators differ substantially in an application to the economic consequences of hospitalization."
            },
            {
                "arxivId": "1803.09015",
                "title": "Difference-in-Differences with Multiple Time Periods",
                "abstract": "Difference-in-Differences (DID) is one of the most important and popular designs for evaluating causal effects of policy changes. In its standard format, there are two time periods and two groups: in the first period no one is treated, and in the second period a \"treatment group\" becomes treated, whereas a \"control group\" remains untreated. However, many em- pirical applications of the DID design have more than two periods and variation in treatment timing. In this article, we consider identification and estimation of treatment effect parameters using DID with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the \"parallel trends assumption\" holds potentially only after conditioning on observed covariates. We propose a simple two-step estimation strategy, establish the asymptotic prop- erties of the proposed estimators, and prove the validity of a computationally convenient bootstrap procedure. Furthermore we propose a semiparametric data-driven testing procedure to assess the credibility of the DID design in our context. Finally, we analyze the effect of the minimum wage on teen employment from 2001-2007. By using our proposed methods we confront the challenges related to variation in the timing of the state-level minimum wage policy changes. Open-source software is available for implementing the proposed methods."
            },
            {
                "arxivId": "1712.04912",
                "title": "Quasi-oracle estimation of heterogeneous treatment effects",
                "abstract": "\n Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-01.json",
        "arxivId": "2401.07075",
        "category": "q-fin",
        "title": "Heterogeneous treatment effect estimation with high-dimensional data in public policy evaluation -- an application to the conditioning of cash transfers in Morocco using causal machine learning",
        "abstract": "Causal machine learning methods can be used to search for treatment effect heterogeneity in high-dimensional datasets even where we lack a strong enough theoretical framework to select variables or make parametric assumptions about data. This paper uses causal machine learning methods to estimate heterogeneous treatment effects in the case of an experimental study carried out in Morocco which evaluated the effect of conditionalizing a cash transfer program on several outcomes including maths test scores which is the focus of this work. We explore treatment effects across a dataset of 1936 pre-treatment variables. For the most part, heterogeneity is modelled by two different factors, participation in education (at the baseline) and more general measures of poverty. Those who are more disadvantaged at the baseline benefit less from any treatment. While conditioning generally has a negative effect this more disadvantaged group is also hurt more by conditioning. The second purpose of this paper is to demonstrate and reflect upon a causal machine learning approach to policy evaluation. We propose a novel causal tree method for interpretable modelling of causal effects and reflect on the difficulty of explaining atheoretical results.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-01.json",
        "arxivId": "2403.19735",
        "category": "q-fin",
        "title": "Enhancing Anomaly Detection in Financial Markets with an LLM-based Multi-Agent Framework",
        "abstract": "This paper introduces a Large Language Model (LLM)-based multi-agent framework designed to enhance anomaly detection within financial market data, tackling the longstanding challenge of manually verifying system-generated anomaly alerts. The framework harnesses a collaborative network of AI agents, each specialised in distinct functions including data conversion, expert analysis via web research, institutional knowledge utilization or cross-checking and report consolidation and management roles. By coordinating these agents towards a common objective, the framework provides a comprehensive and automated approach for validating and interpreting financial data anomalies. I analyse the S&P 500 index to demonstrate the framework's proficiency in enhancing the efficiency, accuracy and reduction of human intervention in financial market monitoring. The integration of AI's autonomous functionalities with established analytical methods not only underscores the framework's effectiveness in anomaly detection but also signals its broader applicability in supporting financial market monitoring.",
        "references": [
            {
                "arxivId": "2308.08155",
                "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "abstract": "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "1901.03407",
                "title": "Deep Learning for Anomaly Detection: A Survey",
                "abstract": "Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-01.json",
        "arxivId": "2403.19781",
        "category": "q-fin",
        "title": "Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior",
        "abstract": "Investors and regulators can greatly benefit from a realistic market simulator that enables them to anticipate the consequences of their decisions in real markets. However, traditional rule-based market simulators often fall short in accurately capturing the dynamic behavior of market participants, particularly in response to external market impact events or changes in the behavior of other participants. In this study, we explore an agent-based simulation framework employing reinforcement learning (RL) agents. We present the implementation details of these RL agents and demonstrate that the simulated market exhibits realistic stylized facts observed in real-world markets. Furthermore, we investigate the behavior of RL agents when confronted with external market impacts, such as a flash crash. Our findings shed light on the effectiveness and adaptability of RL-based agents within the simulation, offering insights into their response to significant market events.",
        "references": [
            {
                "arxivId": "2311.07738",
                "title": "Revisiting Stylized Facts for Modern Stock Markets",
                "abstract": "In 2001, Rama Cont introduced a now-widely used set of \u2018stylized facts\u2019 to synthesize empirical studies of financial time series, resulting in 11 qualitative properties presumed to be universal to all financial markets. Here, we replicate Cont\u2019s analyses for a convenience sample of stocks drawn from the U.S. stock market following a fundamental shift in market regulation. Our study relies on the same authoritative data as that used by the U.S. regulator. We find conclusive evidence in the modern market for eight of Cont\u2019s original facts, while we find weak support for one additional fact and no support for the remaining two. Our study represents the first test of the original set of 11 stylized facts against a consistent set of stocks, therefore providing insight into how Cont\u2019s stylized facts should be viewed in the context of modern stock markets."
            },
            {
                "arxivId": "2210.07184",
                "title": "Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations",
                "abstract": "We study a game between liquidity provider (LP) and liquidity taker agents interacting in an over\u2010the\u2010counter market, for which the typical example is foreign exchange. We show how a suitable design of parameterized families of reward functions coupled with shared policy learning constitutes an efficient solution to this problem. By playing against each other, our deep\u2010reinforcement\u2010learning\u2010driven agents learn emergent behaviors relative to a wide spectrum of objectives encompassing profit\u2010and\u2010loss, optimal execution, and market share. In particular, we find that LPs naturally learn to balance hedging and skewing, where skewing refers to setting their buy and sell prices asymmetrically as a function of their inventory. We further introduce a novel RL\u2010based calibration algorithm, which we found performed well at imposing constraints on the game equilibrium. On the theoretical side, we are able to show convergence rates for our multi\u2010agent policy gradient algorithm under a transitivity assumption, closely related to generalized ordinal potential\u00a0games."
            },
            {
                "arxivId": "2110.06829",
                "title": "Towards a fully rl-based market simulator",
                "abstract": "We present a new financial framework where two families of RL-based agents representing the Liquidity Providers and Liquidity Takers learn simultaneously to satisfy their objective. Thanks to a parametrized reward formulation and the use of Deep RL, each group learns a shared policy able to generalize and interpolate over a wide range of behaviors. This is a step towards a fully RL-based market simulator replicating complex market conditions particularly suited to study the dynamics of the financial market under various scenarios."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-01.json",
        "arxivId": "2403.20039",
        "category": "q-fin",
        "title": "Variability in Aggregate Personal Income Across Industrial Sectors During COVID-19 Shock: A Time-Series Exploration (preprint)",
        "abstract": "This study explored the variability in Aggregate Personal Income (PI) across 13 major industrial sectors in the US during the COVID-19 pandemic. Utilizing time-series data from 2010 Q1 to 2019 Q4, we employed Autoregressive Integrated Moving Average (ARIMA) models to establish baseline trends in Personal Income (PI) before the pandemic. We then extended these models to forecast PI values for the subsequent 14 quarters, from 2020 Q1 to 2023 Q2, as if the pandemic had never happened. This forecasted data was compared with the actual PI data collected during the pandemic to quantify its impacts. This approach allowed for the assessment of both immediate and extended effects of COVID-19 on sector-specific PI. Our study highlighted the resilience of PI in sectors like Utilities, Retail, Finance, Real Estate, and Healthcare, with Farming showing an early recovery in PI, despite significant initial setbacks. In contrast, PI in Accommodation and Food Services experienced delayed recovery, contributing significantly to the overall impact variance alongside Farming (53.26\\% and 33.26\\% respectively). Finance and Utilities demonstrated positive deviations, suggesting a lesser impact or potential benefit in early pandemic stages. Meanwhile, sectoral PI in Manufacturing, Wholesale and Education showed moderate recovery, whereas Construction and Government lagged in resilience. The aggregate economic impact, initially negative at -0.027 in 2020 Q1, drastically worsened to -1.42 in Q2, but improved by Q4, reflecting a broader trend of adaptation and resilience across all the sectors during the pandemic.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-01.json",
        "arxivId": "2403.20171",
        "category": "q-fin",
        "title": "Risk exchange under infinite-mean Pareto models",
        "abstract": "We study the optimal decisions of agents who aim to minimize their risks by allocating their positions over extremely heavy-tailed (i.e., infinite-mean) and possibly dependent losses. The loss distributions of our focus are super-Pareto distributions which include the class of extremely heavy-tailed Pareto distributions. For a portfolio of super-Pareto losses, non-diversification is preferred by decision makers equipped with well-defined and monotone risk measures. The phenomenon that diversification is not beneficial in the presence of super-Pareto losses is further illustrated by an equilibrium analysis in a risk exchange market. First, agents with super-Pareto losses will not share risks in a market equilibrium. Second, transferring losses from agents bearing super-Pareto losses to external parties without any losses may arrive at an equilibrium which benefits every party involved. The empirical studies show that extremely heavy tails exist in real datasets.",
        "references": [
            {
                "arxivId": "2203.02599",
                "title": "A Reverse ES (CVaR) Optimization Formula",
                "abstract": "The celebrated Expected Shortfall (ES) optimization formula implies that ES at a fixed probability level is the minimum of a linear real function plus a scaled mean excess function. We establish a reverse ES optimization formula, which says that a mean excess function at any fixed threshold is the maximum of an ES curve minus a linear function. Despite being a simple result, this formula reveals elegant symmetries between the mean excess function and the ES curve, as well as their optimizers. The reverse ES optimization formula is closely related to the Fenchel-Legendre transforms, and our formulas are generalized from ES to optimized certainty equivalents, a popular class of convex risk measures. We analyze worst-case values of the mean excess function under two popular settings of model uncertainty to illustrate the usefulness of the reverse ES optimization formula, and this is further demonstrated with an application using insurance datasets."
            },
            {
                "arxivId": "1207.5674",
                "title": "Exploring the limits of safety analysis in complex technological systems",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-01.json",
        "arxivId": "2403.20310",
        "category": "q-fin",
        "title": "Predicting the impact of e-commerce indices on international trade in Iran and other selected members of the Organization for Economic Co-operation and Development (OECD) by using the artificial intelligence and P-VAR model",
        "abstract": "This study aims at predicting the impact of e-commerce indicators on international trade of the selected OECD countries and Iran, by using the artificial intelligence approach and P-VAR. According to the nature of export, import, GDP, and ICT functions, and the characteristics of nonlinearity, this analysis is performed by using the MPL neural network. The export, import, GDP, and ICT findings were examined with 99 percent accuracy. Using the P-VAR model in the Eviews software, the initial database and predicted data were applied to estimate the impact of e-commerce on international trade. The findings from analyzing the data show that there is a bilateral correlation between e-commerce which means that ICT and international trade affect each other and the Goodness of fit of the studied model is confirmed.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "1806.01166",
        "category": "q-fin",
        "title": "Dynamic risk measures with fluctuation of market volatility under Bochne-Lebesgue space",
        "abstract": "Starting from the global financial crisis to the more recent disruptions brought about by geopolitical tensions and public health crises, the volatility of risk in financial markets has increased significantly. This underscores the necessity for comprehensive risk measures capable of capturing the complexity and heightened fluctuations in market volatility. This need is crucial not only for new financial assets but also for the traditional financial market in the face of a rapidly changing financial environment and global landscape. In this paper, we consider the risk measures on a special space $L^{p(\\cdot)}$, where the variable exponent $p(\\cdot)$ is no longer a given real number as in the conventional risk measure space $L^{p}$, but rather a random variable reflecting potential fluctuations in volatility within financial markets. Through further development of axioms related to this class of risk measures, we also establish dual representations for them.",
        "references": [
            {
                "arxivId": "2010.05745",
                "title": "Variable exponent Bochner-Lebesgue spaces with symmetric gradient structure.",
                "abstract": null
            },
            {
                "arxivId": "2010.02817",
                "title": "Fixed point properties and reflexivity in variable Lebesgue spaces",
                "abstract": null
            },
            {
                "arxivId": "1810.10149",
                "title": "Recursive Utility Processes, Dynamic Risk Measures and Quadratic Backward Stochastic Volterra Integral Equations",
                "abstract": null
            },
            {
                "arxivId": "1212.5563",
                "title": "Multi-portfolio time consistency for set-valued convex and coherent risk measures",
                "abstract": null
            },
            {
                "arxivId": "1201.1483",
                "title": "Time consistency of dynamic risk measures in markets with transaction costs",
                "abstract": "Abstract Set-valued dynamic risk measures are defined on with and with an image space in the power set of . Primal and dual representations of dynamic risk measures are deduced. Definitions of different time consistency properties in the set-valued framework are given. It is shown that the recursive form for multivariate risk measures as well as an additive property for the acceptance sets is equivalent to a stronger time consistency property called multi-portfolio time consistency."
            },
            {
                "arxivId": "1002.3627",
                "title": "Risk assessment for uncertain cash flows: model ambiguity, discounting ambiguity, and the role of bubbles",
                "abstract": null
            },
            {
                "arxivId": "0711.2354",
                "title": "Function spaces of variable smoothness and integrability",
                "abstract": null
            },
            {
                "arxivId": "math/0410453",
                "title": "Dynamic Monetary Risk Measures for Bounded Discrete-Time Processes",
                "abstract": "We study dynamic monetary risk measures that depend on bounded discrete-time processes describing the evolution of financial values. The time horizon can be finite or infinite. We call a dynamic risk measure time-consistent if it assigns to a process of financial values the same risk irrespective of whether it is calculated directly or in two steps backwards in time. We show that this condition translates into a decomposition property for the corresponding acceptance sets, and we demonstrate how time-consistent dynamic monetary risk measures can be constructed by pasting together one-period risk measures. For conditional coherent and convex monetary risk measures, we provide dual representations of Legendre--Fenchel type based on linear functionals induced by adapted increasing processes of integrable variation. Then we give dual characterizations of time-consistency for dynamic coherent and convex monetary risk measures. To this end, we introduce a concatenation operation for adapted increasing processes of integrable variation, which generalizes the pasting of probability measures. In the coherent case, time-consistency corresponds to stability under concatenation in the dual. For dynamic convex monetary risk measures, the dual characterization of time-consistency generalizes to a condition on the family of convex conjugates of the conditional risk measures at different times. The theoretical results are applied by discussing the time-consistency of various specific examples of dynamic monetary risk measures that depend on bounded discrete-time processes."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2010.09227",
        "category": "q-fin",
        "title": "Influencing Competition Through Shelf Design",
        "abstract": "Shelf design decisions strongly influence product demand. In particular, placing products in desirable locations increases demand. This primary effect on shelf position is clear, but there is a secondary effect based on the relative positioning of nearby products. Intuitively, products located next to each other are more likely to be compared having positive and negative effects. On the one hand, locations closer to relatively strong products will be undesirable, as these strong products will draw demand from others -- an effect that is stronger for those in close proximity. On the other hand, because strong products tend to attract more traffic, locations closer to them elicit high consumer attention by increased visibility. Modifying the GEV class of models to allow demand to be moderated by competitors' proximity, these two effects emerge naturally. We found that although the competition effect is usually stronger, it is not always the dominating effect. Shelf displays can achieve higher profits by exploiting the relative influence on competition from shelf design to shift demand to higher profitability products. In the paper towel category, we found profitability differences of up to 7\\% and displays with 3\\% higher gross profits over the best shelf design present in our data.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2303.16012",
        "category": "q-fin",
        "title": "On the number of terms in the COS method for European option pricing",
        "abstract": null,
        "references": [
            {
                "arxivId": "1901.08943",
                "title": "Pricing options and computing implied volatilities using neural networks",
                "abstract": "This paper proposes a data-driven approach, by means of an Artificial Neural Network (ANN), to value financial options and to calculate implied volatilities with the aim of accelerating the corresponding numerical methods. With ANNs being universal function approximators, this method trains an optimized ANN on a data set generated by a sophisticated financial model, and runs the trained ANN as an agent of the original solver in a fast and efficient way. We test this approach on three different types of solvers, including the analytic solution for the Black-Scholes equation, the COS method for the Heston stochastic volatility model and Brent\u2019s iterative root-finding method for the calculation of implied volatilities. The numerical results show that the ANN solver can reduce the computing time significantly."
            },
            {
                "arxivId": "1907.09856",
                "title": "On the shapes of bilateral Gamma densities",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0203046",
                "title": "Probability distribution of returns in the Heston model with stochastic volatility",
                "abstract": "Abstract We study the Heston model, where the stock price dynamics is governed by a geometrical (multiplicative) Brownian motion with stochastic variance. We solve the corresponding Fokker\u2010Planck equation exactly and, after integrating out the variance, find an analytic formula for the time\u2010dependent probability distribution of stock price changes (returns). The formula is in excellent agreement with the Dow\u2010Jones index for time lags from 1 to 250 trading days. For large returns, the distribution is exponential in log\u2010returns with a time\u2010dependent exponent, whereas for small returns it is Gaussian. For time lags longer than the relaxation time of variance, the probability distribution can be expressed in a scaling form using a Bessel function. The Dow\u2010Jones data for 1982\u20132001 follow the scaling function for seven orders of magnitude."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00012",
        "category": "q-fin",
        "title": "Stress Index Strategy Enhanced With Financial News Sentiment Analysis for the Equity Markets",
        "abstract": "This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&P 500 and the six major equity markets, indicating that the method generalises across equities markets.",
        "references": [
            {
                "arxivId": "2304.00228",
                "title": "Large language models can rate news outlet credibility",
                "abstract": "Although large language models (LLMs) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations. State-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue by gathering information directly from the internet to ground their answers. In this setting, the capacity to distinguish trustworthy sources is critical for providing appropriate accuracy contexts to users. Here we assess whether ChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With appropriate instructions, ChatGPT can provide ratings for a diverse set of news outlets, including those in non-English languages and satirical sources, along with contextual explanations. Our results show that these ratings correlate with those from human experts (Spearmam's $\\rho=0.54, p<0.001$). These findings suggest that LLMs could be an affordable reference for credibility ratings in fact-checking applications. Future LLMs should enhance their alignment with human expert judgments of source credibility to improve information accuracy."
            },
            {
                "arxivId": "1908.10063",
                "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models",
                "abstract": "Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods."
            },
            {
                "arxivId": "1812.01207",
                "title": "Practical Text Classification With Large Pre-Trained Language Models",
                "abstract": "Multi-emotion sentiment classification is a natural language processing (NLP) problem with valuable use cases on real-world data. We demonstrate that large-scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets, including those with label class imbalance and domain-specific context. By training an attention-based Transformer network (Vaswani et al. 2017) on 40GB of text (Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our model achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional emotion classification problem (Mohammad et al. 2018), based on the Plutchik wheel of emotions (Plutchik 1979). These results are competitive with state of the art models, including strong F1 scores on difficult (emotion) categories such as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive results on rare categories such as Anticipation (0.42) and Surprise (0.37). Furthermore, we demonstrate our application on a real world text classification task. We create a narrowly collected text dataset of real tweets on several topics, and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin. We also perform a variety of additional studies, investigating properties of deep learning architectures, datasets and algorithms for achieving practical multidimensional sentiment classification. Overall, we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on real-world sentiment classification."
            },
            {
                "arxivId": "1801.07883",
                "title": "Deep learning for sentiment analysis: A survey",
                "abstract": "Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state\u2010of\u2010the\u2010art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00013",
        "category": "q-fin",
        "title": "Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction",
        "abstract": "This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge database to be used for imputation and overcome the need to access the entire database repetitively for each missing value. This method is then implemented and tested for the prediction of bankruptcy with the Polish Bankruptcy dataset. It provides an efficient solution for big and high-dimensional datasets even with large imputation rates. Then an AI-driven pipeline for bankruptcy prediction has been designed using the proposed granular semantic-based data filling method followed by the solutions to the issues like high dimensional dataset and high class-imbalance in the dataset. The rest of the pipeline consists of feature selection with the random forest for reducing dimensionality, data balancing with SMOTE, and prediction with six different popular classifiers including deep NN. All methods defined here have been experimentally verified with suitable comparative studies and proven to be effective on all the data sets captured over the five years.",
        "references": [
            {
                "arxivId": "1802.05326",
                "title": "Analysis of Financial Credit Risk Using Machine Learning",
                "abstract": "Corporate insolvency can have a devastating effect on the economy. With an increasing number of companies making expansion overseas to capitalize on foreign resources, a multinational corporate bankruptcy can disrupt the world's financial ecosystem. Corporations do not fail instantaneously; objective measures and rigorous analysis of qualitative (e.g. brand) and quantitative (e.g. econometric factors) data can help identify a company's financial risk. Gathering and storage of data about a corporation has become less difficult with recent advancements in communication and information technologies. The remaining challenge lies in mining relevant information about a company's health hidden under the vast amounts of data, and using it to forecast insolvency so that managers and stakeholders have time to react. In recent years, machine learning has become a popular field in big data analytics because of its success in learning complicated models. Methods such as support vector machines, adaptive boosting, artificial neural networks, and Gaussian processes can be used for recognizing patterns in the data (with a high degree of accuracy) that may not be apparent to human analysts. This thesis studied corporate bankruptcy of manufacturing companies in Korea and Poland using experts' opinions and financial measures, respectively. Using publicly available datasets, several machine learning methods were applied to learn the relationship between the company's current state and its fate in the near future. Results showed that predictions with accuracy greater than 95% were achievable using any machine learning technique when informative features like experts' assessment were used. However, when using purely financial factors to predict whether or not a company will go bankrupt, the correlation is not as strong."
            },
            {
                "arxivId": "1106.1813",
                "title": "SMOTE: Synthetic Minority Over-sampling Technique",
                "abstract": "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00060",
        "category": "q-fin",
        "title": "Temporal Graph Networks for Graph Anomaly Detection in Financial Networks",
        "abstract": "This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of each module. In conclusion, we demonstrated that, even with variations within TGN, it is possible to achieve good performance in the anomaly detection task.",
        "references": [
            {
                "arxivId": "2306.16424",
                "title": "Realistic Synthetic Financial Transactions for Anti-Money Laundering Models",
                "abstract": "With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\\% of global GDP or \\$0.8 - \\$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area. To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and made the datasets public. We describe the generator in detail and demonstrate how the datasets generated can help compare different machine learning models in terms of their AML abilities. In a key way, using synthetic data in these comparisons can be even better than using real data: the ground truth labels are complete, whilst many laundering transactions in real data are never detected."
            },
            {
                "arxivId": "2207.03579",
                "title": "DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection",
                "abstract": "Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental work. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes."
            },
            {
                "arxivId": "2106.13264",
                "title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks",
                "abstract": "Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable efficient processing of hypergraph-structured data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on many datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. Furthermore, AllSet draws on new connections between hypergraph neural networks and recent advances in deep learning of multiset functions. In particular, the proposed architecture utilizes Deep Sets and Set Transformer architectures that allow for significant modeling flexibility and offer high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that AllSet has the unique ability to consistently either match or outperform all other hypergraph neural networks across the tested datasets."
            },
            {
                "arxivId": "2006.12278",
                "title": "HNHN: Hypergraph Networks with Hyperedge Neurons",
                "abstract": "Hypergraphs provide a natural representation for many real world datasets. We propose a novel framework, HNHN, for hypergraph representation learning. HNHN is a hypergraph convolution network with nonlinear activation functions applied to both hypernodes and hyperedges, combined with a normalization scheme that can flexibly adjust the importance of high-cardinality hyperedges and high-degree vertices depending on the dataset. We demonstrate improved performance of HNHN in both classification accuracy and speed on real world datasets when compared to state of the art methods."
            },
            {
                "arxivId": "2006.10637",
                "title": "Temporal Graph Networks for Deep Learning on Dynamic Graphs",
                "abstract": "Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs."
            },
            {
                "arxivId": "1901.08150",
                "title": "Hypergraph Convolution and Hypergraph Attention",
                "abstract": null
            },
            {
                "arxivId": "1710.10903",
                "title": "Graph Attention Networks",
                "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
            },
            {
                "arxivId": "1706.02216",
                "title": "Inductive Representation Learning on Large Graphs",
                "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00183",
        "category": "q-fin",
        "title": "Shared Hardships Strengthen Bonds: Negative Shocks, Embeddedness and Employee Retention",
        "abstract": "Unexpected events --\"shocks\"-- are the motive force in explaining changes in embeddedness and retention within the unfolding model of labor turnover. Substantial research effort has examined strategies for insulating valued employees from adverse shocks. However, this paper provides empirical evidence that unambiguously negative shocks can increase employee retention when underlying firm and employee incentives with respect to these shocks are aligned. Using survival analysis on a unique data set of 466,236 communication records and 45,873 employment spells from 21 trucking companies, we show how equipment-related shocks tend to increase the duration of employment. Equipment shocks also generate paradoxically positive sentiments that demonstrate an increase in employees' affective commitment to the firm. Our results highlight the important moderating role aligned incentives have in how shocks ultimately translate into retention. Shared hardships strengthen bonds in employment as in other areas.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00187",
        "category": "q-fin",
        "title": "Portfolio management using graph centralities: Review and comparison",
        "abstract": "We investigate an application of network centrality measures to portfolio optimization, by generalizing the method in [Pozzi, Di Matteo and Aste, \\emph{Spread of risks across financial markets: better to invest in the peripheries}, Scientific Reports 3:1665, 2013], that however had significant limitations with respect to the state of the art in network theory. In this paper, we systematically compare many possible variants of the originally proposed method on S\\&P 500 stocks. We use daily data from twenty-seven years as training set and their following year as test set. We thus select the best network-based methods according to different viewpoints including for instance the highest Sharpe Ratio and the highest expected return. We give emphasis in new centrality measures and we also conduct a thorough analysis, which reveals significantly stronger results compared to those with more traditional methods. According to our analysis, this graph-theoretical approach to investment can be used successfully by investors with different investment profiles leading to high risk-adjusted returns.",
        "references": [
            {
                "arxivId": "2202.02888",
                "title": "Weighted Enumeration of Nonbacktracking Walks on Weighted Graphs",
                "abstract": "We extend the notion of nonbacktracking walks from unweighted graphs to graphs whose edges have a nonnegative weight. Here the weight associated with a walk is taken to be the product over the weights along the individual edges. We give two ways to compute the associated generating function, and corresponding node centrality measures. One method works directly on the original graph and one uses a line graph construction followed by a projection. The first method is more efficient, but the second has the advantage of extending naturally to time-evolving graphs. Computational results are also provided."
            },
            {
                "arxivId": "physics/0605251",
                "title": "Correlation based networks of equity returns sampled at different time horizons",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00424",
        "category": "q-fin",
        "title": "From attention to profit: quantitative trading strategy based on transformer",
        "abstract": "In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced transformer architecture and designs a novel factor based on the model. By transfer learning from sentiment analysis, the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model's superior performance in predicting stock trends compared with other 100 factor-based quantitative strategies with lower turnover rates and a more robust half-life period. Notably, the model's innovative use transformer to establish factors, in conjunction with market sentiment information, has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.",
        "references": [
            {
                "arxivId": "2307.09288",
                "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
            },
            {
                "arxivId": "2304.04912",
                "title": "Financial Time Series Forecasting using CNN and Transformer",
                "abstract": "Time series forecasting is important across various domains for decision-making. In particular, financial time series such as stock prices can be hard to predict as it is difficult to model short-term and long-term temporal dependencies between data points. Convolutional Neural Networks (CNN) are good at capturing local patterns for modeling short-term dependencies. However, CNNs cannot learn long-term dependencies due to the limited receptive field. Transformers on the other hand are capable of learning global context and long-term dependencies. In this paper, we propose to harness the power of CNNs and Transformers to model both short-term and long-term dependencies within a time series, and forecast if the price would go up, down or remain the same (flat) in the future. In our experiments, we demonstrated the success of the proposed method in comparison to commonly adopted statistical and deep learning methods on forecasting intraday stock price change of S&P 500 constituents."
            },
            {
                "arxivId": "2303.17564",
                "title": "BloombergGPT: A Large Language Model for Finance",
                "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT."
            },
            {
                "arxivId": "2303.08774",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
            },
            {
                "arxivId": "2203.05556",
                "title": "On Embeddings for Numerical Features in Tabular Deep Learning",
                "abstract": "Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial for many backbones, not only for Transformers. Specifically, after proper embeddings, simple MLP-like models can perform on par with the attention-based architectures. Overall, we highlight embeddings for numerical features as an important design aspect with good potential for further improvements in tabular DL."
            },
            {
                "arxivId": "2012.07436",
                "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
                "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem."
            },
            {
                "arxivId": "2010.06925",
                "title": "DA-Transformer: Distance-aware Transformer",
                "abstract": "Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants."
            },
            {
                "arxivId": "2006.08097",
                "title": "FinBERT: A Pretrained Language Model for Financial Communications",
                "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at this https URL. We hope this will be useful for practitioners and researchers working on financial NLP tasks."
            },
            {
                "arxivId": "2005.14165",
                "title": "Language Models are Few-Shot Learners",
                "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
            },
            {
                "arxivId": "1912.01703",
                "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            {
                "arxivId": "1809.09441",
                "title": "Temporal Relational Ranking for Stock Prediction",
                "abstract": "Stock prediction aims to predict the future trends of a stock in order to help investors make good investment decisions. Traditional solutions for stock prediction are based on time-series models. With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction. However, most existing deep learning solutions are not optimized toward the target of investment, i.e., selecting the best stock with the highest expected revenue. Specifically, they typically formulate stock prediction as a classification (to predict stock trends) or a regression problem (to predict stock prices). More importantly, they largely treat the stocks as independent of each other. The valuable signal in the rich relations between stocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer relation, is not considered. In this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock prediction. Our RSR method advances existing solutions in two major aspects: (1) tailoring the deep learning models for stock ranking, and (2) capturing the stock relations in a time-sensitive manner. The key novelty of our work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution, which jointly models the temporal evolution and relation network of stocks. To validate our method, we perform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments demonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions achieving an average return ratio of 98% and 71% on NYSE and NASDAQ, respectively."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            {
                "arxivId": "1706.02515",
                "title": "Self-Normalizing Neural Networks",
                "abstract": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL."
            },
            {
                "arxivId": "1511.08458",
                "title": "An Introduction to Convolutional Neural Networks",
                "abstract": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. \nThis document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning."
            },
            {
                "arxivId": "1412.6980",
                "title": "Adam: A Method for Stochastic Optimization",
                "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            {
                "arxivId": "1412.3555",
                "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00806",
        "category": "q-fin",
        "title": "Algorithmic Collusion by Large Language Models",
        "abstract": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions (\"prompts\") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.",
        "references": [
            {
                "arxivId": "2401.15794",
                "title": "Regulation of Algorithmic Collusion",
                "abstract": "Consider sellers in a competitive market that use algorithms to adapt their prices from data that they collect. In such a context it is plausible that algorithms could arrive at prices that are higher than the competitive prices and this may benefit sellers at the expense of consumers (i.e., the buyers in the market). This paper gives a definition of plausible algorithmic non-collusion for pricing algorithms. The definition allows a regulator to empirically audit algorithms by applying a statistical test to the data that they collect. Algorithms that are good, i.e., approximately optimize prices to market conditions, can be augmented to contain the data sufficient to pass the audit. Algorithms that have colluded on, e.g., supra-competitive prices cannot pass the audit. The definition allows sellers to possess useful side information that may be correlated with supply and demand and could affect the prices used by good algorithms. The paper provides an analysis of the statistical complexity of such an audit, i.e., how much data is sufficient for the test of non-collusion to be accurate."
            },
            {
                "arxivId": "2307.03172",
                "title": "Lost in the Middle: How Language Models Use Long Contexts",
                "abstract": "Abstract While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models."
            },
            {
                "arxivId": "2305.16291",
                "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
                "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/."
            },
            {
                "arxivId": "2304.03442",
                "title": "Generative Agents: Interactive Simulacra of Human Behavior",
                "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
            },
            {
                "arxivId": "2301.07543",
                "title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?",
                "abstract": "Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution."
            },
            {
                "arxivId": "2212.08073",
                "title": "Constitutional AI: Harmlessness from AI Feedback",
                "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels."
            },
            {
                "arxivId": "2207.13243",
                "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
                "abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, \u201cinner\u201d interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications."
            },
            {
                "arxivId": "2205.04661",
                "title": "Pricing with algorithms",
                "abstract": "This paper studies Markov perfect equilibria in a repeated duopoly model where sellers choose algorithms. An algorithm is a mapping from the competitor's price to own price. Once set, algorithms respond quickly. Customers arrive randomly and so do opportunities to revise the algorithm. In the simple game with two possible prices, monopoly outcome is the unique equilibrium for standard functional forms of the profit function. More generally, with multiple prices, exercise of market power is the rule -- in all equilibria, the expected payoff of both sellers is above the competitive outcome, and that of at least one seller is close to or above the monopoly outcome. Sustenance of such collusion seems outside the scope of standard antitrust laws for it does not involve any direct communication."
            },
            {
                "arxivId": "2202.05947",
                "title": "Artificial Intelligence and Auction Design",
                "abstract": "Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about the lowest bid to win, as introduced by Google at the time of the switch to first-price auctions, increases competitiveness of auctions."
            },
            {
                "arxivId": "2201.11903",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
            },
            {
                "arxivId": "2110.11855",
                "title": "Auctions between Regret-Minimizing Agents",
                "abstract": "We analyze a scenario in which software agents implemented as regret-minimizing algorithms engage in a repeated auction on behalf of their users. We study first-price and second-price auctions, as well as their generalized versions (e.g., as those used for ad auctions). Using both theoretical analysis and simulations, we show that, surprisingly, in second-price auctions the players have incentives to misreport their true valuations to their own learning agents, while in first-price auctions it is a dominant strategy for all players to truthfully report their valuations to their agents."
            },
            {
                "arxivId": "2009.03300",
                "title": "Measuring Massive Multitask Language Understanding",
                "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-02.json",
        "arxivId": "2404.00825",
        "category": "q-fin",
        "title": "Using Machine Learning to Forecast Market Direction with Efficient Frontier Coefficients",
        "abstract": "The authors propose a novel method to improve estimation of asset returns for portfolio optimization. This approach first performs a monthly directional market forecast using an online decision tree. The decision tree is trained on a novel set of features engineered from portfolio theory: the efficient frontier functional coefficients. Efficient frontiers can be decomposed to their functional form, a square-root second-order polynomial, and the coefficients of this function capture the information of all the constituents that compose the market in the current time period. To make these forecasts actionable, these directional forecasts are integrated to a portfolio optimization framework using expected returns conditional on the market forecast as an estimate for the return vector. This conditional expectation is calculated using the inverse Mills ratio, and the capital asset pricing model is used to translate the market forecast to individual asset forecasts. This novel method outperforms baseline portfolios, as well as other feature sets including technical indicators and the Fama\u2013French factors. To empirically validate the proposed model, the authors employ a set of market sector exchange-traded funds.",
        "references": [
            {
                "arxivId": "1803.06917",
                "title": "Universal features of price formation in financial markets: perspectives from deep learning",
                "abstract": "Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary relation between order flow history and the direction of price moves. The universal price formation model exhibits a remarkably stable out-of-sample accuracy across a wide range of stocks and time periods. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. The universal model\u2014trained on data from all stocks\u2014outperforms asset-specific models trained on time series of any given stock. This weighs in favor of pooling together financial data from various stocks, rather than designing asset- or sector-specific models, as is currently commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations improves forecast accuracy, indicating that there is path-dependence in price dynamics."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2302.10562",
        "category": "q-fin",
        "title": "Renewable energy expansion under taxes and subsidies: A transmission operator\u2019s perspective",
        "abstract": null,
        "references": [
            {
                "arxivId": "1908.03167",
                "title": "Utility-Scale Energy Storage in an Imperfectly Competitive Power Sector",
                "abstract": null
            },
            {
                "arxivId": "1704.05492",
                "title": "The benefits of cooperation in a highly renewable European electricity network",
                "abstract": null
            },
            {
                "arxivId": "1411.1607",
                "title": "Julia: A Fresh Approach to Numerical Computing",
                "abstract": "Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be \u201claws of nature\" by practitioners of numerical computing: \\beginlist \\item High-level dynamic programs have to be slow. \\item One must prototype in one language and then rewrite in another language for speed or deployment. \\item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \\endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after dif..."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01333",
        "category": "q-fin",
        "title": "Methods of Stochastic Field Theory in Non-Equilibrium Systems --- Spontaneous Symmetry Breaking of Ergodicity",
        "abstract": "Recently, a couple of investigations related to symmetry breaking phenomena, 'spontaneous stochasticity' and 'ergodicity breaking' have led to significant impacts in a variety of fields related to the stochastic processes such as economics and finance. We investigate on the origins and effects of those original symmetries in the action from the mathematical and the effective field theory points of view. It is naturally expected that whenever the system respects any symmetry, it would be spontaneously broken once the system falls into a vacuum state which minimizes an effective action of the dynamical system.",
        "references": [
            {
                "arxivId": "2310.03526",
                "title": "Multifractal dimensions for orthogonal-to-unitary crossover ensemble.",
                "abstract": "Multifractal analysis is a powerful approach for characterizing ergodic or localized nature of eigenstates in complex quantum systems. In this context, the eigenvectors of random matrices belonging to invariant ensembles naturally serve as models for ergodic states. However, it has been found that the finite-size versions of multifractal dimensions for these eigenvectors converge to unity logarithmically slowly with increasing system size N. In fact, this strong finite-size effect is capable of distinguishing the ergodicity behavior of orthogonal and unitary invariant classes. Motivated by this observation, in this work, we provide semi-analytical expressions for the ensemble-averaged multifractal dimensions associated with eigenvectors in the orthogonal-to-unitary crossover ensemble. Additionally, we explore shifted and scaled variants of multifractal dimensions, which, in contrast to the multifractal dimensions themselves, yield distinct values in the orthogonal and unitary limits as N\u2192\u221e and, therefore, may serve as a convenient measure for studying the crossover. We substantiate our results using Monte Carlo simulations of the underlying crossover random matrix model. We then apply our results to analyze the multifractal dimensions in a quantum kicked rotor, a Sinai billiard system, and a correlated spin-chain model in a random field. The orthogonal-to-unitary crossover in these systems is realized by tuning relevant system parameters, and we find that in the crossover regime, the observed finite-dimension multifractal dimensions can be captured very well with our results."
            },
            {
                "arxivId": "2302.09670",
                "title": "Ergodic characterization of nonergodic anomalous diffusion processes",
                "abstract": "Canonical characterization techniques that rely upon mean squared displacement ($\\mathrm{MSD}$) break down for non-ergodic processes, making it challenging to characterize anomalous diffusion from an individual time-series measurement. Non-ergodicity reigns when the time-averaged mean square displacement $\\mathrm{TA}$-$\\mathrm{MSD}$ differs from the ensemble-averaged mean squared displacement $\\mathrm{EA}$-$\\mathrm{MSD}$ even in the limit of long measurement series. In these cases, the typical theoretical results for ensemble averages cannot be used to understand and interpret data acquired from time averages. The difficulty then lies in obtaining statistical descriptors of the measured diffusion process that are not non-ergodic. We show that linear descriptors such as the standard deviation ($SD$), coefficient of variation ($CV$), and root mean square ($RMS$) break ergodicity in proportion to non-ergodicity in the diffusion process. In contrast, time series of descriptors addressing sequential structure and its potential nonlinearity: multifractality change in a time-independent way and fulfill the ergodic assumption, largely independent of the time series' non-ergodicity. We show that these findings follow the multiplicative cascades underlying these diffusion processes. Adding fractal and multifractal descriptors to typical linear descriptors would improve the characterization of anomalous diffusion processes. Two particular points bear emphasis here. First, as an appropriate formalism for encoding the nonlinearity that might generate non-ergodicity, multifractal modeling offers descriptors that can behave ergodically enough to meet the needs of linear modeling. Second, this capacity to describe non-ergodic processes in ergodic terms offers the possibility that multifractal modeling could unify several disparate non-ergodic diffusion processes into a common framework."
            },
            {
                "arxivId": "1512.04465",
                "title": "Spontaneously stochastic solutions in one-dimensional inviscid systems",
                "abstract": "In this paper, we study the inviscid limit of the Sabra shell model of turbulence, which is considered as a particular case of a viscous conservation law in one space dimension with a nonlocal quadratic flux function. We present a theoretical argument (with a detailed numerical confirmation) showing that a classical deterministic solution before a finite-time blowup, t\u2009\u2009<\u2009\u2009tb, must be continued as a stochastic process after the blowup, t\u2009\u2009>\u2009\u2009tb, representing a unique physically relevant description in the inviscid limit. This theory is based on the dynamical system formulation written for the logarithmic time \u03c4=log\u2061(t\u2212tb), which features a stable traveling wave solution for the inviscid Burgers equation, but a stochastic traveling wave for the Sabra model. The latter describes a universal onset of stochasticity immediately after the blowup."
            },
            {
                "arxivId": "1209.4517",
                "title": "Ergodicity breaking in geometric Brownian motion.",
                "abstract": "Geometric Brownian motion (GBM) is a model for systems as varied as financial instruments and populations. The statistical properties of GBM are complicated by nonergodicity, which can lead to ensemble averages exhibiting exponential growth while any individual trajectory collapses according to its time average. A common tactic for bringing time averages closer to ensemble averages is diversification. In this Letter, we study the effects of diversification using the concept of ergodicity breaking."
            },
            {
                "arxivId": "math-ph/0303009",
                "title": "A Unified Scheme for Generalized Sectors Based on Selection Criteria: Order Parameters of Symmetries and of Thermality and Physical Meanings of Adjunctions",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01337",
        "category": "q-fin",
        "title": "Detection of temporality at discourse level on financial news by combining Natural Language Processing and Machine Learning",
        "abstract": null,
        "references": [
            {
                "arxivId": "2404.07224",
                "title": "Detection of Financial Opportunities in Micro-Blogging Data With a Stacked Classification System",
                "abstract": "Micro-blogging sources such as the Twitter social network provide valuable real-time data for market prediction models. Investors\u2019 opinions in this network follow the fluctuations of the stock markets and often include educated speculations on market opportunities that may have impact on the actions of other investors. In view of this, we propose a novel system to detect positive predictions in tweets, a type of financial emotions which we term \u201copportunities\u201d that are akin to \u201canticipation\u201d in Plutchik\u2019s theory. Specifically, we seek a high detection precision to present a financial operator a substantial amount of such tweets while differentiating them from the rest of financial emotions in our system. We achieve it with a three-layer stacked Machine Learning classification system with sophisticated features that result from applying Natural Language Processing techniques to extract valuable linguistic information. Experimental results on a dataset that has been manually annotated with financial emotion and ticker occurrence tags demonstrate that our system yields satisfactory and competitive performance in financial opportunity detection, with precision values up to 83%. This promising outcome endorses the usability of our system to support investors\u2019 decision making."
            },
            {
                "arxivId": "1103.0398",
                "title": "Natural Language Processing (Almost) from Scratch",
                "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01338",
        "category": "q-fin",
        "title": "Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation",
        "abstract": null,
        "references": [
            {
                "arxivId": "2404.08664",
                "title": "Identifying Banking Transaction Descriptions via Support Vector Machine Short-Text Classification Based on a Specialized Labelled Corpus",
                "abstract": "Short texts are omnipresent in real-time news, social network commentaries, etc. Traditional text representation methods have been successfully applied to self-contained documents of medium size. However, information in short texts is often insufficient, due, for example, to the use of mnemonics, which makes them hard to classify. Therefore, the particularities of specific domains must be exploited. In this article we describe a novel system that combines Natural Language Processing techniques with Machine Learning algorithms to classify banking transaction descriptions for personal finance management, a problem that was not previously considered in the literature. We trained and tested that system on a labelled dataset with real customer transactions that will be available to other researchers on request. Motivated by existing solutions in spam detection, we also propose a short text similarity detector to reduce training set size based on the Jaccard distance. Experimental results with a two-stage classifier combining this detector with a SVM indicate a high accuracy in comparison with alternative approaches, taking into account complexity and computing time. Finally, we present a use case with a personal finance application, CoinScrap, which is available at Google Play and App Store."
            },
            {
                "arxivId": "2404.01337",
                "title": "Detection of temporality at discourse level on financial news by combining Natural Language Processing and Machine Learning",
                "abstract": null
            },
            {
                "arxivId": "1907.02258",
                "title": "The evolution of argumentation mining: From models to social media and emerging tools",
                "abstract": null
            },
            {
                "arxivId": "1711.04305",
                "title": "Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey",
                "abstract": null
            },
            {
                "arxivId": "1606.01323",
                "title": "Improving Coreference Resolution by Learning Entity-Level Distributed Representations",
                "abstract": "A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01451",
        "category": "q-fin",
        "title": "Non-stationary Financial Risk Factors and Macroeconomic Vulnerability for the UK",
        "abstract": "Tracking the build-up of financial vulnerabilities is a key component of financial stability policy. Due to the complexity of the financial system, this task is daunting, and there have been several proposals on how to manage this goal. One way to do this is by the creation of indices that act as a signal for the policy maker. While factor modelling in finance and economics has a rich history, most of the applications tend to focus on stationary factors. Nevertheless, financial stress (and in particular tail events) can exhibit a high degree of inertia. This paper advocates moving away from the stationary paradigm and instead proposes non-stationary factor models as measures of financial stress. Key advantage of a non-stationary factor model is that while some popular measures of financial stress describe the variance-covariance structure of the financial stress indicators, the new index can capture the tails of the distribution. To showcase this, we use the obtained factors as variables in a growth-at-risk exercise. This paper offers an overview of how to construct non-stationary dynamic factors of financial stress using the UK financial market as an example.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01522",
        "category": "q-fin",
        "title": "Watanabe's expansion: A Solution for the convexity conundrum",
        "abstract": "In this paper, we present a new method for pricing CMS derivatives. We use Mallaivin's calculus to establish a model-free connection between the price of a CMS derivative and a quadratic payoff. Then, we apply Watanabe's expansions to quadratic payoffs case under local and stochastic local volatility. Our approximations are generic. To evaluate their accuracy, we will compare the approximations numerically under the normal SABR model against the market standards: Hagan's approximation, and a Monte Carlo simulation.",
        "references": [
            {
                "arxivId": "1908.09640",
                "title": "Expansion Method for Pricing Foreign Exchange Options Under Stochastic Volatility and Interest Rates",
                "abstract": "Some expansion methods have been proposed for approximately pricing options which has no exact closed formula. Benhamou et al. (2010) presents the smart expansion method that directly expands the expectation value of payoff function with respect to the volatility of volatility, then uses it to price options in the stochastic volatility model. In this paper, we apply their method to the stochastic volatility model with stochastic interest rates, and present the expansion formula for pricing options up to the second order. Then the numerical studies are performed to compare our approximation formula with the Monte-Carlo simulation. It is found that our formula shows the numerically comparable results with the method proposed by Grzelak et al. (2012) which uses the approximation of characteristic function."
            },
            {
                "arxivId": "1905.06315",
                "title": "Higher order approximation of call option prices under stochastic volatility models",
                "abstract": "In the present paper, a decomposition formula for the call price due to Al\\`{o}s is transformed into a Taylor type formula containing an infinite series with stochastic terms. The new decomposition may be considered as an alternative to the decomposition of the call price found in a recent paper of Al\\`{o}s, Gatheral and Radoi\\v{c}i\\'{c}. We use the new decomposition to obtain various approximations to the call price in the Heston model with sharper estimates of the error term than in the previously known approximations. One of the formulas obtained in the present paper has five significant terms and an error estimate of the form $O(\\nu^{3}(\\left|\\rho\\right|+\\nu))$, where $\\nu$ is the vol-vol parameter, and $\\rho$ is the correlation coefficient between the price and the volatility in the Heston model. Another approximation formula contains seven more terms and the error estimate is of the form $O(\\nu^4(1+|\\rho|)$. For the uncorrelated Hestom model ($\\rho=0$), we obtain a formula with four significant terms and an error estimate $O(\\nu^6)$. Numerical experiments show that the new approximations to the call price perform especially well in the high volatility mode."
            },
            {
                "arxivId": "1207.0233",
                "title": "From characteristic functions to implied volatility expansions",
                "abstract": "For any strictly positive martingale S = e X for which X has a characteristic function, we provide an expansion for the implied volatility. This expansion is explicit in the sense that it involves no integrals, but only polynomials in the log-strike. We illustrate the versatility of our expansion by computing the approximate implied volatility smile in three well-known martingale models: one finite activity exponential L\u00e9vy model, Merton (1976), one infinite activity exponential L\u00e9vy model (variance gamma), and one stochastic volatility model, Heston (1993). Finally, we illustrate how our expansion can be used to perform a model-free calibration of the empirically observed implied volatility surface."
            },
            {
                "arxivId": "cond-mat/0504317",
                "title": "A General Asymptotic Implied Volatility for Stochastic Volatility Models",
                "abstract": "In this paper, we derive a general asymptotic implied volatility at the first-order for any stochastic volatility model using the heat kernel expansion on a Riemann manifold endowed with an Abelian connection. This formula is particularly useful for the calibration procedure. As an application, we obtain an asymptotic smile for a SABR model with a mean-reversion term, called lambda-SABR, corresponding in our geometric framework to the Poincare hyperbolic plane. When the lambda-SABR model degenerates into the SABR-model, we show that our asymptotic implied volatility is a better approximation than the classical Hagan-al expression. Furthermore, in order to show the strength of this geometric framework, we give an exact solution of the SABR model with beta=0 or 1. In a next paper, we will show how our method can be applied in other contexts such as the derivation of an asymptotic implied volatility for a Libor market model with a stochastic volatility."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01624",
        "category": "q-fin",
        "title": "Intelligent Optimization of Mine Environmental Damage Assessment and Repair Strategies Based on Deep Learning",
        "abstract": "In recent decades, financial quantification has emerged and matured rapidly. For financial institutions such as funds, investment institutions are increasingly dissatisfied with the situation of passively constructing investment portfolios with average market returns, and are paying more and more attention to active quantitative strategy investment portfolios. This requires the introduction of active stock investment fund management models. Currently, in my country's stock fund investment market, there are many active quantitative investment strategies, and the algorithms used vary widely, such as SVM, random forest, RNN recurrent memory network, etc. This article focuses on this trend, using the emerging LSTM-GRU gate-controlled long short-term memory network model in the field of financial stock investment as a basis to build a set of active investment stock strategies, and combining it with SVM, which has been widely used in the field of quantitative stock investment. Comparing models such as RNN, theoretically speaking, compared to SVM that simply relies on kernel functions for high-order mapping and classification of data, neural network algorithms such as RNN and LSTM-GRU have better principles and are more suitable for processing financial stock data. Then, through multiple By comparison, it was finally found that the LSTM- GRU gate-controlled long short-term memory network has a better accuracy. By selecting the LSTM-GRU algorithm to construct a trading strategy based on the Shanghai and Shenzhen 300 Index constituent stocks, the parameters were adjusted and the neural layer connection was adjusted. Finally, It has significantly outperformed the benchmark index CSI 300 over the long term. The conclusion of this article is that the research results can provide certain quantitative strategy references for financial institutions to construct active stock investment portfolios.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01782",
        "category": "q-fin",
        "title": "Multicriteria Analysis Model in Sustainable Corn Farming Area Planning",
        "abstract": "This study aims to develop a framework for multicriteria analysis to evaluate alternatives for sustainable corn agricultural area planning, considering the integration of ecological, economic, and social aspects as pillars of sustainability. The research method uses qualitative and quantitative approaches to integrate ecological, economic, and social aspects in the multicriteria analysis. The analysis involves land evaluation, subcriteria identification, and data integration using Multidimensional Scaling and Analytical Hierarchy Process methods to prioritize developing sustainable corn agricultural areas. Based on the results of the RAP-Corn analysis, it indicates that the ecological dimension depicts less sustainability. The AHP results yield weight distribution and highly relevant scores that describe tangible preferences. Priority directions are grouped as strategic steps toward achieving the goals of sustainable corn agricultural area planning.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-03.json",
        "arxivId": "2404.01866",
        "category": "q-fin",
        "title": "Supervised Autoencoder Mlp for Financial Time Series Forecasting",
        "abstract": "This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance. It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios. The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022. Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness. However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning. This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling. The results of this study have substantial policy implications, suggesting that financial institutions and regulators could leverage techniques presented to enhance market stability and investor protection, while also encouraging more informed and strategic investment approaches in various financial sectors.",
        "references": [
            {
                "arxivId": "2002.05786",
                "title": "Deep Learning for Financial Applications : A Survey",
                "abstract": null
            },
            {
                "arxivId": "1911.13288",
                "title": "Financial Time Series Forecasting with Deep Learning : A Systematic Literature Review: 2005-2019",
                "abstract": null
            },
            {
                "arxivId": "1904.05315",
                "title": "Bitcoin Price Prediction: An ARIMA Approach",
                "abstract": "Bitcoin is considered the most valuable currency in the world. Besides being highly valuable, its value has also experienced a steep increase, from around 1 dollar in 2010 to around 18000 in 2017. Then, in recent years, it has attracted considerable attention in a diverse set of fields, including economics and computer science. The former mainly focuses on studying how it affects the market, determining reasons behinds its price fluctuations, and predicting its future prices. The latter mainly focuses on its vulnerabilities, scalability, and other techno-crypto-economic issues. Here, we aim at revealing the usefulness of traditional autoregressive integrative moving average (ARIMA) model in predicting the future value of bitcoin by analyzing the price time series in a 3-years-long time period. On the one hand, our empirical studies reveal that this simple scheme is efficient in sub-periods in which the behavior of the time-series is almost unchanged, especially when it is used for short-term prediction, e.g. 1-day. On the other hand, when we try to train the ARIMA model to a 3-years-long period, during which the bitcoin price has experienced different behaviors, or when we try to use it for a long-term prediction, we observe that it introduces large prediction errors. Especially, the ARIMA model is unable to capture the sharp fluctuations in the price, e.g. the volatility at the end of 2017. Then, it calls for more features to be extracted and used along with the price for a more accurate prediction of the price. We have further investigated the bitcoin price prediction using an ARIMA model, trained over a large dataset, and a limited test window of the bitcoin price, with length $w$, as inputs. Our study sheds lights on the interaction of the prediction accuracy, choice of ($p,q,d$), and window size $w$."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-04.json",
        "arxivId": "2007.02553",
        "category": "q-fin",
        "title": "On robust fundamental theorems of asset pricing in discrete time",
        "abstract": "This paper is devoted to a study of robust fundamental theorems of asset pricing in discrete time and finite horizon settings. Uncertainty is modelled by a (possibly uncountable) family of price processes on the same probability space. Our technical assumption is the continuity of the price processes with respect to uncertain parameters. In this setting, we introduce a new topological framework which allows us to use the classical arguments in arbitrage pricing theory involving $L^p$ spaces, the Hahn-Banach separation theorem and other tools from functional analysis. The first result is the equivalence of a ``no robust arbitrage\"condition and the existence of a new ``robust pricing system\". The second result shows superhedging dualities and the existence of superhedging strategies without restrictive conditions on payoff functions, unlike other related studies. The third result discusses completeness in the present robust setting. When other options are available for static trading, we could reduce the set of robust pricing systems and hence the superhedging prices.",
        "references": [
            {
                "arxivId": "1305.6008",
                "title": "Arbitrage and duality in nondominated discrete-time models",
                "abstract": "We consider a nondominated model of a discrete-time financial market where stocks are traded dynamically and options are available for static hedging. In a general measure-theoretic setting, we show that absence of arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of martingale measures. In the arbitrage-free case, we show that optimal superhedging strategies exist for general contingent claims, and that the minimal superhedging price is given by the supremum over the martingale measures. Moreover, we obtain a nondominated version of the Optional Decomposition Theorem."
            },
            {
                "arxivId": "1106.5929",
                "title": "Model-independent bounds for option prices\u2014a mass transport approach",
                "abstract": null
            },
            {
                "arxivId": "0709.2730",
                "title": "Convex compactness and its applications",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-04.json",
        "arxivId": "2404.02343",
        "category": "q-fin",
        "title": "Improved model-free bounds for multi-asset options using option-implied information and deep learning",
        "abstract": "We consider the computation of model-free bounds for multi-asset options in a setting that combines dependence uncertainty with additional information on the dependence structure. More specifically, we consider the setting where the marginal distributions are known and partial information, in the form of known prices for multi-asset options, is also available in the market. We provide a fundamental theorem of asset pricing in this setting, as well as a superhedging duality that allows to transform the maximization problem over probability measures in a more tractable minimization problem over trading strategies. The latter is solved using a penalization approach combined with a deep learning approximation using artificial neural networks. The numerical method is fast and the computational time scales linearly with respect to the number of traded assets. We finally examine the significance of various pieces of additional information. Empirical evidence suggests that\"relevant\"information, i.e. prices of derivatives with the same payoff structure as the target payoff, are more useful that other information, and should be prioritized in view of the trade-off between accuracy and computational efficiency.",
        "references": [
            {
                "arxivId": "2006.14288",
                "title": "Model-Free Bounds for Multi-Asset Options Using Option-Implied Information and Their Exact Computation",
                "abstract": "We consider derivatives written on multiple underlyings in a one-period financial market, and we are interested in the computation of model-free upper and lower bounds for their arbitrage-free prices. We work in a completely realistic setting, in that we only assume the knowledge of traded prices for other single- and multi-asset derivatives and even allow for the presence of bid\u2013ask spread in these prices. We provide a fundamental theorem of asset pricing for this market model, as well as a superhedging duality result, that allows to transform the abstract maximization problem over probability measures into a more tractable minimization problem over vectors, subject to certain constraints. Then, we recast this problem into a linear semi-infinite optimization problem and provide two algorithms for its solution. These algorithms provide upper and lower bounds for the prices that are \u03b5-optimal, as well as a characterization of the optimal pricing measures. These algorithms are efficient and allow the computation of bounds in high-dimensional scenarios (e.g., when d = 60). Moreover, these algorithms can be used to detect arbitrage opportunities and identify the corresponding arbitrage strategies. Numerical experiments using both synthetic and real market data showcase the efficiency of these algorithms, and they also allow understanding of the reduction of model risk by including additional information in the form of known derivative prices. This paper was accepted by Chung Piaw Teo, optimization."
            },
            {
                "arxivId": "1911.05523",
                "title": "BOUNDS ON MULTI-ASSET DERIVATIVES VIA NEURAL NETWORKS",
                "abstract": "Using neural networks, we compute bounds on the prices of multi-asset derivatives given information on prices of related payoffs. As a main example, we focus on European basket options and include information on the prices of other similar options, such as spread options and/or basket options on subindices. We show that, in most cases, adding further constraints gives rise to bounds that are considerably tighter. Our approach follows the literature on constrained optimal transport and, in particular, builds on the work of Eckstein & Kupper (2018) [Computation of optimal transport and related hedging problems via penalization and neural networks, Appl. Math. Optimiz. 1\u201329]."
            },
            {
                "arxivId": "1909.03870",
                "title": "Robust Pricing and Hedging of Options on Multiple Assets and Its Numerics",
                "abstract": "We consider robust pricing and hedging for options written on multiple assets given market option prices for the individual assets. The resulting problem is called the multi-marginal martingale optimal transport problem. We propose two numerical methods to solve such problems: using discretisation and linear programming applied to the primal side and using penalisation and deep neural networks optimisation applied to the dual side. We prove convergence for our methods and compare their numerical performance. We show how adding further information about call option prices at additional maturities can be incorporated and narrows down the no-arbitrage pricing bounds. Finally, we obtain structural results for the case of the payoff given by a weighted sum of covariances between the assets."
            },
            {
                "arxivId": "1802.08539",
                "title": "Computation of Optimal Transport and Related Hedging Problems via Penalization and Neural Networks",
                "abstract": null
            },
            {
                "arxivId": "1709.00641",
                "title": "Marginal and Dependence Uncertainty: Bounds, Optimal Transport, and Sharpness",
                "abstract": "Motivated by applications in model-free finance and quantitative risk management, we consider Fr\\'echet classes of multivariate distribution functions where additional information on the joint distribution is assumed, while uncertainty in the marginals is also possible. We derive optimal transport duality results for these Fr\\'echet classes that extend previous results in the related literature. These proofs are based on representation results for increasing convex functionals and the explicit computation of the conjugates. We show that the dual transport problem admits an explicit solution for the function $f=1_B$, where $B$ is a rectangular subset of $\\mathbb R^d$, and provide an intuitive geometric interpretation of this result. The improved Fr\\'echet--Hoeffding bounds provide ad-hoc upper bounds for these Fr\\'echet classes. We show that the improved Fr\\'echet--Hoeffding bounds are pointwise sharp for these classes in the presence of uncertainty in the marginals, while a counterexample yields that they are not pointwise sharp in the absence of uncertainty in the marginals, even in dimension 2. The latter result sheds new light on the improved Fr\\'echet--Hoeffding bounds, since Tankov [30] has showed that, under certain conditions, these bounds are sharp in dimension 2."
            },
            {
                "arxivId": "1602.08894",
                "title": "Improved Fr\\'echet$-$Hoeffding bounds on $d$-copulas and applications in model-free finance",
                "abstract": "We derive upper and lower bounds on the expectation of $f(\\mathbf{S})$ under dependence uncertainty, i.e. when the marginal distributions of the random vector $\\mathbf{S}=(S_1,\\dots,S_d)$ are known but their dependence structure is partially unknown. We solve the problem by providing improved \\FH bounds on the copula of $\\mathbf{S}$ that account for additional information. In particular, we derive bounds when the values of the copula are given on a compact subset of $[0,1]^d$, the value of a functional of the copula is prescribed or different types of information are available on the lower dimensional marginals of the copula. We then show that, in contrast to the two-dimensional case, the bounds are quasi-copulas but fail to be copulas if $d>2$. Thus, in order to translate the improved \\FH bounds into bounds on the expectation of $f(\\mathbf{S})$, we develop an alternative representation of multivariate integrals with respect to copulas that admits also quasi-copulas as integrators. By means of this representation, we provide an integral characterization of orthant orders on the set of quasi-copulas which relates the improved \\FH bounds to bounds on the expectation of $f(\\mathbf{S})$. Finally, we apply these results to compute model-free bounds on the prices of multi-asset options that take partial information on the dependence structure into account, such as correlations or market prices of other traded derivatives. The numerical results show that the additional information leads to a significant improvement of the option price bounds compared to the situation where only the marginal distributions are known."
            },
            {
                "arxivId": "1509.08988",
                "title": "Duality for increasing convex functionals with countably many marginal constraints",
                "abstract": "The main result of this paper is a convex dual representation for increasing convex functionals that are defined on a space of real-valued Borel measurable functions living on a countable product of metric spaces. Our principal assumption is that the functionals fulfill convex marginal constraints satisfying a tightness condition. In the special case where the marginal constraints are given by expectations or maxima of expectations, we obtain linear and sublinear versions of Kantorovich's transport duality and the recently discovered martingale transport duality on products of countably many metric spaces."
            },
            {
                "arxivId": "1004.4153",
                "title": "Improved Fr\u00e9chet Bounds and Model-Free Pricing of Multi-Asset Options",
                "abstract": "Improved bounds on the copula of a bivariate random vector are computed when partial information is available, such as the values of the copula on a given subset of [0, 1]2, or the value of a functional of the copula, monotone with respect to the concordance order. These results are then used to compute model-free bounds on the prices of two-asset options which make use of extra information about the dependence structure, such as the price of another two-asset option."
            },
            {
                "arxivId": "math/0302243",
                "title": "Static arbitrage bounds on basket option prices",
                "abstract": "We consider the problem of computing upper and lower bounds on the price of an European basket call option, given prices on other similar options. Although this problem is hard to solve exactly in the general case, we show that in some instances the upper and lower bounds can be computed via simple closed-form expressions, or linear programs. We also introduce an efficient linear programming relaxation of the general problem based on an integral transform interpretation of the call price function. We show that this relaxation is tight in some of the special cases examined before."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-04.json",
        "arxivId": "2404.02498",
        "category": "q-fin",
        "title": "From Time-inconsistency to Time-consistency for Optimal Stopping Problems",
        "abstract": "For optimal stopping problems with time-inconsistent preference, we measure the inherent level of time-inconsistency by taking the time needed to turn the naive strategies into the sophisticated ones. In particular, when in a repeated experiment the naive agent can observe her actual sequence of actions which are inconsistent with what she has planned at the initial time, she then chooses her immediate action based on the observations on her later actual behavior. The procedure is repeated until her actual sequence of actions are consistent with her plan at any time. We show that for the preference value of cumulative prospect theory, in which the time-inconsistency is due to the probability distortion, the higher the degree of probability distortion, the more severe the level of time-inconsistency, and the more time required to turn the naive strategies into the sophisticated ones.",
        "references": [
            {
                "arxivId": "1709.03535",
                "title": "General stopping behaviors of na\u00efve and noncommitted sophisticated agents, with application to probability distortion",
                "abstract": "We consider the problem of stopping a diffusion process with a payoff functional that renders the problem time\u2010inconsistent. We study stopping decisions of na\u00efve agents who reoptimize continuously in time, as well as equilibrium strategies of sophisticated agents who anticipate but lack control over their future selves' behaviors. When the state process is one dimensional and the payoff functional satisfies some regularity conditions, we prove that any equilibrium can be obtained as a fixed point of an operator. This operator represents strategic reasoning that takes the future selves' behaviors into account. We then apply the general results to the case when the agents distort probability and the diffusion process is a geometric Brownian motion. The problem is inherently time\u2010inconsistent as the level of distortion of a same event changes over time. We show how the strategic reasoning may turn a na\u00efve agent into a sophisticated one. Moreover, we derive stopping strategies of the two types of agent for various parameter specifications of the problem, illustrating rich behaviors beyond the extreme ones such as \u201cnever\u2010stopping\u201d or \u201cnever\u2010starting.\u201d"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-04.json",
        "arxivId": "2404.02595",
        "category": "q-fin",
        "title": "QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection",
        "abstract": "This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.",
        "references": [
            {
                "arxivId": "2310.10315",
                "title": "A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead",
                "abstract": "Quantum Computing (QC) claims to improve the efficiency of solving complex problems, compared to classical computing. When QC is applied to Machine Learning (ML) applications, it forms a Quantum Machine Learning (QML) system. After discussing the basic concepts of QC and its advantages over classical computing, this paper reviews the key aspects of QML in a comprehensive manner. We discuss different QML algorithms and their domain applicability, quantum datasets, hardware technologies, software tools, simulators, and applications. In this survey, we provide valuable information and resources for readers to jumpstart into the current state-of-the-art techniques in the QML field."
            },
            {
                "arxivId": "2309.01127",
                "title": "Financial Fraud Detection using Quantum Graph Neural Networks",
                "abstract": null
            },
            {
                "arxivId": "2208.13251",
                "title": "A Preprocessing Perspective for Quantum Machine Learning Classification Advantage in Finance Using NISQ Algorithms",
                "abstract": "Quantum Machine Learning (QML) has not yet demonstrated extensively and clearly its advantages compared to the classical machine learning approach. So far, there are only specific cases where some quantum-inspired techniques have achieved small incremental advantages, and a few experimental cases in hybrid quantum computing are promising, considering a mid-term future (not taking into account the achievements purely associated with optimization using quantum-classical algorithms). The current quantum computers are noisy and have few qubits to test, making it difficult to demonstrate the current and potential quantum advantage of QML methods. This study shows that we can achieve better classical encoding and performance of quantum classifiers by using Linear Discriminant Analysis (LDA) during the data preprocessing step. As a result, the Variational Quantum Algorithm (VQA) shows a gain of performance in balanced accuracy with the LDA technique and outperforms baseline classical classifiers."
            },
            {
                "arxivId": "2208.01203",
                "title": "Unsupervised quantum machine learning for fraud detection",
                "abstract": "We develop quantum protocols for anomaly detection and apply them to the task of credit card fraud detection (FD). First, we establish classical benchmarks based on supervised and unsupervised machine learning methods, where average precision is chosen as a robust metric for detecting anomalous data. We focus on kernel-based approaches for ease of direct comparison, basing our unsupervised modelling on one-class support vector machines (OC-SVM). Next, we employ quantum kernels of di\ufb00erent type for performing anomaly detection, and observe that quantum FD can challenge equivalent classical protocols at increasing number of features (equal to the number of qubits for data embedding). Performing simulations with registers up to 20 qubits, we \ufb01nd that quantum kernels with re-uploading demonstrate better average precision, with the advantage increasing with system size. Speci\ufb01cally, at 20 qubits we reach the quantum-classical separation of average precision being equal to 15%. We discuss the prospects of fraud detection with near- and mid-term quantum hardware, and describe possible future improvements."
            },
            {
                "arxivId": "2011.01125",
                "title": "Evaluating the noise resilience of variational quantum algorithms",
                "abstract": "We simulate the effects of different types of noise in state preparation circuits of variational quantum algorithms. We first use a variational quantum eigensolver to find the ground state of a Hamiltonian in presence of noise, and adopt two quality measures in addition to the energy, namely fidelity and concurrence. We then extend the task to the one of constructing, with a layered quantum circuit ansatz, a set of general random target states. We determine the optimal circuit depth for different types and levels of noise, and observe that the variational algorithms mitigate the effects of noise by adapting the optimised parameters. We find that the inclusion of redundant parameterised gates makes the quantum circuits more resilient to noise. For such overparameterised circuits different sets of parameters can result in the same final state in the noiseless case, which we denote as parameter degeneracy. Numerically, we show that this degeneracy can be lifted in the presence of noise, with some states being significantly more resilient to noise than others. We also show that the average deviation from the target state is linear in the noise level, as long as this is small compared to a circuit-dependent threshold. In this region the deviation is well described by a stochastic model. Above the threshold, the optimisation can converge to states with largely different physical properties from the true target state, so that for practical applications it is critical to ensure that noise levels are below this threshold."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-04.json",
        "arxivId": "2404.02687",
        "category": "q-fin",
        "title": "Karma: An Experimental Study",
        "abstract": "A system of non-tradable credits that flow between individuals like karma, hence proposed under that name, is a mechanism for repeated resource allocation that comes with attractive efficiency and fairness properties, in theory. In this study, we test karma in an online experiment in which human subjects repeatedly compete for a resource with time-varying and stochastic individual preferences or urgency to acquire the resource. We confirm that karma has significant and sustained welfare benefits even in a population with no prior training. We identify mechanism usage in contexts with sporadic high urgency, more so than with frequent moderate urgency, and implemented as an easy (binary) karma bidding scheme as particularly effective for welfare improvements: relatively larger aggregate efficiency gains are realized that are (almost) Pareto superior. These findings provide guidance for further testing and for future implementation plans of such mechanisms in the real world.",
        "references": [
            {
                "arxivId": "2305.17222",
                "title": "Karma: Resource Allocation for Dynamic Demands",
                "abstract": "We consider the problem of fair resource allocation in a system where user demands are dynamic, that is, where user demands vary over time. Our key observation is that the classical max-min fairness algorithm for resource allocation provides many desirable properties (e.g., Pareto efficiency, strategy-proofness, and fairness), but only under the strong assumption of user demands being static over time. For the realistic case of dynamic user demands, the max-min fairness algorithm loses one or more of these properties. We present Karma, a new resource allocation mechanism for dynamic user demands. The key technical contribution in Karma is a credit-based resource allocation algorithm: in each quantum, users donate their unused resources and are assigned credits when other users borrow these resources; Karma carefully orchestrates the exchange of credits across users (based on their instantaneous demands, donated resources and borrowed resources), and performs prioritized resource allocation based on users' credits. We theoretically establish Karma guarantees related to Pareto efficiency, strategy-proofness, and fairness for dynamic user demands. Empirical evaluations over production workloads show that these properties translate well into practice: Karma is able to reduce disparity in performance across users to a bare minimum while maintaining Pareto-optimal system-wide performance."
            },
            {
                "arxivId": "2302.09127",
                "title": "Robust Pseudo-Markets for Reusable Public Resources",
                "abstract": "We study non-monetary mechanisms for the fair and efficient allocation of reusable public resources. We consider settings where a limited resource is repeatedly shared among a set of agents, each of whom may request to use the resource over multiple consecutive rounds, receiving some utility only if they get to use the resource for the full duration of their request. Such settings are of particular significance in scientific research where large-scale instruments such as electron microscopes, particle colliders, or telescopes are shared between multiple research groups; this model also subsumes and extends existing models of repeated non-monetary allocation where the resource is demanded only for a single round."
            },
            {
                "arxivId": "2207.00495",
                "title": "A self-contained karma economy for the dynamic allocation of common resources",
                "abstract": null
            },
            {
                "arxivId": "2011.11595",
                "title": "Urgency-aware Optimal Routing in Repeated Games through Artificial Currencies",
                "abstract": null
            },
            {
                "arxivId": "1907.09198",
                "title": "Today Me, Tomorrow Thee: Efficient Resource Allocation in Competitive Settings using Karma Games",
                "abstract": "We present a new type of coordination mechanism among multiple agents for the allocation of a finite resource, such as the allocation of time slots for passing an intersection. We consider the setting where we associate one counter to each agent, which we call karma value, and where there is an established mechanism to decide resource allocation based on agents exchanging karma. The idea is that agents might be inclined to pass on using resources today, in exchange for karma, which will make it easier for them to claim the resource use in the future. To understand whether such a system might work robustly, we only design the protocol and not the agents\u2019 policies. We take a game-theoretic perspective and compute policies corresponding to Nash equilibria for the game. We find, surprisingly, that the Nash equilibria for a society of self-interested agents are very close in social welfare to a centralized cooperative solution. These results suggest that many resource allocation problems can have a simple, elegant, and robust solution, assuming the availability of a karma accounting mechanism."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-04.json",
        "arxivId": "2404.02858",
        "category": "q-fin",
        "title": "The Life Care Annuity: enhancing product features and refining pricing methods",
        "abstract": "In this paper we provide more general features for the variable annuity contract with LTC payouts and GLWB proposed by the state-of-the-art and we refine its pricing methods. In particular, as to product features, we allow dynamic withdrawal strategies, including the surrender option. Furthermore, we consider stochastic interest rate, described by a Cox-Ingersoll-Ross (CIR) process. As to the numerical methods, we solve the stochastic control problem involved by the selection of the optimal withdrawal strategy by means of a robust tree method. We use such a method to estimate the fair price of the product. Furthermore, our numerical results show how the optimal withdrawal strategy varies over time with the health status of the policyholder. Our proposed tree method, we name Tree-LTC, proves to be efficient and reliable, when tested against the Monte Carlo approach.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2103.02754",
        "category": "q-fin",
        "title": "Beyond Unbounded Beliefs: How Preferences and Information Interplay in Social Learning",
        "abstract": "When does society eventually learn the truth, or take the correct action, via observational learning? In a general model of sequential learning over social networks, we identify a simple condition for learning dubbed excludability. Excludability is a joint property of agents' preferences and their information. We develop two classes of preferences and information that jointly satisfy excludability: (i) for a one-dimensional state, preferences with single-crossing differences and a new informational condition, directionally unbounded beliefs; and (ii) for a multi-dimensional state, intermediate preferences and subexponential location-shift information. These applications exemplify that with multiple states\"unbounded beliefs\"is not only unnecessary for learning, but incompatible with familiar informational structures like normal information. Unbounded beliefs demands that a single agent can identify the correct action. Excludability, on the other hand, only requires that a single agent must be able to displace any wrong action, even if she cannot take the correct action.",
        "references": [
            {
                "arxivId": "2212.12009",
                "title": "Single-Crossing Differences in Convex Environments",
                "abstract": "An agent's preferences depend on an ordered parameter or type. We characterize the set of utility functions with single-crossing differences (SCD) in convex environments. These include preferences over lotteries, both in expected utility and rank-dependent utility frameworks, and preferences over bundles of goods and over consumption streams. Our notion of SCD does not presume an order on the choice space. This unordered SCD is necessary and sufficient for ''interval choice'' comparative statics. We present applications to cheap talk, observational learning, and collective choice, showing how convex environments arise in these problems and how SCD/interval choice are useful. Methodologically, our main characterization stems from a result on linear aggregations of single-crossing functions."
            },
            {
                "arxivId": "1707.02689",
                "title": "The speed of sequential asymptotic learning",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2209.09878",
        "category": "q-fin",
        "title": "A unifying view on the irreversible investment exercise boundary in a stochastic, time-inhomogeneous capacity expansion problem",
        "abstract": "Aiming at studying the investment exercise boundary, this paper devises a way to apply the Bank and El Karoui Representation Theorem to a quite general stochastic, continuous time capacity expansion problem with irreversible investment on the finite time interval and including a state dependent scrap value associated with the production facility at the terminal time T. The capacity process is a time-inhomogeneous diffusion in which a monotone non-decreasing, possibly singular, control process representing the cumulative investment enters additively. The functional to be maximized admits a supergradient, hence the optimal control satisfies some first order conditions which are solved by means of the Bank and El Karoui Representation Theorem. Its application in the case of non-zero scrap value at time T is not obvious and, as far as we know, it is new in the literature on singular stochastic control. In fact, due to the scrap value, in the supergradient appears also a non integral term. This challenge is overcome by suitably extending the horizon. The optimal investment process is shown to become active at the so-called base capacity level, given in terms of the optional solution of the Representation Theorem. Contrary to what happens in the no scrap value case, here the base capacity depends on the initial capacity y. Hence, a priori, it is not clear if and how it is related to the investment exercise boundary associated to the capacity expansion problem. Under the assumption of deterministic coefficients, discount factor, conversion factor, wage rate and interest rate, the investment boundary is shown to coincide with the base capacity. Therefore, unifying views, the base capacity is deterministic and independent of y, and its integral equation may be used to characterize the investment boundary, without any a priori regularity of it.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2305.07362",
        "category": "q-fin",
        "title": "The use of trade data in the analysis of global phosphate flows",
        "abstract": "In this paper we present a new method to trace the flows of phosphate from the countries where it is mined to the countries where it is used in agricultural production. We achieve this by combining data on phosphate rock mining with data on fertilizer use and data on international trade of phosphate-related products. We show that by making adjustments to data on net exports and by estimating weighting factors we can derive the matrix of phosphate flows on the country level to a large degree and thus contribute to the accuracy of material flow analyses, a results that is important for improving environmental accounting, not only for phosphorus but for many other resources.",
        "references": [
            {
                "arxivId": "1504.03508",
                "title": "Systemic trade risk of critical resources",
                "abstract": "Price volatility of critical resources can largely be understood through the topology of their trade networks. In the wake of the 2008 financial crisis, the role of strongly interconnected markets in causing systemic instability has been increasingly acknowledged. Trade networks of commodities are susceptible to cascades of supply shocks that increase systemic trade risks and pose a threat to geopolitical stability. We show that supply risk, scarcity, and price volatility of nonfuel mineral resources are intricately connected with the structure of the worldwide trade networks spanned by these resources. At the global level, we demonstrate that the scarcity of a resource is closely related to the susceptibility of the trade network with respect to cascading shocks. At the regional level, we find that, to some extent, region-specific price volatility and supply risk can be understood by centrality measures that capture systemic trade risk. The resources associated with the highest systemic trade risk indicators are often those that are produced as by-products of major metals. We identify significant strategic shortcomings in the management of systemic trade risk, in particular in the European Union."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2306.12965",
        "category": "q-fin",
        "title": "Improved Financial Forecasting via Quantum Machine Learning",
        "abstract": "Quantum algorithms have the potential to enhance machine learning across a variety of domains and applications. In this work, we show how quantum machine learning can be used to improve financial forecasting. First, we use classical and quantum Determinantal Point Processes to enhance Random Forest models for churn prediction, improving precision by almost 6%. Second, we design quantum neural network architectures with orthogonal and compound layers for credit risk assessment, which match classical performance with significantly fewer parameters. Our results demonstrate that leveraging quantum ideas can effectively enhance the performance of machine learning, both today as quantum-inspired classical ML solutions, and even more in the future, with the advent of better quantum hardware.",
        "references": [
            {
                "arxivId": "2303.17893",
                "title": "Improved clinical data imputation via classical and quantum determinantal point processes",
                "abstract": "Imputing data is a critical issue for machine learning practitioners, including in the life sciences domain, where missing clinical data is a typical situation and the reliability of the imputation is of great importance. Currently, there is no canonical approach for imputation of clinical data and widely used algorithms introduce variance in the downstream classification. Here we propose novel imputation methods based on determinantal point processes that enhance popular techniques such as the Multivariate Imputation by Chained Equations (MICE) and MissForest. Their advantages are two-fold: improving the quality of the imputed data demonstrated by increased accuracy of the downstream classification; and providing deterministic and reliable imputations that remove the variance from the classification results. We experimentally demonstrate the advantages of our methods by performing extensive imputations on synthetic and real clinical data. We also perform quantum hardware experiments by applying the quantum circuits for DPP sampling, since such quantum algorithms provide a computational advantage with respect to classical ones. We demonstrate competitive results with up to ten qubits for small-scale imputation tasks on a state-of-the-art IBM quantum processor. Our classical and quantum methods improve the effectiveness and robustness of clinical data prediction modeling by providing better and more reliable data imputations. These improvements can add significant value in settings demanding high precision, such as in pharmaceutical drug trials where our approach can provide higher confidence in the predictions made."
            },
            {
                "arxivId": "2212.07389",
                "title": "Quantum Methods for Neural Networks and Application to Medical Image Classification",
                "abstract": "Quantum machine learning techniques have been proposed as a way to potentially enhance performance in machine learning applications.\nIn this paper, we introduce two new quantum methods for neural networks. The first one is a quantum orthogonal neural network, which is based on a quantum pyramidal circuit as the building block for implementing orthogonal matrix multiplication. We provide an efficient way for training such orthogonal neural networks; novel algorithms are detailed for both classical and quantum hardware, where both are proven to scale asymptotically better than previously known training algorithms.\nThe second method is quantum-assisted neural networks, where a quantum computer is used to perform inner product estimation for inference and training of classical neural networks.\nWe then present extensive experiments applied to medical image classification tasks using current state of the art quantum hardware, where we compare different quantum methods with classical ones, on both real quantum hardware and simulators. Our results show that quantum and classical neural networks generates similar level of accuracy, supporting the promise that quantum methods can be useful in solving visual tasks, given the advent of better quantum hardware."
            },
            {
                "arxivId": "2212.03223",
                "title": "Financial Risk Management on a Neutral Atom Quantum Processor",
                "abstract": "Machine Learning models capable of handling the large datasets collected in the financial world can often become black boxes expensive to run. The quantum computing paradigm suggests new optimization techniques, that combined with classical algorithms, may deliver competitive, faster and more interpretable models. In this work we propose a quantum-enhanced machine learning solution for the prediction of credit rating downgrades, also known as fallen-angels forecasting in the financial risk management field. We implement this solution on a neutral atom Quantum Processing Unit with up to 60 qubits on a real-life dataset. We report competitive performances against the state-of-the-art Random Forest benchmark whilst our model achieves better interpretability and comparable training times. We examine how to improve performance in the near-term validating our ideas with Tensor Networks-based numerical simulations."
            },
            {
                "arxivId": "2209.08867",
                "title": "Quantum computational finance: martingale asset pricing for incomplete markets",
                "abstract": "A derivative is a \ufb01nancial security whose value is a function of underlying traded assets and market outcomes. Pricing a \ufb01nancial derivative involves setting up a market model, \ufb01nding a martingale (\u201cfair game\u201d) probability measure for the model from the given asset prices, and using that probability measure to price the derivative. When the number of underlying assets and/or the number of market outcomes in the model is large, pricing can be computationally demanding. We show that a variety of quantum techniques can be applied to the pricing problem in \ufb01nance, with a particular focus on incomplete markets. We discuss three different methods that are distinct from previous works: they do not use the quantum algorithms for Monte Carlo estimation and they extract the martingale measure from market variables akin to bootstrapping, a common practice among \ufb01nancial institutions. The \ufb01rst two methods are based on a formulation of the pricing problem into a linear program and are using respectively the quantum zero-sum game algorithm and the quantum simplex algorithm as subroutines. For the last algorithm, we formalize a new market assumption milder than market completeness for which quantum linear systems solvers can be applied with the associated potential for large speedups. As a prototype use case, we conduct numerical experiments in the framework of the Black-Scholes-Merton model."
            },
            {
                "arxivId": "2209.08167",
                "title": "Quantum Vision Transformers",
                "abstract": "In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibiting more quantum characteristics. As building blocks of the quantum transformer, we propose a novel method for loading a matrix as quantum states as well as two new trainable quantum orthogonal layers adaptable to different levels of connectivity and quality of quantum computers. We performed extensive simulations of the quantum transformers on standard medical image datasets that showed competitively, and at times better performance compared to the classical benchmarks, including the best-in-class classical vision transformers. The quantum transformers we trained on these small-scale datasets require fewer parameters compared to standard classical benchmarks. Finally, we implemented our quantum transformers on superconducting quantum computers and obtained encouraging results for up to six qubit experiments."
            },
            {
                "arxivId": "2207.03670",
                "title": "Dynamical decoupling for superconducting qubits: A performance survey",
                "abstract": "Dynamical Decoupling (DD) is perhaps the simplest and least resource-intensive error suppression strategy for improving quantum computer performance. Here we report on a large-scale survey of the performance of 60 different DD sequences from 10 families, including basic as well as advanced sequences with high order error cancellation properties and built-in robustness. The survey is performed using three different superconducting-qubit IBMQ devices, with the goal of assessing the relative performance of the different sequences in the setting of arbitrary quantum state preservation. We find that the high-order universally robust (UR) and quadratic DD (QDD) sequences generally outperform all other sequences across devices and pulse interval settings. Surprisingly, we find that DD performance for basic sequences such as CPMG and XY4 can be made to nearly match that of UR and QDD by optimizing the pulse interval, with the optimal interval being substantially larger than the minimum interval possible on each device."
            },
            {
                "arxivId": "2202.00599",
                "title": "Quantum Machine Learning in Finance: Time Series Forecasting",
                "abstract": "We explore the efficacy of the novel use of parametrised quantum circuits (PQCs) as quantum neural networks (QNNs) for forecasting time series signals with simulated quantum forward propagation. The temporal signals consist of several sinusoidal components (deterministic signal), blended together with trends and additive noise. The performance of the PQCs is compared against that of classical bidirectional long short-term memory (BiLSTM) neural networks. Our results show that for time series signals consisting of small amplitude noise variations (up to 40 per cent of the amplitude of the deterministic signal) PQCs, with only a few parameters, perform similar to classical BiLSTM networks, with thousands of parameters, and outperform them for signals with higher amplitude noise variations. Thus, QNNs can be used effectively to model time series having, at the same time, the significant advantage of being trained significantly faster than a classical machine learning model in a quantum computer."
            },
            {
                "arxivId": "2202.00054",
                "title": "Quantum machine learning with subspace states",
                "abstract": "We introduce a new approach for quantum linear algebra based on quantum subspace states and present three new quantum machine learning algorithms. The first is a quantum determinant sampling algorithm that samples from the distribution $\\Pr[S]= det(X_{S}X_{S}^{T})$ for $|S|=d$ using $O(nd)$ gates and with circuit depth $O(d\\log n)$. The state of art classical algorithm for the task requires $O(d^{3})$ operations \\cite{derezinski2019minimax}. The second is a quantum singular value estimation algorithm for compound matrices $\\mathcal{A}^{k}$, the speedup for this algorithm is potentially exponential. It decomposes a $\\binom{n}{k}$ dimensional vector of order-$k$ correlations into a linear combination of subspace states corresponding to $k$-tuples of singular vectors of $A$. The third algorithm reduces exponentially the depth of circuits used in quantum topological data analysis from $O(n)$ to $O(\\log n)$. Our basic tool are quantum subspace states, defined as $|Col(X)\\rangle = \\sum_{S\\subset [n], |S|=d} det(X_{S}) |S\\rangle$ for matrices $X \\in \\mathbb{R}^{n \\times d}$ such that $X^{T} X = I_{d}$, that encode $d$-dimensional subspaces of $\\mathbb{R}^{n}$. We develop two efficient state preparation techniques, the first using Givens circuits uses the representation of a subspace as a sequence of Givens rotations, while the second uses efficient implementations of unitaries $\\Gamma(x) = \\sum_{i} x_{i} Z^{\\otimes (i-1)} \\otimes X \\otimes I^{n-i}$ with $O(\\log n)$ depth circuits that we term Clifford loaders."
            },
            {
                "arxivId": "2201.02773",
                "title": "A Survey of Quantum Computing for Finance",
                "abstract": "Quantum computers are expected to surpass the computational capabilities of classical computers during this decade and have transformative impact on numerous industry sectors, particularly finance. In fact, finance is estimated to be the first industry sector to benefit from quantum computing, not only in the medium and long terms, but even in the short term. This survey paper presents a comprehensive summary of the state of the art of quantum computing for financial applications, with particular emphasis on stochastic modeling, optimization, and machine learning, describing how these solutions, adapted to work on a quantum computer, can potentially help to solve financial problems, such as derivative pricing, risk modeling, portfolio optimization, natural language processing, and fraud detection, more efficiently and accurately. We also discuss the feasibility of these algorithms on nearterm quantum computers with various hardware implementations and demonstrate how they relate to a wide range of use cases in finance. We hope this article will not only serve as a reference for academic researchers and industry practitioners but also inspire new ideas for future research. These authors contributed equally to this work. i ar X iv :2 20 1. 02 77 3v 4 [ qu an tph ] 2 7 Ju n 20 22"
            },
            {
                "arxivId": "2111.15332",
                "title": "Quantum Algorithm for Stochastic Optimal Stopping Problems with Applications in Finance",
                "abstract": "The famous least squares Monte Carlo (LSM) algorithm combines linear least square regression with Monte Carlo simulation to approximately solve problems in stochastic optimal stopping theory. In this work, we propose a quantum LSM based on quantum access to a stochastic process, on quantum circuits for computing the optimal stopping times, and on quantum techniques for Monte Carlo. For this algorithm, we elucidate the intricate interplay of function approximation and quantum algorithms for Monte Carlo. Our algorithm achieves a nearly quadratic speedup in the runtime compared to the LSM algorithm under some mild assumptions. Specifically, our quantum algorithm can be applied to American option pricing and we analyze a case study for the common situation of Brownian motion and geometric Brownian motion processes."
            },
            {
                "arxivId": "2109.04298",
                "title": "Quantum Machine Learning for Finance",
                "abstract": "Quantum computers are expected to surpass the computational capabilities of classical computers during this decade, and achieve disruptive impact on numerous industry sectors, particularly finance. In fact, finance is estimated to be the first industry sector to benefit from Quantum Computing not only in the medium and long terms, but even in the short term. This review paper presents the state of the art of quantum algorithms for financial applications, with particular focus to those use cases that can be solved via Machine Learning."
            },
            {
                "arxivId": "2108.12518",
                "title": "Scalable Mitigation of Measurement Errors on Quantum Computers",
                "abstract": "We present a method for mitigating measurement errors on quantum computing platforms that does not form the full assignment matrix, or its inverse, and works in a subspace defined by the noisy input bit-strings. This method accommodates both uncorrelated and correlated errors, and allows for computing accurate error bounds. Additionally, we detail a matrix-free preconditioned iterative solution method that converges in $\\mathcal{O}(1)$ steps that is performant and uses orders of magnitude less memory than direct factorization. We demonstrate the validity of our method, and mitigate errors in a few seconds on numbers of qubits that would otherwise be intractable."
            },
            {
                "arxivId": "2012.04145",
                "title": "Nearest centroid classification on a trapped ion quantum computer",
                "abstract": null
            },
            {
                "arxivId": "2012.03348",
                "title": "Low depth algorithms for quantum amplitude estimation",
                "abstract": "We design and analyze two new low depth algorithms for amplitude estimation (AE) achieving an optimal tradeoff between the quantum speedup and circuit depth. For $\\beta \\in (0,1]$, our algorithms require $N= O( \\frac{1}{ \\epsilon^{1+\\beta}})$ oracle calls and require the oracle to be called sequentially $D= O( \\frac{1}{ \\epsilon^{1-\\beta}})$ times to perform amplitude estimation within additive error $\\epsilon$. These algorithms interpolate between the classical algorithm $(\\beta=1)$ and the standard quantum algorithm ($\\beta=0$) and achieve a tradeoff $ND= O(1/\\epsilon^{2})$. These algorithms bring quantum speedups for Monte Carlo methods closer to realization, as they can provide speedups with shallower circuits. The first algorithm (Power law AE) uses power law schedules in the framework introduced by Suzuki et al \\cite{S20}. The algorithm works for $\\beta \\in (0,1]$ and has provable correctness guarantees when the log-likelihood function satisfies regularity conditions required for the Bernstein Von-Mises theorem. The second algorithm (QoPrime AE) uses the Chinese remainder theorem for combining lower depth estimates to achieve higher accuracy. The algorithm works for discrete $\\beta =q/k$ where $k \\geq 2$ is the number of distinct coprime moduli used by the algorithm and $1 \\leq q \\leq k-1$, and has a fully rigorous correctness proof. We analyze both algorithms in the presence of depolarizing noise and provide experimental comparisons with the state of the art amplitude estimation algorithms."
            },
            {
                "arxivId": "2011.06492",
                "title": "Prospects and challenges of quantum finance",
                "abstract": "Quantum computers are expected to have substantial impact on the finance industry, as they will be able to solve certain problems considerably faster than the best known classical algorithms. In this article we describe such potential applications of quantum computing to finance, starting with the state-of-the-art and focusing in particular on recent works by the QC Ware team. We consider quantum speedups for Monte Carlo methods, portfolio optimization, and machine learning. For each application we describe the extent of quantum speedup possible and estimate the quantum resources required to achieve a practical speedup. The near-term relevance of these quantum finance algorithms varies widely across applications - some of them are heuristic algorithms designed to be amenable to near-term prototype quantum computers, while others are proven speedups which require larger-scale quantum computers to implement. We also describe powerful ways to bring these speedups closer to experimental feasibility - in particular describing lower depth algorithms for Monte Carlo methods and quantum machine learning, as well as quantum annealing heuristics for portfolio optimization. This article is targeted at financial professionals and no particular background in quantum computation is assumed."
            },
            {
                "arxivId": "2010.02174",
                "title": "A rigorous and robust quantum speed-up in supervised machine learning",
                "abstract": null
            },
            {
                "arxivId": "2006.14510",
                "title": "Quantum Computing for Finance: State-of-the-Art and Future Prospects",
                "abstract": "This article outlines our point of view regarding the applicability, state-of-the-art, and potential of quantum computing for problems in finance. We provide an introduction to quantum computing as well as a survey on problem classes in finance that are computationally challenging classically and for which quantum computing algorithms are promising. In the main part, we describe in detail quantum algorithms for specific applications arising in financial services, such as those involving simulation, optimization, and machine learning problems. In addition, we include demonstrations of quantum algorithms on IBM Quantum back-ends and discuss the potential benefits of quantum algorithms for problems in financial services. We conclude with a summary of technical challenges and future prospects."
            },
            {
                "arxivId": "2006.16947",
                "title": "Sampling from a k-DPP without looking at all items",
                "abstract": "Determinantal point processes (DPPs) are a useful probabilistic model for selecting a small diverse subset out of a large collection of items, with applications in summarization, stochastic optimization, active learning and more. Given a kernel function and a subset size $k$, our goal is to sample $k$ out of $n$ items with probability proportional to the determinant of the kernel matrix induced by the subset (a.k.a. $k$-DPP). Existing $k$-DPP sampling algorithms require an expensive preprocessing step which involves multiple passes over all $n$ items, making it infeasible for large datasets. A naive heuristic addressing this problem is to uniformly subsample a fraction of the data and perform $k$-DPP sampling only on those items, however this method offers no guarantee that the produced sample will even approximately resemble the target distribution over the original dataset. In this paper, we develop an algorithm which adaptively builds a sufficiently large uniform sample of data that is then used to efficiently generate a smaller set of $k$ items, while ensuring that this set is drawn exactly from the target distribution defined on all $n$ items. We show empirically that our algorithm produces a $k$-DPP sample after observing only a small fraction of all elements, leading to several orders of magnitude faster performance compared to the state-of-the-art."
            },
            {
                "arxivId": "2005.03185",
                "title": "Determinantal Point Processes in Randomized Numerical Linear Algebra",
                "abstract": "Randomized Numerical Linear Algebra (RandNLA) uses randomness to develop improved algorithms for matrix problems that arise in scientific computing, data science, machine learning, etc. Determinantal Point Processes (DPPs), a seemingly unrelated topic in pure and applied mathematics, is a class of stochastic point processes with probability distribution characterized by sub-determinants of a kernel matrix. Recent work has uncovered deep and fruitful connections between DPPs and RandNLA which lead to new guarantees and improved algorithms that are of interest to both areas. We provide an overview of this exciting new line of research, including brief introductions to RandNLA and DPPs, as well as applications of DPPs to classical linear algebra tasks such as least squares regression, low-rank approximation and the Nystrom method. For example, random sampling with a DPP leads to new kinds of unbiased estimators for least squares, enabling more refined statistical and inferential understanding of these algorithms; a DPP is, in some sense, an optimal randomized algorithm for the Nystrom method; and a RandNLA technique called leverage score sampling can be derived as the marginal distribution of a DPP. We also discuss recent algorithmic developments, illustrating that, while not quite as efficient as standard RandNLA techniques, DPP-based algorithms are only moderately more expensive."
            },
            {
                "arxivId": "1908.10778",
                "title": "Classical versus quantum models in machine learning: insights from a finance application",
                "abstract": "Although several models have been proposed towards assisting machine learning (ML) tasks with quantum computers, a direct comparison of the expressive power and efficiency of classical versus quantum models for datasets originating from real-world applications is one of the key milestones towards a quantum ready era. Here, we take a first step towards addressing this challenge by performing a comparison of the widely used classical ML models known as restricted Boltzmann machines (RBMs), against a recently proposed quantum model, now known as quantum circuit Born machines (QCBMs). Both models address the same hard tasks in unsupervised generative modeling, with QCBMs exploiting the probabilistic nature of quantum mechanics and a candidate for near-term quantum computers, as experimentally demonstrated in three different quantum hardware architectures to date. To address the question of the performance of the quantum model on real-world classical data sets, we construct scenarios from a probabilistic version out of the well-known portfolio optimization problem in finance, by using time-series pricing data from asset subsets of the S&P500 stock market index. It is remarkable to find that, under the same number of resources in terms of parameters for both classical and quantum models, the quantum models seem to have superior performance on typical instances when compared with the canonical training of the RBMs. Our simulations are grounded on a hardware efficient realization of the QCBMs on ion-trap quantum computers, by using their native gate sets, and therefore readily implementable in near-term quantum devices."
            },
            {
                "arxivId": "1908.08040",
                "title": "Quantum Algorithms for Portfolio Optimization",
                "abstract": "We develop the first quantum algorithm for the constrained portfolio optimization problem. The algorithm has running time \u00d5 (n\u221ar \u03b6k/\u03b42 log (1/\u03f5)), where r is the number of positivity and budget constraints, n is the number of assets in the portfolio, \u03f5 the desired precision, and \u03b4, \u03ba, \u03b6 are problem-dependent parameters related to the well-conditioning of the intermediate solutions. If only a moderately accurate solution is required, our quantum algorithm can achieve a polynomial speedup over the best classical algorithms with complexity \u00d5 (\u221arn\u03c9 log(1/\u03f5)), where \u03c9 is the matrix multiplication exponent that has a theoretical value of around 2.373, but is closer to 3 in practice. We also provide some experiments to bound the problem-dependent factors arising in the running time of the quantum algorithm, and these experiments suggest that for most instances the quantum algorithm can potentially achieve an O(n) speedup over its classical counterpart."
            },
            {
                "arxivId": "1906.07682",
                "title": "Parameterized quantum circuits as machine learning models",
                "abstract": "Hybrid quantum\u2013classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications."
            },
            {
                "arxivId": "1905.13476",
                "title": "Exact sampling of determinantal point processes with sublinear time preprocessing",
                "abstract": "We study the complexity of sampling from a distribution over all index subsets of the set $\\{1,...,n\\}$ with the probability of a subset $S$ proportional to the determinant of the submatrix $\\mathbf{L}_S$ of some $n\\times n$ p.s.d. matrix $\\mathbf{L}$, where $\\mathbf{L}_S$ corresponds to the entries of $\\mathbf{L}$ indexed by $S$. Known as a determinantal point process, this distribution is used in machine learning to induce diversity in subset selection. In practice, we often wish to sample multiple subsets $S$ with small expected size $k = E[|S|] \\ll n$ from a very large matrix $\\mathbf{L}$, so it is important to minimize the preprocessing cost of the procedure (performed once) as well as the sampling cost (performed repeatedly). For this purpose, we propose a new algorithm which, given access to $\\mathbf{L}$, samples exactly from a determinantal point process while satisfying the following two properties: (1) its preprocessing cost is $n \\cdot \\text{poly}(k)$, i.e., sublinear in the size of $\\mathbf{L}$, and (2) its sampling cost is $\\text{poly}(k)$, i.e., independent of the size of $\\mathbf{L}$. Prior to our results, state-of-the-art exact samplers required $O(n^3)$ preprocessing time and sampling time linear in $n$ or dependent on the spectral properties of $\\mathbf{L}$. We also give a reduction which allows using our algorithm for exact sampling from cardinality constrained determinantal point processes with $n\\cdot\\text{poly}(k)$ time preprocessing."
            },
            {
                "arxivId": "1905.05929",
                "title": "Orthogonal Deep Neural Networks",
                "abstract": "In this paper, we introduce the algorithms of Orthogonal Deep Neural Networks (OrthDNNs) to connect with recent interest of spectrally regularized deep learning methods. OrthDNNs are theoretically motivated by generalization analysis of modern DNNs, with the aim to find solution properties of network weights that guarantee better generalization. To this end, we first prove that DNNs are of local isometry on data distributions of practical interest; by using a new covering of the sample space and introducing the local isometry property of DNNs into generalization analysis, we establish a new generalization error bound that is both scale- and range-sensitive to singular value spectrum of each of networks\u2019 weight matrices. We prove that the optimal bound w.r.t. the degree of isometry is attained when each weight matrix has a spectrum of equal singular values, among which orthogonal weight matrix or a non-square one with orthonormal rows or columns is the most straightforward choice, suggesting the algorithms of OrthDNNs. We present both algorithms of strict and approximate OrthDNNs, and for the later ones we propose a simple yet effective algorithm called Singular Value Bounding (SVB), which performs as well as strict OrthDNNs, but at a much lower computational cost. We also propose Bounded Batch Normalization (BBN) to make compatible use of batch normalization with OrthDNNs. We conduct extensive comparative studies by using modern architectures on benchmark image classification. Experiments show the efficacy of OrthDNNs."
            },
            {
                "arxivId": "1904.10246",
                "title": "Amplitude estimation without phase estimation",
                "abstract": null
            },
            {
                "arxivId": "1812.00068",
                "title": "GDPP: Learning Diverse Generations Using Determinantal Point Process",
                "abstract": "Generative models have proven to be an outstanding tool for representing high-dimensional probability distributions and generating realistic-looking images. An essential characteristic of generative models is their ability to produce multi-modal outputs. However, while training, they are often susceptible to mode collapse, that is models are limited in mapping input noise to only a few modes of the true data distribution. In this work, we draw inspiration from Determinantal Point Process (DPP) to propose an unsupervised penalty loss that alleviates mode collapse while producing higher quality samples. DPP is an elegant probabilistic measure used to model negative correlations within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real data as well as in synthetic data. Then, we devise an objective term that encourages generators to synthesize data with similar diversity to real data. In contrast to previous state-of-the-art generative models that tend to use additional trainable parameters or complex training paradigms, our method does not change the original training scheme. Embedded in an adversarial training and variational autoencoder, our Generative DPP approach shows a consistent resistance to mode-collapse on a wide variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA, while outperforming state-of-the-art methods for data-efficiency, generation quality, and convergence-time whereas being 5.8x faster than its closest competitor."
            },
            {
                "arxivId": "1811.03975",
                "title": "Quantum computational finance: quantum algorithm for portfolio optimization",
                "abstract": "We present a quantum algorithm for portfolio optimization. We discuss the market data input, the processing of such data via quantum operations, and the output of financially relevant results. Given quantum access to the historical record of returns, the algorithm determines the optimal risk-return tradeoff curve and allows one to sample from the optimal portfolio. The algorithm can in principle attain a run time of ${\\rm poly}(\\log(N))$, where $N$ is the size of the historical return dataset. Direct classical algorithms for determining the risk-return curve and other properties of the optimal portfolio take time ${\\rm poly}(N)$ and we discuss potential quantum speedups in light of the recent works on efficient classical sampling approaches."
            },
            {
                "arxivId": "1809.02573",
                "title": "Tackling the Qubit Mapping Problem for NISQ-Era Quantum Devices",
                "abstract": "Due to little considerations in the hardware constraints, e.g., limited connections between physical qubits to enable two-qubit gates, most quantum algorithms cannot be directly executed on the Noisy Intermediate-Scale Quantum (NISQ) devices. Dynamically remapping logical qubits to physical qubits in the compiler is needed to enable the two-qubit gates in the algorithm, which introduces additional operations and inevitably reduces the fidelity of the algorithm. Previous solutions in finding such remapping suffer from high complexity, poor initial mapping quality, and limited flexibility and control. To address these drawbacks mentioned above, this paper proposes a SWAP-based Bidirectional heuristic search algorithm (SABRE), which is applicable to NISQ devices with arbitrary connections between qubits. By optimizing every search attempt, globally optimizing the initial mapping using a novel reverse traversal technique, introducing the decay effect to enable the trade-off between the depth and the number of gates of the entire algorithm, SABRE outperforms the best known algorithm with exponential speedup and comparable or better results on various benchmarks."
            },
            {
                "arxivId": "1804.11326",
                "title": "Supervised learning with quantum-enhanced feature spaces",
                "abstract": null
            },
            {
                "arxivId": "1802.06749",
                "title": "Leveraged volume sampling for linear regression",
                "abstract": "Suppose an n x d design matrix in a linear regression problem is given, but the response for each point is hidden unless explicitly requested. The goal is to sample only a small number k << n of the responses, and then produce a weight vector whose sum of squares loss over *all* points is at most 1+epsilon times the minimum. When k is very small (e.g., k=d), jointly sampling diverse subsets of points is crucial. One such method called \"volume sampling\" has a unique and desirable property that the weight vector it produces is an unbiased estimate of the optimum. It is therefore natural to ask if this method offers the optimal unbiased estimate in terms of the number of responses k needed to achieve a 1+epsilon loss approximation. Surprisingly we show that volume sampling can have poor behavior when we require a very accurate approximation -- indeed worse than some i.i.d. sampling techniques whose estimates are biased, such as leverage score sampling. We then develop a new rescaled variant of volume sampling that produces an unbiased estimate which avoids this bad behavior and has at least as good a tail bound as leverage score sampling: sample size k=O(d log d + d/epsilon) suffices to guarantee total loss at most 1+epsilon times the minimum with high probability. Thus, we improve on the best previously known sample size for an unbiased estimator, k=O(d^2/epsilon). Our rescaling procedure leads to a new efficient algorithm for volume sampling which is based on a \"determinantal rejection sampling\" technique with potentially broader applications to determinantal point processes. Other contributions include introducing the combinatorics needed for rescaled volume sampling and developing tail bounds for sums of dependent random matrices which arise in the process."
            },
            {
                "arxivId": "1608.01008",
                "title": "Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and Constrained Sampling",
                "abstract": "We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds."
            },
            {
                "arxivId": "1605.00361",
                "title": "Monte Carlo with determinantal point processes",
                "abstract": "We show that repulsive random variables can yield Monte Carlo methods with faster convergence rates than the typical $N^{-1/2}$, where $N$ is the number of integrand evaluations. More precisely, we propose stochastic numerical quadratures involving determinantal point processes associated with multivariate orthogonal polynomials, and we obtain root mean square errors that decrease as $N^{-(1+1/d)/2}$, where $d$ is the dimension of the ambient space. First, we prove a central limit theorem (CLT) for the linear statistics of a class of determinantal point processes, when the reference measure is a product measure supported on a hypercube, which satisfies the Nevai-class regularity condition, a result which may be of independent interest. Next, we introduce a Monte Carlo method based on these determinantal point processes, and prove a CLT with explicit limiting variance for the quadrature error, when the reference measure satisfies a stronger regularity condition. As a corollary, by taking a specific reference measure and using a construction similar to importance sampling, we obtain a general Monte Carlo method, which applies to any measure with continuously derivable density. Loosely speaking, our method can be interpreted as a stochastic counterpart to Gaussian quadrature, which, at the price of some convergence rate, is easily generalizable to any dimension and has a more explicit error term."
            },
            {
                "arxivId": "1602.05242",
                "title": "Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes",
                "abstract": "Strongly Rayleigh distributions are natural generalizations of product and determinantal probability distributions and satisfy strongest form of negative dependence properties. We show that the \"natural\" Monte Carlo Markov Chain (MCMC) is rapidly mixing in the support of a {\\em homogeneous} strongly Rayleigh distribution. As a byproduct, our proof implies Markov chains can be used to efficiently generate approximate samples of a $k$-determinantal point process. This answers an open question raised by Deshpande and Rademacher."
            },
            {
                "arxivId": "1511.06464",
                "title": "Unitary Evolution Recurrent Neural Networks",
                "abstract": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies."
            },
            {
                "arxivId": "1207.6083",
                "title": "Determinantal Point Processes for Machine Learning",
                "abstract": "Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning."
            },
            {
                "arxivId": "0804.4050",
                "title": "Matchgates and classical simulation of quantum circuits",
                "abstract": "Let G(A,\u200aB) denote the two-qubit gate that acts as the one-qubit SU(2) gates A and B in the even and odd parity subspaces, respectively, of two qubits. Using a Clifford algebra formalism, we show that arbitrary uniform families of circuits of these gates, restricted to act only on nearest neighbour (n.n.) qubit lines, can be classically efficiently simulated. This reproduces a result originally proved by Valiant using his matchgate formalism, and subsequently related by others to free fermionic physics. We further show that if the n.n. condition is slightly relaxed, to allow the same gates to act only on n.n. and next n.n. qubit lines, then the resulting circuits can efficiently perform universal quantum computation. From this point of view, the gap between efficient classical and quantum computational power is bridged by a very modest use of a seemingly innocuous resource (qubit swapping). We also extend the simulation result above in various ways. In particular, by exploiting properties of Clifford operations in conjunction with the Jordan\u2013Wigner representation of a Clifford algebra, we show how one may generalize the simulation result above to provide further classes of classically efficiently simulatable quantum circuits, which we call Gaussian quantum circuits."
            },
            {
                "arxivId": "math/0503110",
                "title": "Determinantal Processes and Independence",
                "abstract": "We give a probabilistic introduction to determinantal and per- manental point processes. Determinantal processes arise in physics (fermions, eigenvalues of random matrices) and in combinatorics (nonintersecting paths, random spanning trees). They have the striking property that the number of points in a region D is a sum of independent Bernoulli random variables, with parameters which are eigenvalues of the relevant operator on L 2 (D). Moreover, any determinantal process can be represented as a mixture of determinantal projection processes. We give a simple explanation for these known facts, and establish analogous representations for permanental pro- cesses, with geometric variables replacing the Bernoulli variables. These representations lead to simple proofs of existence criteria and central limit theorems, and unify known results on the distribution of absolute values in certain processes with radially symmetric distributions."
            },
            {
                "arxivId": "quant-ph/9803057",
                "title": "DYNAMICAL SUPPRESSION OF DECOHERENCE IN TWO-STATE QUANTUM SYSTEMS",
                "abstract": "The dynamics of a decohering two-level system driven by a suitable control Hamiltonian is studied. The control procedure is implemented as a sequence of radio-frequency pulses that repetitively flip the state of the system, a technique that can be termed quantum ``bang-bang'' control after its classical analog. Decoherence introduced by the system's interaction with a quantum environment is shown to be washed out completely in the limit of continuous flipping and greatly suppressed provided the interval between the pulses is made comparable to the correlation time of the environment. The model suggests a strategy to fight against decoherence that complements existing quantum error-correction techniques."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2310.14973",
        "category": "q-fin",
        "title": "Reconciling Open Interest with Traded Volume in Perpetual Swaps",
        "abstract": "\n\n\nPerpetual swaps are derivative contracts that allow traders to speculate on, or hedge, the price movements of cryptocurrencies. Unlike futures contracts, perpetual swaps have no settlement or expiration in the traditional sense. The funding rate acts as the mechanism that tethers the perpetual swap to its underlying with the help of arbitrageurs. Open interest, in the context of perpetual swaps and derivative contracts in general, refers to the total number of outstanding contracts at a given point in time. It is a critical metric in derivatives markets as it can provide insight into market activity, sentiment and overall liquidity. It also provides a way to estimate a lower bound on the collateral required for every cryptocurrency market on an exchange. This number, cumulated across all markets on the exchange in combination with proof of reserves, can be used to gauge whether the exchange in question operates with unsustainable levels of leverage, which could have solvency implications. We find that open interest in Bitcoin perpetual swaps is systematically misquoted by some of the largest derivatives exchanges; however, the degree varies, with some exchanges reporting open interest that is wholly implausible to others that seem to be delaying messages of forced trades, i.e., liquidations. We identify these incongruities by analyzing tick-by-tick data for two time periods in 2023 by connecting directly to seven of the most liquid cryptocurrency derivatives exchanges.\n\n\n",
        "references": [
            {
                "arxivId": "2212.06888",
                "title": "Fundamentals of Perpetual Futures",
                "abstract": "Perpetual futures -- swap contracts that never expire -- are the most popular derivative traded in cryptocurrency markets, with more than \\$100 billion traded daily. Perpetuals provide investors with leveraged exposure to cryptocurrencies, which does not require rollover or direct cryptocurrency holding. To keep the gap between perpetual futures and spot prices small, long position holders periodically pay short position holders a funding rate proportional to this gap. The funding rate incentivizes trades that tend to narrow the futures-spot gap. But unlike fixed-maturity futures, perpetuals are not guaranteed to converge to the spot price of their underlying asset at any time, and familiar no-arbitrage prices for perpetuals are not available, as the contracts have no expiry date to enforce arbitrage. Here, using a weaker notion of random-maturity arbitrage, we derive no-arbitrage prices for perpetual futures in frictionless markets and no-arbitrage bounds for markets with trading costs. These no-arbitrage prices provide a valuable benchmark for perpetual futures and simultaneously prescribe a strategy to exploit divergence from these fundamental values. Empirically, we find that deviations of crypto perpetual futures from no-arbitrage prices are considerably larger than those documented in traditional currency markets. These deviations comove across cryptocurrencies and diminish over time as crypto markets develop and become more efficient. A simple trading strategy generates large Sharpe ratios even for investors paying the highest trading costs on Binance, which is currently the largest crypto exchange by volume."
            },
            {
                "arxivId": "2108.10984",
                "title": "Crypto Wash Trading",
                "abstract": "We present the first systematic approach to detect fake transactions on cryptocurrency exchanges by exploiting robust statistical and behavioral regularities associated with authentic trading. Our sample consists of 29 centralized exchanges, among which the regulated ones feature transaction patterns consistently observed in financial markets and nature. In contrast, unregulated exchanges display abnormal first significant digit distributions, size rounding, and transaction tail distributions, indicating widespread manipulation unlikely driven by a specific trading strategy or exchange heterogeneity. We then quantify the wash trading on each unregulated exchange, which averaged more than 70% of the reported volume. We further document how these fabricated volumes (trillions of dollars annually) improve exchange ranking, temporarily distort prices, and relate to exchange characteristics (e.g., age and user base), market conditions, and regulation. Overall, our study cautions against potential market manipulations on centralized crypto exchanges with concentrated power and limited disclosure requirements and highlights the importance of fintech regulation. This paper was accepted by David Simchi-Levi, special issue of Management Science: Blockchains and crypto economics. Funding: This research was partly funded by the Ewing Marion Kauffman Foundation [Grant G-201907-6995], the National Natural Science Foundation of China [Grants 72192802, 72192800, and 72192801], Ripple\u2019s University Blockchain Research Initiative (UBRI), and the FinTech at Cornell Initiative. Supplemental Material: The online appendix and data are available at https://doi.org/10.1287/mnsc.2021.02709 ."
            },
            {
                "arxivId": "1902.01941",
                "title": "Market Manipulation of Bitcoin: Evidence from Mining the Mt. Gox Transaction Network",
                "abstract": "The cryptocurrency market is a very huge market without effective supervision. It is of great importance for investors and regulators to recognize whether there are market manipulation and its manipulation patterns. This paper proposes an approach to mine the transaction networks of exchanges for answering this question. By taking the leaked transaction history of Mt. Gox Bitcoin exchange as a sample, we first divide the accounts into three categories according to its characteristic and then construct the transaction history into three graphs. Many observations and findings are obtained via analyzing the constructed graphs. To evaluate the influence of the accounts\u2019 transaction behavior on the Bitcoin exchange price, the graphs are reconstructed into series and reshaped as matrices. By using singular value decomposition (SVD) on the matrices, we identify many base networks which have a great correlation with the price fluctuation. When further analyzing the most important accounts in the base networks, plenty of market manipulation patterns are found. According to these findings, we conclude that there was serious market manipulation in Mt. Gox exchange and the cryptocurrency market must strengthen the supervision."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2401.03737",
        "category": "q-fin",
        "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection",
        "abstract": "This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4's advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability and acceptance. Through empirical testing on the competitive S&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10% to 30% and achieving a cumulative return of up to 72% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies.",
        "references": [
            {
                "arxivId": "2304.07619",
                "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models",
                "abstract": "We examine the potential of ChatGPT and other large language models in predicting stock market returns using news headlines. We use ChatGPT to assess whether each headline is good, bad, or neutral for firms' stock prices. We document a significantly positive correlation between ChatGPT scores and subsequent daily stock returns. We find that ChatGPT outperforms traditional sentiment analysis methods. More basic models such as GPT-1, GPT-2, and BERT cannot accurately forecast returns, indicating return predictability is an emerging capacity of complex language models. Long-short strategies based on ChatGPT-4 deliver the highest Sharpe ratio. Furthermore, we find predictability in both small and large stocks, suggesting market underreaction to company news. Predictability is stronger among smaller stocks and stocks with bad news, consistent with limits-to-arbitrage also playing an important role. Finally, we propose a new method to evaluate and understand the models' reasoning capabilities. Overall, our results suggest that incorporating advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies."
            },
            {
                "arxivId": "2004.09297",
                "title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
                "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\\footnote{\\url{this https URL}}."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2404.02053",
        "category": "q-fin",
        "title": "BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights",
        "abstract": "This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextual aspects of market sentiment. The integration of advanced NLP techniques like BERTopic with traditional financial analysis methods marks a step forward in developing more sophisticated tools for understanding and predicting market behaviors.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2404.02270",
        "category": "q-fin",
        "title": "Postprocessing of point predictions for probabilistic forecasting of electricity prices: Diversity matters",
        "abstract": "Operational decisions relying on predictive distributions of electricity prices can result in significantly higher profits compared to those based solely on point forecasts. However, the majority of models developed in both academic and industrial settings provide only point predictions. To address this, we examine three postprocessing methods for converting point forecasts into probabilistic ones: Quantile Regression Averaging, Conformal Prediction, and the recently introduced Isotonic Distributional Regression. We find that while IDR demonstrates the most varied performance, combining its predictive distributions with those of the other two methods results in an improvement of ca. 7.5% compared to a benchmark model with normally distributed errors, over a 4.5-year test period in the German power market spanning the COVID pandemic and the war in Ukraine. Remarkably, the performance of this combination is at par with state-of-the-art Distributional Deep Neural Networks.",
        "references": [
            {
                "arxivId": "1803.06730",
                "title": "Combining Probabilistic Load Forecasts",
                "abstract": "Probabilistic load forecasts provide comprehensive information about future load uncertainties. In recent years, many methodologies and techniques have been proposed for probabilistic load forecasting. Forecast combination, a widely recognized best practice in point forecasting literature, has never been formally adopted to combine probabilistic load forecasts. This paper proposes a constrained quantile regression averaging (CQRA) method to create an improved ensemble from several individual probabilistic forecasts. We formulate the CQRA parameter estimation problem as a linear program with the objective of minimizing the pinball loss and the constraints that the parameters are nonnegative and summing up to one. We demonstrate the effectiveness of the proposed method using two publicly available datasets, the ISO New England data and Irish smart meter data. Comparing with the best individual probabilistic forecast, the ensemble can reduce the pinball score by 4.39% on average. The proposed ensemble also demonstrates superior performance over nine other benchmark ensembles."
            },
            {
                "arxivId": "0706.3188",
                "title": "A tutorial on conformal prediction",
                "abstract": "Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability e, together with a method that makes a prediction \u0177 of a label y, it produces a set of labels, typically containing \u0177, that also contains y with probability 1 \u0096 e. Conformal prediction can be applied to any method for producing \u0177: a nearest-neighbor method, a support-vector machine, ridge regression, etc. \n \nConformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 \u0096 e of the time, even though they are based on an accumulating data set rather than on independent data sets. \n \nIn addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. \n \nThis tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2404.03508",
        "category": "q-fin",
        "title": "The Economic Consequences of Geopolitical Fragmentation: Evidence from the Cold War",
        "abstract": "The Cold War was the defining episode of geopolitical fragmentation in the twentieth century. Trade between East and West across the Iron Curtain (a symbolical and physical barrier dividing Europe into two distinct areas) was restricted, but the severity of these restrictions varied over time. We quantify the trade and welfare effects of the Iron Curtain and show how the difficulty of trading across the Iron Curtain fluctuated throughout the Cold War. Using a novel dataset on trade between the two economic blocs and a quantitative trade model, we find that while the Iron Curtain at its height represented a tariff equivalent of 48% in 1951, trade between East and West gradually became easier until the fall of the Berlin Wall in 1989. Despite the easing of trade restrictions, we estimate that the Iron Curtain roughly halved East-West trade flows and caused substantial welfare losses in the Eastern bloc countries that persisted until the end of the Cold War. Conversely, the Iron Curtain led to an increase in intra-bloc trade, especially in the Eastern bloc, which outpaced the integration of Western Europe in the run-up to the formation of the European Union.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-05.json",
        "arxivId": "2404.03581",
        "category": "q-fin",
        "title": "Consumer Behavior under Benevolent Price Discrimination",
        "abstract": "Extensive research shows that consumers are generally averse to price discrimination. However, instruments of differential pricing can benefit consumer surplus and alleviate inequity through targeted price discounts. This paper examines how these outcome considerations influence consumer reactions to price discrimination. Six studies with 3951 participants show that a large share of consumers is willing to costly switch away from a store that introduces a discount for low-income consumers. This happens irrespective of whether income differences are due to luck or merit. While the price-discriminating store does attract some new high-income consumers, it cannot compensate the loss of existing consumers. Allowing for altruistic preferences by simulating a market mechanism increases costly support for price discounts, but does not alleviate consumer aversions. Finally, we provide evidence that warm glow drives costly support for price discounts.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-08.json",
        "arxivId": "2205.06572",
        "category": "q-fin",
        "title": "Dynamic Stochastic Inventory Management in E-Grocery Retailing",
        "abstract": "E-grocery retailing enables ordering products online to be delivered at a future time slot chosen by the customer. This emerging field of business provides retailers with large and comprehensive new data sets, yet creates several challenges for the inventory management process. For example, the risk of a single item's stock-out leading to a complete cancellation of the shopping process is higher in e-grocery than in traditional store retailing. As a consequence, retailers aim at very high service level targets to provide satisfactory customer service and to ensure long-term business growth. When determining replenishment order quantities, it is of crucial importance to precisely account for the full uncertainty in the inventory process. This requires predictive and prescriptive analytics to (1) estimate suitable underlying probability distributions to represent the uncertainty caused by non-stationary customer demand, shelf lives, and supply, and to (2) integrate those forecasts into a comprehensive multi-period optimisation framework. In this paper, we model this stochastic dynamic problem by a sequential decision process that allows us to avoid simplifying assumptions commonly made in the literature, such as the focus on a single demand period. As the resulting problem will typically be analytically intractable, we propose a stochastic lookahead policy incorporating Monte Carlo techniques to fully propagate the associated uncertainties in order to derive replenishment order quantities. This policy naturally integrates probabilistic forecasts and allows us to explicitly derive the value of accounting for probabilistic information compared to myopic or deterministic approaches in a simulation-based setting. In addition, we evaluate our policy in a case study based on real-world data where underlying probability distributions are estimated from historical data and explanatory variables.",
        "references": [
            {
                "arxivId": "1710.08005",
                "title": "Smart \"Predict, then Optimize\"",
                "abstract": "Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart \u201cPredict, then Optimize\u201d (SPO), which directly leverages the optimization problem structure\u2014that is, its objective and constraints\u2014for designing better prediction models. A key component of our framework is the SPO loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear. This paper was accepted by Yinyu Ye, optimization."
            },
            {
                "arxivId": "1103.2372",
                "title": "A general class of zero-or-one inflated beta regression models",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-08.json",
        "arxivId": "2404.03968",
        "category": "q-fin",
        "title": "Regularization for electricity price forecasting",
        "abstract": "The most commonly used form of regularization typically involves defining the penalty function as a L1 or L2 norm. However, numerous alternative approaches remain untested in practical applications. In this study, we apply ten different penalty functions to predict electricity prices and evaluate their performance under two different model structures and in two distinct electricity markets. The study reveals that LQ and elastic net consistently produce more accurate forecasts compared to other regularization types. In particular, they were the only types of penalty functions that consistently produced more accurate forecasts than the most commonly used LASSO. Furthermore, the results suggest that cross-validation outperforms Bayesian information criteria for parameter optimization, and performs as well as models with ex-post parameter selection.",
        "references": [
            {
                "arxivId": "2308.15443",
                "title": "Combining predictive distributions of electricity prices. Does minimizing the CRPS lead to optimal decisions in day-ahead bidding?",
                "abstract": "Probabilistic price forecasting has recently gained attention in power trading because decisions based on such predictions can yield significantly higher profits than those made with point forecasts alone. At the same time, methods are being developed to combine predictive distributions, since no model is perfect and averaging generally improves forecasting performance. In this article, we address the question of whether using CRPS learning, a novel weighting technique minimizing the continuous ranked probability score (CRPS), leads to optimal decisions in day-ahead bidding. To this end, we conduct an empirical study using hourly day-ahead electricity prices from the German EPEX market. We find that increasing the diversity of an ensemble can have a positive impact on accuracy. At the same time, the higher computational cost of using CRPS learning compared to an equal-weighted aggregation of distributions is not offset by higher profits, despite significantly more accurate predictions."
            },
            {
                "arxivId": "2104.14204",
                "title": "Optimal bidding in hourly and quarter-hourly electricity price auctions: Trading large volumes of power with market impact and transaction costs",
                "abstract": null
            },
            {
                "arxivId": "2005.01365",
                "title": "Ensemble Forecasting for Intraday Electricity Prices: Simulating Trajectories",
                "abstract": null
            },
            {
                "arxivId": "1509.01966",
                "title": "Forecasting Electricity Spot Prices Using Lasso: On Capturing the Autoregressive Intraday Structure",
                "abstract": "In this paper we present a regression based model for day-ahead electricity spot prices. We estimate the considered linear regression model by the lasso estimation method. The lasso approach allows for many possible parameters in the model, but also shrinks and sparsifies the parameters automatically to avoid overfitting. Thus, it is able to capture the autoregressive intraday dependency structure of the electricity price well. We discuss in detail the estimation results which provide insights to the intraday behavior of electricity prices. We perform an out-of-sample forecasting study for several European electricity markets. The results illustrate well that the efficient lasso based estimation technique can exhibit advantages from two popular model approaches."
            },
            {
                "arxivId": "1502.06557",
                "title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic time series with applications to AR-ARCH type processes",
                "abstract": null
            },
            {
                "arxivId": "1104.3390",
                "title": "Improved variable selection with Forward-Lasso adaptive shrinkage",
                "abstract": "Recently, considerable interest has focused on variable selection methods in regression situations where the number of predictors, $p$, is large relative to the number of observations, $n$. Two commonly applied variable selection approaches are the Lasso, which computes highly shrunk regression coefficients, and Forward Selection, which uses no shrinkage. We propose a new approach, \"Forward-Lasso Adaptive SHrinkage\" (FLASH), which includes the Lasso and Forward Selection as special cases, and can be used in both the linear regression and the Generalized Linear Model domains. As with the Lasso and Forward Selection, FLASH iteratively adds one variable to the model in a hierarchical fashion but, unlike these methods, at each step adjusts the level of shrinkage so as to optimize the selection of the next variable. We first present FLASH in the linear regression setting and show that it can be fitted using a variant of the computationally efficient LARS algorithm. Then, we extend FLASH to the GLM domain and demonstrate, through numerous simulations and real world data sets, as well as some theoretical analysis, that FLASH generally outperforms many competing approaches."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-08.json",
        "arxivId": "2404.04105",
        "category": "q-fin",
        "title": "Judgment in macroeconomic output growth predictions: Efficiency, accuracy and persistence",
        "abstract": "The present study applies observations of individual predictions of the first three releases of the US output growth rate to evaluate how the applied judgment affects prediction efficiency and accuracy as well as if judgment is persistent. While the first two issues have been assessed in other studies, there is little evidence on the formation of judgment in macroeconomic projections. Most of the forecasters produce unbiased predictions, but employing the median Bloomberg projection as baseline, it turns out that judgment generally does not improve accuracy. There seems to be persistence in the judgment applied by forecasters in the sense that the sign of the adjustment in the first release prediction carries over to the projections of the two following revisions. One possible explanation is that forecasters use some kind of anchor-and-adjustment heuristic.",
        "references": [
            {
                "arxivId": "2205.04216",
                "title": "Forecast combinations: An over 50-year review",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2005.13033",
        "category": "q-fin",
        "title": "Stocks and cryptocurrencies: Antifragile or robust? A novel antifragility measure of the stock and cryptocurrency markets",
        "abstract": "In contrast with robust systems that resist noise or fragile systems that break with noise, antifragility is defined as a property of complex systems that benefit from noise or disorder. Here we define and test a simple measure of antifragility for complex dynamical systems. In this work we use our antifragility measure to analyze real data from return prices in the stock and cryptocurrency markets. Our definition of antifragility is the product of the return price and a perturbation. We explore different types of perturbations that typically arise from within the system. Our results suggest that for both the stock market and the cryptocurrency market, the tendency among the \u2018top performers\u2019 is to be robust rather than antifragile. It would be important to explore other possible definitions of antifragility to understand its role in financial markets and in complex dynamical systems in general.",
        "references": [
            {
                "arxivId": "2205.02082",
                "title": "Persistence in complex systems",
                "abstract": null
            },
            {
                "arxivId": "2103.05623",
                "title": "The physics of financial networks",
                "abstract": null
            },
            {
                "arxivId": "2011.14809",
                "title": "Collective dynamics of stock market efficiency",
                "abstract": null
            },
            {
                "arxivId": "2009.13076",
                "title": "Modeling and analysis of the effect of COVID-19 on the stock price: V and L-shape recovery",
                "abstract": null
            },
            {
                "arxivId": "2002.01571",
                "title": "Antifragility Predicts the Robustness and Evolvability of Biological Networks through Multi-Class Classification with a Convolutional Neural Network",
                "abstract": "Robustness and evolvability are essential properties to the evolution of biological networks. To determine if a biological network is robust and/or evolvable, it is required to compare its functions before and after mutations. However, this sometimes takes a high computational cost as the network size grows. Here, we develop a predictive method to estimate the robustness and evolvability of biological networks without an explicit comparison of functions. We measure antifragility in Boolean network models of biological systems and use this as the predictor. Antifragility occurs when a system benefits from external perturbations. By means of the differences of antifragility between the original and mutated biological networks, we train a convolutional neural network (CNN) and test it to classify the properties of robustness and evolvability. We found that our CNN model successfully classified the properties. Thus, we conclude that our antifragility measure can be used as a predictor of the robustness and evolvability of biological networks."
            },
            {
                "arxivId": "1902.11214",
                "title": "A Multilayer Structure Facilitates the Production of Antifragile Systems in Boolean Network Models",
                "abstract": "Antifragility is a property to not only resist stress and but also to benefit from it. Even though antifragile dynamics are found in various real-world complex systems where multiple subsystems interact with each other, the attribute has not been quantitatively explored yet in those complex systems which can be regarded as multilayer networks. Here we study how the multilayer structure affects the antifragility of the whole system. By comparing single-layer and multilayer Boolean networks based on our recently proposed antifragility measure, we found that the multilayer structure facilitated the production of antifragile systems. Our measure and findings can be utilized for many applications from understanding properties of biological systems with multilayer structures to designing more antifragile engineered systems."
            },
            {
                "arxivId": "1805.08550",
                "title": "Machine Learning the Cryptocurrency Market",
                "abstract": "Machine learning and AI-assisted trading have attracted growing interest for the past few years. Here, we use this approach to test the hypothesis that the inefficiency of the cryptocurrency market can be exploited to generate abnormal profits. We analyse daily data for 1,681 cryptocurrencies for the period between Nov. 2015 and Apr. 2018. We show that simple trading strategies assisted by state-of-the-art machine learning algorithms outperform standard benchmarks. Our results show that nontrivial, but ultimately simple, algorithmic mechanisms can help anticipate the short-term evolution of the cryptocurrency market."
            },
            {
                "arxivId": "1803.03088",
                "title": "Classification of cryptocurrency coins and tokens by the dynamics of their market capitalizations",
                "abstract": "We empirically verify that the market capitalizations of coins and tokens in the cryptocurrency universe follow power-law distributions with significantly different values for the tail exponent falling between 0.5 and 0.7 for coins, and between 1.0 and 1.3 for tokens. We provide a rationale for this, based on a simple proportional growth with birth and death model previously employed to describe the size distribution of firms, cities, webpages, etc. We empirically validate the model and its main predictions, in terms of proportional growth (Gibrat's Law) of the coins and tokens. Estimating the main parameters of the model, the theoretical predictions for the power-law exponents of coin and token distributions are in remarkable agreement with the empirical estimations, given the simplicity of the model. Our results clearly characterize coins as being \u2018entrenched incumbents\u2019 and tokens as an \u2018explosive immature ecosystem\u2019, largely due to massive and exuberant Initial Coin Offering activity in the token space. The theory predicts that the exponent for tokens should converge to 1 in the future, reflecting a more reasonable rate of new entrants associated with genuine technological innovations."
            },
            {
                "arxivId": "1304.1195",
                "title": "Persistence and first-passage properties in nonequilibrium systems",
                "abstract": "In this review, we discuss the persistence and the related first-passage properties in extended many-body nonequilibrium systems. Starting with simple systems with one or few degrees of freedom, such as random walk and random acceleration problems, we progressively discuss the persistence properties in systems with many degrees of freedom. These systems include spin models undergoing phase-ordering dynamics, diffusion equation, fluctuating interfaces, etc. Persistence properties are nontrivial in these systems as the effective underlying stochastic process is non-Markovian. Several exact and approximate methods have been developed to compute the persistence of such non-Markov processes over the last two decades, as reviewed in this article. We also discuss various generalizations of the local site persistence probability. Persistence in systems with quenched disorder is discussed briefly. Although the main emphasis of this review is on the theoretical developments on persistence, we briefly touch upon various experimental systems as well."
            },
            {
                "arxivId": "1208.1189",
                "title": "Mathematical definition, mapping, and detection of (anti)fragility",
                "abstract": "We provide a mathematical definition of fragility and antifragility as negative or positive sensitivity to a semi-measure of dispersion and volatility (a variant of negative or positive \"vega\") and examine the link to nonlinear effects. We integrate model error (and biases) into the fragile or antifragile context. Unlike risk, which is linked to psychological notions such as subjective preferences (hence cannot apply to a coffee cup) we offer a measure that is universal and concerns any object that has a probability distribution (whether such distribution is known or, critically, unknown). We propose a detection of fragility, robustness, and antifragility using a single \"fast-and-frugal\", model-free, probability free heuristic that also picks up exposure to model error. The heuristic lends itself to immediate implementation, and uncovers hidden risks related to company size, forecasting problems, and bank tail exposures (it explains the forecasting biases). While simple to implement, it improves on stress testing and bypasses the cillib flaws in Value-at-Risk."
            },
            {
                "arxivId": "1201.4845",
                "title": "Brownian motors and stochastic resonance.",
                "abstract": "We study the transport properties for a walker on a ratchet potential. The walker consists of two particles coupled by a bistable potential that allow the interchange of the order of the particles while moving through a one-dimensional asymmetric periodic ratchet potential. We consider the stochastic dynamics of the walker on a ratchet with an external periodic forcing, in the overdamped case. The coupling of the two particles corresponds to a single effective particle, describing the internal degree of freedom, in a bistable potential. This double-well potential is subjected to both a periodic forcing and noise and therefore is able to provide a realization of the phenomenon of stochastic resonance. The main result is that there is an optimal amount of noise where the amplitude of the periodic response of the system is maximum, a signal of stochastic resonance, and that precisely for this optimal noise, the average velocity of the walker is maximal, implying a strong link between stochastic resonance and the ratchet effect."
            },
            {
                "arxivId": "0807.1283",
                "title": "Artificial Brownian motors: Controlling transport on the nanoscale",
                "abstract": "In systems possessing spatial or dynamical symmetry breaking, Brownian motion combined with unbiased external input signals, deterministic and random alike, can assist directed motion of particles at submicron scales. In such cases, one speaks of ``Brownian motors.'' In this review the constructive role of Brownian motion is exemplified for various physical and technological setups, which are inspired by the cellular molecular machinery: the working principles and characteristics of stylized devices are discussed to show how fluctuations, either thermal or extrinsic, can be used to control diffusive particle transport. Recent experimental demonstrations of this concept are surveyed with particular attention to transport in artificial, i.e., nonbiological, nanopores, lithographic tracks, and optical traps, where single-particle currents were first measured. Much emphasis is given to two- and three-dimensional devices containing many interacting particles of one or more species; for this class of artificial motors, noise rectification results also from the interplay of particle Brownian motion and geometric constraints. Recently, selective control and optimization of the transport of interacting colloidal particles and magnetic vortices have been successfully achieved, thus leading to the new generation of microfluidic and superconducting devices presented here. The field has recently been enriched with impressive experimental achievements in building artificial Brownian motor devices that even operate within the quantum domain by harvesting quantum Brownian motion. Sundry akin topics include activities aimed at noise-assisted shuttling other degrees of freedom such as charge, spin, or even heat and the assembly of chemical synthetic molecular motors. This review ends with a perspective for future pathways and potential new applications."
            },
            {
                "arxivId": "0704.0773",
                "title": "Collective behavior of stock price movements in an emerging market.",
                "abstract": "To investigate the universality of the structure of interactions in different markets, we analyze the cross-correlation matrix C of stock price fluctuations in the National Stock Exchange (NSE) of India. We find that this emerging market exhibits strong correlations in the movement of stock prices compared to developed markets, such as the New York Stock Exchange (NYSE). This is shown to be due to the dominant influence of a common market mode on the stock prices. By comparison, interactions between related stocks, e.g., those belonging to the same business sector, are much weaker. This lack of distinct sector identity in emerging markets is explicitly shown by reconstructing the network of mutually interacting stocks. Spectral analysis of C for NSE reveals that, the few largest eigenvalues deviate from the bulk of the spectrum predicted by random matrix theory, but they are far fewer in number compared to, e.g., NYSE. We show this to be due to the relative weakness of intrasector interactions between stocks, compared to the market mode, by modeling stock price dynamics with a two-factor model. Our results suggest that the emergence of an internal structure comprising multiple groups of strongly coupled components is a signature of market development."
            },
            {
                "arxivId": "physics/0507020",
                "title": "Volatility, persistence, and survival in financial markets.",
                "abstract": "We study the temporal fluctuations in time-dependent stock prices (both individual and composite) as a stochastic phenomenon using general techniques and methods of nonequilibrium statistical mechanics. In particular, we analyze stock price fluctuations as a non-Markovian stochastic process using the first-passage statistical concepts of persistence and survival. We report the results of empirical measurements of the normalized qth-order correlation functions fq(t), survival probability S(t), and persistence probability P(t) for several stock market dynamical sets. We analyze both minute-to-minute and higher-frequency stock market recordings (i.e., with the sampling time deltat of the order of days). We find that the fluctuating stock price is multifractal and the choice of deltat has no effect on the qualitative multifractal behavior displayed by the 1/q dependence of the generalized Hurst exponent Hq associated with the power-law evolution of the correlation function fq(t) approximately tHq. The probability S(t) of the stock price remaining above the average up to time t is very sensitive to the total measurement time tm and the sampling time. The probability P(t) of the stock not returning to the initial value within an interval t has a universal power-law behavior P(t) approximately t(-theta), with a persistence exponent theta close to 0.5 that agrees with the prediction theta=1-H2. The empirical financial stocks also present an interesting feature found in turbulent fluids, the extended self-similarity."
            },
            {
                "arxivId": "cond-mat/9905305",
                "title": "Scaling of the distribution of fluctuations of financial market indices.",
                "abstract": "We study the distribution of fluctuations of the S&P 500 index over a time scale deltat by analyzing three distinct databases. Database (i) contains approximately 1 200 000 records, sampled at 1-min intervals, for the 13-year period 1984-1996, database (ii) contains 8686 daily records for the 35-year period 1962-1996, and database (iii) contains 852 monthly records for the 71-year period 1926-1996. We compute the probability distributions of returns over a time scale deltat, where deltat varies approximately over a factor of 10(4)-from 1 min up to more than one month. We find that the distributions for deltat<or= 4 d (1560 min) are consistent with a power-law asymptotic behavior, characterized by an exponent alpha approximately 3, well outside the stable L\u00e9vy regime 0<alpha<2. To test the robustness of the S&P result, we perform a parallel analysis on two other financial market indices. Database (iv) contains 3560 daily records of the NIKKEI index for the 14-year period 1984-1997, and database (v) contains 4649 daily records of the Hang-Seng index for the 18-year period 1980-1997. We find estimates of alpha consistent with those describing the distribution of S&P 500 daily returns. One possible reason for the scaling of these distributions is the long persistence of the autocorrelation function of the volatility. For time scales longer than (deltat)x approximately 4 d, our results are consistent with a slow convergence to Gaussian behavior."
            },
            {
                "arxivId": "cond-mat/9606123",
                "title": "Global Persistence Exponent for Nonequilibrium Critical Dynamics.",
                "abstract": "probability, p(t) \ufffd t \ufffd , that the global order parameter has not changed sign in the time interval t following a quench to the critical point from a disordered state. This exponent is calculated in mean-field theory, in the n = 1 limit of the O(n) model, to first order in \u01eb = 4 d, and for the 1-d Ising model. Numerical results are obtained for the 2-d Ising model. We argue that \u03b8 is a new independent exponent. For many years it was believed that critical phenomena were characterized by a set of three critical exponents, comprising two independent static exponents (other static exponents being related to these by scaling laws) and the dynamical exponent z. Then, quite recently, it was discovered that there is another dynamical exponent, the \u2018non-equilibrium\u2019 (or \u2018short-time\u2019) exponent \u03bb, needed to describe two-time correlations in a system relaxing to the critical state from a disordered initial condition [1,2]. It is natural to ask \u2018Are there any more independent critical exponents?\u2019. In this Letter we propose such an exponent \u2013 the \u2018persistence exponent\u2019 \u03b8 associated with the probability, p(t) \u223c t \u2212\u03b8 , that the global order parameter has not changed sign in time t following a quench to the critical point. We calculate \u03b8 in mean-field theory, in the n = \u221e limit of the O(n) model, to first order in \u01eb = 4\u2212d (d = dimension of space) and for the d = 1 Ising model. In fact, it turns out that all these results satisfy the scaling law \u03b8z = \u03bb\u2212d+1\u2212\u03b7/2, which can be derived on the assumption that the dynamics of the global order parameter is a Markov process. We shall argue, however, that this process is in general non-Markovian, so that \u03b8 is in general a new, non-trivial critical exponent. The persistence exponent \u03b8 was first introduced in the context of the non-equilibrium coarsening dynamics of systems at zero temperature [3,4]. In that context it describes the power-law decay, p(t) \u223c t \u2212\u03b8 , of the probability that the local order parameter \u03c6(x) has not changed sign during the time interval t after the quench to T = 0. Equivalently, it gives the fraction of space in which the order parameter has not changed sign up to time t. More generally, one can consider the probability p0(t1, t2) of no sign changes between t1 and t2. Scaling considerations suggest p0(t1, t2) = f(t1/t2) \u223c (t1/t2) \u03b8 for t2 \u226b t1. Exact solutions for one-dimensional systems [4,5] indicate that, in general, \u03b8 is a new non-trivial exponent for coarsening dynamics. Recently, we have shown that even the diffusion equation exhibits a nontrivial persistence exponent, and have developed a rather accurate approximate theory for this case [6]. The diffusion equation is itself a model of ordering dynamics, via the approximate theory of Ohta, Jasnow and Kawasaki (OJK) [7], and also describes, in its essential features, the ordering kinetics of the nonconserved O(n) model in the large-n limit [8]: The exponents \u03b8 for these systems (OJK and large-n) are just those of the diffusion equation. In this Letter we introduce and calculate the analogous exponent \u03b8 for non-equilibrium critical dynamics. In this case however, one needs to consider the global, rather than the local order parameter. This is because individual degrees of freedom (\u2018spins\u2019, say) are rapidly flipping so that the probability of not flipping in an interval t has an exponential tail. We shall see, however, that the probability for the global order parameter not to have flipped indeed decays as a power"
            },
            {
                "arxivId": "chao-dyn/9307006",
                "title": "Stochastic resonance",
                "abstract": "Stochastic resonance (SR) - a counter-intuitive phenomenon in which the signal due to a weak periodic force in a nonlinear system can be {\\it enhanced} by the addition of external noise - is reviewed. A theoretical approach based on linear response theory (LRT) is described. It is pointed out that, although the LRT theory of SR is by definition restricted to the small signal limit, it possesses substantial advantages in terms of simplicity, generality and predictive power. The application of LRT to overdamped motion in a bistable potential, the most commonly studied form of SR, is outlined. Two new forms of SR, predicted on the basis of LRT and subsequently observed in analogue electronic experiments, are described."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2201.01330",
        "category": "q-fin",
        "title": "The credit spread curve. I: Fundamental concepts, fitting, par-adjusted spread, and expected return",
        "abstract": "The notion of a credit spread curve is fundamental in fixed income investing, but in practice it is not `given' and needs to be constructed from bond prices either for a particular issuer, or for a sector rating-by-rating. Rather than attempting to fit spreads -- and as we discuss here, the Z-spread is unsuitable -- we fit parametrised survival curves. By deriving a valuation formula for a risky bond, we explain and avoid the problem that bonds with a high dollar price trade at a higher yield or spread than those with low dollar price (at the same maturity point), even though they do not necessarily offer better value. In fact, a concise treatment of this effect is elusive, and much of the academic literature on risky bond pricing, including a well-known paper by Duffie and Singleton (1997), is fundamentally incorrect. We then proceed to show how to calculate carry, rolldown and relative value for bonds/CDS. Also, once curve construction has been programmed and automated we can run it historically and assess the way a curve has moved over time. This provides the necessary grounding for econometric and arbitrage-free models of curve dynamics, which will be pursued in later work, as well as assessing how the perceived relative value of a particular instrument varies over time.",
        "references": [
            {
                "arxivId": "2006.11146",
                "title": "Credit migration: Generating generators.",
                "abstract": "Markovian credit migration models are a reasonably standard tool nowadays, but there are fundamental difficulties with calibrating them. We show how these are resolved using a simplified form of matrix generator and explain why risk-neutral calibration cannot be done without volatility information. We also show how to use elementary ideas from differential geometry to make general inferences about calibration stability."
            },
            {
                "arxivId": "1804.09056",
                "title": "Emerging Market Corporate Bonds as First-to-Default Baskets",
                "abstract": "Emerging market hard-currency bonds are an asset class of growing importance, and contain exposure to an EM sovereign and the underlying industry. The authors investigate how to model this as a modification of the well-known first-to-default (FtD) basket, using the structural model, and find the approach feasible."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2204.10275",
        "category": "q-fin",
        "title": "Do t-Statistic Hurdles Need to be Raised?",
        "abstract": "Many scholars have called for raising statistical hurdles to guard against false discoveries in academic publications. I show these calls may be difficult to justify empirically. Published data exhibit bias: results that fail to meet existing hurdles are often unobserved. These unobserved results must be extrapolated, which can lead to weak identification of revised hurdles. In contrast, statistics that can target only published findings (e.g. empirical Bayes shrinkage and the FDR) can be strongly identified, as data on published findings is plentiful. I demonstrate these results theoretically and in an empirical analysis of the cross-sectional return predictability literature.",
        "references": [
            {
                "arxivId": "2209.13623",
                "title": "Publication Bias in Asset Pricing Research",
                "abstract": "Researchers are more likely to share notable findings. As a result, published findings tend to overstate the magnitude of real-world phenomena. This bias is a natural concern for asset pricing research, which has found hundreds of return predictors and little consensus on their origins. Empirical evidence on publication bias comes from large scale meta-studies. Meta-studies of cross-sectional return predictability have settled on four stylized facts that demonstrate publication bias is not a dominant factor: (1) almost all findings can be replicated, (2) predictability persists out-of-sample, (3) empirical $t$-statistics are much larger than 2.0, and (4) predictors are weakly correlated. Each of these facts has been demonstrated in at least three meta-studies. Empirical Bayes statistics turn these facts into publication bias corrections. Estimates from three meta-studies find that the average correction (shrinkage) accounts for only 10 to 15 percent of in-sample mean returns and that the risk of inference going in the wrong direction (the false discovery rate) is less than 10%. Meta-studies also find that $t$-statistic hurdles exceed 3.0 in multiple testing algorithms and that returns are 30 to 50 percent weaker in alternative portfolio tests. These facts are easily misinterpreted as evidence of publication bias effects. We clarify these misinterpretations and others, including the conflating of ``mostly false findings'' with ``many insignificant findings,'' ``data snooping'' with ``liquidity effects,'' and ``failed replications'' with ``insignificant ad-hoc trading strategies.'' Meta-studies outside of the cross-sectional literature are rare. The four facts from cross-sectional meta-studies provide a framework for future research. We illustrate with a preliminary re-examination of equity premium predictability."
            },
            {
                "arxivId": "2208.09638",
                "title": "Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability",
                "abstract": "What is the purpose of pre-analysis plans, and how should they be designed? We propose a principal-agent model where a decision-maker relies on selective but truthful reports by an analyst. The analyst has data access, and non-aligned objectives. In this model, the implementation of statistical decision rules (tests, estimators) requires an incentive-compatible mechanism. We first characterize which decision rules can be implemented. We then characterize optimal statistical decision rules subject to implementability. We show that implementation requires pre-analysis plans. Focussing specifically on hypothesis tests, we show that optimal rejection rules pre-register a valid test for the case when all data is reported, and make worst-case assumptions about unreported data. Optimal tests can be found as a solution to a linear-programming problem."
            },
            {
                "arxivId": "2206.15365",
                "title": "Most Claimed Statistical Findings in Cross-Sectional Return Predictability Are Likely True",
                "abstract": "Harvey, Liu, and Zhu (2016) \u201cargue that most claimed research findings in financial economics are likely false.\u201d Surprisingly, their false discovery rate (FDR) estimates suggest most are true. I revisit their results by developing non- and semi-parametric FDR estimators that account for publication bias and empirical correlations. These estimators provide simple closed-form expressions and reliably produce an upper bound on the FDR in simulations that cluster-bootstrap from empirical predictor returns. Applying these estimators to the Chen-Zimmermann dataset of 205 predictors, I find that most claimed statistical findings in the cross-sectional predictability literature are likely true."
            },
            {
                "arxivId": "1709.07588",
                "title": "Abandon Statistical Significance",
                "abstract": "ABSTRACT We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm\u2014and the p-value thresholds intrinsic to it\u2014as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to \u201cban\u201d p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly."
            },
            {
                "arxivId": "1711.10527",
                "title": "Identification of and Correction for Publication Bias",
                "abstract": "Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study\u2019s results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.(JEL C13, C90, I23, J23, J38, L82)"
            },
            {
                "arxivId": "1709.10193",
                "title": "Forecasting with Dynamic Panel Data Models",
                "abstract": "This paper considers the problem of forecasting a collection of short time series using cross\u2010sectional information in panel data. We construct point predictors using Tweedie's formula for the posterior mean of heterogeneous coefficients under a correlated random effects distribution. This formula utilizes cross\u2010sectional information to transform the unit\u2010specific (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coefficients. We show that the risk of a predictor based on a nonparametric kernel estimate of the Tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated random effects distribution as known (ratio optimality). Our empirical Bayes predictor performs well compared to various competitors in a Monte Carlo study. In an empirical application, we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions."
            },
            {
                "arxivId": "0808.0597",
                "title": "Comment: Microarrays, Empirical Bayes and the Two-Groups Model",
                "abstract": "Abstract. Brad Efron\u2019s paper has inspired a return to the ideas be-hind Bayes, frequency and empirical Bayes. The latter preferably wouldnot be limited to exchangeable models for the data and hyperparam-eters. Parallels are revealed between microarray analyses and pro\ufb01lingof hospitals, with advances suggesting more decision modeling for geneidenti\ufb01cation also. Then good multilevel and empirical Bayes modelsfor random e\ufb00ects should be sought when regression toward the meanis anticipated. Key words and phrases: Bayes, frequency, interval estimation, ex-changeable, general model, random e\ufb00ects.1. FREQUENCY, BAYES, EMPIRICAL BAYESAND A GENERAL MODELBrad Efron\u2019s two-groups approach and the empir-ical null (\u201cnull\u201d refers to a distribution, not to ahypothesis) extension of his local fdr addresses test-ing many hypotheses simultaneously, with model-ing enabled by the repeated presence of many simi-lar problems. He assumes two-level models for ran-dom e\ufb00ects, developing theory by drawing on andcombining ideas from frequency, Bayesian and em-pirical Bayesian perspectives. The last half-centuryin statistics has seen exciting developments frommany perspectives for simultaneous estimation ofrandom e\ufb00ects, but there has been little explicit par-allel work on the complementary problem of hypoth-esis testing. That changes in Brad\u2019s paper,especiallyfor testing many hypotheses when exchangeabilityrestrictions are plausible.\u201cEmpirical Bayes\u201d is in the paper\u2019s title, said inSection 3 to be a \u201cbipolar\u201d methodology that draws"
            },
            {
                "arxivId": "math/0406519",
                "title": "A stochastic process approach to false discovery control",
                "abstract": "This paper extends the theory of false discovery rates (FDR) pioneered by Benjamini and Hochberg [J. Roy. Statist. Soc. Ser B 57 (1995) 289-300]. We develop a framework in which the False Discovery Proportion (FDP)-the number of false rejections divided by the number of rejections-is treated as a stochastic process. After obtaining the limiting distribution of the process, we demonstrate the validity of a class of procedures for controlling the False Discovery Rate (the expected FDP). We construct a confidence envelope for the whole FDP process. From these envelopes we derive confidence thresholds, for controlling the quantiles of the distribution of the FDP as well as controlling the number of false discoveries. We also investigate methods for estimating the p-value distribution."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2207.04794",
        "category": "q-fin",
        "title": "LASSO principal component averaging: A fully automated approach for point forecast pooling",
        "abstract": null,
        "references": [
            {
                "arxivId": "1811.08604",
                "title": "The value of forecasts: Quantifying the economic gains of accurate quarter-hourly electricity price forecasts",
                "abstract": null
            },
            {
                "arxivId": "1805.06649",
                "title": "Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks",
                "abstract": null
            },
            {
                "arxivId": "1402.7027",
                "title": "Efficient modeling and forecasting of electricity spot prices",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2210.10585",
        "category": "q-fin",
        "title": "The Minimum Wage as an Anchor: Effects on Determinations of Fairness by Humans and AI",
        "abstract": "I study the role of minimum wage as an anchor for judgements of the fairness of wages by both human subjects and artificial intelligence (AI). Through surveys of human subjects enrolled in the crowdsourcing platform Prolific.co and queries submitted to the OpenAI's language model GPT-3, I test whether the numerical response for what wage is deemed fair for a particular job description changes when respondents and GPT-3 are prompted with additional information that includes a numerical minimum wage, whether realistic or unrealistic, relative to a control where no minimum wage is stated. I find that the minimum wage influences the distribution of responses for the wage considered fair by shifting the mean response toward the minimum wage, thus establishing the minimum wage's role as an anchor for judgements of fairness. However, for unrealistically high minimum wages, namely $50 and $100, the distribution of responses splits into two distinct modes, one that approximately follows the anchor and one that remains close to the control, albeit with an overall upward shift towards the anchor. The anchor exerts a similar effect on the AI bot; however, the wage that the AI bot perceives as fair exhibits a systematic downward shift compared to human subjects' responses. For unrealistic values of the anchor, the responses of the bot also split into two modes but with a smaller proportion of the responses adhering to the anchor compared to human subjects. As with human subjects, the remaining responses are close to the control group for the AI bot but also exhibit a systematic shift towards the anchor. During experimentation, I noted some variability in the bot responses depending on small perturbations of the prompt, so I also test variability in the bot's responses with respect to more meaningful differences in gender and race cues in the prompt, finding anomalies in the distribution of responses.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2211.16643",
        "category": "q-fin",
        "title": "Security Issuance, Institutional Investors and Quid Pro Quo: Insights from SPACs",
        "abstract": "Security issuance through intermediaries is subject to informational and agency-related frictions. However, separating their effects on securities has been difficult. We estimate those effects separately using SPAC data. To that end, we identify\"premium\"investors who produce value-relevant information. Their participation is associated with lower liquidation risk and higher returns. In contrast,\"non-premium\"investors engage only in quid pro quo arrangements. They receive high returns from an intermediary (quid) for participating in weaker future deals initiated by that intermediary (quo). Thus, quid pro quo is not pure agency cost; it includes transfers enabling more firms to go public.",
        "references": [
            {
                "arxivId": "2102.05739",
                "title": "Coordinated Capacity Reductions and Public Communication in the Airline Industry",
                "abstract": "We investigate whether legacy U.S. airlines communicated via earnings calls to coordinate with other legacy airlines in offering fewer seats on competitive routes. To this end, we first use text analytics to build a novel dataset on communication among airlines about their capacity choices. Estimates from our preferred specification show that when all legacy airlines in a market discuss the concept of \u201ccapacity discipline,\u201d they reduce offered seats by 1.79%. We verify that this reduction materializes only when airlines communicate concurrently, and that it cannot be explained by other possibilities, including that airlines are simply announcing to investors their unilateral intentions to reduce capacity, and then following through on those announcements. Additional results from conditional-exogeneity tests and control function estimates confirm our interpretation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2304.14870",
        "category": "q-fin",
        "title": "Using a Deep Learning Model to Simulate Human Stock Trader's Methods of Chart Analysis",
        "abstract": "Despite the efficient market hypothesis, many studies suggest the existence of inefficiencies in the stock market leading to the development of techniques to gain above-market returns. Systematic trading has undergone significant advances in recent decades with deep learning schemes emerging as a powerful tool for analyzing and predicting market behavior. In this paper, a method is proposed that is inspired by how professional technical analysts trade. This scheme looks at stock prices of the previous 600 days and predicts whether the stock price will rise or fall 10% or 20% within the next D days. The proposed method uses the Resnet's (a deep learning model) skip connections and logits to increase the probability of the prediction. The model was trained and tested using historical data from both the Korea and US stock markets. The backtest is done using the data from 2020 to 2022. Using the proposed method for the Korea market it gave return of 75.36% having Sharpe ratio of 1.57, which far exceeds the market return by 36% and 0.61, respectively. On the US market it gives total return of 27.17% with Sharpe ratio of 0.61, which outperforms other benchmarks such as NASDAQ, S&P500, DOW JONES index by 17.69% and 0.27, respectively.",
        "references": [
            {
                "arxivId": "2004.10178",
                "title": "Forecasting directional movements of stock prices for intraday trading using LSTM and random forests",
                "abstract": null
            },
            {
                "arxivId": "1903.12258",
                "title": "Using Deep Learning Neural Networks and Candlestick Chart Representation to Predict Stock Market",
                "abstract": "Stock market prediction is still a challenging problem because there are many factors effect to the stock market price such as company news and performance, industry performance, investor sentiment, social media sentiment and economic factors. This work explores the predictability in the stock market using Deep Convolutional Network and candlestick charts. The outcome is utilized to design a decision support framework that can be used by traders to provide suggested indications of future stock price direction. We perform this work using various types of neural networks like convolutional neural network, residual network and visual geometry group network. From stock market historical data, we converted it to candlestick charts. Finally, these candlestick charts will be feed as input for training a Convolutional Neural Network model. This Convolutional Neural Network model will help us to analyze the patterns inside the candlestick chart and predict the future movements of stock market. The effectiveness of our method is evaluated in stock market prediction with a promising results 92.2% and 92.1% accuracy for Taiwan and Indonesian stock market dataset respectively. The constructed model have been implemented as a web-based system freely available at http://140.138.155.216/deepcandle/ for predicting stock market using candlestick chart and deep learning neural networks."
            },
            {
                "arxivId": "1811.07522",
                "title": "Practical Deep Reinforcement Learning Approach for Stock Trading",
                "abstract": "Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns."
            },
            {
                "arxivId": "1611.06455",
                "title": "Time series classification from scratch with deep neural networks: A strong baseline",
                "abstract": "We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics."
            },
            {
                "arxivId": "1512.03385",
                "title": "Deep Residual Learning for Image Recognition",
                "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2305.09472",
        "category": "q-fin",
        "title": "Equity Protection Swaps: A New Type of Investment Insurance for Holders of Superannuation Accounts",
        "abstract": "We propose to develop a new class of investment insurance products for holders of superannuation accounts in Australia, which we tentatively call equity protection swaps (EPSs). An EPS is a standalone financial derivative, which is reminiscent of a total return swap but also shares some features with the variable annuity known as the registered index-linked annuity (RILA). The buyer of an EPS obtains partial protection against losses on a reference portfolio and, in exchange, agrees to share portfolio gains with the insurance provider if the realized return on a reference portfolio is above a predetermined threshold. Formally, a generic EPS consists of protection and fee legs with participation rates agreed upon by the provider and holder. A general fair pricing formula for an EPS is obtained by considering a static hedging strategy based on traded European options. It is argued that to make the contract appealing to holders, the provider should select appropriate protection and fee rates that make the fair premium at the contract's inception equal to zero. A numerical study based on the Black-Scholes model and empirical tests based on market data for S\\&P~500 and S&P/ASX~200 indices for 2020-2022 demonstrates the benefits of an EPS as an efficient investment insurance tool for superannuation accounts.",
        "references": [
            {
                "arxivId": "1411.5453",
                "title": "Valuation of Variable Annuities with Guaranteed Minimum Withdrawal and Death Benefits via Stochastic Control Optimization",
                "abstract": null
            },
            {
                "arxivId": "1410.8609",
                "title": "Fast Numerical Method for Pricing of Variable Annuities with Guaranteed Minimum Withdrawal Benefit Under Optimal Withdrawal Strategy",
                "abstract": "A variable annuity contract with Guaranteed Minimum Withdrawal Benefit (GMWB) promises to return the entire initial investment through cash withdrawals during the policy life plus the remaining account balance at maturity, regardless of the portfolio performance. Under the optimal withdrawal strategy of a policyholder, the pricing of variable annuities with GMWB becomes an optimal stochastic control problem. So far in the literature these contracts have only been evaluated by solving partial differential equations (PDE) using the finite difference method. The well-known Least-Squares or similar Monte Carlo methods cannot be applied to pricing these contracts because the paths of the underlying wealth process are affected by optimal cash withdrawals (control variables) and thus cannot be simulated forward in time. In this paper we present a very efficient new algorithm for pricing these contracts in the case when transition density of the underlying asset between withdrawal dates or its moments are known. This algorithm relies on computing the expected contract value through a high order Gauss-Hermite quadrature applied on a cubic spline interpolation. Numerical results from the new algorithm for a series of GMWB contract are then presented, in comparison with results using the finite difference method solving corresponding PDE. The comparison demonstrates that the new algorithm produces results in very close agreement with those of the finite difference method, but at the same time it is significantly faster; virtually instant results on a standard desktop PC."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2306.09862",
        "category": "q-fin",
        "title": "DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting",
        "abstract": "Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distribution in favor of profitable updates. Complemented by data adaptation, we can confidently adapt the model parameters under mitigated distribution shifts. We cast each incremental learning task as a meta-learning task and automatically optimize the adapters for desirable data adaptation and parameter initialization. Experiments on real-world stock datasets demonstrate that DoubleAdapt achieves state-of-the-art predictive performance and shows considerable efficiency.",
        "references": [
            {
                "arxivId": "2208.07239",
                "title": "ROLAND: Graph Learning Framework for Dynamic Graphs",
                "abstract": "Graph Neural Networks (GNNs) have been successfully applied to many real-world static graphs. However, the success of static graphs has not fully translated to dynamic graphs due to the limitations in model design, evaluation settings, and training strategies. Concretely, existing dynamic GNNs do not incorporate state-of-the-art designs from static GNNs, which limits their performance. Current evaluation settings for dynamic GNNs do not fully reflect the evolving nature of dynamic graphs. Finally, commonly used training methods for dynamic GNNs are not scalable. Here we propose ROLAND, an effective graph representation learning framework for real-world dynamic graphs. At its core, the ROLAND framework can help researchers easily repurpose any static GNN to dynamic graphs. Our insight is to view the node embeddings at different GNN layers as hierarchical node states and then recurrently update them over time. We then introduce a live-update evaluation setting for dynamic graphs that mimics real-world use cases, where GNNs are making predictions and being updated on a rolling basis. Finally, we propose a scalable and efficient training approach for dynamic GNNs via incremental training and meta-learning. We conduct experiments over eight different dynamic graph datasets on future link prediction tasks. Models built using the ROLAND framework achieve on average 62.7% relative mean reciprocal rank (MRR) improvement over state-of-the-art baselines under the standard evaluation settings on three datasets. We find state-of-the-art baselines experience out-of-memory errors for larger datasets, while ROLAND can easily scale to dynamic graphs with 56 million edges. After re-implementing these baselines using the ROLAND training strategy, ROLAND models still achieve on average 15.5% relative MRR improvement over the baselines."
            },
            {
                "arxivId": "2201.04038",
                "title": "DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation",
                "abstract": "In many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as the concept drift in the literature. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work. In this paper, we propose a novel method DDG-DA, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data. We conduct experiments on three real-world tasks (forecasting on stock price trend, electricity load and solar irradiance) and obtained significant improvement on multiple widely-used models."
            },
            {
                "arxivId": "2110.13716",
                "title": "HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared Information",
                "abstract": "Stock trend forecasting, which forecasts stock prices' future trends, plays an essential role in investment. The stocks in a market can share information so that their stock prices are highly correlated. Several methods were recently proposed to mine the shared information through stock concepts (e.g., technology, Internet Retail) extracted from the Web to improve the forecasting results. However, previous work assumes the connections between stocks and concepts are stationary, and neglects the dynamic relevance between stocks and concepts, limiting the forecasting results. Moreover, existing methods overlook the invaluable shared information carried by hidden concepts, which measure stocks' commonness beyond the manually defined stock concepts. To overcome the shortcomings of previous work, we proposed a novel stock trend forecasting framework that can adequately mine the concept-oriented shared information from predefined concepts and hidden concepts. The proposed framework simultaneously utilize the stock's shared information and individual information to improve the stock trend forecasting performance. Experimental results on the real-world tasks demonstrate the efficiency of our framework on stock trend forecasting. The investment simulation shows that our framework can achieve a higher investment return than the baselines."
            },
            {
                "arxivId": "2108.04443",
                "title": "AdaRNN: Adaptive Learning and Forecasting of Time Series",
                "abstract": "Time series has wide applications in the real world and is known to be difficult to forecast. Since its statistical properties change over time, its distribution also changes temporally, which will cause severe distribution shift problem to existing methods. However, it remains unexplored to model the time series in the distribution perspective. In this paper, we term this as Temporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to tackle the TCS problem by building an adaptive model that generalizes well on the unseen test data. AdaRNN is sequentially composed of two novel algorithms. First, we propose Temporal Distribution Characterization to better characterize the distribution information in the TS. Second, we propose Temporal Distribution Matching to reduce the distribution mismatch in TS to learn the adaptive TS model. AdaRNN is a general framework with flexible distribution distances integrated. Experiments on human activity recognition, air quality prediction, and financial analysis show that AdaRNN outperforms the latest methods by a classification accuracy of 2.6% and significantly reduces the RMSE by 9.0%. We also show that the temporal distribution matching algorithm can be extended in Transformer structure to boost its performance."
            },
            {
                "arxivId": "2106.12950",
                "title": "Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport",
                "abstract": "Successful quantitative investment usually relies on precise predictions of the future movement of the stock price. Recently, machine learning based solutions have shown their capacity to give more accurate stock prediction and become indispensable components in modern quantitative investment systems. However, the i.i.d. assumption behind existing methods is inconsistent with the existence of diverse trading patterns in the stock market, which inevitably limits their ability to achieve better stock prediction performance. In this paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to empower existing stock prediction models with the ability to model multiple stock trading patterns. Essentially, TRA is a lightweight module that consists of a set of independent predictors for learning multiple patterns as well as a router to dispatch samples to different predictors. Nevertheless, the lack of explicit pattern identifiers makes it quite challenging to train an effective TRA-based model. To tackle this challenge, we further design a learning algorithm based on Optimal Transport (OT) to obtain the optimal sample to predictor assignment and effectively optimize the router with such assignment through an auxiliary loss term. Experiments on the real-world stock ranking task show that compared to the state-of-the-art baselines, e.g., Attention LSTM and Transformer, the proposed method can improve information coefficient (IC) from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used in this work are publicly available2: https://github.com/microsoft/qlib."
            },
            {
                "arxivId": "2102.07372",
                "title": "REST: Relational Event-driven Stock Trend Forecasting",
                "abstract": "Stock trend forecasting, aiming at predicting the stock future trends, is crucial for investors to seek maximized profits from the stock market. Many event-driven methods utilized the events extracted from news, social media, and discussion board to forecast the stock trend in recent years. However, existing event-driven methods have two main shortcomings: 1) overlooking the influence of event information differentiated by the stock-dependent properties; 2) neglecting the effect of event information from other related stocks. In this paper, we propose a relational event-driven stock trend forecasting (REST) framework, which can address the shortcoming of existing methods. To remedy the first shortcoming, we propose to model the stock context and learn the effect of event information on the stocks under different contexts. To address the second shortcoming, we construct a stock graph and design a new propagation layer to propagate the effect of event information from related stocks. The experimental studies on the real-world data demonstrate the efficiency of our REST framework. The results of investment simulation show that our framework can achieve a higher return of investment than baselines."
            },
            {
                "arxivId": "2009.11189",
                "title": "Qlib: An AI-oriented Quantitative Investment Platform",
                "abstract": "Quantitative investment aims to maximize the return and minimize the risk in a sequential trading period over a set of financial instruments. Recently, inspired by rapid development and great potential of AI technologies in generating remarkable innovation in quantitative investment, there has been increasing adoption of AI-driven workflow for quantitative research and practical investment. In the meantime of enriching the quantitative investment methodology, AI technologies have raised new challenges to the quantitative investment system. Particularly, the new learning paradigms for quantitative investment call for an infrastructure upgrade to accommodate the renovated workflow; moreover, the data-driven nature of AI technologies indeed indicates a requirement of the infrastructure with more powerful performance; additionally, there exist some unique challenges for applying AI technologies to solve different tasks in the financial scenarios. To address these challenges and bridge the gap between AI technologies and quantitative investment, we design and develop Qlib that aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment."
            },
            {
                "arxivId": "2005.13258",
                "title": "How to Retrain Recommender System?: A Sequential Meta-Learning Method",
                "abstract": "Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community. Our first belief is that retraining the model on historical data is unnecessary, since the model has been trained on it before. Nevertheless, normal training on new data only may easily cause overfitting and forgetting issues, since the new data is of a smaller scale and contains fewer information on long-term user preference. To address this dilemma, we propose a new training method, aiming to abandon the historical data during retraining through learning to transfer the past training experience.Specifically, we design a neural network-based transfer component, which transforms the old model to a new model that is tailored for future recommendations. To learn the transfer component well, we optimize the \"future performance'' -- i.e., the recommendation accuracy evaluated in the next time period. Our Sequential Meta-Learning(SML) method offers a general training paradigm that is applicable to any differentiable model. We demonstrate SML on matrix factorization and conduct experiments on two real-world datasets. Empirical results show that SML not only achieves significant speed-up, but also outperforms the full model retraining in recommendation accuracy, validating the effectiveness of our proposals. We release our codes at: https://github.com/zyang1580/SML."
            },
            {
                "arxivId": "2003.10580",
                "title": "Meta Pseudo Labels",
                "abstract": "We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art [16]. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student\u2019s performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student.1"
            },
            {
                "arxivId": "2003.03284",
                "title": "TaskNorm: Rethinking Batch Normalization for Meta-Learning",
                "abstract": "Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms."
            },
            {
                "arxivId": "1906.05201",
                "title": "Task Agnostic Continual Learning via Meta Learning",
                "abstract": "While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering -- i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism."
            },
            {
                "arxivId": "1812.07671",
                "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL",
                "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances."
            },
            {
                "arxivId": "1802.01569",
                "title": "Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization",
                "abstract": "Significance Artificial neural networks can suffer from catastrophic forgetting, in which learning a new task causes the network to forget how to perform previous tasks. While previous studies have proposed various methods that can alleviate forgetting over small numbers (\u2a7d10) of tasks, it is uncertain whether they can prevent forgetting across larger numbers of tasks. In this study, we propose a neuroscience-inspired scheme, called \u201ccontext-dependent gating,\u201d in which mostly nonoverlapping sets of units are active for any one task. Importantly, context-dependent gating has a straightforward implementation, requires little extra computational overhead, and when combined with previous methods to stabilize connection weights, can allow networks to maintain high performance across large numbers of sequentially presented tasks. Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks (ANNs) on new tasks typically causes them to forget previously learned tasks. This phenomenon is the result of \u201ccatastrophic forgetting,\u201d in which training an ANN disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of ANNs that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows ANNs to maintain high performance across large numbers of sequentially presented tasks, particularly when combined with weight stabilization. We show that this method works for both feedforward and recurrent network architectures, trained using either supervised or reinforcement-based learning. This suggests that using multiple, complementary methods, akin to what is believed to occur in the brain, can be a highly effective strategy to support continual learning."
            },
            {
                "arxivId": "1712.02136",
                "title": "Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction",
                "abstract": "Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return."
            },
            {
                "arxivId": "1704.02971",
                "title": "A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction",
                "abstract": "The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction."
            },
            {
                "arxivId": "1703.03400",
                "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
                "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
            },
            {
                "arxivId": "1607.08022",
                "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
                "abstract": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at this https URL. Full paper can be found at arXiv:1701.02096."
            },
            {
                "arxivId": "1412.3555",
                "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2309.02447",
        "category": "q-fin",
        "title": "Economic Complexity Limits Accuracy of Price Probability Predictions by Gaussian Distributions",
        "abstract": "We discuss the economic reasons why the predictions of price and return statistical moments in the coming decades, in the best case, will be limited by their averages and volatilities. That limits the accuracy of the forecasts of price and return probabilities by Gaussian distributions. The economic origin of these restrictions lies in the fact that the predictions of the market-based n-th statistical moments of price and return for n=1,2,.., require the description of the economic variables of the n-th order that are determined by sums of the n-th degrees of values or volumes of market trades. The lack of existing models that describe the evolution of the economic variables determined by the sums of the 2nd degrees of market trades results in the fact that even predictions of the volatilities of price and return are very uncertain. One can ignore existing economic barriers that we highlight but cannot overcome or resolve them. The accuracy of predictions of price and return probabilities substantially determines the reliability of asset pricing models and portfolio theories. The restrictions on the accuracy of predictions of price and return statistical moments reduce the reliability and veracity of modern asset pricing and portfolio theories.",
        "references": [
            {
                "arxivId": "2302.07935",
                "title": "Market-Based Probability of Stock Returns",
                "abstract": "Markets possess all available information on stock returns. The randomness of market trade determines the statistics of stock returns. This paper describes the dependence of the first four market-based statistical moments of stock returns on statistical moments and correlations of current and past trade values. The mean return of trades during the averaging period coincides with Markowitz's definition of portfolio value weighted return. We derive the market-based volatility of return and return-value correlations. We present approximations of the characteristic functions and probability measures of stock return by a finite number of market-based statistical moments. To forecast market-based average return or volatility of return, one should predict the statistical moments and correlations of current and past market trade values at the same time horizon."
            },
            {
                "arxivId": "2208.07839",
                "title": "Why Economic Theories and Policies Fail? Unnoticed Variables and Overlooked Economics",
                "abstract": "Accuracy of economic theories and efficiency of economic policy strictly depend on the choice of the economic variables and processes mostly liable for description of economic reality. That states the general problem of assessment of any possible economic variables and processes chargeable for economic evolution. We show that economic variables and processes described by current economic theories constitute only a negligible fraction of factors responsible for economic dynamics. We consider numerous unnoted economic variables and overlooked economic processes those determine the states and predictions of the real economics. We regard collective economic variables, collective transactions and expectations, mean risks of economic variables and transactions, collective velocities and flows of economic variables, transactions and expectations as overlooked factors of economic evolution. We introduce market-based probability of the asset price and consider unnoticed influence of market stochasticity on randomness of macroeconomic variables. We introduce economic domain composed by continuous numeric risk grades and outline that the bounds of the economic domain result in unnoticed inherent cyclical motion of collective variables, transactions and expectations those are responsible for observed business cycles. Our treatment of unnoticed and overlooked factors of theoretical economics and policy decisions preserves a wide field of studies for many decades for academic researchers, economic authorities and high-level politicians."
            },
            {
                "arxivId": "2205.07256",
                "title": "Market-Based Asset Price Probability",
                "abstract": "We consider the randomness of market trade values and volumes as the origin of asset price stochasticity. We define the first four market-based price statistical moments that depend on statistical moments and correlations of market trade values and volumes. Market-based price statistical moments coincide with conventional frequency-based ones if all trade volumes are constant during the time averaging interval. We present approximations of market-based price probability by a finite number of price statistical moments. We consider the consequences of the use of market-based price statistical moments for asset-pricing models and Value-at-Risk. We show that the use of volume weighted average price results in zero price-volume correlations. We derive market-based correlations between price and squares of volume and between squares of price and volume. To forecast market-based price volatility at horizon T one should predict the first two statistical moments of market trade values and volumes and their correlations at the same horizon T."
            },
            {
                "arxivId": "2112.04566",
                "title": "Theoretical Economics and the Second-Order Economic Theory. What is it?",
                "abstract": "The economic and financial variables of economic agents determine macroeconomic variables. Current models consider agents' variables that are determined by the sums of values and volumes of agents' trades during some time interval {\\Delta}. We call them first-order economic variables. We describe how the volatilities and correlations of market trade values and volumes determine price volatility. We argue that such a link requests consideration of agents' economic variables of the second order that are composed of sums of squares of agents' transactions during {\\Delta}. Almost any variable of the first order should be complemented by its second-order pair. Respectively, the sums of agents' second-order variables introduce macroeconomic variables of the second order. The description of the first- and second-order macroeconomic variables establishes the subject of second-order economic theory. We highlight that the complexity of second-order economic theory essentially restricts any hopes for precise predictions of price probability and, at best, could provide estimates of price volatility. That limits the predictions of price probability to Gauss's approximations only."
            },
            {
                "arxivId": "2105.13903",
                "title": "Three Remarks On Asset Pricing",
                "abstract": "We consider the time interval \u0394 during which the market trade time-series are averaged as the key factor of the consumption-based asset-pricing model that causes modification of the basic pricing equation. The duration of \u0394 determines Taylor series of investor\u2019s utility over current and future values of consumption. We present consumption at current and future moments as sums of their mean values and perturbations during \u0394 of the price at current moment t and perturbations of the payoff at day t+1. For linear and quadratic Taylor series approximations of the basic equation we obtain new relations on mean price, mean payoff, their volatilities, skewness and amount of asset \u03bemax that delivers max to investor\u2019s utility. The stochasticity of market trade time-series defines random properties of the asset price time-series during \u0394. We introduce new market-based price probability measure entirely determined by frequency-based probability measures of the market trade value and volume. The conventional frequency-based price probability is a very special case of the market-based price probability measure when all trade volumes during \u0394 equal unit. Prediction of the market-based price probability at horizon T equals forecast of the market trade value and volume probabilities at same horizon. The similar Taylor series and probability measures alike to market-based price probability can be used as approximations of different versions of asset pricing, financial and economic models that describe relations between economic and financial variables averaged during some time interval \u0394."
            },
            {
                "arxivId": "2012.04506",
                "title": "Business Cycles as Collective Risk Fluctuations",
                "abstract": "We suggest use continuous numerical risk grades [0,1] of R for a single risk or the unit cube in Rn for n risks as the economic domain. We consider risk ratings of economic agents as their coordinates in the economic domain. Economic activity of agents, economic or other factors change agents risk ratings and that cause motion of agents in the economic domain. Aggregations of variables and transactions of individual agents in small volume of economic domain establish the continuous economic media approximation that describes collective variables, transactions and their flows in the economic domain as functions of risk coordinates. Any economic variable A(t,x) defines mean risk XA(t) as risk weighted by economic variable A(t,x). Collective flows of economic variables in bounded economic domain fluctuate from secure to risky area and back. These fluctuations of flows cause time oscillations of macroeconomic variables A(t) and their mean risks XA(t) in economic domain and are the origin of any business and credit cycles. We derive equations that describe evolution of collective variables, transactions and their flows in the economic domain. As illustration we present simple self-consistent equations of supply-demand cycles that describe fluctuations of supply, demand and their mean risks."
            },
            {
                "arxivId": "2009.14278",
                "title": "Price, Volatility and the Second-Order Economic Theory",
                "abstract": "We introduce the new price probability measure, which entirely depends on the probability measures of the value and the volume of the market trades. We define the nth statistical moment of the price as the ratio of the nth statistical moment of the value to the nth statistical moment of the volume of all trades performed during an averaging time interval \u0394. The set of the price statistical moments determines the price characteristic function and its Fourier transform defines the price probability measure. The price volatility depends on the 1st and the 2nd statistical moments of the value and the volume of the trades. The prediction of the price volatility requires a description of the sums of squares of the value and the volume of the market trades during the interval \u0394 and we call it the second-order economic theory. To develop that theory, we introduce numerical continuous risk ratings and distribute the agents by the risk ratings as coordinates. Based on distributions of the agents by the risk coordinates, we introduce a continuous economic media approximation that describes the collective trades. The agents perform the trades under the action of their expectations. We model the mutual impact of the expectations and the trades and derive equations that describe their evolution. To illustrate the benefits of our approach, in a linear approximation we describe perturbations of the mean price, the mean square price and the price volatility as functions of the first and the second-degree trades\u2019 disturbances."
            },
            {
                "arxivId": "1709.00282",
                "title": "Econophysics of Business Cycles: Aggregate Economic Fluctuations, Mean Risks and Mean Square Risks",
                "abstract": "This paper presents hydrodynamic-like model of business cycles aggregate fluctuations of economic and financial variables. We model macroeconomics as ensemble of economic agents on economic space and agent's risk ratings play role of their coordinates. Sum of economic variables of agents with coordinate x define macroeconomic variables as functions of time and coordinates x. We describe evolution and interactions between macro variables on economic space by hydrodynamic-like equations. Integral of macro variables over economic space defines aggregate economic or financial variables as functions of time t only. Hydrodynamic-like equations define fluctuations of aggregate variables. Motion of agents from low risk to high risk area and back define the origin for repeated fluctuations of aggregate variables. Economic or financial variables on economic space may define statistical moments like mean risk, mean square risk and higher. Fluctuations of statistical moments describe phases of financial and economic cycles. As example we present a simple model relations between Assets and Revenue-on-Assets and derive hydrodynamic-like equations that describe evolution and interaction between these variables. Hydrodynamic-like equations permit derive systems of ordinary differential equations that describe fluctuations of aggregate Assets, Assets mean risks and Assets mean square risks. Our approach allows describe business cycle aggregate fluctuations induced by interactions between any number of economic or financial variables."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2310.09098",
        "category": "q-fin",
        "title": "Growth, Poverty Trap and Escape",
        "abstract": "The well-known Solow growth model is the workhorse model of the theory of economic growth, which studies capital accumulation in a model economy as a function of time with capital stock, labour and technology efficiency as the basic ingredients. The capital is assumed to be in the form of manufacturing equipments and materials. Two important parameters of the model are: the saving fraction $s$ of the output of a production function and the technology efficiency parameter $A$, appearing in the production function. The saved fraction of the output is fully invested in the generation of new capital and the rest is consumed. The capital stock also depreciates as a function of time due to the wearing out of old capital and the increase in the size of the labour population. We propose a stochastic Solow growth model assuming the saving fraction to be a sigmoidal function of the per capita capital $k_p$. We derive analytically the steady state probability distribution $P(k_p)$ and demonstrate the existence of a poverty trap, of central concern in development economics. In a parameter regime, $P(k_p)$ is bimodal with the twin peaks corresponding to states of poverty and well-being respectively. The associated potential landscape has two valleys with fluctuation-driven transitions between them. The mean exit times from the valleys are computed and one finds that the escape from a poverty trap is more favourable at higher values of $A$. We identify a critical value of $A_c$ below (above) which the state of poverty (well-being) dominates and propose two early signatures of the regime shift occurring at $A_c$. The economic model, with conceptual foundation in nonlinear dynamics and statistical mechanics, shares universal features with dynamical models from diverse disciplines like ecology and cell biology.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2311.06718",
        "category": "q-fin",
        "title": "Sustainable Development Goal (SDG) 8: New Zealand\u2019s Prospects while Yield Curve Inverts in Central Bank Digital Currency (CBDC) Era",
        "abstract": "In the inverted yield curve environment, I intend to assess the feasibility of fulfilling Sustainable Development Goal (SDG) 8, decent work and economic growth, of the United Nations by 2030 in New Zealand. Central Bank Digital Currency (CBDC) issuance supports SDG 8, based on the Cobb-Douglas production function, the growth accounting relation, and the Theory of Aggregate Demand. Bright prospects exist for New Zealand.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2401.01622",
        "category": "q-fin",
        "title": "Non-Atomic Arbitrage in Decentralized Finance",
        "abstract": "The prevalence of maximal extractable value (MEV) in the Ethereum ecosystem has led to a characterization of the latter as a dark forest. Studies of MEV have thus far largely been restricted to purely on-chain MEV, i.e., sandwich attacks, cyclic arbitrage, and liquidations. In this work, we shed light on the prevalence of non-atomic arbitrage on decentralized exchanges (DEXes) on the Ethereum blockchain. Importantly, non-atomic arbitrage exploits price differences between DEXes on the Ethereum blockchain as well as exchanges outside the Ethereum blockchain (i.e., centralized exchanges or DEXes on other blockchains). Thus, non-atomic arbitrage is a type of MEV that involves actions on and off the Ethereum blockchain. In our study of non-atomic arbitrage, we uncover that more than a fourth of the volume on Ethereum's biggest five DEXes from the merge until 31 October 2023 can likely be attributed to this type of MEV. We further highlight that only eleven searchers are responsible for more than 80% of the identified non-atomic arbitrage volume sitting at a staggering $132 billion and draw a connection between the centralization of the block construction market and non-atomic arbitrage. Finally, we discuss the security implications of these high-value transactions that account for more than 10% of Ethereum's total block value and outline possible mitigations.",
        "references": [
            {
                "arxivId": "2306.10777",
                "title": "Ethereum Proof-of-Stake Consensus Layer: Participation and Decentralization",
                "abstract": "In September 2022, Ethereum transitioned from Proof-of-Work (PoW) to Proof-of-Stake (PoS) during\"the merge\"- making it the largest PoS cryptocurrency in terms of market capitalization. With this work, we present a comprehensive measurement study of the current state of the Ethereum PoS consensus layer on the beacon chain. We perform a longitudinal study of the history of the beacon chain. Our work finds that all dips in network participation are caused by network upgrades, issues with major consensus clients, or issues with service operators controlling a large number of validators. Further, our longitudinal staking power decentralization analysis reveals that Ethereum PoS fairs similarly to its PoW counterpart in terms of decentralization and exhibits the immense impact of (liquid) staking services on staking power decentralization. Finally, we highlight the heightened security concerns in Ethereum PoS caused by high degrees of centralization."
            },
            {
                "arxivId": "2305.19150",
                "title": "The Centralizing Effects of Private Order Flow on Proposer-Builder Separation",
                "abstract": "The current Proposer-Builder Separation (PBS) equilibrium has several builders with different backgrounds winning blocks consistently. This paper considers how that equilibrium will shift when transactions are sold privately via order flow auctions (OFAs) rather than forwarded directly to the public mempool. We discuss a novel model that highlights the augmented value of private order flow for integrated builder searchers. We show that private order flow is complementary to top-of-block opportunities, and therefore integrated builder-searchers are more likely to participate in OFAs and outbid non integrated builders. They will then parlay access to these private transactions into an advantage in the PBS auction, winning blocks more often and extracting higher profits than non-integrated builders. To validate our main assumptions, we construct a novel dataset pairing post-merge PBS outcomes with realized 12-second volatility on a leading CEX (Binance). Our results show that integrated builder-searchers are more likely to win in the PBS auction when realized volatility is high, suggesting that indeed such builders have an advantage in extracting top-of-block opportunities. Our findings suggest that modifying PBS to disentangle the intertwined dynamics between top-of-block extraction and private order flow would pave the way for a fairer and more decentralized Ethereum."
            },
            {
                "arxivId": "2305.19037",
                "title": "Ethereum's Proposer-Builder Separation: Promises and Realities",
                "abstract": "With Ethereum's transition from Proof-of-Work to Proof-of-Stake in September 2022 came another paradigm shift, the Proposer-Builder Separation (PBS) scheme. PBS was introduced to decouple the roles of selecting and ordering transactions in a block (i.e., the builder), from those validating its contents and proposing the block to the network as the new head of the blockchain (i.e., the proposer). In this landscape, proposers are the validators in the Proof-of-Stake consensus protocol, while now relying on specialized block builders for creating blocks with the highest value for the proposer. Additionally, relays act as mediators between builders and proposers. We study PBS adoption and show that the current landscape exhibits significant centralization amongst the builders and relays. Further, we explore whether PBS effectively achieves its intended objectives of enabling hobbyist validators to maximize block profitability and preventing censorship. Our findings reveal that although PBS grants validators the opportunity to access optimized and competitive blocks, it tends to stimulate censorship rather than reduce it. Additionally, we demonstrate that relays do not consistently uphold their commitments and may prove unreliable. Specifically, proposers do not always receive the complete promised value, and the censorship or filtering capabilities pledged by relays exhibit significant gaps."
            },
            {
                "arxivId": "2305.18545",
                "title": "Blockchain Censorship",
                "abstract": "Permissionless blockchains promise to be resilient against censorship by a single entity. This suggests that deterministic rules, and not third-party actors, are responsible for deciding if a transaction is appended to the blockchain or not. In 2022, the U.S. Office of Foreign Assets Control (OFAC) sanctioned a Bitcoin mixer and an Ethereum application, putting the neutrality of permissionless blockchains to the test. In this paper, we formalize quantify and analyze the security impact of blockchain censorship. We start by defining censorship, followed by a quantitative assessment of current censorship practices. We find that 46% of Ethereum blocks were made by censoring actors that intend to comply with OFAC sanctions, indicating the significant impact of OFAC sanctions on the neutrality of public blockchains. We further uncover that censorship not only impacts neutrality, but also security. We show how after Ethereum's move to Proof-of-Stake (PoS) and adoption of Proposer-Builder Separation (PBS) the inclusion of censored transactions was delayed by an average of 85%. Inclusion delays compromise a transaction's security by, e.g., strengthening a sandwich adversary. Finally we prove a fundamental limitation of PoS and Proof-of-Work (PoW) protocols against censorship resilience."
            },
            {
                "arxivId": "2305.16468",
                "title": "Time to Bribe: Measuring Block Construction Market",
                "abstract": "With the emergence of Miner Extractable Value (MEV), block construction markets on blockchains have evolved into a competitive arena. Following Ethereum's transition from Proof of Work (PoW) to Proof of Stake (PoS), the Proposer Builder Separation (PBS) mechanism has emerged as the dominant force in the Ethereum block construction market. This paper presents an in-depth longitudinal study of the Ethereum block construction market, spanning from the introduction of PoS and PBS in September 2022 to May 2023. We analyze the market shares of builders and relays, their temporal changes, and the financial dynamics within the PBS system, including payments among builders and block proposers -- commonly referred to as bribes. We introduce an MEV-time law quantifying the expected MEV revenue wrt. the time elapsed since the last proposed block. We provide empirical evidence that moments of crisis (e.g. the FTX collapse, USDC stablecoin de-peg) coincide with significant spikes in MEV payments compared to the baseline. Despite the intention of the PBS architecture to enhance decentralization by separating actor roles, it remains unclear whether its design is optimal. Implicit trust assumptions and conflicts of interest may benefit particular parties and foster the need for vertical integration. MEV-Boost was explicitly designed to foster decentralization, causing the side effect of enabling risk-free sandwich extraction from unsuspecting users, potentially raising concerns for regulators."
            },
            {
                "arxivId": "2305.14604",
                "title": "Automated Market Making and Arbitrage Profits in the Presence of Fees",
                "abstract": "We consider the impact of trading fees on the profits of arbitrageurs trading against an automated marker marker (AMM) or, equivalently, on the adverse selection incurred by liquidity providers due to arbitrage. We extend the model of Milionis et al. [2022] for a general class of two asset AMMs to both introduce fees and discrete Poisson block generation times. In our setting, we are able to compute the expected instantaneous rate of arbitrage profit in closed form. When the fees are low, in the fast block asymptotic regime, the impact of fees takes a particularly simple form: fees simply scale down arbitrage profits by the fraction of time that an arriving arbitrageur finds a profitable trade."
            },
            {
                "arxivId": "2305.09032",
                "title": "Time is Money: Strategic Timing Games in Proof-of-Stake Protocols",
                "abstract": "We propose a model suggesting that honest-but-rational consensus participants may play timing games, and strategically delay their block proposal to optimize MEV capture, while still ensuring the proposal's timely inclusion in the canonical chain. In this context, ensuring economic fairness among consensus participants is critical to preserving decentralization. We contend that a model grounded in honest-but-rational consensus participation provides a more accurate portrayal of behavior in economically incentivized systems such as blockchain protocols. We empirically investigate timing games on the Ethereum network and demonstrate that while timing games are worth playing, they are not currently being exploited by consensus participants. By quantifying the marginal value of time, we uncover strong evidence pointing towards their future potential, despite the limited exploitation of MEV capture observed at present."
            },
            {
                "arxivId": "2305.05206",
                "title": "A Fair and Resilient Decentralized Clock Network for Transaction Ordering",
                "abstract": "Traditional blockchain design gives miners or validators full control over transaction ordering, i.e., they can freely choose which transactions to include or exclude, as well as in which order. While not an issue initially, the emergence of decentralized finance has introduced new transaction order dependencies allowing parties in control of the ordering to make a profit by front-running others' transactions. In this work, we present the Decentralized Clock Network, a new approach for achieving fair transaction ordering. Users submit their transactions to the network's clocks, which run an agreement protocol that provides each transaction with a timestamp of receipt which is then used to define the transactions' order. By separating agreement from ordering, our protocol is efficient and has a simpler design compared to other available solutions. Moreover, our protocol brings to the blockchain world the paradigm of asynchronous fallback, where the algorithm operates with stronger fairness guarantees during periods of synchronous use, switching to an asynchronous mode only during times of increased network delay."
            },
            {
                "arxivId": "2212.05111",
                "title": "SoK: MEV Countermeasures: Theory and Practice",
                "abstract": "Blockchains offer strong security guarantees, but they cannot protect the ordering of transactions. Powerful players, such as miners, sequencers, and sophisticated bots, can reap significant profits by selectively including, excluding, or re-ordering user transactions. Such profits are called Miner/Maximal Extractable Value or MEV. MEV bears profound implications for blockchain security and decentralization. While numerous countermeasures have been proposed, there is no agreement on the best solution. Moreover, solutions developed in academic literature differ quite drastically from what is widely adopted by practitioners. For these reasons, this paper systematizes the knowledge of the theory and practice of MEV countermeasures. The contribution is twofold. First, we present a comprehensive taxonomy of 30 proposed MEV countermeasures, covering four different technical directions. Secondly, we empirically studied the most popular MEV-auction-based solution with rich blockchain and mempool data. We also present the Mempool Guru system, a public service system that collects, persists, and analyzes the Ethereum mempool data for research. In addition to gaining insights into MEV auction platforms' real-world operations, our study shed light on the prevalent censorship by MEV auction platforms as a result of the recent OFAC sanction, and its implication on blockchain properties."
            },
            {
                "arxivId": "2208.06046",
                "title": "Automated Market Making and Loss-Versus-Rebalancing",
                "abstract": "Automated market making (AMM) protocols such as Uniswap have recently emerged as an alternative to the most common market structure for electronic trading, the limit order book. Relative to limit order books, AMMs are both more computationally efficient and do not require the participation of active market making intermediaries. As such, AMMs have emerged as the dominant market mechanism for trust-less decentralized exchanges (DEXs). We develop a model the underlying economics of AMMs from the perspective of their passive liquidity providers (LPs). Our central contribution is a\"Black-Scholes formula for AMMs\". Like the Black-Scholes formula, we consider the return to LPs once market risk has been hedged. We identify the main adverse selection cost incurred by LPs, which we call\"loss-versus-rebalancing\"(LVR, pronounced\"lever\"). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. In a continuous time Black-Scholes setting, we are able to derive closed-form expressions for this adverse selection cost, for all automated market makers, including constant function market makers and those featuring concentrated liquidity (e.g., Uniswap v3). Qualitatively, we highlight the main forces that drive AMM LP returns, including asset characteristics (volatility) and AMM characteristics (curvature/marginal liquidity). Quantitatively, we illustrate how our model's expressions for LP returns match actual LP returns for the Uniswap v2 WETH-USDC trading pair. Our model provides tradable insight into both the ex ante and ex post assessment of AMM LP investment decisions. LVR can also inform the design of the next generation of DEX market mechanisms -- in fact, in the short time since our work has been released,\"LVR mitigation\"has already emerged as the dominant challenge among practitioners in the AMM protocol designer community."
            },
            {
                "arxivId": "2205.08904",
                "title": "Risks and Returns of Uniswap V3 Liquidity Providers",
                "abstract": "Trade execution on Decentralized Exchanges (DEXes) is automatic and does not require individual buy and sell orders to be matched. Instead, liquidity aggregated in pools from individual liquidity providers enables trading between cryptocurrencies. The largest DEX measured by trading volume, Uniswap V3, promises a DEX design optimized for capital efficiency. However, Uniswap V3 requires far more decisions from liquidity providers than previous DEX designs. In this work, we develop a theoretical model to illustrate the choices faced by Uniswap V3 liquidity providers and their implications. Our model suggests that providing liquidity on Uniswap V3 is highly complex and requires many considerations from a user. Our supporting data analysis of the risks and returns of real Uniswap V3 liquidity providers underlines that liquidity providing in Uniswap V3 is incredibly complicated, and performances can vary wildly. While there are simple and profitable strategies for liquidity providers in liquidity pools characterized by negligible price volatilities, these strategies only yield modest returns. Instead, significant returns can only be obtained by accepting increased financial risks and at the cost of active management. Thus, providing liquidity has become a game reserved for sophisticated players with the introduction of Uniswap V3, where retail traders do not stand a chance."
            },
            {
                "arxivId": "2203.11520",
                "title": "SoK: Preventing Transaction Reordering Manipulations in Decentralized Finance",
                "abstract": "User transactions on Ethereum's peer-to-peer network are at risk of being attacked. The smart contracts building decentralized finance (DeFi) have introduced a new transaction ordering dependency to the Ethereum blockchain. As a result, attackers can profit from front- and back-running transactions. Multiple approaches to mitigate transaction reordering manipulations have surfaced recently. However, the success of individual approaches in mitigating such attacks and their impact on the entire blockchain remains largely unstudied. In this systematization of knowledge (SoK), we categorize and analyze state-of-the-art transaction reordering manipulation mitigation schemes. Instead of restricting our analysis to a scheme's success at preventing transaction reordering attacks, we evaluate its full impact on the blockchain. Therefore, we are able to provide a complete picture of the strengths and weaknesses of current mitigation schemes. We find that currently no scheme fully meets all the demands of the blockchain ecosystem. In fact, all approaches demonstrate unsatisfactory performance in at least one area relevant to the blockchain ecosystem."
            },
            {
                "arxivId": "2202.03762",
                "title": "Eliminating Sandwich Attacks with the Help of Game Theory",
                "abstract": "Predatory trading bots lurking in Ethereum's mempool present invisible taxation of traders on automated market makers (AMMs). AMM traders specify a slippage tolerance to indicate the maximum price movement they are willing to accept. This way, traders avoid automatic transaction failure in case of small price movements before their trade request executes. However, while a too-small slippage tolerance may lead to trade failures, a too-large slippage tolerance allows predatory trading bots to profit from sandwich attacks. These bots can extract the difference between the slippage tolerance and the actual price movement as profit. In this work, we introduce the sandwich game to analyze sandwich attacks analytically from both the attacker and victim perspectives. Moreover, we provide a simple and highly effective algorithm that traders can use to set the slippage tolerance. We unveil that most broadcasted transactions can avoid sandwich attacks while simultaneously only experiencing a low risk of transaction failure. Thereby, we demonstrate that a constant auto-slippage cannot adjust to varying trade sizes and pool characteristics. Our algorithm outperforms the constant auto-slippage suggested by the biggest AMM, Uniswap, in all performed tests. Specifically, our algorithm repeatedly demonstrates a cost reduction exceeding a factor of 100."
            },
            {
                "arxivId": "2112.06615",
                "title": "Quick Order Fairness",
                "abstract": null
            },
            {
                "arxivId": "2106.07371",
                "title": "A2MM: Mitigating Frontrunning, Transaction Reordering and Consensus Instability in Decentralized Exchanges",
                "abstract": "The asset trading volume on blockchain-based exchanges (DEX) increased substantially since the advent of Automated Market Makers (AMM). Yet, AMMs and their forks compete on the same blockchain, incurring unnecessary network and block-space overhead, by attracting sandwich attackers and arbitrage competitions. Moreover, conceptually speaking, a blockchain is one database, and we find little reason to partition this database into multiple competing exchanges, which then necessarily require price synchronization through arbitrage. This paper shows that DEX arbitrage and trade routing among similar AMMs can be performed efficiently and atomically on-chain within smart contracts. These insights lead us to create a new AMM design, an Automated Arbitrage Market Maker, short A2MM DEX. A2MM aims to unite multiple AMMs to reduce overheads, costs and increase blockchain security. With respect to Miner Extractable Value (MEV), A2MM serves as a decentralized design for users to atomically collect MEV, mitigating the dangers of centralized MEV relay services. We show that A2MM offers essential security benefits. First, A2MM strengthens the blockchain consensus security by mitigating the competitive exploitation of MEV, therefore reducing the risks of consensus forks. A2MM reduces the network layer overhead of competitive transactions, improves network propagation, leading to less stale blocks and better blockchain security. Through trade routing, A2MM reduces the predatory risks of sandwich attacks by taking advantage of the minimum profitable victim input. A2MM also offers financial benefits to traders. Failed swap transactions from competitive trading occupy valuable block space, implying an upward pressure on transaction fees. Our evaluations shows that A2MM frees up 32.8% block-space of AMM-related transactions. In expectation, A2MM's revenue allows to reduce swap fees by 90%."
            },
            {
                "arxivId": "2106.06389",
                "title": "An empirical study of DeFi liquidations: incentives, risks, and instabilities",
                "abstract": "Financial speculators often seek to increase their potential gains with leverage. Debt is a popular form of leverage, and with over 39.88B USD of total value locked (TVL), the Decentralized Finance (DeFi) lending markets are thriving. Debts, however, entail the risks of liquidation, the process of selling the debt collateral at a discount to liquidators. Nevertheless, few quantitative insights are known about the existing liquidation mechanisms. In this paper, to the best of our knowledge, we are the first to study the breadth of the borrowing and lending markets of the Ethereum DeFi ecosystem. We focus on Aave, Compound, MakerDAO, and dYdX, which collectively represent over 85% of the lending market on Ethereum. Given extensive liquidation data measurements and insights, we systematize the prevalent liquidation mechanisms and are the first to provide a methodology to compare them objectively. We find that the existing liquidation designs well incentivize liquidators but sell excessive amounts of discounted collateral at the borrowers' expenses. We measure various risks that liquidation participants are exposed to and quantify the instabilities of existing lending protocols. Moreover, we propose an optimal strategy that allows liquidators to increase their liquidation profit, which may aggravate the loss of borrowers."
            },
            {
                "arxivId": "2105.13822",
                "title": "Behavior of Liquidity Providers in Decentralized Exchanges",
                "abstract": "Decentralized exchanges (DEXes) have introduced an innovative trading mechanism, where it is not necessary to match buy-orders and sell-orders to execute a trade. DEXes execute each trade individually, and the exchange rate is automatically determined by the ratio of assets reserved in the market. Therefore, apart from trading, financial players can also liquidity providers, benefiting from transaction fees from trades executed in DEXes. Although liquidity providers are essential for the functionality of DEXes, it is not clear how liquidity providers behave in such markets. In this paper, we aim to understand how liquidity providers react to market information and how they benefit from providing liquidity in DEXes. We measure the operations of liquidity providers on Uniswap and analyze how they determine their investment strategy based on market changes. We also reveal their returns and risks of investments in different trading pair categories, i.e., stable pairs, normal pairs, and exotic pairs. Further, we investigate the movement of liquidity between trading pools. To the best of our knowledge, this is the first work that systematically studies the behavior of liquidity providers in DEXes."
            },
            {
                "arxivId": "2007.08303",
                "title": "Wendy, the Good Little Fairness Widget: Achieving Order Fairness for Blockchains",
                "abstract": "The advent of decentralized trading markets introduces a number of new challenges for consensus protocols. In addition to the 'usual' attacks -- a subset of the validators trying to prevent agreement -- there is now the possibility of financial fraud, which can abuse properties not normally considered critical in consensus protocols. We investigate the issues of attackers manipulating or exploiting the order in which transactions are scheduled in the blockchain. More concretely, we look into order fairness, i.e., ways we can assure that the order of transactions relative to each other is fair. We show that one of the more intuitive definitions of fairness is impossible to achieve. We then present Wendy, a group of low overhead protocols that can implement different concepts of fairness. Wendy acts as an additional widget for an existing blockchain, and is largely agnostic to the underlying blockchain and its security assumptions, as long as they provide a known and always active set of validators. Furthermore, it is possible to implement fairness for some subsets of the transactions, and thus run several independent fair markets (as well as some unfair ones) on the same chain."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2402.16509",
        "category": "q-fin",
        "title": "On short-time behavior of implied volatility in a market model with indexes",
        "abstract": "This paper investigates short-term behaviors of implied volatility of derivatives written on indexes in equity markets when the index processes are constructed by using a ranking procedure. Even in simple market settings where stock prices follow geometric Brownian motion dynamics, the ranking mechanism can produce the observed term structure of at-the-money (ATM) implied volatility skew for equity indexes. Our proposed models showcase the ability to reconcile two seemingly contradictory features found in empirical data from equity markets: the long memory of volatilities and the power law of ATM skews. Furthermore, the models allow for the capture of a novel phenomenon termed the quasi-blow-up phenomenon.",
        "references": [
            {
                "arxivId": "2203.13820",
                "title": "Rough Volatility: Fact or Artefact?",
                "abstract": "We investigate the statistical evidence for the use of \u2018rough\u2019 fractional processes with Hurst exponent $$H< 0.5$$ H < 0.5 for modeling the volatility of financial assets, using a model-free approach. We introduce a non-parametric method for estimating the roughness of a function based on discrete sample, using the concept of normalized p -th variation along a sequence of partitions. Detailed numerical experiments based on sample paths of fractional Brownian motion and other fractional processes reveal good finite sample performance of our estimator for measuring the roughness of sample paths of stochastic processes. We then apply this method to estimate the roughness of realized volatility signals based on high-frequency observations. Detailed numerical experiments based on stochastic volatility models show that, even when the instantaneous volatility has diffusive dynamics with the same roughness as Brownian motion, the realized volatility exhibits rough behaviour corresponding to a Hurst exponent significantly smaller than 0.5. Comparison of roughness estimates for realized and instantaneous volatility in fractional volatility models with different values of Hurst exponent shows that, irrespective of the roughness of the spot volatility process, realized volatility always exhibits \u2018rough\u2019 behaviour with an apparent Hurst index $$\\widehat{H}<0.5$$ H ^ < 0.5 . These results suggest that the origin of the roughness observed in realized volatility time series lies in the estimation error rather than the volatility process itself."
            },
            {
                "arxivId": "1801.08675",
                "title": "Short-Term at-the-Money Asymptotics Under Stochastic Volatility Models",
                "abstract": "A small-time Edgeworth expansion of the density of an asset price is given under a general stochastic volatility model, from which asymptotic expansions of put option prices and at-the-money implied volatilities follow. A limit theorem for at-the-money implied volatility skew and curvature is also given as a corollary. The rough Bergomi model is treated as an example."
            },
            {
                "arxivId": "1702.02777",
                "title": "Rough volatility: Evidence from option prices",
                "abstract": "ABSTRACT It has been recently shown that spot volatilities can be closely modeled by rough stochastic volatility-type dynamics. In such models, the log-volatility follows a fractional Brownian motion with Hurst parameter smaller than half. This result has been established using high-frequency volatility estimations from historical price data. We revisit this finding by studying implied volatility-based approximations of the spot volatility. Using at-the-money options on the S&P500 index with short maturity, we are able to confirm that volatility is rough. The Hurst parameter found here, of order 0.3, is slightly larger than that usually obtained from historical data. This is easily explained from a smoothing effect due to the remaining time to maturity of the considered options."
            },
            {
                "arxivId": "1502.05442",
                "title": "Extreme-strike asymptotics for general Gaussian stochastic volatility models",
                "abstract": null
            },
            {
                "arxivId": "1108.0384",
                "title": "Convergence rates for rank-based models with applications to portfolio theory",
                "abstract": null
            },
            {
                "arxivId": "0909.0065",
                "title": "HYBRID ATLAS MODELS",
                "abstract": "We study Atlas-type models of equity markets with local characteristics that depend on both name and rank, and in ways that induce a stable capital distribution. Ergodic properties and rankings of processes are examined with reference to the theory of reflected Brownian motions in polyhedral domains. In the context of such models we discuss properties of various investment strategies, including the so-called growth-optimal and universal portfolios."
            },
            {
                "arxivId": "0911.2834",
                "title": "Coupling index and stocks",
                "abstract": "In this paper, we are interested in continuous-time models in which the index level induces feedback on the dynamics of its composing stocks. More precisely, we propose a model in which the log-returns of each stock may be decomposed into a systemic part proportional to the log-returns of the index plus an idiosyncratic part. We show that, when the number of stocks in the index is large, this model may be approximated by a local volatility model for the index and a stochastic volatility model for each stock with volatility driven by the index. This result is useful from a calibration perspective: it suggests that one should first calibrate the local volatility of the index and then calibrate the dynamics of each stock. We explain how to do so in the limiting simplified model and in the original model."
            },
            {
                "arxivId": "0810.2149",
                "title": "On collisions of Brownian particles",
                "abstract": "We examine the behavior of n Brownian particles diffusing on the real line, with bounded, measurable drift and bounded, piecewise continuous diffusion coefficients that depend on the currentconfigura- tion of particles. Sufficient conditions are established forthe absence of triple collisions, as well as for the presence of (infinitely-many) triple collisions among the particles. As an application to the Atlas model of equity markets, we study a special construction of such systems of diffusing particles using Brownian motions with reflection on polyhedral domains."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2403.09265",
        "category": "q-fin",
        "title": "Zonal vs. Nodal Pricing: An Analysis of Different Pricing Rules in the German Day-Ahead Market",
        "abstract": "The European electricity market is based on large pricing zones with a uniform day-ahead price. The energy transition leads to shifts in supply and demand and increasing redispatch costs. In an attempt to ensure efficient market clearing and congestion management, the EU Commission has mandated the Bidding Zone Review (BZR) to reevaluate the configuration of European bidding zones. Based on a unique data set published in the context of the BZR, we compare various pricing rules for the German power market. We compare market clearing and pricing for national, zonal, and nodal models, including their generation costs and associated redispatch costs. Moreover, we investigate different non-uniform pricing rules and their economic implications for the German electricity market. Our results indicate that the differences in the average prices in different zones are small. The total costs across different configurations are similar and the reduction of standard deviations in prices is also small based on this data set. A nodal pricing rule leads to the lowest total costs. We also analyze the quality of different pricing rules and their differences with respect to the quality of the price signals and the necessary uplift payments. While the study focuses on Germany, the analysis is relevant beyond and feeds into the broader discussion about pricing rules.",
        "references": [
            {
                "arxivId": "2209.07386",
                "title": "Pricing Optimal Outcomes in Coupled and Non-Convex Markets: Theory and Applications to Electricity Markets",
                "abstract": "According to the fundamental theorems of welfare economics, any competitive equilibrium is Pareto efficient. Unfortunately, competitive equilibrium prices only exist under strong assumptions such as perfectly divisible goods and convex preferences. In many real-world markets, participants have non-convex preferences and the allocation problem needs to consider complex constraints. Electricity markets are a prime example, but similar problems appear in many real-world markets, which has led to a growing literature in market design. Power markets use heuristic pricing rules based on the dual of a relaxed allocation problem today. With increasing levels of renewables, these rules have come under scrutiny as they lead to high out-of-market side-payments to some participants and to inadequate congestion signals. We show that existing pricing heuristics optimize specific design goals that can be conflicting. The trade-offs can be substantial, and we establish that the design of pricing rules is fundamentally a multi-objective optimization problem addressing different incentives. In addition to traditional multi-objective optimization techniques using weighing of individual objectives, we introduce a novel parameter-free pricing rule that minimizes incentives for market participants to deviate locally. Our theoretical and experimental findings show how the new pricing rule capitalizes on the upsides of existing pricing rules under scrutiny today. It leads to prices that incur low make-whole payments while providing adequate congestion signals and low lost opportunity costs. Our suggested pricing rule does not require weighing of objectives, it is computationally scalable, and balances trade-offs in a principled manner, addressing an important policy issue in electricity markets."
            },
            {
                "arxivId": "2207.05216",
                "title": "Numerical Comparisons of Linear Power Flow Approximations: Optimality, Feasibility, and Computation Time",
                "abstract": "Linear approximations of the AC power flow equations are of great significance for the computational efficiency of large-scale optimal power flow (OPF) problems. Put differently, the feasibility of the obtained solution is essential for practical use cases of OPF. However, most studies focus on approximation error and come short of comprehensively studying the AC feasibility of different linear approximations of power flow. This paper discusses the merits of widely-used linear approximations of active power in OPF problems. The advantages and disadvantages of the linearized models are discussed with respect to four criteria; accuracy of the linear approximation, optimality, feasibility, and computation time. Each method is tested on five different systems."
            },
            {
                "arxivId": "1605.05002",
                "title": "A Convex Primal Formulation for Convex Hull Pricing",
                "abstract": "In certain electricity markets, because of nonconvexities that arise from their operating characteristics, generators that follow the independent system operator's (ISO's) decisions may fail to recover their cost through sales of energy at locational marginal prices. The ISO makes discriminatory side payments to incentivize the compliance of generators. Convex hull pricing is a uniform pricing scheme that minimizes these side payments. The Lagrangian dual problem of the unit commitment problem has been solved in the dual space to determine convex hull prices. However, this approach is computationally expensive. We propose a polynomially solvable primal formulation for the Lagrangian dual problem. This formulation explicitly describes for each generating unit the convex hull of its feasible set and the convex envelope of its cost function. We cast our formulation as a second-order cone program when the cost functions are quadratic, and a linear program when the cost functions are piecewise linear. A 96-period 76-unit transmission-constrained example is solved in less than 15\u00a0s on a personal computer."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04276",
        "category": "q-fin",
        "title": "Recursive index for assessing value added of individual scientific publications",
        "abstract": "An aggregated recursive K-index is proposed as a new scientometric indicator of added value and scientific research output of individual publications. This index can be used instead of or in addition to the H-index (J.E. Hirsch. An index to quantify an individual's scientific research output, arXiv:physics/0508025). In particular, it is proposed to switch from a pure strategy for assessing the quality and effectiveness of R&D using the H-index (Hirsch index) to a mixed strategy (in the context of publication activity as a combination of cooperative and noncooperative games) using the K-index on subnational and H-index on international or differentiated levels. In the context of a hybrid strategy of the scientist's payoff functions. This transition is correct and in demand for a number of national scientific systems with limited financial, material, infrastructural and linguistic (in terms of the English language) potential. Scientific systems with highly developed indigenous (autochthonous) characteristics are also needed in some scientific areas.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04282",
        "category": "q-fin",
        "title": "Analyzing Economic Convergence Across the Americas: A Survival Analysis Approach to GDP per Capita Trajectories",
        "abstract": "By integrating survival analysis, machine learning algorithms, and economic interpretation, this research examines the temporal dynamics associated with attaining a 5 percent rise in purchasing power parity-adjusted GDP per capita over a period of 120 months (2013-2022). A comparative investigation reveals that DeepSurv is proficient at capturing non-linear interactions, although standard models exhibit comparable performance under certain circumstances. The weight matrix evaluates the economic ramifications of vulnerabilities, risks, and capacities. In order to meet the GDPpc objective, the findings emphasize the need of a balanced approach to risk-taking, strategic vulnerability reduction, and investment in governmental capacities and social cohesiveness. Policy guidelines promote individualized approaches that take into account the complex dynamics at play while making decisions.",
        "references": [
            {
                "arxivId": "1708.04649",
                "title": "Machine Learning for Survival Analysis: A Survey",
                "abstract": "Accurately predicting the time of occurrence of an event of interest is a critical problem in longitudinal data analysis. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04506",
        "category": "q-fin",
        "title": "Super Apps and the Digital Markets Act",
        "abstract": "The Digital Markets Act (DMA) aims to ensure contestability and fairness in digital markets, particularly focusing on regulating Big Tech companies. The paper explores the DMA's capacity to address both current and future challenges in digital market contestability and fairness, spotlighting the trend towards platform integration and the potential rise of\"super-apps\"akin to WeChat and KakaoTalk. Specifically, it investigates WhatsApp, owned by Meta, as a gatekeeper that might expand its service offerings, integrating additional functionalities like AI and metaverse technologies. The paper discusses whether the DMA's obligations, such as mandated interoperability and data portability, can mitigate the emergent risks to market fairness and contestability from such integrations. Despite recognizing that the DMA has the potential to address many issues arising from platform integration, it suggests the necessity for adaptability and a complementary relationship with traditional antitrust law to ensure sustained contestability and fairness in evolving digital markets.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04543",
        "category": "q-fin",
        "title": "Early Adoption of Generative AI by Global Business Leaders: Insights from an INSEAD Alumni Survey",
        "abstract": "How are new technologies like generative AI quickly adopted and used by executive and managerial leaders to create value in organizations? A survey of INSEAD's global alumni base revealed several intriguing insights into perceptions and engagements with generative AI across a broad spectrum of demographics, industries, and geographies. Notably, there's a prevailing optimism about the role of generative AI in enhancing productivity and innovation, as evidenced by the 90% of respondents being excited about its time-saving and efficiency benefits. Analysis revealed different attitudes about adoption and use across demographic variables. Younger respondents are significantly more excited about generative AI and more likely to be using it at work and in personal life than older participants. Those in Europe have a somewhat more distant view of generative AI than those in North America in Asia, in that they see the gains more likely to be captured by organizations than individuals, and are less likely to be using it in professional and personal contexts than those in North America and Asia. This may also be related to the fact that those in Europe are more likely to be working in Financial Services and less likely to be working in Information Technology industries than those in North America and Asia. Despite this, those in Europe are more likely to see AGI happening faster than those in North America, although this may reflect less interaction with generative AI in personal and professional contexts. These findings collectively underscore the complex and multifaceted perceptions of generative AI's role in society, pointing to both its promising potential and the challenges it presents.",
        "references": [
            {
                "arxivId": "2306.10052",
                "title": "Assigning AI: Seven Approaches for Students, with Prompts",
                "abstract": "This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the\"human in the loop,\"the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms"
            },
            {
                "arxivId": "2305.09573",
                "title": "Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs",
                "abstract": "Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate \u201cethics washing.\u201d Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies."
            },
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            },
            {
                "arxivId": "2303.08774",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04707",
        "category": "q-fin",
        "title": "Gender Bias in Emerging New Research Topics: The Impact of COVID-19 on Women in Science (preprint)",
        "abstract": "We investigate the impact of new research opportunities on the long-standing under-representation of women in medical and academic leadership by assessing the impact of the emergence of COVID-19 as a new research topic in the life sciences on women's authorship. After collecting publication data from 2019 and 2020 on biomedical publications, where the position of first and last author is most important for future career development, we use the major Medical Subject Heading (MeSH) terms to identify the main research area of each publication and measure the relation of each paper to COVID-19. Using a Difference-in-Difference approach, we find that although the general female authorship trend is upwards, papers in areas related to COVID-19 are less likely to have a woman as first or last author compared to research areas not related to COVID-19. Conversely, new publication opportunities in the COVID-19 research field increase the proportion of women in middle, less-relevant, author positions. Stay-at-home mandates, journal importance, and access to new funds do not fully explain the drop in women's outcomes. The decline in female first authorship is related to the increase of teams in which both lead authors have no prior experience in the COVID-related research field. In addition, pre-existing publishing teams show reduced bias in female key authorship with respect to new teams specifically formed for COVID-related research. This suggests that opportunistic teams, transitioning into research areas with emerging interests, possess greater flexibility in choosing the primary and final authors, potentially reducing uncertainties associated with engaging in productions divergent from their past scientific experiences by excluding women scientists from key authorship positions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04709",
        "category": "q-fin",
        "title": "Two-Sided Flexibility in Platforms",
        "abstract": "Flexibility is a cornerstone of operations management, crucial to hedge stochasticity in product demands, service requirements, and resource allocation. In two-sided platforms, flexibility is also two-sided and can be viewed as the compatibility of agents on one side with agents on the other side. Platform actions often influence the flexibility on either the demand or the supply side. But how should flexibility be jointly allocated across different sides? Whereas the literature has traditionally focused on only one side at a time, our work initiates the study of two-sided flexibility in matching platforms. We propose a parsimonious matching model in random graphs and identify the flexibility allocation that optimizes the expected size of a maximum matching. Our findings reveal that flexibility allocation is a first-order issue: for a given flexibility budget, the resulting matching size can vary greatly depending on how the budget is allocated. Moreover, even in the simple and symmetric settings we study, the quest for the optimal allocation is complicated. In particular, easy and costly mistakes can be made if the flexibility decisions on the demand and supply side are optimized independently (e.g., by two different teams in the company), rather than jointly. To guide the search for optimal flexibility allocation, we uncover two effects, flexibility cannibalization, and flexibility abundance, that govern when the optimal design places the flexibility budget only on one side or equally on both sides. In doing so we identify the study of two-sided flexibility as a significant aspect of platform efficiency.",
        "references": [
            {
                "arxivId": "2104.14740",
                "title": "Driver Positioning and Incentive Budgeting with an Escrow Mechanism for Ridesharing Platforms",
                "abstract": "Drivers on the Lyft ride-share platform do not always know where the areas of supply shortage are in real time. This lack of information hurts both riders trying to find a ride and drivers trying to determine how to maximize their earnings opportunities. Lyft\u2019s Personal Power Zone (PPZ) product helps the company to maintain high levels of service on the platform by influencing the spatial distribution of drivers in real time via monetary incentives that encourage them to reposition their vehicles. The underlying system that powers the product has two main components: (1) a novel \u201cescrow mechanism\u201d that tracks available incentive budgets tied to locations within a city in real time, and (2) an algorithm that solves the stochastic driver-positioning problem to maximize short-run revenue from riders\u2019 fares. The optimization problem is a multiagent dynamic program that is too complicated to solve optimally for our large-scale application. Our approach is to decompose it into two subproblems. The first determines the set of drivers to incentivize and where to incentivize them to position themselves. The second determines how to fund each incentive using the escrow budget. By formulating it as two convex programs, we are able to use commercial solvers that find the optimal solution in a matter of seconds. Rolled out to all 320 cities in which Lyft operates in a little more than a year, the system now generates millions of bonuses that incentivize hundreds of thousands of active drivers to optimally position themselves in anticipation of ride requests every week. Together, the PPZ product and its underlying algorithms represent a paradigm shift in how Lyft drivers drive and generate earnings on the platform. Its direct business impact has been a 0.5% increase in incremental bookings, amounting to tens of millions of dollars per year. In addition, the product has brought about significant improvements to the driver and rider experience on the platform. These include statistically significant reductions in pick-up times and ride cancellations. Finally, internal surveys reveal that the vast majority of drivers prefer PPZs over the legacy system."
            },
            {
                "arxivId": "1505.07648",
                "title": "Flexible Queueing Architectures",
                "abstract": "We study a multiserver model with n flexible servers and n queues, connected through a bipartite graph, where the level of flexibility is captured by an upper bound on the graph\u2019s average degree, dn. Applications in content replication in data centers, skill-based routing in call centers, and flexible supply chains are among our main motivations. We focus on the scaling regime where the system size n tends to infinity, while the overall traffic intensity stays fixed. We show that a large capacity region and an asymptotically vanishing queueing delay are simultaneously achievable even under limited flexibility (dn \u226a n). Our main results demonstrate that, when dn \u226b ln n, a family of expander-graph-based flexibility architectures has a capacity region that is within a constant factor of the maximum possible, while simultaneously ensuring a diminishing queueing delay for all arrival rate vectors in the capacity region. Our analysis is centered around a new class of virtual-queue-based scheduling policies that..."
            },
            {
                "arxivId": "cond-mat/0603350",
                "title": "The number of matchings in random graphs",
                "abstract": "We study matchings on sparse random graphs by means of the cavity method. We first show how the method reproduces several known results about maximum and perfect matchings in regular and Erd\u00f6s\u2013R\u00e9nyi random graphs. Our main new result is the computation of the entropy, i.e. the leading order of the logarithm of the number of solutions, of matchings with a given size. We derive both an algorithm to compute this entropy for an arbitrary graph with a girth that diverges in the large size limit, and an analytic result for the entropy in regular and Erd\u00f6s\u2013R\u00e9nyi random graph ensembles."
            },
            {
                "arxivId": "math/0309441",
                "title": "Maximum weight independent sets and matchings in sparse random graphs. Exact results using the local weak convergence method",
                "abstract": "Let G(n,c/n) and Gr(n) be an n\u2010node sparse random graph and a sparse random r\u2010regular graph, respectively, and let I(n,r) and I(n,c) be the sizes of the largest independent set in G(n,c/n) and Gr(n). The asymptotic value of I(n,c)/n as n \u2192 \u221e, can be computed using the Karp\u2010Sipser algorithm when c \u2264 e. For random cubic graphs, r = 3, it is only known that .432 \u2264 lim infn I(n,3)/n \u2264 lim supn I(n,3)/n \u2264 .4591 with high probability (w.h.p.) as n \u2192 \u221e, as shown in Frieze and Suen [Random Structures Algorithms 5 (1994), 649\u2013664] and Bollabas [European J Combin 1 (1980), 311\u2013316], respectively. In this paper we assume in addition that the nodes of the graph are equipped with nonnegative weights, independently generated according to some common distribution, and we consider instead the maximum weight of an independent set. Surprisingly, we discover that for certain weight distributions, the limit limn I(n,c)/n can be computed exactly even when c > e, and limn I(n,r)/n can be computed exactly for some r \u2265 1. For example, when the weights are exponentially distributed with parameter 1, limn I(n,2e)/n \u2248 .5517, and limn I(n,3)/n \u2248 .6077. Our results are established using the recently developed local weak convergence method further reduced to a certain local optimality property exhibited by the models we consider. We extend our results to maximum weight matchings in G(n,c/n) and Gr(n). For the case of exponential distributions, we compute the corresponding limits for every c > 0 and every r \u2265 2. \u00a9 2005 Wiley Periodicals, Inc. Random Struct. Alg., 2006"
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04710",
        "category": "q-fin",
        "title": "Explaining Indian Stock Market through Geometry of Scale free Networks",
        "abstract": "This paper presents an analysis of the Indian stock market using a method based on embedding the network in a hyperbolic space using Machine learning techniques. We claim novelty on four counts. First, it is demonstrated that the hyperbolic clusters resemble the topological network communities more closely than the Euclidean clusters. Second, we are able to clearly distinguish between periods of market stability and volatility through a statistical analysis of hyperbolic distance and hyperbolic shortest path distance corresponding to the embedded network. Third, we demonstrate that using the modularity of the embedded network significant market changes can be spotted early. Lastly, the coalescent embedding is able to segregate the certain market sectors thereby underscoring its natural clustering ability.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04962",
        "category": "q-fin",
        "title": "A Comparison of Cryptocurrency Volatility-benchmarking New and Mature Asset Classes",
        "abstract": "The paper analyzes the cryptocurrency ecosystem at both the aggregate and individual levels to understand the factors that impact future volatility. The study uses high-frequency panel data from 2020 to 2022 to examine the relationship between several market volatility drivers, such as daily leverage, signed volatility and jumps. Several known autoregressive model specifications are estimated over different market regimes, and results are compared to equity data as a reference benchmark of a more mature asset class. The panel estimations show that the positive market returns at the high-frequency level increase price volatility, contrary to what is expected from the classical financial literature. We attributed this effect to the price dynamics over the last year of the dataset (2022) by repeating the estimation on different time spans. Moreover, the positive signed volatility and negative daily leverage positively impact the cryptocurrencies' future volatility, unlike what emerges from the same study on a cross-section of stocks. This result signals a structural difference in a nascent cryptocurrency market that has to mature yet. Further individual-level analysis confirms the findings of the panel analysis and highlights that these effects are statistically significant and commonly shared among many components in the selected universe.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.04989",
        "category": "q-fin",
        "title": "Towards a representative social cost of carbon",
        "abstract": "The majority of estimates of the social cost of carbon use preference parameters calibrated to data for North America and Europe. We here use representative data for attitudes to time and risk across the world. The social cost of carbon is substantially higher in the global north than in the south. The difference is more pronounced if we count people rather than countries.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.05214",
        "category": "q-fin",
        "title": "Generalized measure Black\u2013Scholes equation: towards option self-similar pricing",
        "abstract": null,
        "references": [
            {
                "arxivId": "1802.09925",
                "title": "The finite difference method for the heat equation on Sierpi\u0144ski simplices",
                "abstract": "ABSTRACT In the sequel, we extend our previous work on the Minkowski Curve to Sierpi\u0144ski simplices (Gasket and Tetrahedron), in the case of the heat equation. First, we build the finite difference scheme. Then, we give a theoretical study of the error, compute the scheme error, give stability conditions, and prove the convergence of the scheme. Contrary to existing work, we do not call for approximations of the eigenvalues."
            },
            {
                "arxivId": "1703.09206",
                "title": "Control of the Black-Scholes equation",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.05230",
        "category": "q-fin",
        "title": "Non-concave distributionally robust stochastic control in a discrete time finite horizon setting",
        "abstract": "In this article we present a general framework for non-concave distributionally robust stochastic control problems in a discrete time finite horizon setting. Our framework allows to consider a variety of different path-dependent ambiguity sets of probability measures comprising, as a natural example, the ambiguity set defined via Wasserstein-balls around path-dependent reference measures, as well as parametric classes of probability distributions. We establish a dynamic programming principle which allows to derive both optimal control and worst-case measure by solving recursively a sequence of one-step optimization problems. As a concrete application, we study the robust hedging problem of a financial derivative under an asymmetric (and non-convex) loss function accounting for different preferences of sell- and buy side when it comes to the hedging of financial derivatives. As our entirely data-driven ambiguity set of probability measures, we consider Wasserstein-balls around the empirical measure derived from real financial data. We demonstrate that during adverse scenarios such as a financial crisis, our robust approach outperforms typical model-based hedging strategies such as the classical Delta-hedging strategy as well as the hedging strategy obtained in the non-robust setting with respect to the empirical measure and therefore overcomes the problem of model misspecification in such critical periods.",
        "references": [
            {
                "arxivId": "2403.11824",
                "title": "Nonconcave Robust Utility Maximization under Projective Determinacy",
                "abstract": "We study a robust utility maximization problem in a general discrete-time frictionless market. The investor is assumed to have a random, nonconcave and nondecreasing utility function, which may or may not be finite on the whole real-line. She also faces model ambiguity on her beliefs about the market, which is modeled through a set of priors. We prove, using only primal methods, the existence of an optimal investment strategy when the utility function is also upper-semicontinuous. For that, we introduce the new notion of projectively measurable functions. We show basic properties of these functions as stability under sums, differences, products, suprema, infima and compositions but also assuming the set-theoretical axiom of Projective Determinacy (PD) stability under integration and existence of $\\epsilon$-optimal selectors. We consider projectively measurable random utility function and price process and assume that the graphs of the sets of local priors are projective sets. Our other assumptions are stated on a prior-by-prior basis and correspond to generally accepted assumptions in the literature on markets without ambiguity."
            },
            {
                "arxivId": "2403.09532",
                "title": "Robust SGLD algorithm for solving non-convex distributionally robust optimisation problems",
                "abstract": "In this paper we develop a Stochastic Gradient Langevin Dynamics (SGLD) algorithm tailored for solving a certain class of non-convex distributionally robust optimisation problems. By deriving non-asymptotic convergence bounds, we build an algorithm which for any prescribed accuracy $\\varepsilon>0$ outputs an estimator whose expected excess risk is at most $\\varepsilon$. As a concrete application, we employ our robust SGLD algorithm to solve the (regularised) distributionally robust Mean-CVaR portfolio optimisation problem using real financial data. We empirically demonstrate that the trading strategy obtained by our robust SGLD algorithm outperforms the trading strategy obtained when solving the corresponding non-robust Mean-CVaR portfolio optimisation problem using, e.g., a classical SGLD algorithm. This highlights the practical relevance of incorporating model uncertainty when optimising portfolios in real financial markets."
            },
            {
                "arxivId": "2307.11919",
                "title": "Discrete time optimal investment under model uncertainty",
                "abstract": "We study a robust utility maximization problem in a general discrete-time frictionless market under quasi-sure no-arbitrage. The investor is assumed to have a random and concave utility function defined on the whole real-line. She also faces model ambiguity on her beliefs about the market, which is modeled through a set of priors. We prove the existence of an optimal investment strategy using only primal methods. For that we assume classical assumptions on the market and on the random utility function as asymptotic elasticity constraints. Most of our other assumptions are stated on a prior-by-prior basis and correspond to generally accepted assumptions in the literature on markets without ambiguity. We also propose a general setting including utility functions with benchmark for which our assumptions are easily checked."
            },
            {
                "arxivId": "2209.04976",
                "title": "Data-Driven Nonparametric Robust Control under Dependence Uncertainty",
                "abstract": "We consider a multi-period stochastic control problem where the multivariate driving stochastic factor of the system has known marginal distributions but uncertain dependence structure. To solve the problem, we propose to implement the nonparametric adaptive robust control framework. We aim to find the optimal control against the worst-case copulae in a sequence of shrinking uncertainty sets which are generated from continuously observing the data. Then, we use a stochastic gradient descent ascent algorithm to numerically handle the corresponding high dimensional dynamic inf-sup optimization problem. We present the numerical results in the context of utility maximization and show that the controller benefits from knowing more information about the uncertain model."
            },
            {
                "arxivId": "2208.10735",
                "title": "Robust Control Problems of BSDEs Coupled with Value Functions",
                "abstract": "A robust control problem is considered in this paper, where the controlled stochastic differential equations (SDEs) include ambiguity parameters and their coefficients satisfy non-Lipschitz continuous and non-linear growth conditions, the objective function is expressed as a backward stochastic differential equation (BSDE) with the generator depending on the value function. We establish the existence and uniqueness of the value function in a proper space and provide a verification theorem. Moreover, we apply the results to solve two typical optimal investment problems in the market with ambiguity, one of which is with Heston stochastic volatility model. In particular, we establish some sharp estimations for Heston model with ambiguity parameters."
            },
            {
                "arxivId": "2206.06109",
                "title": "Markov decision processes under model uncertainty",
                "abstract": "We introduce a general framework for Markov decision problems under model uncertainty in a discrete\u2010time infinite horizon setting. By providing a dynamic programming principle, we obtain a local\u2010to\u2010global paradigm, namely solving a local, that is, a one time\u2010step robust optimization problem leads to an optimizer of the global (i.e., infinite time\u2010steps) robust stochastic optimal control problem, as well as to a corresponding worst\u2010case measure. Moreover, we apply this framework to portfolio optimization involving data of the S&P500$S\\&P\\nobreakspace 500$ . We present two different types of ambiguity sets; one is fully data\u2010driven given by a Wasserstein\u2010ball around the empirical measure, the second one is described by a parametric set of multivariate normal distributions, where the corresponding uncertainty sets of the parameters are estimated from the data. It turns out that in scenarios where the market is volatile or bearish, the optimal portfolio strategies from the corresponding robust optimization problem outperforms the ones without model uncertainty, showcasing the importance of taking model uncertainty into account."
            },
            {
                "arxivId": "2202.10391",
                "title": "Nonparametric Adaptive Robust Control under Model Uncertainty",
                "abstract": "We consider a discrete time stochastic Markovian control problem under model uncertainty. Such uncertainty not only comes from the fact that the true probability law of the underlying stochastic process is unknown, but the parametric family of probability distributions which the true law belongs to is also unknown. We propose a nonparametric adaptive robust control methodology to deal with such problem. Our approach hinges on the following building concepts: first, using the adaptive robust paradigm to incorporate online learning and uncertainty reduction into the robust control problem; second, learning the unknown probability law through the empirical distribution, and representing uncertainty reduction in terms of a sequence of Wasserstein balls around the empirical distribution; third, using Lagrangian duality to convert the optimization over Wasserstein balls to a scalar optimization problem, and adopting a machine learning technique to achieve efficient computation of the optimal control. We illustrate our methodology by considering a utility maximization problem. Numerical comparisons show that the nonparametric adaptive robust control approach is preferable to the traditional robust frameworks."
            },
            {
                "arxivId": "2107.11340",
                "title": "Deep Equal Risk Pricing of Financial Derivatives with Non-Translation Invariant Risk Measures",
                "abstract": "The objective is to study the use of non-translation invariant risk measures within the equal risk pricing (ERP) methodology for the valuation of financial derivatives. The ability to move beyond the class of convex risk measures considered in several prior studies provides more flexibility within the pricing scheme. In particular, suitable choices for the risk measure embedded in the ERP framework, such as the semi-mean-square-error (SMSE), are shown herein to alleviate the price inflation phenomenon observed under the tail value at risk-based ERP as documented in previous work. The numerical implementation of non-translation invariant ERP is performed through deep reinforcement learning, where a slight modification is applied to the conventional deep hedging training algorithm so as to enable obtaining a price through a single training run for the two neural networks associated with the respective long and short hedging strategies. The accuracy of the neural network training procedure is shown in simulation experiments not to be materially impacted by such modification of the training algorithm."
            },
            {
                "arxivId": "2004.07162",
                "title": "On linear optimization over Wasserstein balls",
                "abstract": null
            },
            {
                "arxivId": "2001.04727",
                "title": "Wasserstein Distributionally Robust Motion Control for Collision Avoidance Using Conditional Value-at-Risk",
                "abstract": "In this article, a risk-aware motion control scheme is considered for mobile robots to avoid randomly moving obstacles when the true probability distribution of uncertainty is unknown. We propose a novel model-predictive control (MPC) method for limiting the risk of unsafety even when the true distribution of the obstacles\u2019 movements deviates, within an ambiguity set, from the empirical distribution obtained using a limited amount of sample data. By choosing the ambiguity set as a statistical ball with its radius measured by the Wasserstein metric, we achieve a probabilistic guarantee of the out-of-sample risk, evaluated using new sample data generated independently of the training data. To resolve the infinite-dimensionality issue inherent in the distributionally robust MPC problem, we reformulate it as a finite-dimensional nonlinear program using modern distributionally robust optimization techniques based on the Kantorovich duality principle. To find a globally optimal solution in the case of affine dynamics and output equations, a spatial branch-and-bound algorithm is designed using McCormick relaxation. The performance of the proposed method is demonstrated and analyzed through simulation studies using nonlinear dynamic and kinematic vehicle models and a linearized quadrotor model. The simulation results indicate that, even when the sample size is small, the proposed method can successfully avoid randomly moving obstacles with a guarantee of out-of-sample risk, while its sample average approximation counterpart fails to do so."
            },
            {
                "arxivId": "1911.10106",
                "title": "Speculative trading, prospect theory and transaction costs",
                "abstract": "A speculative agent with prospect theory preference chooses the optimal time to purchase and then to sell an indivisible risky asset to maximise the expected utility of the round-trip profit net of transaction costs. The optimisation problem is formulated as a sequential optimal stopping problem, and we provide a complete characterisation of the solution. Depending on the preference and market parameters, the optimal strategy can be \u201cbuy and hold\u201d, \u201cbuy low, sell high\u201d, \u201cbuy high, sell higher\u201d or \u201cno trading\u201d. Behavioural preference and market friction interact in a subtle way which yields surprising implications on the agent\u2019s trading patterns. For example, increasing the market entry fee does not necessarily curb speculative trading, but instead may induce a higher reference point under which the agent becomes more risk-seeking and in turn is more likely to trade."
            },
            {
                "arxivId": "1909.00748",
                "title": "Portfolio liquidation under factor uncertainty",
                "abstract": "We study an optimal liquidation problem under the ambiguity with respect to price impact parameters. Our main results show that the value function and the optimal trading strategy can be characterized by the solution to a semi-linear PDE with superlinear gradient, monotone generator and singular terminal value. We also establish an asymptotic analysis of the robust model for small amount of uncertainty and analyse the effect of robustness on optimal trading strategies and liquidation costs. In particular, in our model ambiguity aversion is observationally equivalent to increased risk aversion. This suggests that ambiguity aversion increases liquidation rates."
            },
            {
                "arxivId": "1812.09808",
                "title": "Wasserstein Distributionally Robust Stochastic Control: A Data-Driven Approach",
                "abstract": "Standard stochastic control methods assume that the probability distribution of uncertain variables is available. Unfortunately, in practice, obtaining accurate distribution information is a challenging task. To resolve this issue, in this article we investigate the problem of designing a control policy that is robust against errors in the empirical distribution obtained from data. This problem can be formulated as a two-player zero-sum dynamic game problem, where the action space of the adversarial player is a Wasserstein ball centered at the empirical distribution. A dynamic programming solution is provided exploiting the reformulation techniques for Wasserstein distributionally robust optimization. We show that the contraction property of associated Bellman operators extends a single-stage out-of-sample performance guarantee, obtained using a measure concentration inequality, to the corresponding multistage guarantee without any degradation in the confidence level. Furthermore, we characterize an explicit form of the optimal control policy and the worst-case distribution policy for linear-quadratic problems with Wasserstein penalty."
            },
            {
                "arxivId": "1801.06860",
                "title": "On utility maximization under model uncertainty in discrete\u2010time markets",
                "abstract": "We study the problem of maximizing terminal utility for an agent facing model uncertainty, in a frictionless discrete\u2010time market with one safe asset and finitely many risky assets. We show that an optimal investment strategy exists if the utility function, defined either on the positive real line or on the whole real line, is bounded from above. We further find that the boundedness assumption can be dropped, provided that we impose suitable integrability conditions, related to some strengthened form of no\u2010arbitrage. These results are obtained in an alternative framework for model uncertainty, where all possible dynamics of the stock prices are represented by a collection of stochastic processes on the same filtered probability space, rather than by a family of probability measures."
            },
            {
                "arxivId": "1712.07699",
                "title": "Robust expected utility maximization with medial limits",
                "abstract": null
            },
            {
                "arxivId": "1711.03875",
                "title": "Nonconcave robust optimization with discrete strategies under Knightian uncertainty",
                "abstract": null
            },
            {
                "arxivId": "1610.09230",
                "title": "Robust Utility Maximization in Discrete-Time Markets with Friction",
                "abstract": "We study a robust stochastic optimization problem in the quasi-sure setting in discrete-time. We show that under a lineality-type condition the problem admits a maximizer. This condition is implied by the no-arbitrage condition in models of financial markets. As a corollary, we obtain existence of an utility maximizer in the frictionless market model, markets with proportional transaction costs and also more general convex costs, like in the case of market impact."
            },
            {
                "arxivId": "1610.06805",
                "title": "Robust Markowitz mean\u2010variance portfolio selection under ambiguous covariance matrix",
                "abstract": "This paper studies a robust continuous\u2010time Markowitz portfolio selection problem where the model uncertainty affects the covariance matrix of multiple risky assets. This problem is formulated into a min\u2013max mean\u2010variance problem over a set of nondominated probability measures that is solved by a McKean\u2013Vlasov dynamic programming approach, which allows us to characterize the solution in terms of a Bellman\u2013Isaacs equation in the Wasserstein space of probability measures. We provide explicit solutions for the optimal robust portfolio strategies and illustrate our results in the case of uncertain volatilities and ambiguous correlation between two risky assets. We then derive the robust efficient frontier in closed form, and obtain a lower bound for the Sharpe ratio of any robust efficient portfolio strategy. Finally, we compare the performance of Sharpe ratios for a robust investor and for an investor with a misspecified model."
            },
            {
                "arxivId": "1610.00999",
                "title": "Exponential utility maximization under model uncertainty for unbounded endowments",
                "abstract": "We consider the robust exponential utility maximization problem in discrete time: An investor maximizes the worst case expected exponential utility with respect to a family of non-dominated probabilistic models of her endowment by dynamically investing in a financial market. We show that, for any measurable random endowment (regardless of whether the problem is finite or not) an optimal strategy exists, a dual representation in terms of martingale measures holds true, and that the problem satisfies the dynamic programming principle."
            },
            {
                "arxivId": "1604.01446",
                "title": "Quantifying Distributional Model Risk Via Optimal Transport",
                "abstract": "This paper deals with the problem of quantifying the impact of model misspecification when computing general expected values of interest. The methodology that we propose is applicable in great generality, in particular, we provide examples involving path dependent expectations of stochastic processes. Our approach consists in computing bounds for the expectation of interest regardless of the probability measure used, as long as the measure lies within a prescribed tolerance measured in terms of a flexible class of distances from a suitable baseline model. These distances, based on optimal transportation between probability measures, include Wasserstein's distances as particular cases. The proposed methodology is well-suited for risk analysis, as we demonstrate with a number of applications. We also discuss how to estimate the tolerance region non-parametrically using Skorokhod-type embeddings in some of these applications."
            },
            {
                "arxivId": "1505.05116",
                "title": "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations",
                "abstract": null
            },
            {
                "arxivId": "1307.3597",
                "title": "UTILITY MAXIMIZATION UNDER MODEL UNCERTAINTY IN DISCRETE TIME",
                "abstract": "We give a general formulation of the utility maximization problem under nondominated model uncertainty in discrete time and show that an optimal portfolio exists for any utility function that is bounded from above. In the unbounded case, integrability conditions are needed as nonexistence may arise even if the value function is finite."
            },
            {
                "arxivId": "1305.6008",
                "title": "Arbitrage and duality in nondominated discrete-time models",
                "abstract": "We consider a nondominated model of a discrete-time financial market where stocks are traded dynamically and options are available for static hedging. In a general measure-theoretic setting, we show that absence of arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of martingale measures. In the arbitrage-free case, we show that optimal superhedging strategies exist for general contingent claims, and that the minimal superhedging price is given by the supremum over the martingale measures. Moreover, we obtain a nondominated version of the Optional Decomposition Theorem."
            },
            {
                "arxivId": "1003.4431",
                "title": "Quasi-sure Stochastic Analysis through Aggregation",
                "abstract": "This paper is on developing stochastic analysis simultaneously under a general family of probability measures that are not dominated by a single probability measure. The interest in this question originates from the probabilistic representations of fully nonlinear partial differential equations and applications to mathematical finance. The existing literature relies either on the capacity theory (Denis and Martini), or on the underlying nonlinear partial differential equation (Peng). In both approaches, the resulting theory requires certain smoothness, the so-called quasi-sure continuity, of the corresponding processes and random variables in terms of the underlying canonical process. In this paper, we investigate this question for a larger class of ``non-smooth\" processes, but with a restricted family of non-dominated probability measures. For smooth processes, our approach leads to similar results as in previous literature, provided the restricted family satisfies an additional density property."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-09.json",
        "arxivId": "2404.05372",
        "category": "q-fin",
        "title": "The Peal Method: A Mathematical Framework to Streamline Securitization Structuring",
        "abstract": "Securitization is a financial process where the cash flows of income-generating assets are sold to institutional investors as securities, liquidating illiquid assets. This practice presents persistent challenges due to the absence of a comprehensive mathematical framework for structuring asset-backed securities. While existing literature provides technical analysis of credit risk modeling, there remains a need for a definitive framework detailing the allocation of the inbound cash flows to the outbound positions. To fill this gap, we introduce the PEAL Method: a 10-step mathematical framework to streamline the securitization structuring across all time periods. The PEAL Method offers a rigorous and versatile approach, allowing practitioners to structure various types of securitizations, including those with complex vertical positions. By employing standardized equations, it facilitates the delineation of payment priorities and enhances risk characterization for both the asset and the liability sides throughout the securitization life cycle. In addition to its technical contributions, the PEAL Method aims to elevate industry standards by addressing longstanding challenges in securitization. By providing detailed information to investors and enabling transparent risk profile comparisons, it promotes market transparency and enables stronger regulatory oversight. In summary, the PEAL Method represents a significant advancement in securitization literature, offering a standardized framework for precision and efficiency in structuring transactions. Its adoption has the potential to drive innovation and enhance risk management practices in the securitization market.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-10.json",
        "arxivId": "2303.09682",
        "category": "q-fin",
        "title": "Quantum Monte Carlo simulations for financial risk analytics: scenario generation for equity, rate, and credit risk factors",
        "abstract": "Monte Carlo (MC) simulations are widely used in financial risk management, from estimating value-at-risk (VaR) to pricing over-the-counter derivatives. However, they come at a significant computational cost due to the number of scenarios required for convergence. If a probability distribution is available, Quantum Amplitude Estimation (QAE) algorithms can provide a quadratic speed-up in measuring its properties as compared to their classical counterparts. Recent studies have explored the calculation of common risk measures and the optimisation of QAE algorithms by initialising the input quantum states with pre-computed probability distributions. If such distributions are not available in closed form, however, they need to be generated numerically, and the associated computational cost may limit the quantum advantage. In this paper, we bypass this challenge by incorporating scenario generation \u2013 i.e. simulation of the risk factor evolution over time to generate probability distributions \u2013 into the quantum computation; we refer to this process as Quantum MC (QMC) simulations. Specifically, we assemble quantum circuits that implement stochastic models for equity (geometric Brownian motion), interest rate (mean-reversion models), and credit (structural, reduced-form, and rating migration credit models) risk factors. We then integrate these models with QAE to provide end-to-end examples for both market and credit risk use cases.",
        "references": [
            {
                "arxivId": "2310.03011",
                "title": "Quantum algorithms: A survey of applications and end-to-end complexities",
                "abstract": "The anticipated applications of quantum computers span across science and industry, ranging from quantum chemistry and many-body physics to optimization, finance, and machine learning. Proposed quantum solutions in these areas typically combine multiple quantum algorithmic primitives into an overall quantum algorithm, which must then incorporate the methods of quantum error correction and fault tolerance to be implemented correctly on quantum hardware. As such, it can be difficult to assess how much a particular application benefits from quantum computing, as the various approaches are often sensitive to intricate technical details about the underlying primitives and their complexities. Here we present a survey of several potential application areas of quantum algorithms and their underlying algorithmic primitives, carefully considering technical caveats and subtleties. We outline the challenges and opportunities in each area in an\"end-to-end\"fashion by clearly defining the problem being solved alongside the input-output model, instantiating all\"oracles,\"and spelling out all hidden costs. We also compare quantum solutions against state-of-the-art classical methods and complexity-theoretic limitations to evaluate possible quantum speedups. The survey is written in a modular, wiki-like fashion to facilitate navigation of the content. Each primitive and application area is discussed in a standalone section, with its own bibliography of references and embedded hyperlinks that direct to other relevant sections. This structure mirrors that of complex quantum algorithms that involve several layers of abstraction, and it enables rapid evaluation of how end-to-end complexities are impacted when subroutines are altered."
            },
            {
                "arxivId": "2307.14310",
                "title": "Derivative Pricing using Quantum Signal Processing",
                "abstract": "Pricing financial derivatives on quantum computers typically includes quantum arithmetic components which contribute heavily to the quantum resources required by the corresponding circuits. In this manuscript, we introduce a method based on Quantum Signal Processing (QSP) to encode financial derivative payoffs directly into quantum amplitudes, alleviating the quantum circuits from the burden of costly quantum arithmetic. Compared to current state-of-the-art approaches in the literature, we find that for derivative contracts of practical interest, the application of QSP significantly reduces the required resources across all metrics considered, most notably the total number of T-gates by $\\sim 16$x and the number of logical qubits by $\\sim 4$x. Additionally, we estimate that the logical clock rate needed for quantum advantage is also reduced by a factor of $\\sim 5$x. Overall, we find that quantum advantage will require $4.7$k logical qubits, and quantum devices that can execute $10^9$ T-gates at a rate of $45$MHz. While in this work we focus specifically on the payoff component of the derivative pricing process where the method we present is most readily applicable, similar techniques can be employed to further reduce the resources in other applications, such as state preparation."
            },
            {
                "arxivId": "2307.11230",
                "title": "Quantum computing for finance",
                "abstract": null
            },
            {
                "arxivId": "2303.04945",
                "title": "A Survey of Quantum Alternatives to Randomized Algorithms: Monte Carlo Integration and Beyond",
                "abstract": "Monte Carlo sampling is a powerful toolbox of algorithmic techniques widely used for a number of applications wherein some noisy quantity, or summary statistic thereof, is sought to be estimated. In this paper, we survey the literature for implementing Monte Carlo procedures using quantum circuits, focusing on the potential to obtain a quantum advantage in the computational speed of these procedures. We revisit the quantum algorithms that could replace classical Monte Carlo and then consider both the existing quantum algorithms and the potential quantum realizations that include adaptive enhancements as alternatives to the classical procedure."
            },
            {
                "arxivId": "2210.14892",
                "title": "Quantum state preparation without coherent arithmetic",
                "abstract": "We introduce a versatile method for preparing a quantum state whose amplitudes are given by some known function. Unlike existing approaches, our method does not require handcrafted reversible arithmetic circuits, or quantum memory loads, to encode the function values. Instead, we use a template quantum eigenvalue transformation circuit to convert a low cost block encoding of the sine function into the desired function. Our method uses only 4 ancilla qubits (3 if the approximating polynomial has definite parity), providing order-of-magnitude qubit count reductions compared to state-of-the-art approaches, while using a similar number of Toffoli gates if the function can be well represented by a polynomial or Fourier approximation. Like black-box methods, the complexity of our approach depends on the 'L2-norm filling-fraction' of the function. We demonstrate the efficiency of our method for preparing states commonly used in quantum algorithms, such as Gaussian and Kaiser window states."
            },
            {
                "arxivId": "2203.04924",
                "title": "Quantum advantage for multi-option portfolio pricing and valuation adjustments",
                "abstract": "A critical problem in the financial world deals with the management of risk, from regulatory risk to portfolio risk. Many such problems involve the analysis of securities modelled by complex dynamics that cannot be captured analytically, and hence rely on numerical techniques that simulate the stochastic nature of the underlying variables. These techniques may be computationally difficult or demanding. Hence, improving these methods offers a variety of opportunities for quantum algorithms. In this work, we study the problem of Credit Valuation Adjustments (CVAs) which have significant importance in the valuation of derivative portfolios. We propose quantum algorithms that accelerate statistical sampling processes to approximate the CVA under different measures of dispersion, using known techniques in Quantum Monte Carlo (QMC) and analyse the conditions under which we may employ these techniques."
            },
            {
                "arxivId": "2201.02773",
                "title": "A Survey of Quantum Computing for Finance",
                "abstract": "Quantum computers are expected to surpass the computational capabilities of classical computers during this decade and have transformative impact on numerous industry sectors, particularly finance. In fact, finance is estimated to be the first industry sector to benefit from quantum computing, not only in the medium and long terms, but even in the short term. This survey paper presents a comprehensive summary of the state of the art of quantum computing for financial applications, with particular emphasis on stochastic modeling, optimization, and machine learning, describing how these solutions, adapted to work on a quantum computer, can potentially help to solve financial problems, such as derivative pricing, risk modeling, portfolio optimization, natural language processing, and fraud detection, more efficiently and accurately. We also discuss the feasibility of these algorithms on nearterm quantum computers with various hardware implementations and demonstrate how they relate to a wide range of use cases in finance. We hope this article will not only serve as a reference for academic researchers and industry practitioners but also inspire new ideas for future research. These authors contributed equally to this work. i ar X iv :2 20 1. 02 77 3v 4 [ qu an tph ] 2 7 Ju n 20 22"
            },
            {
                "arxivId": "2111.15332",
                "title": "Quantum Algorithm for Stochastic Optimal Stopping Problems with Applications in Finance",
                "abstract": "The famous least squares Monte Carlo (LSM) algorithm combines linear least square regression with Monte Carlo simulation to approximately solve problems in stochastic optimal stopping theory. In this work, we propose a quantum LSM based on quantum access to a stochastic process, on quantum circuits for computing the optimal stopping times, and on quantum techniques for Monte Carlo. For this algorithm, we elucidate the intricate interplay of function approximation and quantum algorithms for Monte Carlo. Our algorithm achieves a nearly quadratic speedup in the runtime compared to the LSM algorithm under some mild assumptions. Specifically, our quantum algorithm can be applied to American option pricing and we analyze a case study for the common situation of Brownian motion and geometric Brownian motion processes."
            },
            {
                "arxivId": "2111.12509",
                "title": "Towards Quantum Advantage in Financial Market Risk using Quantum Gradient Algorithms",
                "abstract": "We introduce a quantum algorithm to compute the market risk of financial derivatives. Previous work has shown that quantum amplitude estimation can accelerate derivative pricing quadratically in the target error and we extend this to a quadratic error scaling advantage in market risk computation. We show that employing quantum gradient estimation algorithms can deliver a further quadratic advantage in the number of the associated market sensitivities, usually called greeks. By numerically simulating the quantum gradient estimation algorithms on financial derivatives of practical interest, we demonstrate that not only can we successfully estimate the greeks in the examples studied, but that the resource requirements can be significantly lower in practice than what is expected by theoretical complexity bounds. This additional advantage in the computation of financial market risk lowers the estimated logical clock rate required for financial quantum advantage from Chakrabarti et al. [Quantum 5, 463 (2021)] by a factor of ~7, from 50MHz to 7MHz, even for a modest number of greeks by industry standards (four). Moreover, we show that if we have access to enough resources, the quantum algorithm can be parallelized across 60 QPUs, in which case the logical clock rate of each device required to achieve the same overall runtime as the serial execution would be ~100kHz. Throughout this work, we summarize and compare several different combinations of quantum and classical approaches that could be used for computing the market risk of financial derivatives."
            },
            {
                "arxivId": "2109.03687",
                "title": "Variational quantum amplitude estimation",
                "abstract": "We propose to perform amplitude estimation with the help of constant-depth quantum circuits that variationally approximate states during amplitude amplification. In the context of Monte Carlo (MC) integration, we numerically show that shallow circuits can accurately approximate many amplitude amplification steps. We combine the variational approach with maximum likelihood amplitude estimation [Y. Suzuki et al., Quantum Inf. Process. 19, 75 (2020)] in variational quantum amplitude estimation (VQAE). VQAE typically has larger computational requirements than classical MC sampling. To reduce the variational cost, we propose adaptive VQAE and numerically show in 6 to 12 qubit simulations that it can outperform classical MC sampling."
            },
            {
                "arxivId": "2106.02678",
                "title": "A universal quantum circuit design for periodical functions",
                "abstract": "We propose a universal quantum circuit design that can estimate any arbitrary one-dimensional periodic functions based on the corresponding Fourier expansion. The quantum circuit contains N-qubits to store the information on the different N-Fourier components and M + 2 auxiliary qubits with M = \u2308log2\u2009 N\u2309 for control operations. The desired output will be measured in the last qubit q N with a time complexity of the computation of O(N2\u2308log2N\u23092) , which leads to polynomial speedup under certain circumstances. We illustrate the approach by constructing the quantum circuit for the square wave function with accurate results obtained by direct simulations using the IBM-QASM simulator. The approach is general and can be applied to any arbitrary periodic function."
            },
            {
                "arxivId": "2105.12087",
                "title": "Quantum algorithm for credit valuation adjustments",
                "abstract": "Quantum mechanics is well known to accelerate statistical sampling processes over classical techniques. In quantitative finance, statistical samplings arise broadly in many use cases. Here we focus on a particular one of such use cases, credit valuation adjustment (CVA), and identify opportunities and challenges towards quantum advantage for practical instances. To build a NISQ-friendly quantum circuit able to solve such problem, we draw on various heuristics that indicate the potential for significant improvement over well-known techniques such as reversible logical circuit synthesis. In minimizing the resource requirements for amplitude amplification while maximizing the speedup gained from the quantum coherence of a noisy device, we adopt a recently developed Bayesian variant of quantum amplitude estimation using engineered likelihood functions. We perform numerical analyses to characterize the prospect of quantum speedup in concrete CVA instances over classical Monte Carlo simulations."
            },
            {
                "arxivId": "2101.02240",
                "title": "No quantum speedup with Grover-Rudolph state preparation for quantum Monte Carlo integration.",
                "abstract": "We prove that there is no quantum speedup when using quantum Monte Carlo integration to estimate the mean (and other moments) of analytically defined log-concave probability distributions prepared as quantum states using the Grover-Rudolph method."
            },
            {
                "arxivId": "2012.06283",
                "title": "Quantum-accelerated multilevel Monte Carlo methods for stochastic differential equations in mathematical finance",
                "abstract": "Inspired by recent progress in quantum algorithms for ordinary and partial differential equations, we study quantum algorithms for stochastic differential equations (SDEs). Firstly we provide a quantum algorithm that gives a quadratic speed-up for multilevel Monte Carlo methods in a general setting. As applications, we apply it to compute expectation values determined by classical solutions of SDEs, with improved dependence on precision. We demonstrate the use of this algorithm in a variety of applications arising in mathematical finance, such as the Black-Scholes and Local Volatility models, and Greeks. We also provide a quantum algorithm based on sublinear binomial sampling for the binomial option pricing model with the same improvement."
            },
            {
                "arxivId": "2012.03819",
                "title": "A Threshold for Quantum Advantage in Derivative Pricing",
                "abstract": "We give an upper bound on the resources required for valuable quantum advantage in pricing derivatives. To do so, we give the first complete resource estimates for useful quantum derivative pricing, using autocallable and Target Accrual Redemption Forward (TARF) derivatives as benchmark use cases. We uncover blocking challenges in known approaches and introduce a new method for quantum derivative pricing \u2013 the re-parameterization method \u2013 that avoids them. This method combines pre-trained variational circuits with fault-tolerant quantum computing to dramatically reduce resource requirements. We find that the benchmark use cases we examine require 8k logical qubits and a T-depth of 54 million. We estimate that quantum advantage would require executing this program at the order of a second. While the resource requirements given here are out of reach of current systems, we hope they will provide a roadmap for further improvements in algorithms, implementations, and planned hardware architectures."
            },
            {
                "arxivId": "2011.02165",
                "title": "Quantum speedup of Monte Carlo integration with respect to the number of dimensions and its application to finance",
                "abstract": null
            },
            {
                "arxivId": "2008.04110",
                "title": "Quantum computation for pricing the collateralized debt obligations",
                "abstract": "Collateral debt obligation (CDO) has been one of the most commonly used structured financial products and is intensively studied in quantitative finance. By setting the asset pool into different tranches, it effectively works out and redistributes credit risks and returns to meet the risk preferences for different tranche investors. The copula models of various kinds are normally used for pricing CDOs, and the Monte Carlo simulations are required to get their numerical solution. Here we implement two typical CDO models, the single-factor Gaussian copula model and Normal Inverse Gaussian copula model, and by applying the conditional independence approach, we manage to load each model of distribution in quantum circuits. We then apply quantum amplitude estimation as an alternative to Monte Carlo simulation for CDO pricing. We demonstrate the quantum computation results using IBM Qiskit. Our work addresses a useful task in finance instrument pricing, significantly broadening the application scope for quantum computing in finance."
            },
            {
                "arxivId": "2006.14510",
                "title": "Quantum Computing for Finance: State-of-the-Art and Future Prospects",
                "abstract": "This article outlines our point of view regarding the applicability, state-of-the-art, and potential of quantum computing for problems in finance. We provide an introduction to quantum computing as well as a survey on problem classes in finance that are computationally challenging classically and for which quantum computing algorithms are promising. In the main part, we describe in detail quantum algorithms for specific applications arising in financial services, such as those involving simulation, optimization, and machine learning problems. In addition, we include demonstrations of quantum algorithms on IBM Quantum back-ends and discuss the potential benefits of quantum algorithms for problems in financial services. We conclude with a summary of technical challenges and future prospects."
            },
            {
                "arxivId": "2005.07711",
                "title": "Efficient State Preparation for Quantum Amplitude Estimation",
                "abstract": "Quantum Amplitude Estimation (QAE) can achieve a quadratic speed-up for applications classically solved by Monte Carlo simulation. A key requirement to realize this advantage is efficient state preparation. If state preparation is too expensive, it can diminish the quantum advantage. Preparing arbitrary quantum states has exponential complexity with respect to the number of qubits, thus, is not applicable. Currently known efficient techniques require problems based on log-concave probability distributions, involve learning an unknown distribution from empirical data, or fully rely on quantum arithmetic. In this paper, we introduce an approach to simplify state preparation, together with a circuit optimization technique, both of which can help reduce the circuit complexity for QAE state preparation significantly. We demonstrate the introduced techniques for a numerical integration example on real quantum hardware, as well as for option pricing under the Heston model, i.e., based on a stochastic volatility process, using simulation."
            },
            {
                "arxivId": "1912.05559",
                "title": "Iterative quantum amplitude estimation",
                "abstract": null
            },
            {
                "arxivId": "1907.03044",
                "title": "Credit Risk Analysis Using Quantum Computers",
                "abstract": "We present and analyze a quantum algorithm to estimate credit risk more efficiently than Monte Carlo simulations can do on classical computers. More precisely, we estimate the economic capital requirement, i.e. the difference between the Value at Risk and the expected value of a given loss distribution. The economic capital requirement is an important risk metric because it summarizes the amount of capital required to remain solvent at a given confidence level. We implement this problem for a realistic loss distribution and analyze its scaling to a realistic problem size. In particular, we provide estimates of the total number of required qubits, the expected circuit depth, and how this translates into an expected runtime under reasonable assumptions on future fault-tolerant quantum hardware."
            },
            {
                "arxivId": "1905.02666",
                "title": "Option Pricing using Quantum Computers",
                "abstract": "We present a methodology to price options and portfolios of options on a gate-based quantum computer using amplitude estimation, an algorithm which provides a quadratic speedup compared to classical Monte Carlo methods. The options that we cover include vanilla options, multi-asset options and path-dependent options such as barrier options. We put an emphasis on the implementation of the quantum circuits required to build the input states and operators needed by amplitude estimation to price the different option types. Additionally, we show simulation results to highlight how the circuits that we implement price the different option contracts. Finally, we examine the performance of option pricing circuits on quantum hardware using the IBM Q Tokyo quantum device. We employ a simple, yet effective, error mitigation scheme that allows us to significantly reduce the errors arising from noisy two-qubit gates."
            },
            {
                "arxivId": "1904.10246",
                "title": "Amplitude estimation without phase estimation",
                "abstract": null
            },
            {
                "arxivId": "1904.00043",
                "title": "Quantum Generative Adversarial Networks for learning and loading random distributions",
                "abstract": null
            },
            {
                "arxivId": "1807.03890",
                "title": "Quantum computing for finance: Overview and prospects",
                "abstract": null
            },
            {
                "arxivId": "1806.06893",
                "title": "Quantum risk analysis",
                "abstract": null
            },
            {
                "arxivId": "1805.00109",
                "title": "Quantum computational finance: Monte Carlo pricing of financial derivatives",
                "abstract": "Financial derivatives are contracts that can have a complex payoff dependent upon underlying benchmark assets. In this work, we present a quantum algorithm for the Monte Carlo pricing of financial derivatives. We show how the relevant probability distributions can be prepared in quantum superposition, the payoff functions can be implemented via quantum circuits, and the price of financial derivatives can be extracted via quantum measurements. We show how the amplitude estimation algorithm can be applied to achieve a quadratic quantum speedup in the number of steps required to obtain an estimate for the price with high confidence. This work provides a starting point for further research at the interface of quantum computing and finance."
            },
            {
                "arxivId": "1801.00862",
                "title": "Quantum Computing in the NISQ era and beyond",
                "abstract": "Noisy Intermediate-Scale Quantum (NISQ) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. NISQ devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away - we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing."
            },
            {
                "arxivId": "1504.06987",
                "title": "Quantum speedup of Monte Carlo methods",
                "abstract": "Monte Carlo methods use random sampling to estimate numerical quantities which are hard to compute deterministically. One important example is the use in statistical physics of rapidly mixing Markov chains to approximately compute partition functions. In this work, we describe a quantum algorithm which can accelerate Monte Carlo methods in a very general setting. The algorithm estimates the expected output value of an arbitrary randomized or quantum subroutine with bounded variance, achieving a near-quadratic speedup over the best possible classical algorithm. Combining the algorithm with the use of quantum walks gives a quantum speedup of the fastest known classical algorithms with rigorous performance bounds for computing partition functions, which use multiple-stage Markov chain Monte Carlo techniques. The quantum algorithm can also be used to estimate the total variation distance between probability distributions efficiently."
            },
            {
                "arxivId": "quant-ph/0208112",
                "title": "Creating superpositions that correspond to efficiently integrable probability distributions",
                "abstract": "We give a simple and efficient process for generating a quantum superposition of states which form a discrete approximation of any efficiently integrable (such as log concave) probability density functions."
            },
            {
                "arxivId": "quant-ph/0005055",
                "title": "Quantum Amplitude Amplification and Estimation",
                "abstract": "Consider a Boolean function $\\chi: X \\to \\{0,1\\}$ that partitions set $X$ between its good and bad elements, where $x$ is good if $\\chi(x)=1$ and bad otherwise. Consider also a quantum algorithm $\\mathcal A$ such that $A |0\\rangle= \\sum_{x\\in X} \\alpha_x |x\\rangle$ is a quantum superposition of the elements of $X$, and let $a$ denote the probability that a good element is produced if $A |0\\rangle$ is measured. If we repeat the process of running $A$, measuring the output, and using $\\chi$ to check the validity of the result, we shall expect to repeat $1/a$ times on the average before a solution is found. *Amplitude amplification* is a process that allows to find a good $x$ after an expected number of applications of $A$ and its inverse which is proportional to $1/\\sqrt{a}$, assuming algorithm $A$ makes no measurements. This is a generalization of Grover's searching algorithm in which $A$ was restricted to producing an equal superposition of all members of $X$ and we had a promise that a single $x$ existed such that $\\chi(x)=1$. Our algorithm works whether or not the value of $a$ is known ahead of time. In case the value of $a$ is known, we can find a good $x$ after a number of applications of $A$ and its inverse which is proportional to $1/\\sqrt{a}$ even in the worst case. We show that this quadratic speedup can also be obtained for a large family of search problems for which good classical heuristics exist. Finally, as our main result, we combine ideas from Grover's and Shor's quantum algorithms to perform amplitude estimation, a process that allows to estimate the value of $a$. We apply amplitude estimation to the problem of *approximate counting*, in which we wish to estimate the number of $x\\in X$ such that $\\chi(x)=1$. We obtain optimal quantum algorithms in a variety of settings."
            },
            {
                "arxivId": "quant-ph/9605043",
                "title": "A fast quantum mechanical algorithm for database search",
                "abstract": "were proposed in the early 1980\u2019s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80\u2019s and early 90\u2019s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------"
            },
            {
                "arxivId": "quant-ph/9511018",
                "title": "Quantum networks for elementary arithmetic operations.",
                "abstract": "Quantum computers require quantum arithmetic. We provide an explicit construction of quantum networks effecting basic arithmetic operations: from addition to modular exponentiation. Quantum modular exponentiation seems to be the most difficult (time and space consuming) part of Shor's quantum factorizing algorithm. We show that the auxiliary memory required to perform this operation in a reversible way grows linearly with the size of the number to be factorized. \\textcopyright{} 1996 The American Physical Society."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-10.json",
        "arxivId": "2403.08811",
        "category": "q-fin",
        "title": "The UK Universities Superannuation Scheme valuations 2014-2023: gilt yield dependence, self-sufficiency and metrics",
        "abstract": "This review considers the Universities Superannuation Scheme (USS) valuations from 2014 to 2023. USS is a 70-80 billion GBP Defined Benefit pension scheme with over 500,000 members who are employed (or have been employed) at around 70 UK universities. Disputes over USS have led to a decade of industrial action. New results are presented showing the high dependence of USS pension contributions on the return from UK government bonds (the gilt yield). The two conditions of the USS-specific 'self-sufficiency' (SfS) definition are examined. USS data are presented along with new analysis. It is shown that the second SfS condition of 'maintaining a high funding ratio' dominates USS modelling to amplify gilt yield dependence, inflating the SfS liabilities beyond the regulatory requirements, and leading to excessive prudence. The Red, Amber and Green status of USS metrics 'Actual' and 'Target' Reliance are also examined. It is shown that Target Reliance tethers the cost of future pensions to the SfS definition and that Actual Reliance can simultaneously be Green and Red. Implications for regulatory intervention are considered. An aim of this review is to support evidence-based decision making and consensus building.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-10.json",
        "arxivId": "2404.04335",
        "category": "q-fin",
        "title": "Estimating contagion mechanism in global equity market with time\u2010zone effect",
        "abstract": "This paper proposes a time-zone vector autoregression (VAR) model to investigate comovements in the global financial market. Analyzing daily data from 36 national equity markets, we explore the subprime and European debt crises using static analysis and the COVID-19 crisis through a rolling window method. Our study of comovements using VAR coefficients reveals a resonance effect in the global system. Findings on densities and assortativities suggest the existence of the transmission mechanism in all periods and abnormal structural changes during the crises. Strength analysis uncovers the information transmission mechanism across continents over normal and turmoil periods and emphasizes specific stock markets' unique roles. We examine dynamic continent strengths to demonstrate the contagion mechanism in the global equity market over an extended period. Incorporating the time-zone effect significantly enhances the VAR model's interpretability. Signed networks provide more information on global equity markets and better identifies critical contagion patterns than unsigned networks.",
        "references": [
            {
                "arxivId": "2403.19363",
                "title": "Dynamic correlation of market connectivity, risk spillover and abnormal volatility in stock price",
                "abstract": null
            },
            {
                "arxivId": "2403.19439",
                "title": "Dynamic Analyses of Contagion Risk and Module Evolution on the SSE A-Shares Market Based on Minimum Information Entropy",
                "abstract": "The interactive effect is significant in the Chinese stock market, exacerbating the abnormal market volatilities and risk contagion. Based on daily stock returns in the Shanghai Stock Exchange (SSE) A-shares, this paper divides the period between 2005 and 2018 into eight bull and bear market stages to investigate interactive patterns in the Chinese financial market. We employ the Least Absolute Shrinkage and Selection Operator (LASSO) method to construct the stock network, compare the heterogeneity of bull and bear markets, and further use the Map Equation method to analyse the evolution of modules in the SSE A-shares market. Empirical results show that (1) the connected effect is more significant in bear markets than bull markets and gives rise to abnormal volatilities in the stock market; (2) a system module can be found in the network during the first four stages, and the industry aggregation effect leads to module differentiation in the last four stages; (3) some stocks have leading effects on others throughout eight periods, and medium- and small-cap stocks with poor financial conditions are more likely to become risk sources, especially in bear markets. Our conclusions are beneficial to improving investment strategies and making regulatory policies."
            },
            {
                "arxivId": "2012.12702",
                "title": "Systemic Risk in Financial Networks: A Survey",
                "abstract": "We provide an overview of the relationship between financial networks and systemic risk. We present a taxonomy of different types of systemic risk, differentiating between direct externalities between financial organizations (e.g., defaults, correlated portfolios, fire sales), and perceptions and feedback effects (e.g., bank runs, credit freezes). We also discuss optimal regulation and bailouts, measurements of systemic risk and financial centrality, choices by banks regarding their portfolios and partnerships, and the changing nature of financial networks."
            },
            {
                "arxivId": "1711.00097",
                "title": "Bayesian Markov Switching Tensor Regression For Time-Varying Networks",
                "abstract": "We propose a new Bayesian Markov switching regression model for multi-dimensional arrays (tensors) of binary time series. We assume a zero-inflated logit dynamics with time-varying parameters and apply it to multi-layer temporal networks. The original contribution is threefold. First, in order to avoid over-fitting we propose a parsimonious parametrization of the model, based on a low-rank decomposition of the tensor of regression coefficients. Second, the parameters of the tensor model are driven by a hidden Markov chain, thus allowing for structural changes. The regimes are identified through prior constraints on the mixing probability of the zero-inflated model. Finally, we model the jointly dynamics of the network and of a set of variables of interest. We follow a Bayesian approach to inference, exploiting the Polya-Gamma data augmentation scheme for logit models in order to provide an efficient Gibbs sampler for posterior approximation. We show the effectiveness of the sampler on simulated datasets of medium-big sizes, finally we apply the methodology to a real dataset of financial networks."
            },
            {
                "arxivId": "cond-mat/0311416",
                "title": "The architecture of complex weighted networks.",
                "abstract": "Networked structures arise in a wide array of different contexts such as technological and transportation infrastructures, social phenomena, and biological systems. These highly interconnected systems have recently been the focus of a great deal of attention that has uncovered and characterized their topological complexity. Along with a complex topological structure, real networks display a large heterogeneity in the capacity and intensity of the connections. These features, however, have mainly not been considered in past studies where links are usually represented as binary states, i.e., either present or absent. Here, we study the scientific collaboration network and the world-wide air-transportation network, which are representative examples of social and large infrastructure systems, respectively. In both cases it is possible to assign to each edge of the graph a weight proportional to the intensity or capacity of the connections among the various elements of the network. We define appropriate metrics combining weighted and topological observables that enable us to characterize the complex statistical properties and heterogeneity of the actual strength of edges and vertices. This information allows us to investigate the correlations among weighted quantities and the underlying topological structure of the network. These results provide a better description of the hierarchies and organizational principles at the basis of the architecture of weighted networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-10.json",
        "arxivId": "2404.06472",
        "category": "q-fin",
        "title": "High-skilled Human Workers in Non-Routine Jobs are Susceptible to AI Automation but Wage Benefits Differ between Occupations",
        "abstract": "Artificial Intelligence (AI) will change human work by taking over specific job tasks, but there is a debate which tasks are susceptible to automation, and whether AI will augment or replace workers and affect wages. By combining data on job tasks with a measure of AI susceptibility, we show that more highly skilled workers are more susceptible to AI automation, and that analytical non-routine tasks are at risk to be impacted by AI. Moreover, we observe that wage growth premiums for the lowest and the highest required skill level appear unrelated to AI susceptibility and that workers in occupations with many routine tasks saw higher wage growth if their work was more strongly susceptible to AI. Our findings imply that AI has the potential to affect human workers differently than canonical economic theories about the impact of technology on work these theories predict.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "1603.00751",
        "category": "q-fin",
        "title": "Equity forecast: Predicting long term stock price movement using machine learning",
        "abstract": "Long term investment is one of the major investment strategies. However, calculating intrinsic value of some company and evaluating shares for long term investment is not easy, since analyst have to care about a large number of financial indicators and evaluate them in a right manner. So far, little help in predicting the direction of the company value over the longer period of time has been provided from the machines. In this paper we present a machine learning aided approach to evaluate the equity's future price over the long time. Our method is able to correctly predict whether some company's value will be 10% higher or not over the period of one year in 76.5% of cases.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2101.06077",
        "category": "q-fin",
        "title": "ESTIMATION OF FUTURE DISCRETIONARY BENEFITS IN TRADITIONAL LIFE INSURANCE",
        "abstract": "Abstract In the context of life insurance with profit participation, the future discretionary benefits (FDB), which are a central item for Solvency II reporting, are generally calculated by computationally expensive Monte Carlo algorithms. We derive analytic formulas to estimate lower and upper bounds for the FDB. This yields an estimation interval for the FDB, and the average of lower and upper bound is a simple estimator. These formulae are designed for real world applications, and we compare the results to publicly available reporting data.",
        "references": [
            {
                "arxivId": "1802.07009",
                "title": "Analytical validation formulas for best estimate calculation in traditional life insurance",
                "abstract": null
            },
            {
                "arxivId": "1607.04100",
                "title": "Insurance valuation : A computable multi-period cost-of-capital approach",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2303.18161",
        "category": "q-fin",
        "title": "Nash equilibria for relative investors with (non)linear price impact",
        "abstract": null,
        "references": [
            {
                "arxivId": "2302.10485",
                "title": "Optimal Investment with a Noisy Signal of Future Stock Prices",
                "abstract": null
            },
            {
                "arxivId": "2206.05425",
                "title": "Mean field portfolio games with consumption",
                "abstract": null
            },
            {
                "arxivId": "2111.02310",
                "title": "Nash equilibria for relative investors via no-arbitrage arguments",
                "abstract": null
            },
            {
                "arxivId": "2106.06185",
                "title": "Mean field portfolio games",
                "abstract": null
            },
            {
                "arxivId": "2012.01235",
                "title": "Forward Utility and Market Adjustments in Relative Investment-Consumption Games of Many Players",
                "abstract": "We study a portfolio management problem featuring many-player and mean-field competition, investment and consumption, and relative performance concerns under the forward performance processes (FPP) framework. We focus on agents using power (CRRA) type FPPs for their investment-consumption optimization problem an under common noise Merton market model and we solve both the many-player and mean-field game providing closed-form expressions for the solutions where the limit of the former yields the latter. In our case, the FPP framework yields a continuum of solutions for the consumption component as indexed to a market parameter we coin \"market consumption intensity\". The parameter permits the agent to set a preference for their consumption going forward in time that, in the competition case, reflects a common market behaviour. We show the FPP framework, under both competition and no-competition, allows the agent to disentangle his risk-tolerance and elasticity of intertemporal substitution (EIS) just like Epstein-Zin preferences under recursive utility framework and unlike the classical utility theory one. This, in turn, allows a finer analysis on the agent's consumption \"income\" and \"substitution\" regimes, and, of independent interest, motivates a new strand of economics research on EIS under the FPP framework. We find that competition rescales the agent's perception of consumption in a non-trivial manner in addition to a time-dependent \"elasticity of conformity\" of the agent to the market's consumption intensity."
            },
            {
                "arxivId": "2006.07684",
                "title": "Mean Field Exponential Utility Game: A Probabilistic Approach",
                "abstract": "We study an $N$-player and a mean field exponential utility game. Each player manages two stocks; one is driven by an individual shock and the other is driven by a common shock. Moreover, each player is concerned not only with her own terminal wealth but also with the relative performance of her competitors. We use the probabilistic approach to study these two games. We show the unique equilibrium of the $N$-player game and the mean field game can be characterized by a novel multi-dimensional FBSDE with quadratic growth and a novel mean-field FBSDEs, respectively. The well-posedness result and the convergence result are established."
            },
            {
                "arxivId": "2005.09461",
                "title": "Forward Utilities and Mean-Field Games Under Relative Performance Concerns",
                "abstract": null
            },
            {
                "arxivId": "1905.11782",
                "title": "Many-player games of optimal consumption and investment under relative performance criteria",
                "abstract": null
            },
            {
                "arxivId": "1807.03813",
                "title": "Nash Equilibrium for Risk-Averse Investors in a Market Impact Game with Transient Price Impact",
                "abstract": "We consider a market impact game for [Formula: see text] risk-averse agents that are competing in a market model with linear transient price impact and additional transaction costs. For both finite and infinite time horizons, the agents aim to minimize a mean-variance functional of their costs or to maximize the expected exponential utility of their revenues. We give explicit representations for corresponding Nash equilibria and prove uniqueness in the case of mean-variance optimization. A qualitative analysis of these Nash equilibria is conducted by means of numerical analysis."
            },
            {
                "arxivId": "1804.04911",
                "title": "A Mean Field Game of Optimal Portfolio Liquidation",
                "abstract": "We consider a mean field game (MFG) of optimal portfolio liquidation under asymmetric information. We prove that the solution to the MFG can be characterized in terms of a forward-backward stochastic differential equation (FBSDE) with a possibly singular terminal condition on the backward component or, equivalently, in terms of an FBSDE with a finite terminal value yet a singular driver. Extending the method of continuation to linear-quadratic FBSDEs with a singular driver, we prove that the MFG has a unique solution. Our existence and uniqueness result allows proving that the MFG with a possibly singular terminal condition can be approximated by a sequence of MFGs with finite terminal values."
            },
            {
                "arxivId": "1703.07685",
                "title": "Mean field and n\u2010agent games for optimal investment under relative performance criteria",
                "abstract": "We analyze a family of portfolio management problems under relative performance criteria, for fund managers having CARA or CRRA utilities and trading in a common investment horizon in log\u2010normal markets. We construct explicit constant equilibrium strategies for both the finite population games and the corresponding mean field games, which we show are unique in the class of constant equilibria. In the CARA case, competition drives agents to invest more in the risky asset than they would otherwise, while in the CRRA case competitive agents may over\u2010 or underinvest, depending on their levels of risk tolerance."
            },
            {
                "arxivId": "1509.08281",
                "title": "High-Frequency Limit of Nash Equilibria in a Market Impact Game with Transient Price Impact",
                "abstract": "We study the high-frequency limits of strategies and costs in a Nash equilibrium for two agents that are competing to minimize liquidation costs in a discrete-time market impact model with exponentially decaying price impact and quadratic transaction costs of size $\\theta\\ge0$. We show that, for $\\theta=0$, equilibrium strategies and costs will oscillate indefinitely between two accumulation points. For $\\theta>0$, however, strategies, costs, and total transaction costs will converge towards limits that are independent of $\\theta$. We then show that the limiting strategies form a Nash equilibrium for a continuous-time version of the model with $\\theta$ equal to a certain critical value $\\theta^*>0$, and that the corresponding expected costs coincide with the high-frequency limits of the discrete-time equilibrium costs. For $\\theta\\neq\\theta^*$, however, continuous-time Nash equilibria will typically not exist. Our results permit us to give mathematically rigorous proofs of numerical observations made in [..."
            },
            {
                "arxivId": "1312.7360",
                "title": "A STATE\u2010CONSTRAINED DIFFERENTIAL GAME ARISING IN OPTIMAL PORTFOLIO LIQUIDATION",
                "abstract": "We consider n risk\u2010averse agents who compete for liquidity in an Almgren\u2013Chriss market impact model. Mathematically, this situation can be described by a Nash equilibrium for a certain linear quadratic differential game with state constraints. The state constraints enter the problem as terminal boundary conditions for finite and infinite time horizons. We prove existence and uniqueness of Nash equilibria and give closed\u2010form solutions in some special cases. We also analyze qualitative properties of the equilibrium strategies and provide corresponding financial interpretations."
            },
            {
                "arxivId": "1305.4013",
                "title": "A Market Impact Game Under Transient Price Impact",
                "abstract": "We consider a Nash equilibrium between two high-frequency traders in a simple market impact model with transient price impact and additional quadratic transaction costs. Extending a result by Sch\\\"oneborn (2008), we prove existence and uniqueness of the Nash equilibrium and show that for small transaction costs the high-frequency traders engage in a \"hot-potato game\", in which the same asset position is sold back and forth. We then identify a critical value for the size of the transaction costs above which all oscillations disappear and strategies become buy-only or sell-only. Numerical simulations show that for both traders the expected costs can be lower with transaction costs than without. Moreover, the costs can increase with the trading frequency when there are no transaction costs, but decrease with the trading frequency when transaction costs are sufficiently high. We argue that these effects occur due to the need of protection against predatory trading in the regime of low transaction costs."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2306.08760",
        "category": "q-fin",
        "title": "Do Productivity Shocks Cause Inputs Misallocation?",
        "abstract": "This paper investigates how productivity dispersion relates to input misallocation using a model with staggered productivity shocks that create wedges between anticipated and realized productivity for any production input. With inputs allocated optimally ex ante but suboptimally ex post, dispersion in realized productivity contributes to ex post input misallocation. Analyzing European firm data from 2000-2017 reveals significant co-movement between productivity dispersion and capital/labor misallocation across industries. Productivity dispersion explains a substantial share of capital and labor misallocation (40% and 70%), and 10% of materials misallocation, confirming its key role in allocation frictions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2307.11732",
        "category": "q-fin",
        "title": "Advancing Ad Auction Realism: Practical Insights & Modeling Implications",
        "abstract": "Contemporary real-world online ad auctions differ from canonical models [Edelman et al., 2007; Varian, 2009] in at least four ways: (1) values and click-through rates can depend upon users' search queries, but advertisers can only partially\"tune\"their bids to specific queries; (2) advertisers do not know the number, identity, and precise value distribution of competing bidders; (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially known to bidders. These features make it virtually impossible to fully characterize equilibrium bidding behavior. This paper shows that, nevertheless, one can still gain useful insight into modern ad auctions by modeling advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. To demonstrate our approach, we first simulate\"soft-floor\"auctions [Zeithammer, 2019], a complex, real-world pricing rule for which no complete equilibrium characterization is known. We find that (i) when values and click-through rates are query-dependent, soft floors can improve revenues relative to standard auction formats even if bidder types are drawn from the same distribution; and (ii) with distributional asymmetries that reflect relevant real-world scenario, we find that soft floors yield lower revenues than suitably chosen reserve prices, even restricting attention to a single query. We then demonstrate how to infer advertiser value distributions from observed bids for a variety of pricing rules, and illustrate our approach with aggregate data from an e-commerce website.",
        "references": [
            {
                "arxivId": "2003.09795",
                "title": "Optimal No-regret Learning in Repeated First-price Auctions",
                "abstract": "We study online learning in repeated first-price auctions with censored feedback, where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid in order to maximize her cumulative payoff. To achieve this goal, the bidder faces a challenging dilemma: if she wins the bid--the only way to achieve positive payoffs--then she is not able to observe the highest bid of the other bidders, which we assume is iid drawn from an unknown distribution. This dilemma, despite being reminiscent of the exploration-exploitation trade-off in contextual bandits, cannot directly be addressed by the existing UCB or Thompson sampling algorithms in that literature, mainly because contrary to the standard bandits setting, when a positive reward is obtained here, nothing about the environment can be learned. \nIn this paper, by exploiting the structural properties of first-price auctions, we develop the first learning algorithm that achieves $O(\\sqrt{T}\\log^2 T)$ regret bound when the bidder's private values are stochastically generated. We do so by providing an algorithm on a general class of problems, which we call monotone group contextual bandits, where the same regret bound is established under stochastically generated contexts. Further, by a novel lower bound argument, we characterize an $\\Omega(T^{2/3})$ lower bound for the case where the contexts are adversarially generated, thus highlighting the impact of the contexts generation mechanism on the fundamental learning limit. Despite this, we further exploit the structure of first-price auctions and develop a learning algorithm that operates sample-efficiently (and computationally efficiently) in the presence of adversarially generated private values. We establish an $O(\\sqrt{T}\\log^3 T)$ regret bound for this algorithm, hence providing a complete characterization of optimal learning guarantees for this problem."
            },
            {
                "arxivId": "1802.08365",
                "title": "Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising",
                "abstract": "Real-time bidding (RTB) is an important mechanism in online display advertising, where a proper bid for each page view plays an essential role for good marketing results. Budget constrained bidding is a typical scenario in RTB where the advertisers hope to maximize the total value of the winning impressions under a pre-set budget constraint. However, the optimal bidding strategy is hard to be derived due to the complexity and volatility of the auction environment. To address these challenges, in this paper, we formulate budget constrained bidding as a Markov Decision Process and propose a model-free reinforcement learning framework to resolve the optimization problem. Our analysis shows that the immediate reward from environment is misleading under a critical resource constraint. Therefore, we innovate a reward function design methodology for the reinforcement learning problems with constraints. Based on the new reward design, we employ a deep neural network to learn the appropriate reward so that the optimal policy can be learned effectively. Different from the prior model-based work, which suffers from the scalability problem, our framework is easy to be deployed in large-scale industrial applications. The experimental evaluations demonstrate the effectiveness of our framework on large-scale real datasets."
            },
            {
                "arxivId": "1711.01333",
                "title": "Learning to Bid Without Knowing your Value",
                "abstract": "We address online learning in complex auction settings, such as sponsored search auctions, where the value of the bidder is unknown to her, evolving in an arbitrary manner and observed only if the bidder wins an allocation. We leverage the structure of the utility of the bidder and the partial feedback that bidders typically receive in auctions, in order to provide algorithms with regret rates against the best fixed bid in hindsight, that are exponentially faster in convergence in terms of dependence on the action space, than what would have been derived by applying a generic bandit algorithm and almost equivalent to what would have been achieved in the full information setting. Our results are enabled by analyzing a new online learning setting with outcome-based feedback, which generalizes learning with feedback graphs. We provide an online learning algorithm for this setting, of independent interest, with regret that grows only logarithmically with the number of actions and linearly only in the number of potential outcomes (the latter being very small in most auction settings). Last but not least, we show that our algorithm outperforms the bandit approach experimentally and that this performance is robust to dropping some of our theoretical assumptions or introducing noise in the feedback that the bidder receives."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2311.12183",
        "category": "q-fin",
        "title": "Optimal Transport Divergences Induced by Scoring Functions",
        "abstract": "We employ scoring functions, used in statistics for eliciting risk functionals, as cost functions in the Monge-Kantorovich (MK) optimal transport problem. This gives raise to a rich variety of novel asymmetric MK divergences, which subsume the family of Bregman-Wasserstein divergences. We show that for distributions on the real line, the comonotonic coupling is optimal for the majority of the new divergences. Specifically, we derive the optimal coupling of the MK divergences induced by functionals including the mean, generalised quantiles, expectiles, and shortfall measures. Furthermore, we show that while any elicitable law-invariant coherent risk measure gives raise to infinitely many MK divergences, the comonotonic coupling is simultaneously optimal. The novel MK divergences, which can be efficiently calculated, open an array of applications in robust stochastic optimisation. We derive sharp bounds on distortion risk measures under a Bregman-Wasserstein divergence constraint, and solve for cost-efficient payoffs under benchmark constraints.",
        "references": [
            {
                "arxivId": "2302.05833",
                "title": "Bregman-Wasserstein divergence: geometry and applications",
                "abstract": "Consider the Monge-Kantorovich optimal transport problem where the cost function is given by a Bregman divergence. The associated transport cost, which we call the Bregman-Wasserstein divergence, presents a natural asymmetric extension of the squared $2$-Wasserstein metric and has recently found applications in statistics and machine learning. On the other hand, Bregman divergence is a fundamental object in information geometry and induces a dually flat geometry on the underlying manifold. Using the Bregman-Wasserstein divergence, we lift this dualistic geometry to the space of probability measures, thus extending Otto's weak Riemannian structure of the Wasserstein space to statistical manifolds. We do so by generalizing Lott's formal geometric computations on the Wasserstein space. In particular, we define primal and dual connections on the space of probability measures and show that they are conjugate with respect to Otto's metric. We also define primal and dual displacement interpolations which satisfy the corresponding geodesic equations. As applications, we study displacement convexity and the Bregman-Wasserstein barycenter."
            },
            {
                "arxivId": "2205.08850",
                "title": "Robust Distortion Risk Measures",
                "abstract": "Robustness of risk measures to changes in underlying loss distributions (distributional uncertainty) is of crucial importance when making well-informed risk management decisions. In this paper, we quantify for any given distortion risk measure its robustness to distributional uncertainty by deriving its range of attainable values when the underlying loss distribution has a known mean and variance and furthermore lies within a ball - specified through the Wasserstein distance - around a reference distribution. We extend our results to account for uncertainty in the first two moments and provide an application to model risk assessment."
            },
            {
                "arxivId": "1910.07912",
                "title": "Forecast evaluation of quantiles, prediction intervals, and other set-valued functionals",
                "abstract": "We introduce a theoretical framework of elicitability and identifiability of set-valued functionals, such as quantiles, prediction intervals, and systemic risk measures. A functional is elicitable if it is the unique minimiser of an expected scoring function, and identifiable if it is the unique zero of an expected identification function; both notions are essential for forecast ranking and validation, and $M$- and $Z$-estimation. Our framework distinguishes between exhaustive forecasts, being set-valued and aiming at correctly specifying the entire functional, and selective forecasts, content with solely specifying a single point in the correct functional. We establish a mutual exclusivity result: A set-valued functional can be either selectively elicitable or exhaustively elicitable or not elicitable at all. Notably, since quantiles are well known to be selectively elicitable, they fail to be exhaustively elicitable. We further show that the class of prediction intervals and Vorob'ev quantiles turn out to be exhaustively elicitable and selectively identifiable. In particular, we provide a mixture representation of elementary exhaustive scores, leading the way to Murphy diagrams. We give possibility and impossibility results for the shortest prediction interval and prediction intervals specified by an endpoint or a midpoint. We end with a comprehensive literature review on common practice in forecast evaluation of set-valued functionals."
            },
            {
                "arxivId": "1603.09491",
                "title": "On the properties of the Lambda value at risk: robustness, elicitability and consistency",
                "abstract": "Recently, the financial industry and regulators have enhanced the debate on the good properties of a risk measure. A fundamental issue is the evaluation of the quality of a risk estimation. On the one hand, a backtesting procedure is desirable for assessing the accuracy of such an estimation and this can be naturally achieved by elicitable risk measures. For the same objective, an alternative approach has been introduced by Davis [Stat. Risk Model. Appl. Finance Insurance, 2016, 33, 67\u201393] through the so-called consistency property. On the other hand, a risk estimation should be less sensitive with respect to small changes in the available data-set and exhibit qualitative robustness. A new risk measure, the Lambda value at risk (), has been recently proposed by Frittelli et al. [Math. Finance, 2014, 24, 442\u2013463], as a generalization of VaR with the ability to discriminate the risk among P&L distributions with different tail behaviour. In this article, we show that also satisfies the properties of robustness, elicitability and consistency under some conditions."
            },
            {
                "arxivId": "1503.08123",
                "title": "Higher order elicitability and Osband\u2019s principle",
                "abstract": "A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability for multi-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the `correct' quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications."
            },
            {
                "arxivId": "0912.0902",
                "title": "Making and Evaluating Point Forecasts",
                "abstract": "Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2401.09955",
        "category": "q-fin",
        "title": "Consistent asset modelling with random coefficients and switches between regimes",
        "abstract": null,
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2404.05101",
        "category": "q-fin",
        "title": "StockGPT: A GenAI Model for Stock Prediction and Trading",
        "abstract": "This paper introduces StockGPT, an autoregressive ``number'' model trained and tested on 70 million daily U.S. stock returns over nearly 100 years. Treating each return series as a sequence of tokens, StockGPT automatically learns the hidden patterns predictive of future returns via its attention mechanism. On a held-out test sample from 2001 to 2023, a daily rebalanced long-short portfolio formed from StockGPT predictions earns an annual return of 119% with a Sharpe ratio of 6.5. The StockGPT-based portfolio completely spans momentum and long-/short-term reversals, eliminating the need for manually crafted price-based strategies, and also encompasses most leading stock market factors. This highlights the immense promise of generative AI in surpassing human in making complex financial investment decisions.",
        "references": [
            {
                "arxivId": "2403.04667",
                "title": "The Social Impact of Generative AI: An Analysis on ChatGPT",
                "abstract": "In recent months, the impact of Artificial Intelligence (AI) on citizens\u2019 lives has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a citizen-centric AI."
            },
            {
                "arxivId": "2304.07619",
                "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models",
                "abstract": "We examine the potential of ChatGPT and other large language models in predicting stock market returns using news headlines. We use ChatGPT to assess whether each headline is good, bad, or neutral for firms' stock prices. We document a significantly positive correlation between ChatGPT scores and subsequent daily stock returns. We find that ChatGPT outperforms traditional sentiment analysis methods. More basic models such as GPT-1, GPT-2, and BERT cannot accurately forecast returns, indicating return predictability is an emerging capacity of complex language models. Long-short strategies based on ChatGPT-4 deliver the highest Sharpe ratio. Furthermore, we find predictability in both small and large stocks, suggesting market underreaction to company news. Predictability is stronger among smaller stocks and stocks with bad news, consistent with limits-to-arbitrage also playing an important role. Finally, we propose a new method to evaluate and understand the models' reasoning capabilities. Overall, our results suggest that incorporating advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies."
            },
            {
                "arxivId": "2006.08097",
                "title": "FinBERT: A Pretrained Language Model for Financial Communications",
                "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at this https URL. We hope this will be useful for practitioners and researchers working on financial NLP tasks."
            },
            {
                "arxivId": "1706.03762",
                "title": "Attention is All you Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2404.07132",
        "category": "q-fin",
        "title": "Hedonic Models Incorporating ESG Factors for Time Series of Average Annual Home Prices",
        "abstract": "Using data from 2000 through 2022, we analyze the predictive capability of the annual numbers of new home constructions and four available environmental, social, and governance factors on the average annual price of homes sold in eight major U.S. cities. We contrast the predictive capability of a P-spline generalized additive model (GAM) against a strictly linear version of the commonly used generalized linear model (GLM). As the data for the annual price and predictor variables constitute non-stationary time series, to avoid spurious correlations in the analysis we transform each time series appropriately to produce stationary series for use in the GAM and GLM models. While arithmetic returns or first differences are adequate transformations for the predictor variables, for the average price response variable we utilize the series of innovations obtained from AR(q)-ARCH(1) fits. Based on the GAM results, we find that the influence of ESG factors varies markedly by city, reflecting geographic diversity. Notably, the presence of air conditioning emerges as a strong factor. Despite limitations on the length of available time series, this study represents a pivotal step toward integrating ESG considerations into predictive real estate models.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-11.json",
        "arxivId": "2404.07179",
        "category": "q-fin",
        "title": "Machine learning-based similarity measure to forecast M&A from patent data",
        "abstract": "Defining and finalizing Mergers and Acquisitions (M&A) requires complex human skills, which makes it very hard to automatically find the best partner or predict which firms will make a deal. In this work, we propose the MASS algorithm, a specifically designed measure of similarity between companies and we apply it to patenting activity data to forecast M&A deals. MASS is based on an extreme simplification of tree-based machine learning algorithms and naturally incorporates intuitive criteria for deals; as such, it is fully interpretable and explainable. By applying MASS to the Zephyr and Crunchbase datasets, we show that it outperforms LightGCN, a\"black box\"graph convolutional network algorithm. When similar companies have disjoint patenting activities, on the contrary, LightGCN turns out to be the most effective algorithm. This study provides a simple and powerful tool to model and predict M&A deals, offering valuable insights to managers and practitioners for informed decision-making.",
        "references": [
            {
                "arxivId": "2210.07292",
                "title": "Prediction and visualization of Mergers and Acquisitions using Economic Complexity",
                "abstract": "Mergers and Acquisitions represent important forms of business deals, both because of the volumes involved in the transactions and because of the role of the innovation activity of companies. Nevertheless, Economic Complexity methods have not been applied to the study of this field. By considering the patent activity of about one thousand companies, we develop a method to predict future acquisitions by assuming that companies deal more frequently with technologically related ones. We address both the problem of predicting a pair of companies for a future deal and that of finding a target company given an acquirer. We compare different forecasting methodologies, including machine learning and network-based algorithms, showing that a simple angular distance with the addition of the industry sector information outperforms the other approaches. Finally, we present the Continuous Company Space, a two-dimensional representation of firms to visualize their technological proximity and possible deals. Companies and policymakers can use this approach to identify companies most likely to pursue deals or explore possible innovation strategies."
            },
            {
                "arxivId": "2210.01001",
                "title": "Urban economic fitness and complexity from patent data",
                "abstract": null
            },
            {
                "arxivId": "2202.00458",
                "title": "Machine Learning to Assess Relatedness: The Advantage of Using Firm-Level Data",
                "abstract": "The relatedness between a country or a firm and a product is a measure of the feasibility of that economic activity. As such, it is a driver for investments at a private and institutional level. Traditionally, relatedness is measured using networks derived by country-level co-occurrences of product pairs, that is counting how many countries export both. In this work, we compare networks and machine learning algorithms trained not only on country-level data, but also on firms, which is something not much studied due to the low availability of firm-level data. We quantitatively compare the different measures of relatedness, by using them to forecast the exports at the country and firm level, assuming that more related products have a higher likelihood to be exported in the future. Our results show that relatedness is scale dependent: the best assessments are obtained by using machine learning on the same typology of data one wants to predict. Moreover, we found that while relatedness measures based on country data are not suitable for firms, firm-level data are very informative also for the development of countries. In this sense, models built on firm data provide a better assessment of relatedness. We also discuss the effect of using parameter optimization and community detection algorithms to identify clusters of related companies and products, finding that a partition into a higher number of blocks decreases the computational time while maintaining a prediction performance well above the network-based benchmarks."
            },
            {
                "arxivId": "2110.02004",
                "title": "Which will be your firm\u2019s next technology? Comparison between machine learning and network-based algorithms",
                "abstract": "We reconstruct the innovation dynamics of about two hundred thousand companies by following their patenting activity for about ten years. We define the technology portfolios of these companies as the set of the technological sectors present in the patents they submit. By assuming that companies move more frequently towards related sectors, we leverage their past activity to build network-based and machine learning algorithms to forecast the future submissions of patents in new sectors. We compare different prediction methodologies using suitable evaluation metrics, showing that tree-based machine learning algorithms outperform the standard methods based on networks of co-occurrences. This methodology can be applied by firms and policymakers to disentangle, given the present innovation activity, the feasible technological sectors from those that are out of reach."
            },
            {
                "arxivId": "2105.03391",
                "title": "Meta-validation of bipartite network projections",
                "abstract": null
            },
            {
                "arxivId": "2002.02126",
                "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                "abstract": "Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance. In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives."
            },
            {
                "arxivId": "1708.03511",
                "title": "Technology networks: the autocatalytic origins of innovation",
                "abstract": "We analyse the autocatalytic structure of technological networks and evaluate its significance for the dynamics of innovation patenting. To this aim, we define a directed network of technological fields based on the International Patents Classification, in which a source node is connected to a receiver node via a link if patenting activity in the source field anticipates patents in the receiver field in the same region more frequently than we would expect at random. We show that the evolution of the technology network is compatible with the presence of a growing autocatalytic structure, i.e. a portion of the network in which technological fields mutually benefit from being connected to one another. We further show that technological fields in the core of the autocatalytic set display greater fitness, i.e. they tend to appear in a greater number of patents, thus suggesting the presence of positive spillovers as well as positive reinforcement. Finally, we observe that core shifts take place whereby different groups of technology fields alternate within the autocatalytic structure; this points to the importance of recombinant innovation taking place between close as well as distant fields of the hierarchical classification of technological fields."
            },
            {
                "arxivId": "1707.05146",
                "title": "Unfolding the innovation system for the development of countries: coevolution of Science, Technology and Production",
                "abstract": null
            },
            {
                "arxivId": "1706.02216",
                "title": "Inductive Representation Learning on Large Graphs",
                "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
            },
            {
                "arxivId": "1612.05810",
                "title": "Patent portfolio analysis of cities: statistics and maps of technological inventiveness",
                "abstract": "ABSTRACT Cities can be considered as engines of the knowledge-based economy, because they are the primary sites of knowledge production activities that subsequently shape the rate and direction of technological change and economic growth. Patents provide rich information to analyse the knowledge specialization of specific places, such as technological details and information on inventors and entities involved. The technology codes attributed at the level of individual patent documents can be used to indicate the diversity and scope of the knowledge claims underlying a specific invention. In this study we introduce tools for portfolio analysis in terms of patents that provide insights into the technological specialization of cities. The mapping and analysis of patent portfolios of cities exploits data derived from the Unites States Patent and Trademark Office (USPTO) and dedicated tools (at https://leydesdorff.net/software/patents/). The results allow policy makers and other stakeholders to identify promising areas of further knowledge development, including smart specialization strategies."
            },
            {
                "arxivId": "1609.02907",
                "title": "Semi-Supervised Classification with Graph Convolutional Networks",
                "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
            },
            {
                "arxivId": "1603.02754",
                "title": "XGBoost: A Scalable Tree Boosting System",
                "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
            },
            {
                "arxivId": "1408.2138",
                "title": "How the Taxonomy of Products Drives the Economic Development of Countries",
                "abstract": "We introduce an algorithm able to reconstruct the relevant network structure on which the time evolution of country-product bipartite networks takes place. The significant links are obtained by selecting the largest values of the projected matrix. We first perform a number of tests of this filtering procedure on synthetic cases and a toy model. Then we analyze the bipartite network constituted by countries and exported products, using two databases for a total of almost 50 years. It is then possible to build a hierarchically directed network, in which the taxonomy of products emerges in a natural way. We study the influence of the structure of this taxonomy network on countries' development; in particular, guided by an example taken from the industrialization of South Korea, we link the structure of the taxonomy network to the empirical temporal connections between product activations, finding that the most relevant edges for countries' development are the ones suggested by our network. These results suggest paths in the product space which are easier to achieve, and so can drive countries' policies in the industrialization process."
            },
            {
                "arxivId": "0909.3890",
                "title": "The building blocks of economic complexity",
                "abstract": "For Adam Smith, wealth was related to the division of labor. As people and firms specialize in different activities, economic efficiency increases, suggesting that development is associated with an increase in the number of individual activities and with the complexity that emerges from the interactions between them. Here we develop a view of economic growth and development that gives a central role to the complexity of a country's economy by interpreting trade data as a bipartite network in which countries are connected to the products they export, and show that it is possible to quantify the complexity of a country's economy by characterizing the structure of this network. Furthermore, we show that the measures of complexity we derive are correlated with a country's level of income, and that deviations from this relationship are predictive of future growth. This suggests that countries tend to converge to the level of income dictated by the complexity of their productive structures, indicating that development efforts should focus on generating the conditions that would allow complexity to emerge to generate sustained growth and prosperity."
            },
            {
                "arxivId": "0708.2090",
                "title": "The Product Space Conditions the Development of Nations",
                "abstract": "Economies grow by upgrading the products they produce and export. The technology, capital, institutions, and skills needed to make newer products are more easily adapted from some products than from others. Here, we study this network of relatedness between products, or \u201cproduct space,\u201d finding that more-sophisticated products are located in a densely connected core whereas less-sophisticated products occupy a less-connected periphery. Empirically, countries move through the product space by developing goods close to those they currently produce. Most countries can reach the core only by traversing empirically infrequent distances, which may help explain why poor countries have trouble developing more competitive exports and fail to converge to the income levels of rich countries."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-12.json",
        "arxivId": "2404.07221",
        "category": "q-fin",
        "title": "Improving Retrieval for RAG based Question Answering Models on Financial Documents",
        "abstract": "The effectiveness of Large Language Models (LLMs) in generating accurate responses relies heavily on the quality of input provided, particularly when employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by sourcing the most relevant text chunk(s) to base queries upon. Despite the significant advancements in LLMs' response quality in recent years, users may still encounter inaccuracies or irrelevant answers; these issues often stem from suboptimal text chunk retrieval by RAG rather than the inherent capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine the RAG process. This paper explores the existing constraints of RAG pipelines and introduces methodologies for enhancing text retrieval. It delves into strategies such as sophisticated chunking techniques, query expansion, the incorporation of metadata annotations, the application of re-ranking algorithms, and the fine-tuning of embedding algorithms. Implementing these approaches can substantially improve the retrieval quality, thereby elevating the overall performance and reliability of LLMs in processing and responding to queries.",
        "references": [
            {
                "arxivId": "2312.05934",
                "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
                "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-12.json",
        "arxivId": "2404.07750",
        "category": "q-fin",
        "title": "The Broken Rung: Gender and the Leadership Gap",
        "abstract": "Addressing female underrepresentation in leadership positions has become a key policy objective. However, little is known about the extent to which leadership appeals differently to women. Collecting new data from a large firm, I document that women are substantially less likely to apply for early-career promotions. Realized application patterns and large-scale surveys reveal the role of an understudied feature of promotions -- having to assume responsibility over a team -- which is less appealing to women. This gender difference is not accounted for by standard explanations, such as success likelihood or confidence, but is rather a product of common design features of leadership positions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-15.json",
        "arxivId": "1906.04822",
        "category": "q-fin",
        "title": "From a stochastic model of economic exchange to measures of inequality",
        "abstract": null,
        "references": [
            {
                "arxivId": "1505.01274",
                "title": "Kinetic models of immediate exchange",
                "abstract": null
            },
            {
                "arxivId": "1305.4173",
                "title": "A model for stock returns and volatility",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0203046",
                "title": "Probability distribution of returns in the Heston model with stochastic volatility",
                "abstract": "Abstract We study the Heston model, where the stock price dynamics is governed by a geometrical (multiplicative) Brownian motion with stochastic variance. We solve the corresponding Fokker\u2010Planck equation exactly and, after integrating out the variance, find an analytic formula for the time\u2010dependent probability distribution of stock price changes (returns). The formula is in excellent agreement with the Dow\u2010Jones index for time lags from 1 to 250 trading days. For large returns, the distribution is exponential in log\u2010returns with a time\u2010dependent exponent, whereas for small returns it is Gaussian. For time lags longer than the relaxation time of variance, the probability distribution can be expressed in a scaling form using a Bessel function. The Dow\u2010Jones data for 1982\u20132001 follow the scaling function for seven orders of magnitude."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-15.json",
        "arxivId": "2404.08129",
        "category": "q-fin",
        "title": "One Factor to Bind the Cross-Section of Returns",
        "abstract": "We propose a new non-linear single-factor asset pricing model $r_{it}=h(f_{t}\\lambda_{i})+\\epsilon_{it}$. Despite its parsimony, this model represents exactly any non-linear model with an arbitrary number of factors and loadings -- a consequence of the Kolmogorov-Arnold representation theorem. It features only one pricing component $h(f_{t}\\lambda_{I})$, comprising a nonparametric link function of the time-dependent factor and factor loading that we jointly estimate with sieve-based estimators. Using 171 assets across major classes, our model delivers superior cross-sectional performance with a low-dimensional approximation of the link function. Most known finance and macro factors become insignificant controlling for our single-factor.",
        "references": [
            {
                "arxivId": "2012.03182",
                "title": "Binary Response Models for Heterogeneous Panel Data with Interactive Fixed Effects",
                "abstract": "In this paper, we investigate binary response models for heterogeneous panel data with interactive fixed effects by allowing both the cross sectional dimension and the temporal dimension to diverge. From a practical point of view, the proposed framework can be applied to predict the probability of corporate failure, conduct credit rating analysis, etc. Theoretically and methodologically, we establish a link between a maximum likelihood estimation and a least squares approach, provide a simple information criterion to detect the number of factors, and achieve the asymptotic distributions accordingly. In addition, we conduct intensive simulations to examine the theoretical findings. In the empirical study, we focus on the sign prediction of stock returns, and then use the results of sign forecast to conduct portfolio analysis. By implementing rolling-window based out-of-sample forecasts, we show the finite-sample performance and demonstrate the practical relevance of the proposed model and estimation method."
            },
            {
                "arxivId": "1710.11230",
                "title": "Nonparametric identification in index models of link formation",
                "abstract": null
            },
            {
                "arxivId": "1911.02173",
                "title": "Quantile Factor Models",
                "abstract": "Quantile factor models (QFM) represent a new class of factor models for high\u2010dimensional panel data. Unlike approximate factor models (AFM), which only extract mean factors, QFM also allow unobserved factors to shift other relevant parts of the distributions of observables. We propose a quantile regression approach, labeled Quantile Factor Analysis (QFA), to consistently estimate all the quantile\u2010dependent factors and loadings. Their asymptotic distributions are established using a kernel\u2010smoothed version of the QFA estimators. Two consistent model selection criteria, based on information criteria and rank minimization, are developed to determine the number of factors at each quantile. QFA estimation remains valid even when the idiosyncratic errors exhibit heavy\u2010tailed distributions. An empirical application illustrates the usefulness of QFA by highlighting the role of extra factors in the forecasts of U.S. GDP growth and inflation rates using a large set of predictors."
            },
            {
                "arxivId": "1311.7065",
                "title": "Individual and time effects in nonlinear panel models with large N , T",
                "abstract": null
            },
            {
                "arxivId": "1305.6099",
                "title": "Inference on Treatment Effects after Selection Amongst High-Dimensional Controls",
                "abstract": "In this supplementary appendix we provide additional results, omitted proofs and extensive simulations that complement the analysis of the main text (arXiv:1201.0224)."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-15.json",
        "arxivId": "2404.08456",
        "category": "q-fin",
        "title": "A backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations",
        "abstract": "In this work, we propose a novel backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations (BSDEs), where the deep neural network (DNN) models are trained not only on the inputs and labels but also the differentials of the corresponding labels. This is motivated by the fact that differential deep learning can provide an efficient approximation of the labels and their derivatives with respect to inputs. The BSDEs are reformulated as differential deep learning problems by using Malliavin calculus. The Malliavin derivatives of solution to a BSDE satisfy themselves another BSDE, resulting thus in a system of BSDEs. Such formulation requires the estimation of the solution, its gradient, and the Hessian matrix, represented by the triple of processes $\\left(Y, Z, \\Gamma\\right).$ All the integrals within this system are discretized by using the Euler-Maruyama method. Subsequently, DNNs are employed to approximate the triple of these unknown processes. The DNN parameters are backwardly optimized at each time step by minimizing a differential learning type loss function, which is defined as a weighted sum of the dynamics of the discretized BSDE system, with the first term providing the dynamics of the process $Y$ and the other the process $Z$. An error analysis is carried out to show the convergence of the proposed algorithm. Various numerical experiments up to $50$ dimensions are provided to demonstrate the high efficiency. Both theoretically and numerically, it is demonstrated that our proposed scheme is more efficient compared to other contemporary deep learning-based methodologies, especially in the computation of the process $\\Gamma$.",
        "references": [
            {
                "arxivId": "2403.18552",
                "title": "Generalized convergence of the deep BSDE method: a step towards fully-coupled FBSDEs and applications in stochastic control",
                "abstract": "We are concerned with high-dimensional coupled FBSDE systems approximated by the deep BSDE method of Han et al. (2018). It was shown by Han and Long (2020) that the errors induced by the deep BSDE method admit a posteriori estimate depending on the loss function, whenever the backward equation only couples into the forward diffusion through the Y process. We generalize this result to fully-coupled drift coefficients, and give sufficient conditions for convergence under standard assumptions. The resulting conditions are directly verifiable for any equation. Consequently, unlike in earlier theory, our convergence analysis enables the treatment of FBSDEs stemming from stochastic optimal control problems. In particular, we provide a theoretical justification for the non-convergence of the deep BSDE method observed in recent literature, and present direct guidelines for when convergence can be guaranteed in practice. Our theoretical findings are supported by several numerical experiments in high-dimensional settings."
            },
            {
                "arxivId": "2211.04349",
                "title": "A deep solver for BSDEs with jumps",
                "abstract": "The aim of this work is to propose an extension of the Deep BSDE solver by Han, E, Jentzen (2017) to the case of FBSDEs with jumps. As in the aforementioned solver, starting from a discretized version of the BSDE and parametrizing the (high dimensional) control processes by means of a family of ANNs, the BSDE is viewed as model-based reinforcement learning problem and the ANN parameters are fitted so as to minimize a prescribed loss function. We take into account both finite and infinite jump activity by introducing, in the latter case, an approximation with finitely many jumps of the forward process."
            },
            {
                "arxivId": "2205.09815",
                "title": "Differential learning methods for solving fully nonlinear PDEs",
                "abstract": null
            },
            {
                "arxivId": "2204.05796",
                "title": "A deep learning method for solving stochastic optimal control problems driven by fully-coupled FBSDEs",
                "abstract": "In this paper, we mainly focus on the numerical solution of high-dimensional stochastic optimal control problem driven by fully-coupled forward-backward stochastic differential equations (FBSDEs in short) through deep learning. We first transform the problem into a stochastic Stackelberg differential game(leader-follower problem), then a cross-optimization method (CO method) is developed where the leader's cost functional and the follower's cost functional are optimized alternatively via deep neural networks. As for the numerical results, we compute two examples of the investment-consumption problem solved through stochastic recursive utility models, and the results of both examples demonstrate the effectiveness of our proposed algorithm."
            },
            {
                "arxivId": "2201.06854",
                "title": "Convergence of a robust deep FBSDE method for stochastic control",
                "abstract": "In this paper, we propose a deep learning based numerical scheme for strongly coupled FBSDEs, stemming from stochastic control. It is a modification of the deep BSDE method in which the initial value to the backward equation is not a free parameter, and with a new loss function being the weighted sum of the cost of the control problem, and a variance term which coincides with the mean squared error in the terminal condition. We show by a numerical example that a direct extension of the classical deep BSDE method to FBSDEs, fails for a simple linear-quadratic control problem, and motivate why the new method works. Under regularity and boundedness assumptions on the exact controls of time continuous and time discrete control problems, we provide an error analysis for our method. We show empirically that the method converges for three different problems, one being the one that failed for a direct extension of the deep BSDE method."
            },
            {
                "arxivId": "2111.02636",
                "title": "A novel control method for solving high-dimensional Hamiltonian systems through deep neural networks",
                "abstract": "In this paper, we mainly focus on solving high-dimensional stochastic Hamiltonian systems with boundary condition, which is essentially a Forward Backward Stochastic Differential Equation (FBSDE in short), and propose a novel method from the view of the stochastic control. In order to obtain the approximated solution of the Hamiltonian system, we first introduce a corresponding stochastic optimal control problem such that the extended Hamiltonian system of the control problem is exactly what we need to solve, then we develop two different algorithms suitable for different cases of the control problem and approximate the stochastic control via deep neural networks. From the numerical results, comparing with the Deep FBSDE method developed previously from the view of solving FBSDEs, the novel algorithms converge faster, which means that they require fewer training steps, and demonstrate more stable convergences for different Hamiltonian systems."
            },
            {
                "arxivId": "2110.05421",
                "title": "The One Step Malliavin scheme: new discretization of BSDEs implemented with deep learning regressions",
                "abstract": "\n A novel discretization is presented for decoupled forward\u2013backward stochastic differential equations (FBSDE) with differentiable coefficients, simultaneously solving the BSDE and its Malliavin sensitivity problem. The control process is estimated by the corresponding linear BSDE driving the trajectories of the Malliavin derivatives of the solution pair, which implies the need to provide accurate $\\varGamma $ estimates. The approximation is based on a merged formulation given by the Feynman\u2013Kac formulae and the Malliavin chain rule. The continuous time dynamics is discretized with a theta-scheme. In order to allow for an efficient numerical solution of the arising semidiscrete conditional expectations in possibly high dimensions, it is fundamental that the chosen approach admits to differentiable estimates. Two fully-implementable schemes are considered: the BCOS method as a reference in the one-dimensional framework and neural network Monte Carlo regressions in case of high-dimensional problems, similarly to the recently emerging class of Deep BSDE methods (Han et al. (2018 Solving high-dimensional partial differential equations using deep learning. Proc. Natl. Acad. Sci., 115, 8505\u20138510); Hur\u00e9 et al. (2020 Deep backward schemes for high-dimensional nonlinear PDEs. Math. Comp., 89, 1547\u20131579)). An error analysis is carried out to show $\\mathbb{L}^2$ convergence of order $1/2$, under standard Lipschitz assumptions and additive noise in the forward diffusion. Numerical experiments are provided for a range of different semilinear equations up to $50$ dimensions, demonstrating that the proposed scheme yields a significant improvement in the control estimations."
            },
            {
                "arxivId": "2107.06673",
                "title": "Gradient boosting-based numerical methods for high-dimensional backward stochastic differential equations",
                "abstract": null
            },
            {
                "arxivId": "2102.12051",
                "title": "A learning scheme by sparse grids and Picard approximations for semilinear parabolic PDEs",
                "abstract": "\n Relying on the classical connection between backward stochastic differential equations and nonlinear parabolic partial differential equations (PDEs), we propose a new probabilistic learning scheme for solving high-dimensional semilinear parabolic PDEs. This scheme is inspired by the approach coming from machine learning and developed using deep neural networks in Han et al. (2018, Solving high-dimensional partial differential equations using deep learning. Proc. Natl. Acad. Sci., 115, 8505\u20138510. Our algorithm is based on a Picard iteration scheme in which a sequence of linear-quadratic optimization problem is solved by means of stochastic gradient descent algorithm. In the framework of a linear specification of the approximation space, we manage to prove a convergence result for our scheme, under some smallness condition. In practice, in order to be able to treat high-dimensional examples, we employ sparse-grid approximation spaces. In the case of periodic coefficients and using pre-wavelet basis functions, we obtain an upper bound on the global complexity of our method. It shows, in particular, that the curse of dimensionality is tamed in the sense that in order to achieve a root mean squared error of order $\\varepsilon $, for a prescribed precision $\\varepsilon $, the complexity of the Picard algorithm grows polynomially in $\\varepsilon ^{-1}$ up to some logarithmic factor $|\\!\\log (\\varepsilon )|$, whose exponent grows linearly with respect to the PDE dimension. Various numerical results are presented to validate the performance of our method, and to compare them with some recent machine learning schemes proposed in E et al. (2017, Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations. Commun. Math. Stat., 5, 349\u2013380) and Hur\u00e9 et al. (2020, Deep backward schemes for high-dimensional nonlinear PDEs. Math. Comput., 89, 1547\u20131579)."
            },
            {
                "arxivId": "2101.09890",
                "title": "A New Efficient Approximation Scheme for Solving High-Dimensional Semilinear PDEs: Control Variate Method for Deep BSDE Solver",
                "abstract": null
            },
            {
                "arxivId": "2101.01869",
                "title": "Convergence of the Deep BSDE method for FBSDEs with non-Lipschitz coefficients",
                "abstract": "This paper is dedicated to solving high-dimensional coupled FBSDEs with non-Lipschitz diffusion coefficients numerically. Under mild conditions, we provided a posterior estimate of the numerical solution that holds for any time duration. This posterior estimate validates the convergence of the recently proposed Deep BSDE method. In addition, we developed a numerical scheme based on the Deep BSDE method and presented numerical examples in financial markets to demonstrate the high performance."
            },
            {
                "arxivId": "1809.00324",
                "title": "A multi-step scheme based on cubic spline for solving backward stochastic differential equations",
                "abstract": null
            },
            {
                "arxivId": "1804.07010",
                "title": "Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations",
                "abstract": "Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions."
            },
            {
                "arxivId": "0907.1221",
                "title": "CREDIT RISK PREMIA AND QUADRATIC BSDEs WITH A SINGLE JUMP",
                "abstract": "This paper is concerned with the determination of credit risk premia of defaultable contingent claims by means of indifference valuation principles. Assuming exponential utility preferences we derive representations of indifference premia of credit risk in terms of solutions of Backward Stochastic Differential Equations (BSDE). The class of BSDEs needed for that representation allows for quadratic growth generators and jumps at random times. Since the existence and uniqueness theory for this class of BSDEs has not yet been developed to the required generality, the first part of the paper is devoted to fill that gap. By using a simple constructive algorithm, and known results on continuous quadratic BSDEs, we provide sufficient conditions for the existence and uniqueness of quadratic BSDEs with discontinuities at random times."
            },
            {
                "arxivId": "0801.3203",
                "title": "Time discretization and Markovian iteration for coupled FBSDEs",
                "abstract": "In this paper we lay the foundation for a numerical algorithm to simulate high-dimensional coupled FBSDEs under weak coupling or monotonicity conditions. In particular, we prove convergence of a time discretization and a Markovian iteration. The iteration differs from standard Picard iterations for FBSDEs in that the dimension of the underlying Markovian process does not increase with the number of iterations. This feature seems to be indispensable for an efficient iterative scheme from a numerical point of view. We finally suggest a fully explicit numerical algorithm and present some numerical examples with up to 10-dimensional state space."
            },
            {
                "arxivId": "math/0603250",
                "title": "A Forward-Backward Stochastic Algorithm For Quasi-Linear PDEs",
                "abstract": "We propose a time-space discretization scheme for quasi-linear PDEs. The algorithm relies on the theory of fully coupled Forward-Backward SDEs, which provides an efficient probabilistic representation of this type of equations. The derivated algorithm holds for strong solutions defined on any interval of arbitrary length. As a bypass product, we obtain a discretization procedure for the underlying FBSDE."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-15.json",
        "arxivId": "2404.08620",
        "category": "q-fin",
        "title": "Natural disasters, personal attributes, and social entrepreneurship: an attention-based view",
        "abstract": null,
        "references": [
            {
                "arxivId": "2207.12492",
                "title": "Natural disasters, entrepreneurship activity, and the moderating role of country governance",
                "abstract": null
            },
            {
                "arxivId": "2104.12008",
                "title": "Weathering the Storm: How Foreign Aid and Institutions Affect Entrepreneurship Activity Following Natural Disasters",
                "abstract": "This study examines how foreign aid and institutions affect entrepreneurship activity following natural disasters. We use insights from the entrepreneurship, development, and institutions literature to develop a model of entrepreneurship activity in the aftermath of natural disasters. First, we hypothesize the effect of natural disasters on entrepreneurship activity depends on the amount of foreign aid received. Second, we hypothesize that natural disasters and foreign aid either encourages or discourages entrepreneurship activity depending on two important institutional conditions: the quality of government and economic freedom. The findings from our panel of 85 countries from 2006 to 2016 indicate that natural disasters are negatively associated with entrepreneurship activity, but both foreign aid and economic freedom attenuate this effect. In addition, we observe that foreign aid is positively associated with entrepreneurship activity but only in countries with high quality government. Hence, we conclude that the effect of natural disasters on entrepreneurship depends crucially on the quality of government, economic freedom, and foreign aid. Our findings provide new insights into how natural disasters and foreign aid affect entrepreneurship and highlight the important role of the institutional context."
            },
            {
                "arxivId": "1903.02934",
                "title": "Entrepreneurship, Institutions, and Economic Growth: Does the Level of Development Matter?",
                "abstract": "This study questions the assumption that entrepreneurship unequivocally leads to economic growth. Using insights from institutional theory and development economics, we reevaluate entrepreneurship\u2019s contribution towards economic growth. Our study uses Global Entrepreneurship Monitor (GEM) data for a panel of 83 countries from 2002 to 2014 and highlights several important findings. First, our evidence suggests that entrepreneurship encourages economic growth but not in developing countries. Second, we find that a country\u2019s institutional environment\u2014measured by GEM\u2019s Entrepreneurial Framework Conditions (EFCs)\u2014contributes to economic growth in more developed countries but not in developing countries. Lastly, we find that opportunity-motivated entrepreneurship encourages economic growth in developed countries, while necessity-motivated entrepreneurship discourages economic growth in developing countries. These findings have important policy implications. Namely, our evidence contradicts policy proposals that suggest entrepreneurship and the adoption of pro-market institutions that support it will encourage economic growth in developing countries. Our evidence suggests these policy proposals are unlikely to generate the desired economic growth."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2212.07384",
        "category": "q-fin",
        "title": "Valuing Pharmaceutical Drug Innovations",
        "abstract": "We propose a methodology to estimate the market value of pharmaceutical drugs. Our approach combines an event study with a model of discounted cash flows and uses stock market responses to drug development announcements to infer the values. We estimate that, on average, a successful drug is valued at \\$1.62 billion, and its value at the discovery stage is \\$64.3 million, with substantial heterogeneity across major diseases. Leveraging these estimates, we also determine the average drug development costs at various stages. Furthermore, we explore applying our estimates to design policies that support drug development through drug buyouts and cost-sharing agreements.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2306.12602",
        "category": "q-fin",
        "title": "Social Media Emotions and IPO Returns",
        "abstract": "I examine potential mechanisms behind two stylized facts of initial public offerings (IPOs) returns. By analyzing investor emotions expressed on StockTwits and Twitter, I find that emotions conveyed through these social media platforms can help explain the mispricing of IPO stocks. The abundance of information and opinions shared on social media can generate hype around certain stocks, leading to investors' irrational buying and selling decisions. This can result in an overvaluation of the stock in the short term but often leads to a correction in the long term as the stock's performance fails to meet the inflated expectations. In particular, I find that IPOs with high levels of pre-IPO enthusiasm tend to have a significantly higher first-day return of 29.73%, compared to IPOs with lower levels of pre-IPO investor enthusiasm, which have an average first-day return of 17.59%. However, this initial enthusiasm may be misplaced, as IPOs with high pre-IPO investor enthusiasm demonstrate a much lower average long-run industry-adjusted return of -8.22%, compared to IPOs with lower pre-IPO investor enthusiasm, which have an average long-run industry-adjusted return of -0.14%. Diving deeper into the qualitative aspects of investor discourse, I find that messages rich in financial language or that bolster prevailing information drive my results. Additionally, a trend towards caution emerges among users who frequently engage with IPOs, perhaps a byproduct of lessons from past endeavors. Intriguingly, firms that enjoy high levels of pre-IPO optimism consistently garner post-launch enthusiasm, a trend at odds with their long-term under-performance.",
        "references": [
            {
                "arxivId": "1910.02570",
                "title": "Racial Disparities in Debt Collection",
                "abstract": "A distinct set of disadvantages experienced by black Americans increases their likelihood of experiencing negative financial shocks, decreases their ability to mitigate the impact of such shocks, and ultimately results in debt collection cases being far more common in black neighborhoods than in non-black neighborhoods. In this paper, we create a novel dataset that links debt collection court cases with information from credit reports to document the disparity in debt collection judgments across black and non-black neighborhoods and to explore potential mechanisms that could be driving this judgment gap. We find that majority black neighborhoods experience approximately 40% more judgments than non-black neighborhoods, even after controlling for differences in median incomes, median credit scores, and default rates. The racial disparity in judgments cannot be explained by differences in debt characteristics across black and non-black neighborhoods, nor can it be explained by differences in attorney representation, the share of contested judgments, or differences in neighborhood lending institutions."
            },
            {
                "arxivId": "1908.11498",
                "title": "Predicting Consumer Default: A Deep Learning Approach",
                "abstract": "We develop a model to predict consumer default based on deep learning. We show that the model consistently outperforms standard credit scoring models, even though it uses the same data. Our model is interpretable and is able to provide a score to a larger class of borrowers relative to standard credit scoring models while accurately tracking variations in systemic risk. We argue that these properties can provide valuable insights for the design of policies targeted at reducing consumer default and alleviating its burden on borrowers and lenders, as well as macroprudential regulation."
            },
            {
                "arxivId": "1903.10075",
                "title": "Machine Learning Methods That Economists Should Know About",
                "abstract": "We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models."
            },
            {
                "arxivId": "1804.09790",
                "title": "Adaptive MPC with Chance Constraints for FIR Systems",
                "abstract": "This paper proposes an adaptive stochastic Model Predictive Control (MPC) strategy for stable linear time invariant systems in the presence of bounded disturbances. We consider multi-input multi-output systems that can be expressed by a finite impulse response model, whose parameters we estimate using a linear Recursive Least Squares algorithm. Building on the work of [1], [2], our approach is able to handle hard input constraints and probabilistic output constraints. By using tools from distributionally robust optimization, we formulate our MPC design task as a convex optimization problem that can be solved using existing tools. Furthermore, we show that our adaptive stochastic MPC algorithm is persistently feasible. The efficacy of the developed algorithm is demonstrated in a numerical example and the results are compared with the adaptive robust MPC algorithm of [2]."
            },
            {
                "arxivId": "1010.3003",
                "title": "Twitter mood predicts the stock market",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2312.15563",
        "category": "q-fin",
        "title": "Dynamics of Global Emission Permit Prices and Regional Social Cost of Carbon under Noncooperation",
        "abstract": "We build a dynamic multi-region model of climate and economy with emission permit trading among 12 aggregated regions in the world. We solve for the dynamic Nash equilibrium under noncooperation, wherein each region adheres to the emission cap constraints following commitments that were first outlined in the 2015 Paris Agreement and updated in subsequent years. Our model shows that the emission permit price reaches $811 per ton of carbon by 2050. We demonstrate that a regional carbon tax is complementary to the global cap-and-trade system, and the optimal regional carbon tax is equal to the difference between the regional marginal abatement cost and the permit price.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.07298",
        "category": "q-fin",
        "title": "Predicting Mergers and Acquisitions: Temporal Dynamic Industry Networks",
        "abstract": "M&A activities are pivotal for market consolidation, enabling firms to augment market power through strategic complementarities. Existing research often overlooks the peer effect, the mutual influence of M&A behaviors among firms, and fails to capture complex interdependencies within industry networks. Common approaches suffer from reliance on ad-hoc feature engineering, data truncation leading to significant information loss, reduced predictive accuracy, and challenges in real-world application. Additionally, the rarity of M&A events necessitates data rebalancing in conventional models, introducing bias and undermining prediction reliability. We propose an innovative M&A predictive model utilizing the Temporal Dynamic Industry Network (TDIN), leveraging temporal point processes and deep learning to adeptly capture industry-wide M&A dynamics. This model facilitates accurate, detailed deal-level predictions without arbitrary data manipulation or rebalancing, demonstrated through superior evaluation results from M&A cases between January 1997 and December 2020. Our approach marks a significant improvement over traditional models by providing detailed insights into M&A activities and strategic recommendations for specific firms.",
        "references": [
            {
                "arxivId": "1706.02216",
                "title": "Inductive Representation Learning on Large Graphs",
                "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.08665",
        "category": "q-fin",
        "title": "Targeted aspect-based emotion analysis to detect opportunities and precaution in financial Twitter messages",
        "abstract": null,
        "references": [
            {
                "arxivId": "2005.05810",
                "title": "Handling Concept Drift for Predictions in Business Process Mining",
                "abstract": "Predictive services nowadays play an important role across all business sectors. However, deployed machine learning models are challenged by changing data streams over time which is described as concept drift. Prediction quality of models can be largely influenced by this phenomenon. Therefore, concept drift is usually handled by retraining of the model. However, current research lacks a recommendation which data should be selected for the retraining of the machine learning model. Therefore, we systematically analyze different data selection strategies in this work. Subsequently, we instantiate our findings on a use case in process mining which is strongly affected by concept drift. We can show that we can improve accuracy from 0.5400 to 0.7010 with concept drift handling. Furthermore, we depict the effects of the different data selection strategies."
            },
            {
                "arxivId": "1809.08427",
                "title": "Pachinko Prediction: A Bayesian method for event prediction from social media data",
                "abstract": null
            },
            {
                "arxivId": "2404.01439",
                "title": "Creating emoji lexica from unsupervised sentiment analysis of their descriptions",
                "abstract": null
            },
            {
                "arxivId": "1807.04662",
                "title": "Scikit-Multiflow: A Multi-output Streaming Framework",
                "abstract": "Scikit-multiflow is a multi-output/multi-label and stream data mining framework for the Python programming language. Conceived to serve as a platform to encourage democratization of stream learning research, it provides multiple state of the art methods for stream learning, stream generators and evaluators. scikit-multiflow builds upon popular open source frameworks including scikit-learn, MOA and MEKA. Development follows the FOSS principles and quality is enforced by complying with PEP8 guidelines and using continuous integration and automatic testing. The source code is publicly available at this https URL."
            },
            {
                "arxivId": "1705.00294",
                "title": "Tales of emotion and stock in China: volatility, causality and prediction",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.08712",
        "category": "q-fin",
        "title": "Machine learning and economic forecasting: the role of international trade networks",
        "abstract": "This study examines the effects of de-globalization trends on international trade networks and their role in improving forecasts for economic growth. Using section-level trade data from nearly 200 countries from 2010 to 2022, we identify significant shifts in the network topology driven by rising trade policy uncertainty. Our analysis highlights key global players through centrality rankings, with the United States, China, and Germany maintaining consistent dominance. Using a horse race of supervised regressors, we find that network topology descriptors evaluated from section-specific trade networks substantially enhance the quality of a country's GDP growth forecast. We also find that non-linear models, such as Random Forest, XGBoost, and LightGBM, outperform traditional linear models used in the economics literature. Using SHAP values to interpret these non-linear model's predictions, we find that about half of most important features originate from the network descriptors, underscoring their vital role in refining forecasts. Moreover, this study emphasizes the significance of recent economic performance, population growth, and the primary sector's influence in shaping economic growth predictions, offering novel insights into the intricacies of economic growth forecasting.",
        "references": [
            {
                "arxivId": "2012.12802",
                "title": "Machine Learning Advances for Time Series Forecasting",
                "abstract": "In this paper we survey the most recent advances in supervised machine learning and high-dimensional models for time series forecasting. We consider both linear and nonlinear alternatives. Among the linear methods we pay special attention to penalized regressions and ensemble of models. The nonlinear methods considered in the paper include shallow and deep neural networks, in their feed-forward and recurrent versions, and tree-based methods, such as random forests and boosted trees. We also consider ensemble and hybrid models by combining ingredients from different alternatives. Tests for superior predictive ability are briefly reviewed. Finally, we discuss application of machine learning in economics and finance and provide an illustration with high-frequency financial data."
            },
            {
                "arxivId": "2008.12477",
                "title": "How is Machine Learning Useful for Macroeconomic Forecasting?",
                "abstract": "We move beyond Is Machine Learning Useful for Macroeconomic Forecasting? by adding the how. The current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. To the contrary, we study the usefulness of the underlying features driving ML gains over standard macroeconometric methods. We distinguish four so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. To do so, we design experiments that allow to identify the \u201ctreatment\u201d effects of interest. We conclude that (i) nonlinearity is the true game changer for macroeconomic prediction, (ii) the standard factor model remains the best regularization, (iii) K-fold cross-validation is the best practice and (iv) the L2 is preferred to the e-insensitive in-sample loss. The forecasting gains of nonlinear techniques are associated with high macroeconomic uncertainty, financial stress and housing bubble bursts. This suggests that Machine Learning is useful for macroeconomic forecasting by mostly capturing important nonlinearities that arise in the context of uncertainty and financial frictions."
            },
            {
                "arxivId": "1705.07874",
                "title": "A Unified Approach to Interpreting Model Predictions",
                "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
            },
            {
                "arxivId": "1603.02754",
                "title": "XGBoost: A Scalable Tree Boosting System",
                "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
            },
            {
                "arxivId": "1405.6974",
                "title": "Futility Analysis in the Cross-Validation of Machine Learning Models",
                "abstract": "Many machine learning models have important structural tuning parameters that cannot be directly estimated from the data. The common tactic for setting these parameters is to use resampling methods, such as cross--validation or the bootstrap, to evaluate a candidate set of values and choose the best based on some pre--defined criterion. Unfortunately, this process can be time consuming. However, the model tuning process can be streamlined by adaptively resampling candidate values so that settings that are clearly sub-optimal can be discarded. The notion of futility analysis is introduced in this context. An example is shown that illustrates how adaptive resampling can be used to reduce training time. Simulation studies are used to understand how the potential speed--up is affected by parallel processing techniques."
            },
            {
                "arxivId": "cond-mat/0408566",
                "title": "Characterization and modeling of weighted networks",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0403051",
                "title": "Fitness-dependent topological properties of the world trade web.",
                "abstract": "Among the proposed network models, the hidden variable (or good get richer) one is particularly interesting, even if an explicit empirical test of its hypotheses has not yet been performed on a real network. Here we provide the first empirical test of this mechanism on the world trade web, the network defined by the trade relationships between world countries. We find that the power-law distributed gross domestic product can be successfully identified with the hidden variable (or fitness) determining the topology of the world trade web: all previously studied properties up to third-order correlation structure (degree distribution, degree correlations, and hierarchy) are found to be in excellent agreement with the predictions of the model. The choice of the connection probability is such that all realizations of the network with the same degree sequence are equiprobable."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.08757",
        "category": "q-fin",
        "title": "Strategic Informed Trading and the Value of Private Information",
        "abstract": "We consider a market of risky financial assets where the participants are an informed trader, a mass of uniformed traders and noisy liquidity providers. We prove the existence of a market-clearing equilibrium when the insider internalizes her power to impact prices. In the price-impact equilibrium the insider strategically reveals a noisier (compared to when the insider takes prices as given) signal, and prices are less reactive to the publicly available information. In contrast to the related literature, we show that in the price-impact equilibrium, the insider's ex-ante welfare monotonically increases in the signal precision. This clarifies when a trader with market power is motivated to both obtain and refine her private information. Furthermore, even though the uniformed traders act as price-takers, the effect of price impact is ex-ante welfare improving for them. By contrast, internalization of price impact may reduce insider ex-ante welfare. This happens provided the insider is sufficiently risk averse and the uninformed traders are sufficiently risk tolerant.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.08906",
        "category": "q-fin",
        "title": "Voting participation and engagement in blockchain-based fan tokens",
        "abstract": null,
        "references": [
            {
                "arxivId": "2403.15810",
                "title": "Anticipatory Gains and Event-Driven Losses in Blockchain-Based Fan Tokens: Evidence from the FIFA World Cup",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.08908",
        "category": "q-fin",
        "title": "Reference Model Based Learning in Expectation Formation: Experimental Evidence",
        "abstract": "How do people form expectations about future prices in financial markets? One of the dominant learning rules that explains the forecasting behavior is the Adaptive Expectation Rule (ADA), which suggests that people adjust their predictions by adapting to the most recent prediction error at a constant weight. However, this rule also implies that they will continually learn and adapt until the prediction error is zero, which contradicts recent experimental evidence showing that people usually stop learning long before reaching zero prediction error. A more recent learning rule, Reference Model Based Learning (RMBL), extends and generalizes ADA, hypothesizing that: i) People apply ADA but dynamically adjust the adaptive coefficient with regards to the auto-correlation of the prediction error in the most recent two periods; ii) Meanwhile, they also utilize a satisficing rule so that people would only adjust their adaptive coefficient when the prediction error is higher than their anticipation. This paper utilizes a rich set of experimental data with observations of 41,490 predictions from 801 subjects from the Learning-to-Forecast Experiments (LtFEs), i.e., the experiment that has been used to study expectation formation. Our results concludes that RMBL fits better than ADA in all the experiments.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.08991",
        "category": "q-fin",
        "title": "Business models for the simulation hypothesis",
        "abstract": "The simulation hypothesis suggests that we live in a computer simulation. That notion has attracted significant scholarly and popular interest. This article explores the simulation hypothesis from a business perspective. Due to the lack of a name for a universe consistent with the simulation hypothesis, we propose the term simuverse. We argue that if we live in a simulation, there must be a business justification. Therefore, we ask: If we live in a simuverse, what is its business model? We identify and explore business model scenarios, such as simuverse as a project, service, or platform. We also explore business model pathways and risk management issues. The article contributes to the simulation hypothesis literature and is the first to provide a business model perspective on the simulation hypothesis. The article discusses theoretical and practical implications and identifies opportunities for future research related to sustainability, digital transformation, and Artificial Intelligence (AI).",
        "references": [
            {
                "arxivId": "2211.05040",
                "title": "Digital twins for the designs of systems: a perspective",
                "abstract": null
            },
            {
                "arxivId": "2206.15331",
                "title": "GitHub Copilot AI pair programmer: Asset or Liability?",
                "abstract": "Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans' solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired."
            },
            {
                "arxivId": "2204.01467",
                "title": "On scientific understanding with artificial intelligence",
                "abstract": null
            },
            {
                "arxivId": "2104.03902",
                "title": "The Autodidactic Universe",
                "abstract": "We present an approach to cosmology in which the Universe learns its own physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models. We discover maps that put each of these matrix models in correspondence with both a gauge/gravity theory and a mathematical model of a learning machine, such as a deep recurrent, cyclic neural network. This establishes a correspondence between each solution of the physical theory and a run of a neural network. This correspondence is not an equivalence, partly because gauge theories emerge from N \u2192 \u221e limits of the matrix models, whereas the same limits of the neural networks used here are not well-defined. We discuss in detail what it means to say that learning takes place in autodidactic systems, where there is no supervision. We propose that if the neural network model can be said to learn without supervision, the same can be said for the corresponding physical theory. 1 ar X iv :2 10 4. 03 90 2v 1 [ he pth ] 2 9 M ar 2 02 1 We consider other protocols for autodidactic physical systems, such as optimization of graph variety, subset-replication using self-attention and look-ahead, geometrogenesis guided by reinforcement learning, structural learning using renormalization group techniques, and extensions. These protocols together provide a number of directions in which to explore the origin of physical laws based on putting machine learning architectures in correspondence with physical theories."
            },
            {
                "arxivId": "2008.12254",
                "title": "A Bayesian Approach to the Simulation Argument",
                "abstract": "The Simulation Argument posed by Bostrom (2003) suggests that we may be living inside a sophisticated computer simulation. If post-human civilizations eventually have both the capability and desire to generate such Bostrom-like simulations, then the number of simulated realities would greatly exceed the one base reality, ostensibly indicating a high probability that we do not live in said base reality. In this work, it is argued that since the hypothesis that such simulations are technically possible remains unproven, then statistical calculations need to consider not just the number of state spaces, but the intrinsic model uncertainty. This is achievable through a Bayesian treatment of the problem, which is presented here. Using Bayesian model averaging, it is shown that the probability that we are sims is in fact less than 50%, tending towards that value in the limit of an infinite number of simulations. This result is broadly indifferent as to whether one conditions upon the fact that humanity has not yet birthed such simulations, or ignore it. As argued elsewhere, it is found that if humanity does start producing such simulations, then this would radically shift the odds and make it very probable that we are in fact sims."
            },
            {
                "arxivId": "2007.05995",
                "title": "The Complexity and Information Content of Simulated Universes",
                "abstract": null
            },
            {
                "arxivId": "2001.10439",
                "title": "How Many Simulations Do We Exist In? A Practical Mathematical Solution to the Simulation Argument",
                "abstract": "The Simulation Argument has gained significant traction in the public arena. It has offered a hypothesis based on probabilistic analysis of its assumptions that we are likely to exist within a computer simulation. This has been derived from factors including the prediction of computing power, human existence, extinction and population dynamics, and suggests a very large value for the number of possible simulations within which we may exist. On evaluating this argument through the application of tangible real-world evidence and projections, it is possible to calculate real numerical solutions for the Simulation Argument. This reveals a much smaller number of possible simulations within which we may exist, and offers a novel practicable approach in which to appraise the variety and multitude of conjectures and theories associated with the Simulation Hypothesis."
            },
            {
                "arxivId": "1905.05792",
                "title": "Simulation Typology and Termination Risks",
                "abstract": "The goal of the article is to explore what is the most probable type of simulation in which humanity lives (if any) and how this affects simulation termination risks. We firstly explore the question of what kind of simulation in which humanity is most likely located based on pure theoretical reasoning. We suggest a new patch to the classical simulation argument, showing that we are likely simulated not by our own descendants, but by alien civilizations. Based on this, we provide classification of different possible simulations and we find that simpler, less expensive and one-person-centered simulations, resurrectional simulations, or simulations of the first artificial general intelligence's (AGI's) origin (singularity simulations) should dominate. Also, simulations which simulate the 21st century and global catastrophic risks are probable. We then explore whether the simulation could collapse or be terminated. Most simulations must be terminated after they model the singularity or after they model a global catastrophe before the singularity. Undeniably observed glitches, but not philosophical speculations could result in simulation termination. The simulation could collapse if it is overwhelmed by glitches. The Doomsday Argument in simulations implies termination soon. We conclude that all types of the most probable simulations except resurrectional simulations are prone to termination risks in a relatively short time frame of hundreds of years or less from now."
            },
            {
                "arxivId": "1803.03453",
                "title": "The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities",
                "abstract": "Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems."
            },
            {
                "arxivId": "1703.00058",
                "title": "On testing the simulation theory",
                "abstract": "Can the theory that reality is a simulation be tested? We investigate this question based on the assumption that if the system performing the simulation is finite (i.e. has limited resources), then to achieve low computational complexity, such a system would, as in a video game, render content (reality) only at the moment that information becomes available for observation by a player and not at the moment of detection by a machine (that would be part of the simulation and whose detection would also be part of the internal computation performed by the Virtual Reality server before rendering content to the player). Guided by this principle we describe conceptual wave/particle duality experiments aimed at testing the simulation theory."
            },
            {
                "arxivId": "1210.1847",
                "title": "Constraints on the universe as a numerical simulation",
                "abstract": null
            },
            {
                "arxivId": "0704.0646",
                "title": "The Mathematical Universe",
                "abstract": null
            },
            {
                "arxivId": "quant-ph/0112105",
                "title": "Information and computation: Classical and quantum aspects",
                "abstract": "Quantum theory has found a new field of application in the realm of information and computation during recent years. This paper reviews how quantum physics allows information coding in classically unexpected and subtle nonlocal ways, as well as information processing with an efficiency largely surpassing that of the present and foreseeable classical computers. Some notable aspects of classical and quantum information theory will be addressed here. Quantum teleportation, dense coding, and quantum cryptography are discussed as examples of the impact of quanta on the transmission of information. Quantum logic gates and quantum algorithms are also discussed as instances of the improvement made possible in information processing by a quantum computer. Finally the authors provide some examples of current experimental realizations for quantum computers and future prospects."
            },
            {
                "arxivId": "hep-th/9409089",
                "title": "The world as a hologram",
                "abstract": "According to \u2019t Hooft the combination of quantum mechanics and gravity requires the three\u2010dimensional world to be an image of data that can be stored on a two\u2010dimensional projection much like a holographic image. The two\u2010dimensional description only requires one discrete degree of freedom per Planck area and yet it is rich enough to describe all three\u2010dimensional phenomena. After outlining \u2019t Hooft\u2019s proposal we give a preliminary informal description of how it may be implemented. One finds a basic requirement that particles must grow in size as their momenta are increased far above the Planck scale. The consequences for high\u2010energy particle collisions are described. The phenomenon of particle growth with momentum was previously discussed in the context of string theory and was related to information spreading near black hole horizons. The considerations of this paper indicate that the effect is much more rapid at all but the earliest times. In fact the rate of spreading is found to saturate the bound fro..."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.09090",
        "category": "q-fin",
        "title": "DEX Specs: A Mean-Field Approach to DeFi Currency Exchanges",
        "abstract": "We investigate the behavior of liquidity providers (LPs) by modeling a decentralized cryptocurrency exchange (DEX) based on Uniswap v3. LPs with heterogeneous characteristics choose optimal liquidity positions subject to uncertainty regarding the size of exogenous incoming transactions and the prices of assets in the wider market. They engage in a game among themselves, and the resulting liquidity distribution determines the exchange rate dynamics and potential arbitrage opportunities of the pool. We calibrate the distribution of LP characteristics based on Uniswap data and the equilibrium strategy resulting from this mean-field game produces pool exchange rate dynamics and liquidity evolution consistent with observed pool behavior. We subsequently introduce Maximal Extractable Value (MEV) bots who perform Just-In-Time (JIT) liquidity attacks, and develop a Stackelberg game between LPs and bots. This addition results in more accurate simulated pool exchange rate dynamics and stronger predictive power regarding the evolution of the pool liquidity distribution.",
        "references": [
            {
                "arxivId": "2302.00610",
                "title": "Uniswap Liquidity Provision: An Online Learning Approach",
                "abstract": "Decentralized Exchanges (DEXs) are new types of marketplaces leveraging Blockchain technology. They allow users to trade assets with Automatic Market Makers (AMM), using funds provided by liquidity providers, removing the need for order books. One such DEX, Uniswap v3, allows liquidity providers to allocate funds more efficiently by specifying an active price interval for their funds. This introduces the problem of finding an optimal strategy for choosing price intervals. We formalize this problem as an online learning problem with non-stochastic rewards. We use regret-minimization methods to show a liquidity provision strategy that guarantees a lower bound on the reward. This is true even for non-stochastic changes to asset pricing, and we express this bound in terms of the trading volume."
            },
            {
                "arxivId": "2209.15569",
                "title": "Credible Decentralized Exchange Design via Verifiable Sequencing Rules",
                "abstract": "Trading on decentralized exchanges has been one of the primary use cases for permissionless blockchains with daily trading volume exceeding billions of U.S.\u2004dollars. In the status quo, users broadcast transactions they wish to execute in the exchange and miners are responsible for composing a block of transactions and picking an execution ordering \u2014 the order in which transactions execute in the exchange. Due to the lack of a regulatory framework, it is common to observe miners exploiting their privileged position by front-running transactions and obtaining risk-fee profits. Indeed, the Flashbots service institutionalizes this exploit, with miners auctioning the right to front-run transactions. In this work, we propose to modify the interaction between miners and users and initiate the study of verifiable sequencing rules. As in the status quo, miners can determine the content of a block; however, they commit to respecting a sequencing rule that constrains the execution ordering and is verifiable (there is a polynomial time algorithm that can verify if the execution ordering satisfies such constraints). Thus in the event a miner deviates from the sequencing rule, anyone can generate a proof of non-compliance. We ask if there are sequencing rules that limit price manipulation from miners in a two-token liquidity pool exchange. Our first result is an impossibility theorem: for any sequencing rule, there is an instance of user transactions where the miner can obtain non-zero risk-free profits. In light of this impossibility result, our main result is a verifiable sequencing rule that provides execution price guarantees for users. In particular, for any user transaction A, it ensures that either (1) the execution price of A is at least as good as if A was the only transaction in the block, or (2) the execution price of A is worse than this \u201cstandalone\u201d price and the miner does not gain when including A in the block. Our framework does not require users to use countermeasures against predatory trading strategies, for example, set limit prices or split large transactions into smaller ones. This is likely to improve user experience relative to the status quo."
            },
            {
                "arxivId": "2205.08904",
                "title": "Risks and Returns of Uniswap V3 Liquidity Providers",
                "abstract": "Trade execution on Decentralized Exchanges (DEXes) is automatic and does not require individual buy and sell orders to be matched. Instead, liquidity aggregated in pools from individual liquidity providers enables trading between cryptocurrencies. The largest DEX measured by trading volume, Uniswap V3, promises a DEX design optimized for capital efficiency. However, Uniswap V3 requires far more decisions from liquidity providers than previous DEX designs. In this work, we develop a theoretical model to illustrate the choices faced by Uniswap V3 liquidity providers and their implications. Our model suggests that providing liquidity on Uniswap V3 is highly complex and requires many considerations from a user. Our supporting data analysis of the risks and returns of real Uniswap V3 liquidity providers underlines that liquidity providing in Uniswap V3 is incredibly complicated, and performances can vary wildly. While there are simple and profitable strategies for liquidity providers in liquidity pools characterized by negligible price volatilities, these strategies only yield modest returns. Instead, significant returns can only be obtained by accepting increased financial risks and at the cost of active management. Thus, providing liquidity has become a game reserved for sophisticated players with the introduction of Uniswap V3, where retail traders do not stand a chance."
            },
            {
                "arxivId": "2202.03762",
                "title": "Eliminating Sandwich Attacks with the Help of Game Theory",
                "abstract": "Predatory trading bots lurking in Ethereum's mempool present invisible taxation of traders on automated market makers (AMMs). AMM traders specify a slippage tolerance to indicate the maximum price movement they are willing to accept. This way, traders avoid automatic transaction failure in case of small price movements before their trade request executes. However, while a too-small slippage tolerance may lead to trade failures, a too-large slippage tolerance allows predatory trading bots to profit from sandwich attacks. These bots can extract the difference between the slippage tolerance and the actual price movement as profit. In this work, we introduce the sandwich game to analyze sandwich attacks analytically from both the attacker and victim perspectives. Moreover, we provide a simple and highly effective algorithm that traders can use to set the slippage tolerance. We unveil that most broadcasted transactions can avoid sandwich attacks while simultaneously only experiencing a low risk of transaction failure. Thereby, we demonstrate that a constant auto-slippage cannot adjust to varying trade sizes and pool characteristics. Our algorithm outperforms the constant auto-slippage suggested by the biggest AMM, Uniswap, in all performed tests. Specifically, our algorithm repeatedly demonstrates a cost reduction exceeding a factor of 100."
            },
            {
                "arxivId": "2111.11933",
                "title": "Disentangling Decentralized Finance (DeFi) Compositions",
                "abstract": "We present a measurement study on compositions of Decentralized Finance (DeFi) protocols, which aim to disrupt traditional finance and offer services on top of distributed ledgers, such as Ethereum. Understanding DeFi compositions is of great importance, as they may impact the development of ecosystem interoperability, are increasingly integrated with web technologies, and may introduce risks through complexity. Starting from a dataset of 23 labeled DeFi protocols and 10,663,881 associated Ethereum accounts, we study the interactions of protocols and associated smart contracts. From a network perspective, we find that decentralized exchange (DEX) and lending protocol account nodes have high degree and centrality values, that interactions among protocol nodes primarily occur in a strongly connected component, and that known community detection methods cannot disentangle DeFi protocols. Therefore, we propose an algorithm to decompose a protocol call into a nested set of building blocks that may be part of other DeFi protocols. This allows us to untangle and study protocol compositions. With a ground truth dataset that we have collected, we can demonstrate the algorithm\u2019s capability by finding that swaps are the most frequently used building blocks. As building blocks can be nested, that is, contained in each other, we provide visualizations of composition trees for deeper inspections. We also present a broad picture of DeFi compositions by extracting and flattening the entire nested building block structure across multiple DeFi protocols. Finally, to demonstrate the practicality of our approach, we present a case study that is inspired by the recent collapse of the UST stablecoin in the Terra ecosystem. Under the hypothetical assumption that the stablecoin USD Tether would experience a similar fate, we study which building blocks \u2014 and, thereby, DeFi protocols \u2014 would be affected. Overall, our results and methods contribute to a better understanding of a new family of financial products."
            },
            {
                "arxivId": "2009.14021",
                "title": "High-Frequency Trading on Decentralized On-Chain Exchanges",
                "abstract": "Decentralized exchanges (DEXs) allow parties to participate in financial markets while retaining full custody of their funds. However, the transparency of blockchain-based DEX in combination with the latency for transactions to be processed, makes market-manipulation feasible. For instance, adversaries could perform front-running \u2014 the practice of exploiting (typically non-public) information that may change the price of an asset for financial gain.In this work we formalize, analytically exposit and empirically evaluate an augmented variant of front-running: sandwich attacks, which involve front- and back-running victim transactions on a blockchain-based DEX. We quantify the probability of an adversarial trader being able to undertake the attack, based on the relative positioning of a transaction within a blockchain block. We find that a single adversarial trader can earn a daily revenue of over several thousand USD when performing sandwich attacks on one particular DEX \u2014 Uniswap, an exchange with over 5M USD daily trading volume by June 2020. In addition to a single-adversary game, we simulate the outcome of sandwich attacks under multiple competing adversaries, to account for the real-world trading environment."
            },
            {
                "arxivId": "2003.10001",
                "title": "Improved Price Oracles: Constant Function Market Makers",
                "abstract": "Automated market makers, first popularized by Hanson's logarithmic market scoring rule (or LMSR) for prediction markets, have become important building blocks, called 'primitives,' for decentralized finance. A particularly useful primitive is the ability to measure the price of an asset, a problem often known as the pricing oracle problem. In this paper, we focus on the analysis of a very large class of automated market makers, called constant function market makers (or CFMMs) which includes existing popular market makers such as Uniswap, Balancer, and Curve, whose yearly transaction volume totals to billions of dollars. We give sufficient conditions such that, under fairly general assumptions, agents who interact with these constant function market makers are incentivized to correctly report the price of an asset and that they can do so in a computationally efficient way. We also derive several other useful properties that were previously not known. These include lower bounds on the total value of assets held by CFMMs and lower bounds guaranteeing that no agent can, by any set of trades, drain the reserves of assets held by a given CFMM."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.09462",
        "category": "q-fin",
        "title": "Experimental Analysis of Deep Hedging Using Artificial Market Simulations for Underlying Asset Simulators",
        "abstract": "Derivative hedging and pricing are important and continuously studied topics in financial markets. Recently, deep hedging has been proposed as a promising approach that uses deep learning to approximate the optimal hedging strategy and can handle incomplete markets. However, deep hedging usually requires underlying asset simulations, and it is challenging to select the best model for such simulations. This study proposes a new approach using artificial market simulations for underlying asset simulations in deep hedging. Artificial market simulations can replicate the stylized facts of financial markets, and they seem to be a promising approach for deep hedging. We investigate the effectiveness of the proposed approach by comparing its results with those of the traditional approach, which uses mathematical finance models such as Brownian motion and Heston models for underlying asset simulations. The results show that the proposed approach can achieve almost the same level of performance as the traditional approach without mathematical finance models. Finally, we also reveal that the proposed approach has some limitations in terms of performance under certain conditions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-16.json",
        "arxivId": "2404.09646",
        "category": "q-fin",
        "title": "Derivatives of Risk Measures",
        "abstract": "This paper provides the first and second order derivatives of any risk measures, including VaR and ES for continuous and discrete portfolio loss random variable variables. Also, we give asymptotic results of the first and second order conditional moments for heavy--tailed portfolio loss random variable.",
        "references": [
            {
                "arxivId": "0708.2542",
                "title": "Capital Allocation to Business Units and Sub-Portfolios: the Euler Principle",
                "abstract": "Despite the fact that the Euler allocation principle has been adopted by many financial institutions for their internal capital allocation process, a comprehensive description of Euler allocation seems still to be missing. We try to fill this gap by presenting the theoretical background as well as practical aspects. In particular, we discuss how Euler risk contributions can be estimated for some important risk measures. We furthermore investigate the analysis of CDO tranche expected losses by means of Euler's theorem and suggest an approach to measure the impact of risk factors on non-linear portfolios."
            },
            {
                "arxivId": "math/0104190",
                "title": "Conditional Expectation as Quantile Derivative",
                "abstract": "For a linear combination of random variables, fix some confidence level and consider the quantile of the combination at this level. We are interested in the partial derivatives of the quantile with respect to the weights of the random variables in the combination. It turns out that under suitable conditions on the joint distribution of the random variables the derivatives exist and coincide with the conditional expectations of the variables given that their combination just equals the quantile. Moreover, using this result, we deduce formulas for the derivatives with respect to the weights of the variables for the so-called expected shortfall (first or higher moments) of the combination. Finally, we study in some more detail the coherence properties of the expected shortfall in case it is defined as a first conditional moment. Key words: quantile; value-at-risk; quantile derivative; conditional expectation; expected shortfall; conditional value-at-risk; coherent risk measure."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-18.json",
        "arxivId": "2309.13648",
        "category": "q-fin",
        "title": "Don't Let MEV Slip: The Costs of Swapping on the Uniswap Protocol",
        "abstract": "We present the first in-depth empirical characterization of the costs of trading on a decentralized exchange (DEX). Using quoted prices from the Uniswap Labs interface for two pools -- USDC-ETH (5bps) and PEPE-ETH (30bps) -- we evaluate the efficiency of trading on DEXs. Our main tool is slippage -- the difference between the realized execution price of a trade, and its quoted price -- which we breakdown into its benign and adversarial components. We also present an alternative way to quantify and identify slippage due to adversarial reordering of transactions, which we call reordering slippage, that does not require quoted prices or mempool data to calculate. We find that the composition of transaction costs varies tremendously with the trade's characteristics. Specifically, while for small swaps, gas costs dominate costs, for large swaps price-impact and slippage account for the majority of it. Moreover, when trading PEPE, a popular 'memecoin', the probability of adversarial slippage is about 80% higher than when trading a mature asset like USDC. Overall, our results provide preliminary evidence that DEXs offer a compelling trust-less alternative to centralized exchanges for trading digital assets.",
        "references": [
            {
                "arxivId": "2310.07865",
                "title": "The Specter (and Spectra) of Miner Extractable Value",
                "abstract": "Miner extractable value (MEV) refers to any excess value that a transaction validator can realize by manipulating the ordering of transactions. In this work, we introduce a simple theoretical definition of the 'cost of MEV', prove some basic properties, and show that the definition is useful via a number of examples. In a variety of settings, this definition is related to the 'smoothness' of a function over the symmetric group. From this definition and some basic observations, we recover a number of results from the literature."
            },
            {
                "arxivId": "2306.17742",
                "title": "Blockchain Scaling and Liquidity Concentration on Decentralized Exchanges",
                "abstract": "Liquidity providers (LPs) on decentralized exchanges (DEXs) can protect themselves from adverse selection risk by updating their positions more frequently. However, repositioning is costly, because LPs have to pay gas fees for each update. We analyze the causal relation between repositioning and liquidity concentration around the market price, using the entry of blockchain scaling solutions, Arbitrum and Polygon, as our instruments. Lower gas fees on scaling solutions allow LPs to update more frequently than on Ethereum. Our results demonstrate that higher repositioning intensity and precision lead to greater liquidity concentration, which benefits small trades by reducing their slippage."
            },
            {
                "arxivId": "2305.14604",
                "title": "Automated Market Making and Arbitrage Profits in the Presence of Fees",
                "abstract": "We consider the impact of trading fees on the profits of arbitrageurs trading against an automated marker marker (AMM) or, equivalently, on the adverse selection incurred by liquidity providers due to arbitrage. We extend the model of Milionis et al. [2022] for a general class of two asset AMMs to both introduce fees and discrete Poisson block generation times. In our setting, we are able to compute the expected instantaneous rate of arbitrage profit in closed form. When the fees are low, in the fast block asymptotic regime, the impact of fees takes a particularly simple form: fees simply scale down arbitrage profits by the fraction of time that an arriving arbitrageur finds a profitable trade."
            },
            {
                "arxivId": "2209.15569",
                "title": "Credible Decentralized Exchange Design via Verifiable Sequencing Rules",
                "abstract": "Trading on decentralized exchanges has been one of the primary use cases for permissionless blockchains with daily trading volume exceeding billions of U.S.\u2004dollars. In the status quo, users broadcast transactions they wish to execute in the exchange and miners are responsible for composing a block of transactions and picking an execution ordering \u2014 the order in which transactions execute in the exchange. Due to the lack of a regulatory framework, it is common to observe miners exploiting their privileged position by front-running transactions and obtaining risk-fee profits. Indeed, the Flashbots service institutionalizes this exploit, with miners auctioning the right to front-run transactions. In this work, we propose to modify the interaction between miners and users and initiate the study of verifiable sequencing rules. As in the status quo, miners can determine the content of a block; however, they commit to respecting a sequencing rule that constrains the execution ordering and is verifiable (there is a polynomial time algorithm that can verify if the execution ordering satisfies such constraints). Thus in the event a miner deviates from the sequencing rule, anyone can generate a proof of non-compliance. We ask if there are sequencing rules that limit price manipulation from miners in a two-token liquidity pool exchange. Our first result is an impossibility theorem: for any sequencing rule, there is an instance of user transactions where the miner can obtain non-zero risk-free profits. In light of this impossibility result, our main result is a verifiable sequencing rule that provides execution price guarantees for users. In particular, for any user transaction A, it ensures that either (1) the execution price of A is at least as good as if A was the only transaction in the block, or (2) the execution price of A is worse than this \u201cstandalone\u201d price and the miner does not gain when including A in the block. Our framework does not require users to use countermeasures against predatory trading strategies, for example, set limit prices or split large transactions into smaller ones. This is likely to improve user experience relative to the status quo."
            },
            {
                "arxivId": "2208.06046",
                "title": "Automated Market Making and Loss-Versus-Rebalancing",
                "abstract": "Automated market making (AMM) protocols such as Uniswap have recently emerged as an alternative to the most common market structure for electronic trading, the limit order book. Relative to limit order books, AMMs are both more computationally efficient and do not require the participation of active market making intermediaries. As such, AMMs have emerged as the dominant market mechanism for trust-less decentralized exchanges (DEXs). We develop a model the underlying economics of AMMs from the perspective of their passive liquidity providers (LPs). Our central contribution is a\"Black-Scholes formula for AMMs\". Like the Black-Scholes formula, we consider the return to LPs once market risk has been hedged. We identify the main adverse selection cost incurred by LPs, which we call\"loss-versus-rebalancing\"(LVR, pronounced\"lever\"). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. In a continuous time Black-Scholes setting, we are able to derive closed-form expressions for this adverse selection cost, for all automated market makers, including constant function market makers and those featuring concentrated liquidity (e.g., Uniswap v3). Qualitatively, we highlight the main forces that drive AMM LP returns, including asset characteristics (volatility) and AMM characteristics (curvature/marginal liquidity). Quantitatively, we illustrate how our model's expressions for LP returns match actual LP returns for the Uniswap v2 WETH-USDC trading pair. Our model provides tradable insight into both the ex ante and ex post assessment of AMM LP investment decisions. LVR can also inform the design of the next generation of DEX market mechanisms -- in fact, in the short time since our work has been released,\"LVR mitigation\"has already emerged as the dominant challenge among practitioners in the AMM protocol designer community."
            },
            {
                "arxivId": "2206.04185",
                "title": "A flash(bot) in the pan: measuring maximal extractable value in private pools",
                "abstract": "The rise of Ethereum has lead to a flourishing decentralized marketplace that has, unfortunately, fallen victim to frontrunning and Maximal Extractable Value (MEV) activities, where savvy participants game transaction orderings within a block for profit. One popular solution to address such behavior is Flashbots, a private pool with infrastructure and design goals aimed at eliminating the negative externalities associated with MEV. While Flashbots has established laudable goals to address MEV behavior, no evidence has been provided to show that these goals are achieved in practice. In this paper, we measure the popularity of Flashbots and evaluate if it is meeting its chartered goals. We find that (1) Flashbots miners account for over 99.9% of the hashing power in the Ethereum network, (2) powerful miners are making more than 2X what they were making prior to using Flashbots, while non-miners' slice of the pie has shrunk commensurately, (3) mining is just as centralized as it was prior to Flashbots with more than 90% of Flashbots blocks coming from just two miners, and (4) while more than 80% of MEV extraction in Ethereum is happening through Flashbots, 13.2% is coming from other private pools."
            },
            {
                "arxivId": "2203.11520",
                "title": "SoK: Preventing Transaction Reordering Manipulations in Decentralized Finance",
                "abstract": "User transactions on Ethereum's peer-to-peer network are at risk of being attacked. The smart contracts building decentralized finance (DeFi) have introduced a new transaction ordering dependency to the Ethereum blockchain. As a result, attackers can profit from front- and back-running transactions. Multiple approaches to mitigate transaction reordering manipulations have surfaced recently. However, the success of individual approaches in mitigating such attacks and their impact on the entire blockchain remains largely unstudied. In this systematization of knowledge (SoK), we categorize and analyze state-of-the-art transaction reordering manipulation mitigation schemes. Instead of restricting our analysis to a scheme's success at preventing transaction reordering attacks, we evaluate its full impact on the blockchain. Therefore, we are able to provide a complete picture of the strengths and weaknesses of current mitigation schemes. We find that currently no scheme fully meets all the demands of the blockchain ecosystem. In fact, all approaches demonstrate unsatisfactory performance in at least one area relevant to the blockchain ecosystem."
            },
            {
                "arxivId": "2112.06615",
                "title": "Quick Order Fairness",
                "abstract": null
            },
            {
                "arxivId": "2109.04347",
                "title": "Clockwork Finance: Automated Analysis of Economic Security in Smart Contracts",
                "abstract": "We introduce the Clockwork Finance Framework (CFF), a general purpose, formal verification framework for mechanized reasoning about the economic security properties of composed decentralized-finance (DeFi) smart contracts.CFF features three key properties. It is contract complete, meaning that it can model any smart contract platform and all its contracts\u2014Turing complete or otherwise. It does so with asymptotically constant model overhead. It is also attack-exhaustive by construction, meaning that it can automatically and mechanically extract all possible economic attacks on users\u2019 cryptocurrency across modeled contracts.Thanks to these properties, CFF can support multiple goals: economic security analysis of contracts by developers, analysis of DeFi trading risks by users, fees UX, and optimization of arbitrage opportunities by bots or miners. Because CFF offers composability, it can support these goals with reasoning over any desired set of potentially interacting smart contract models.We instantiate CFF as an executable model for Ethereum contracts that incorporates a state-of-the-art deductive verifier. Building on previous work, we introduce extractable value (EV), a new formal notion of economic security in composed DeFi contracts that is both a basis for CFF and of general interest.We construct modular, human-readable, composable CFF models of four popular, deployed DeFi protocols in Ethereum: Uniswap, Uniswap V2, Sushiswap, and MakerDAO, representing a combined 24 billion USD in value as of March 2022. We use these models along with some other common models such as flash loans, airdrops and voting to show experimentally that CFF is practical and can drive useful, data-based EV-based insights from real world transaction activity. Without any explicitly programmed attack strategies, CFF uncovers on average an expected $56 million of EV per month in the recent past."
            },
            {
                "arxivId": "2102.03347",
                "title": "Frontrunner Jones and the Raiders of the Dark Forest: An Empirical Study of Frontrunning on the Ethereum Blockchain",
                "abstract": "Ethereum prospered the inception of a plethora of smart contract applications, ranging from gambling games to decentralized finance. However, Ethereum is also considered a highly adversarial environment, where vulnerable smart contracts will eventually be exploited. Recently, Ethereum's pool of pending transaction has become a far more aggressive environment. In the hope of making some profit, attackers continuously monitor the transaction pool and try to frontrun their victims' transactions by either displacing or suppressing them, or strategically inserting their transactions. This paper aims to shed some light into what is known as a dark forest and uncover these predators' actions. We present a methodology to efficiently measure the three types of frontrunning: displacement, insertion, and suppression. We perform a large-scale analysis on more than 11M blocks and identify almost 200K attacks with an accumulated profit of 18.41M USD for the attackers, providing evidence that frontrunning is both, lucrative and a prevalent issue."
            },
            {
                "arxivId": "2101.05511",
                "title": "Quantifying Blockchain Extractable Value: How dark is the forest?",
                "abstract": "Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain\u2019s consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.However, related work is currently missing quantitative in-sights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantity the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60, 830 on-chain markets. The highest BEV instance we find amounts to 4.1MUSD, 616.6x the Ethereum block reward.Moreover, while the practitioner\u2019s community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions\u2019 underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37MUSD) over 32 months of past blockchain data.Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security."
            },
            {
                "arxivId": "2009.14021",
                "title": "High-Frequency Trading on Decentralized On-Chain Exchanges",
                "abstract": "Decentralized exchanges (DEXs) allow parties to participate in financial markets while retaining full custody of their funds. However, the transparency of blockchain-based DEX in combination with the latency for transactions to be processed, makes market-manipulation feasible. For instance, adversaries could perform front-running \u2014 the practice of exploiting (typically non-public) information that may change the price of an asset for financial gain.In this work we formalize, analytically exposit and empirically evaluate an augmented variant of front-running: sandwich attacks, which involve front- and back-running victim transactions on a blockchain-based DEX. We quantify the probability of an adversarial trader being able to undertake the attack, based on the relative positioning of a transaction within a blockchain block. We find that a single adversarial trader can earn a daily revenue of over several thousand USD when performing sandwich attacks on one particular DEX \u2014 Uniswap, an exchange with over 5M USD daily trading volume by June 2020. In addition to a single-adversary game, we simulate the outcome of sandwich attacks under multiple competing adversaries, to account for the real-world trading environment."
            },
            {
                "arxivId": "2003.03052",
                "title": "Combining GHOST and Casper",
                "abstract": "We present \"Gasper,\" a proof-of-stake-based consensus protocol, which is an idealized version of the proposed Ethereum 2.0 beacon chain. The protocol combines Casper FFG, a finality tool, with LMD GHOST, a fork-choice rule. We prove safety, plausible liveness, and probabilistic liveness under different sets of assumptions."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-18.json",
        "arxivId": "2404.11063",
        "category": "q-fin",
        "title": "Attitudinal Loyalty Manifestation in Banking CSR: Cross-Buying Behavior and Customer Advocacy",
        "abstract": "This study in the banking industry examines the influence of attitudinal loyalty on customer advocacy and cross buying behavior, alongside the moderating roles of Quality of Life and Corporate Social Responsibility support in the CSR fit and loyalty relationship. Employing Structural Equation Modeling, it reveals that higher attitudinal loyalty significantly boosts customer advocacy and propensity for cross buying. The findings highlight the importance of nurturing customer loyalty through valuable and relevant offerings, as CSR fit alone does not define the loyalty of the banking customer. Banks are advised to target customers with a high Quality of Life and engage with those who support CSR initiatives aligning with the banks objectives, to enhance loyalty and deepen customer relationships.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-18.json",
        "arxivId": "2404.11257",
        "category": "q-fin",
        "title": "Deep Joint Learning valuation of Bermudan Swaptions",
        "abstract": "This paper addresses the problem of pricing involved financial derivatives by means of advanced of deep learning techniques. More precisely, we smartly combine several sophisticated neural network-based concepts like differential machine learning, Monte Carlo simulation-like training samples and joint learning to come up with an efficient numerical solution. The application of the latter development represents a novelty in the context of computational finance. We also propose a novel design of interdependent neural networks to price early-exercise products, in this case, Bermudan swaptions. The improvements in efficiency and accuracy provided by the here proposed approach is widely illustrated throughout a range of numerical experiments. Moreover, this novel methodology can be extended to the pricing of other financial derivatives.",
        "references": [
            {
                "arxivId": "2204.03508",
                "title": "A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods",
                "abstract": "Multi-task learning (MTL) has become increasingly popular in natural language processing (NLP) because it improves the performance of related tasks by exploiting their commonalities and differences. Nevertheless, it is still not understood very well how multi-task learning can be implemented based on the relatedness of training tasks. In this survey, we review recent advances of multi-task learning methods in NLP, with the aim of summarizing them into two general multi-task training methods based on their task relatedness: (i) joint training and (ii) multi-step training. We present examples in various NLP downstream applications, summarize the task relationships and discuss future directions of this promising topic."
            },
            {
                "arxivId": "2201.02587",
                "title": "Pricing Bermudan Options Using Regression Trees/Random Forests",
                "abstract": "The value of an American option is the maximized value of the discounted cash flows from the option. At each time step, one needs to compare the immediate exercise value with the continuation value and decide to exercise as soon as the exercise value is strictly greater than the continuation value. We can formulate this problem as a dynamic programming equation, where the main difficulty comes from the computation of the conditional expectations representing the continuation values at each time step. In (Longstaff and Schwartz, 2001), these conditional expectations were estimated using regressions on a finite-dimensional vector space (typically a polynomial basis). In this paper, we follow the same algorithm; only the conditional expectations are estimated using Regression trees or Random forests. We discuss the convergence of the LS algorithm when the standard least squares regression is replaced with regression trees. Finally, we expose some numerical results with regression trees and random forests. The random forest algorithm gives excellent results in high dimensions."
            },
            {
                "arxivId": "2005.12059",
                "title": "Financial Option Valuation by Unsupervised Learning with Artificial Neural Networks",
                "abstract": "Artificial neural networks (ANNs) have recently also been applied to solve partial differential equations (PDEs). The classical problem of pricing European and American financial options, based on the corresponding PDE formulations, is studied here. Instead of using numerical techniques based on finite element or difference methods, we address the problem using ANNs in the context of unsupervised learning. As a result, the ANN learns the option values for all possible underlying stock values at future time points, based on the minimization of a suitable loss function. For the European option, we solve the linear Black\u2013Scholes equation, whereas for the American option we solve the linear complementarity problem formulation. Two-asset exotic option values are also computed, since ANNs enable the accurate valuation of high-dimensional options. The resulting errors of the ANN approach are assessed by comparing to the analytic option values or to numerical reference solutions (for American options, computed by finite elements). In the short note, previously published, a brief introduction to this work was given, where some ideas to price vanilla options by ANNs were presented, and only European options were addressed. In the current work, the methodology is introduced in much more detail."
            },
            {
                "arxivId": "2005.02633",
                "title": "Deep xVA Solver \u2013 A Neural Network Based Counterparty Credit Risk Management Framework",
                "abstract": "In this paper, we present a novel computational framework for portfolio-wide risk management problems, where the presence of a potentially large number of risk factors makes traditional numerical techniques ineffective. The new method utilises a coupled system of BSDEs for the valuation adjustments (xVA) and solves these by a recursive application of a neural network based BSDE solver. This not only makes the computation of xVA for high-dimensional problems feasible, but also produces hedge ratios and dynamic risk measures for xVA, and allows simulations of the collateral account."
            },
            {
                "arxivId": "1912.11060",
                "title": "Pricing and Hedging American-Style Options with Deep Learning",
                "abstract": "In this paper we introduce a deep learning method for pricing and hedging American-style options. It first computes a candidate optimal stopping policy. From there it derives a lower bound for the price. Then it calculates an upper bound, a point estimate and confidence intervals. Finally, it constructs an approximate dynamic hedging strategy. We test the approach on different specifications of a Bermudan max-call option. In all cases it produces highly accurate prices and dynamic hedging strategies with small replication errors."
            },
            {
                "arxivId": "1901.09647",
                "title": "Deep learning volatility: a deep neural network perspective on pricing and calibration in (rough) volatility models",
                "abstract": "We present a neural network-based calibration method that performs the calibration task within a few milliseconds for the full implied volatility surface. The framework is consistently applicable throughout a range of volatility models\u2014including second-generation stochastic volatility models and the rough volatility family\u2014and a range of derivative contracts. Neural networks in this work are used in an off-line approximation of complex pricing functions, which are difficult to represent or time-consuming to evaluate by other means. The form in which information from available data is extracted and used influences network performance: The grid-based algorithm used for calibration is inspired by representing the implied volatility and option prices as a collection of pixels. We highlight how this perspective opens new horizons for quantitative modelling. The calibration bottleneck posed by a slow pricing of derivative contracts is lifted, and stochastic volatility models (classical and rough) can be handled in great generality as the framework also allows taking the forward variance curve as an input. We demonstrate the calibration performance both on simulated and historical data, on different derivative contracts and on a number of example models of increasing complexity, and also showcase some of the potentials of this approach towards model recognition. The algorithm and examples are provided in the Github repository GitHub: NN-StochVol-Calibrations."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-19.json",
        "arxivId": "2302.12439",
        "category": "q-fin",
        "title": "Simultaneous Upper and Lower Bounds of American Option Prices with Hedging Via Neural Networks",
        "abstract": "In this paper, we introduce two methods to solve the American-style option pricing problem and its dual form at the same time using neural networks. Without applying nested Monte Carlo, the first method uses a series of neural networks to simultaneously compute both the lower and upper bounds of the option price, and the second one accomplishes the same goal with one global network. The avoidance of extra simulations and the use of neural networks significantly reduce the computational complexity and allow us to price Bermudan options with frequent exercise opportunities in high dimensions, as illustrated by the provided numerical experiments. As a by-product, these methods also derive a hedging strategy for the option, which can also be used as a control variate for variance reduction.",
        "references": [
            {
                "arxivId": "2205.04595",
                "title": "Neural Optimal Stopping Boundary",
                "abstract": "A method based on deep artificial neural networks and empirical risk minimization is developed to calculate the boundary separating the stopping and continuation regions in optimal stopping. The algorithm parameterizes the stopping boundary as the graph of a function and introduces relaxed stopping rules based on fuzzy boundaries to facilitate efficient optimization. Several financial instruments, some in high dimensions, are analyzed through this method, demonstrating its effectiveness. The existence of the stopping boundary is also proved under natural structural assumptions."
            },
            {
                "arxivId": "2101.08068",
                "title": "Neural networks-based algorithms for stochastic control and PDEs in finance",
                "abstract": "This paper presents machine learning techniques and deep reinforcement learningbased algorithms for the efficient resolution of nonlinear partial differential equations and dynamic optimization problems arising in investment decisions and derivative pricing in financial engineering. We survey recent results in the literature, present new developments, notably in the fully nonlinear case, and compare the different schemes illustrated by numerical tests on various financial applications. We conclude by highlighting some future research directions."
            },
            {
                "arxivId": "2012.12348",
                "title": "An overview on deep learning-based approximation methods for partial differential equations",
                "abstract": "It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations (PDEs). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional PDEs. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional PDEs. In this article we offer an introduction to this field of research by revisiting selected mathematical results related to deep learning approximation methods for PDEs and reviewing the main ideas of their proofs. We also provide a short overview of the recent literature in this area of research."
            },
            {
                "arxivId": "1911.11362",
                "title": "Neural Network for Pricing and Universal Static Hedging of Contingent Claims",
                "abstract": "We present here a regress later based Monte Carlo approach that uses neural networks for pricing high-dimensional contingent claims. The choice of specific architecture of the neural networks used in the proposed algorithm provides for interpretability of the model, a feature that is often desirable in the financial context. Specifically, the interpretation leads us to demonstrate that any contingent claim -- possibly high dimensional and path-dependent -- under the Markovian and the no-arbitrage assumptions, can be semi-statically hedged using a portfolio of short maturity options. We show how the method can be used to obtain an upper and lower bound to the true price, where the lower bound is obtained by following a sub-optimal policy, while the upper bound by exploiting the dual formulation. Unlike other duality based upper bounds where one typically has to resort to nested simulation for constructing super-martingales, the martingales in the current approach come at no extra cost, without the need for any sub-simulations. We demonstrate through numerical examples the simplicity and efficiency of the method for both pricing and semi-static hedging of path-dependent options"
            },
            {
                "arxivId": "1907.06474",
                "title": "Neural network regression for Bermudan option pricing",
                "abstract": "Abstract The pricing of Bermudan options amounts to solving a dynamic programming principle, in which the main difficulty, especially in high dimension, comes from the conditional expectation involved in the computation of the continuation value. These conditional expectations are classically computed by regression techniques on a finite-dimensional vector space. In this work, we study neural networks approximations of conditional expectations. We prove the convergence of the well-known Longstaff and Schwartz algorithm when the standard least-square regression is replaced by a neural network approximation, assuming an efficient algorithm to compute this approximation. We illustrate the numerical efficiency of neural networks as an alternative to standard regression methods for approximating conditional expectations on several numerical examples."
            },
            {
                "arxivId": "1905.09474",
                "title": "Machine learning for pricing American options in high-dimensional Markovian and non-Markovian models",
                "abstract": "In this paper we propose two efficient techniques which allow one to compute the price of American basket options. In particular, we consider a basket of assets that follow a multi-dimensional Black\u2013Scholes dynamics. The proposed techniques, called GPR Tree (GRP-Tree) and GPR Exact Integration (GPR-EI), are both based on Machine Learning, exploited together with binomial trees or with a closed form formula for integration. Moreover, these two methods solve the backward dynamic programing problem considering a Bermudan approximation of the American option. On the exercise dates, the value of the option is first computed as the maximum between the exercise value and the continuation value and then approximated by means of Gaussian Process Regression. The two methods mainly differ in the approach used to compute the continuation value: a single step of the binomial tree or integration according to the probability density of the process. Numerical results show that these two methods are accurate and reliable in handling American options on very large baskets of assets. Moreover we also consider the rough Bergomi model, which provides stochastic volatility with memory. Despite that this model is only bidimensional, the whole history of the process impacts on the price, and how to handle all this information is not obvious at all. To this aim, we present how to adapt the GPR-Tree and GPR-EI methods and we focus on pricing American options in this non-Markovian framework."
            },
            {
                "arxivId": "1903.11275",
                "title": "Variance Reduction Applied to Machine Learning for Pricing Bermudan/American Options in High Dimension",
                "abstract": "In this paper we propose an efficient method to compute the price of multi-asset American options, based on Machine Learning, Monte Carlo simulations and variance reduction technique. Specifically, the options we consider are written on a basket of assets, each of them following a Black-Scholes dynamics. In the wake of Ludkovski's approach (2018), we implement here a backward dynamic programming algorithm which considers a finite number of uniformly distributed exercise dates. On these dates, the option value is computed as the maximum between the exercise value and the continuation value, which is obtained by means of Gaussian process regression technique and Monte Carlo simulations. Such a method performs well for low dimension baskets but it is not accurate for very high dimension baskets. In order to improve the dimension range, we employ the European option price as a control variate, which allows us to treat very large baskets and moreover to reduce the variance of price estimators. Numerical tests show that the proposed algorithm is fast and reliable, and it can handle also American options on very large baskets of assets, overcoming the problem of the curse of dimensionality."
            },
            {
                "arxivId": "1902.05287",
                "title": "Risk management with machine-learning-based algorithms",
                "abstract": "We propose some machine-learning-based algorithms to solve hedging problems in incomplete markets. Sources of incompleteness cover illiquidity, untradable risk factors, discrete hedging dates and transaction costs. The proposed algorithms resulting strategies are compared to classical stochastic control techniques on several payoffs using a variance criterion. One of the proposed algorithm is flexible enough to be used with several existing risk criteria. We furthermore propose a new moment-based risk criteria."
            },
            {
                "arxivId": "1812.04300",
                "title": "Deep Neural Networks Algorithms for Stochastic Control Problems on Finite Horizon: Convergence Analysis",
                "abstract": "This paper develops algorithms for high-dimensional stochastic control problems based on deep learning and dynamic programming. Unlike classical approximate dynamic programming approaches, we first approximate the optimal policy by means of neural networks in the spirit of deep reinforcement learning, and then the value function by Monte Carlo regression. This is achieved in the dynamic programming recursion by performance or hybrid iteration, and regress now methods from numerical probabilities. We provide a theoretical justification of these algorithms. Consistency and rate of convergence for the control and value function estimates are analyzed and expressed in terms of the universal approximation error of the neural networks, and of the statistical error when estimating network function, leaving aside the optimization error. Numerical results on various applications are presented in a companion paper (arxiv.org/abs/1812.05916) and illustrate the performance of the proposed algorithms."
            },
            {
                "arxivId": "1809.07609",
                "title": "Machine Learning for Semi Linear PDEs",
                "abstract": null
            },
            {
                "arxivId": "1804.07010",
                "title": "Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations",
                "abstract": "Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions."
            },
            {
                "arxivId": "1804.05394",
                "title": "Deep Optimal Stopping",
                "abstract": "In this paper we develop a deep learning method for optimal stopping problems which directly learns the optimal stopping rule from Monte Carlo samples. As such, it is broadly applicable in situations where the underlying randomness can efficiently be simulated. We test the approach on three problems: the pricing of a Bermudan max-call option, the pricing of a callable multi barrier reverse convertible and the problem of optimally stopping a fractional Brownian motion. In all three cases it produces very accurate results in high-dimensional situations with short computing times."
            },
            {
                "arxivId": "1707.02568",
                "title": "Solving high-dimensional partial differential equations using deep learning",
                "abstract": "Significance Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the \u201ccurse of dimensionality.\u201d This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships. Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the \u201ccurse of dimensionality.\u201d This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black\u2013Scholes equation, the Hamilton\u2013Jacobi\u2013Bellman equation, and the Allen\u2013Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships."
            },
            {
                "arxivId": "1509.02179",
                "title": "Kriging Metamodels and Experimental Design for Bermudan Option Pricing",
                "abstract": "We investigate two new strategies for the numerical solution of optimal stopping problems within the Regression Monte Carlo (RMC) framework of Longstaff and Schwartz. First, we propose the use of stochastic kriging (Gaussian process) meta-models for fitting the continuation value. Kriging offers a flexible, nonparametric regression approach that quantifies approximation quality. Second, we connect the choice of stochastic grids used in RMC to the Design of Experiments paradigm. We examine space-filling and adaptive experimental designs; we also investigate the use of batching with replicated simulations at design sites to improve the signal-to-noise ratio. Numerical case studies for valuing Bermudan Puts and Max-Calls under a variety of asset dynamics illustrate that our methods offer significant reduction in simulation budgets over existing approaches."
            },
            {
                "arxivId": "1210.8175",
                "title": "A Probabilistic Numerical Method for Optimal Multiple Switching Problems in High Dimension",
                "abstract": "In this paper, we present a probabilistic numerical algorithm combining dynamic programming, Monte Carlo simulations and local basis regressions to solve non-stationary optimal multiple switching problems in infinite horizon. We provide the rate of convergence of the method in terms of the time step used to discretize the problem, of the regression basis used to approximate conditional expectations, and of the truncating time horizon. To make the method viable for problems in high dimension and long time horizon, we extend a memory reduction method to the general Euler scheme, so that, when performing the numerical resolution, the storage of the Monte Carlo simulation paths is not needed. Then, we apply this algorithm to a model of optimal investment in power plants in dimension eight, i.e. with two different technologies and six random factors."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-19.json",
        "arxivId": "2303.10906",
        "category": "q-fin",
        "title": "The Economic Value of User Tracking for Publishers",
        "abstract": "Regulators and browsers increasingly restrict user tracking to protect users' privacy online. In two large-scale empirical studies, we study the economic implications for publishers relying on selling advertising space to finance their content. In our first study, we draw on 42 million ad impressions from 111 publishers covering EU desktop browsing traffic in 2016. In our second study, we use 218 million ad impressions from 10,526 publishers (i.e., apps) covering EU and US mobile in-app browsing traffic in 2023. The two studies differ in the share of trackable users (Study 1: 85%; Study 2: Apple: 17%, Android: 91%). Still, we find similar average ad impression price decreases (Study 1: 18% and Study 2: 23%) when user tracking is unavailable. More than 90% of the publishers realize lower prices when selling ad impressions for untrackable users. Publishers offering content on sports, cars, lifestyle&shopping, and news&information suffer the most. Premium publishers with high-quality edited content and strong reputations, thematic-focused (niche) publishers, and smaller publishers suffer less from the unavailability of user tracking. In contrast, non-premium publishers with non-edited or user-generated content, thematic-broad (general news) publishers, and larger publishers suffer more. The availability of a user ID generates the highest value for publishers, whereas collecting a user's browsing history, perceived as intrusive by most users, generates only a small value for publishers. These results affirm that ensuring user privacy online has substantial costs for online publishers, but those costs differ across publishers and the type of collected data. This article offers suggestions to reduce these costs.",
        "references": [
            {
                "arxivId": "1912.09012",
                "title": "Inefficiencies in Digital Advertising Markets",
                "abstract": "Digital advertising markets are growing and attracting increased scrutiny. This article explores four market inefficiencies that remain poorly understood: ad effect measurement, frictions between and within advertising channel members, ad blocking, and ad fraud. Although these topics are not unique to digital advertising, each manifests in unique ways in markets for digital ads. The authors identify relevant findings in the academic literature, recent developments in practice, and promising topics for future research."
            },
            {
                "arxivId": "1010.5586",
                "title": "Matching methods for causal inference: A review and a look forward.",
                "abstract": "When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970's, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine, and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods-or developing methods related to matching-do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-19.json",
        "arxivId": "2309.16408",
        "category": "q-fin",
        "title": "Assessing the Solvency of Virtual Asset Service Providers: Are Current Standards Sufficient?",
        "abstract": "Entities like centralized cryptocurrency exchanges fall under the business category of virtual asset service providers (VASPs). As any other enterprise, they can become insolvent. VASPs enable the exchange, custody, and transfer of cryptoassets organized in wallets across distributed ledger technologies (DLTs). Despite the public availability of DLT transactions, the cryptoasset holdings of VASPs are not yet subject to systematic auditing procedures. In this paper, we propose an approach to assess the solvency of a VASP by cross-referencing data from three distinct sources: cryptoasset wallets, balance sheets from the commercial register, and data from supervisory entities. We investigate 24 VASPs registered with the Financial Market Authority in Austria and provide regulatory data insights such as who are the customers and where do they come from. Their yearly incoming and outgoing transaction volume amount to 2 billion EUR for around 1.8 million users. We describe what financial services they provide and find that they are most similar to traditional intermediaries such as brokers, money exchanges, and funds, rather than banks. Next, we empirically measure DLT transaction flows of four VASPs and compare their cryptoasset holdings to balance sheet entries. Data are consistent for two VASPs only. This enables us to identify gaps in the data collection and propose strategies to address them. We remark that any entity in charge of auditing requires proof that a VASP actually controls the funds associated with its on-chain wallets. It is also important to report fiat and cryptoasset and liability positions broken down by asset types at a reasonable frequency.",
        "references": [
            {
                "arxivId": "2207.13914",
                "title": "Anatomy of a Stablecoin's failure: the Terra-Luna case",
                "abstract": null
            },
            {
                "arxivId": "2003.11352",
                "title": "Cryptocurrency trading: a comprehensive survey",
                "abstract": null
            },
            {
                "arxivId": "2108.10984",
                "title": "Crypto Wash Trading",
                "abstract": "We present the first systematic approach to detect fake transactions on cryptocurrency exchanges by exploiting robust statistical and behavioral regularities associated with authentic trading. Our sample consists of 29 centralized exchanges, among which the regulated ones feature transaction patterns consistently observed in financial markets and nature. In contrast, unregulated exchanges display abnormal first significant digit distributions, size rounding, and transaction tail distributions, indicating widespread manipulation unlikely driven by a specific trading strategy or exchange heterogeneity. We then quantify the wash trading on each unregulated exchange, which averaged more than 70% of the reported volume. We further document how these fabricated volumes (trillions of dollars annually) improve exchange ranking, temporarily distort prices, and relate to exchange characteristics (e.g., age and user base), market conditions, and regulation. Overall, our study cautions against potential market manipulations on centralized crypto exchanges with concentrated power and limited disclosure requirements and highlights the importance of fintech regulation. This paper was accepted by David Simchi-Levi, special issue of Management Science: Blockchains and crypto economics. Funding: This research was partly funded by the Ewing Marion Kauffman Foundation [Grant G-201907-6995], the National Natural Science Foundation of China [Grants 72192802, 72192800, and 72192801], Ripple\u2019s University Blockchain Research Initiative (UBRI), and the FinTech at Cornell Initiative. Supplemental Material: The online appendix and data are available at https://doi.org/10.1287/mnsc.2021.02709 ."
            },
            {
                "arxivId": "1910.08820",
                "title": "Rationality is Self-Defeating in Permissionless Systems",
                "abstract": "We outline a metacircular argument explaining why it is rational to be irrational when attacking open-world decentralized systems, and why systems whose security depend on rationality assumptions are insecure."
            },
            {
                "arxivId": "1906.02152",
                "title": "(In)Stability for the Blockchain: Deleveraging Spirals and Stablecoin Attacks",
                "abstract": "We develop a model of stable assets, including noncustodial stablecoins backed by cryptocurrencies. Such stablecoins are popular methods for bootstrapping price stability within public blockchain settings. We demonstrate fundamental results about dynamics and liquidity in stablecoin markets, demonstrate that these markets face deleveraging spirals that cause illiquidity during crises, and show that these stablecoins have `stable' and `unstable' domains. Starting from documented market behaviors, we explain actual stablecoin movements; further our results are robust to a wide range of potential behaviors. In simulations, we show that these systems are susceptible to high tail volatility and failure. Our model builds foundations for stablecoin design. Based on our results, we suggest design improvements that can improve long-term stability and suggest methods for solving pricing problems that arise in existing stablecoins. In addition to the direct risk of instability, our dynamics results suggest a profitable economic attack during extreme events that can induce volatility in the `stable' asset. This attack additionally suggests ways in which stablecoins can cause perverse incentives for miners, posing risks to blockchain consensus."
            },
            {
                "arxivId": "1705.05334",
                "title": "Evolutionary dynamics of the cryptocurrency market",
                "abstract": "The cryptocurrency market surpassed the barrier of $100 billion market capitalization in June 2017, after months of steady growth. Despite its increasing relevance in the financial world, a comprehensive analysis of the whole system is still lacking, as most studies have focused exclusively on the behaviour of one (Bitcoin) or few cryptocurrencies. Here, we consider the history of the entire market and analyse the behaviour of 1469 cryptocurrencies introduced between April 2013 and May 2017. We reveal that, while new cryptocurrencies appear and disappear continuously and their market capitalization is increasing (super-)exponentially, several statistical properties of the market have been stable for years. These include the number of active cryptocurrencies, market share distribution and the turnover of cryptocurrencies. Adopting an ecological perspective, we show that the so-called neutral model of evolution is able to reproduce a number of key empirical observations, despite its simplicity and the assumption of no selective advantage of one cryptocurrency over another. Our results shed light on the properties of the cryptocurrency market and establish a first formal link between ecological modelling and the study of this growing system. We anticipate they will spark further research in this direction."
            },
            {
                "arxivId": "1704.08175",
                "title": "High-Frequency Jump Analysis of the Bitcoin Market",
                "abstract": "We use the database leak of Mt. Gox exchange to analyze the dynamics of the price of bitcoin from June 2011 to November 2013. This gives us a rare opportunity to study an emerging retail-focused, highly speculative and unregulated market with trader identifiers at a tick transaction level. Jumps are frequent events and they cluster in time. The order flow imbalance and the preponderance of aggressive traders, as well as a widening of the bid-ask spread predict them. Jumps have short-term positive impact on market activity and illiquidity and see a persistent change in the price."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2209.10128",
        "category": "q-fin",
        "title": "Efficient Integrated Volatility Estimation in the Presence of Infinite Variation Jumps via Debiased Truncated Realized Variations",
        "abstract": "Statistical inference for stochastic processes based on high-frequency observations has been an active research area for more than two decades. One of the most well-known and widely studied problems has been the estimation of the quadratic variation of the continuous component of an It\\^o semimartingale with jumps. Several rate- and variance-efficient estimators have been proposed in the literature when the jump component is of bounded variation. However, to date, very few methods can deal with jumps of unbounded variation. By developing new high-order expansions of the truncated moments of a locally stable L\\'evy process, we propose a new rate- and variance-efficient volatility estimator for a class of It\\^o semimartingales whose jumps behave locally like those of a stable L\\'evy process with Blumenthal-Getoor index $Y\\in (1,8/5)$ (hence, of unbounded variation). The proposed method is based on a two-step debiasing procedure for the truncated realized quadratic variation of the process and can also cover the case $Y<1$. Our Monte Carlo experiments indicate that the method outperforms other efficient alternatives in the literature in the setting covered by our theoretical framework.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2212.10317",
        "category": "q-fin",
        "title": "Does Peer-Reviewed Research Help Predict Stock Returns?",
        "abstract": "Mining 29,000 accounting ratios for t-statistics over 2.0 leads to cross-sectional predictability similar to the peer review process. For both methods, about 50% of predictability remains after the original sample periods. Data mining generates other features of peer review including the rise in returns as original sample periods end, the speed of post-sample decay, and themes like investment, issuance, and accruals. Predictors supported by peer-reviewed risk explanations underperform data mining. Similarly, the relationship between modeling rigor and post-sample returns is negative. Our results suggest peer review systematically mislabels mispricing as risk, though only 18% of predictors are attributed to risk.",
        "references": [
            {
                "arxivId": "2311.10685",
                "title": "High-Throughput Asset Pricing",
                "abstract": "We use empirical Bayes (EB) to mine data on 140,000 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. This\"high-throughput asset pricing\"produces out-of-sample performance comparable to strategies in top finance journals. But unlike the published strategies, the data-mined strategies are free of look-ahead bias. EB predicts that high returns are concentrated in accounting strategies, small stocks, and pre-2004 samples, consistent with limited attention theories. The intuition is seen in the cross-sectional distribution of t-stats, which is far from the null for equal-weighted accounting strategies. High-throughput methods provide a rigorous, unbiased method for documenting asset pricing facts."
            },
            {
                "arxivId": "2209.13623",
                "title": "Publication Bias in Asset Pricing Research",
                "abstract": "Researchers are more likely to share notable findings. As a result, published findings tend to overstate the magnitude of real-world phenomena. This bias is a natural concern for asset pricing research, which has found hundreds of return predictors and little consensus on their origins. Empirical evidence on publication bias comes from large scale meta-studies. Meta-studies of cross-sectional return predictability have settled on four stylized facts that demonstrate publication bias is not a dominant factor: (1) almost all findings can be replicated, (2) predictability persists out-of-sample, (3) empirical $t$-statistics are much larger than 2.0, and (4) predictors are weakly correlated. Each of these facts has been demonstrated in at least three meta-studies. Empirical Bayes statistics turn these facts into publication bias corrections. Estimates from three meta-studies find that the average correction (shrinkage) accounts for only 10 to 15 percent of in-sample mean returns and that the risk of inference going in the wrong direction (the false discovery rate) is less than 10%. Meta-studies also find that $t$-statistic hurdles exceed 3.0 in multiple testing algorithms and that returns are 30 to 50 percent weaker in alternative portfolio tests. These facts are easily misinterpreted as evidence of publication bias effects. We clarify these misinterpretations and others, including the conflating of ``mostly false findings'' with ``many insignificant findings,'' ``data snooping'' with ``liquidity effects,'' and ``failed replications'' with ``insignificant ad-hoc trading strategies.'' Meta-studies outside of the cross-sectional literature are rare. The four facts from cross-sectional meta-studies provide a framework for future research. We illustrate with a preliminary re-examination of equity premium predictability."
            },
            {
                "arxivId": "2207.13071",
                "title": "Missing values handling for machine learning portfolios",
                "abstract": null
            },
            {
                "arxivId": "2206.15365",
                "title": "Most Claimed Statistical Findings in Cross-Sectional Return Predictability Are Likely True",
                "abstract": "Harvey, Liu, and Zhu (2016) \u201cargue that most claimed research findings in financial economics are likely false.\u201d Surprisingly, their false discovery rate (FDR) estimates suggest most are true. I revisit their results by developing non- and semi-parametric FDR estimators that account for publication bias and empirical correlations. These estimators provide simple closed-form expressions and reliably produce an upper bound on the FDR in simulations that cluster-bootstrap from empirical predictor returns. Applying these estimators to the Chen-Zimmermann dataset of 205 predictors, I find that most claimed statistical findings in the cross-sectional predictability literature are likely true."
            },
            {
                "arxivId": "2006.04269",
                "title": "False (and Missed) Discoveries in Financial Economics",
                "abstract": "Multiple testing plagues many important questions in finance such as fund and factor selection. We propose a new way to calibrate both Type I and Type II errors. Next, using a double-bootstrap method, we establish a t-statistic hurdle that is associated with a specific false discovery rate (e.g., 5%). We also establish a hurdle that is associated with a certain acceptable ratio of misses to false discoveries (Type II error scaled by Type I error), which effectively allows for differential costs of the two types of mistakes. Evaluating current methods, we find that they lack power to detect outperforming managers."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2301.09241",
        "category": "q-fin",
        "title": "Quantum Monte Carlo algorithm for solving Black-Scholes PDEs for high-dimensional option pricing in finance and its complexity analysis",
        "abstract": "In this paper we provide a quantum Monte Carlo algorithm to solve high-dimensional Black-Scholes PDEs with correlation for high-dimensional option pricing. The payoff function of the option is of general form and is only required to be continuous and piece-wise affine (CPWA), which covers most of the relevant payoff functions used in finance. We provide a rigorous error analysis and complexity analysis of our algorithm. In particular, we prove that the computational complexity of our algorithm is bounded polynomially in the space dimension $d$ of the PDE and the reciprocal of the prescribed accuracy $\\varepsilon$. Moreover, we show that for payoff functions which are bounded, our algorithm indeed has a speed-up compared to classical Monte Carlo methods. Furthermore, we provide numerical simulations in one and two dimensions using our developed package within the Qiskit framework tailored to price CPWA options with respect to the Black-Scholes model, as well as discuss the potential extension of the numerical simulations to arbitrary space dimension.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2401.01427",
        "category": "q-fin",
        "title": "Nash Equilibria in Greenhouse Gas Offset Credit Markets",
        "abstract": "In response to the global climate crisis, governments worldwide are introducing legislation to reduce greenhouse gas (GHG) emissions to help mitigate environmental catastrophes. One method to encourage emission reductions is to incentivize carbon capturing and carbon reducing projects while simultaneously penalising excess GHG output. Firms that invest in such projects or reduce their emissions can receive offset credits (OCs) in return. They may then use OCs for regulatory purposes to offset emissions in a compliance period or trade them. Here, we present a novel market framework and characterise the optimal behaviour of GHG OC market participants in both single-player and two-player settings. The single player setting is posed as an optimal stopping and control problem, while the two-player setting is posed as optimal stopping and mixed-Nash equilibria problem. We demonstrate the importance of acting optimally using numerical solutions and Monte Carlo simulations and explore the differences between the homogeneous and heterogeneous players.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2401.03776",
        "category": "q-fin",
        "title": "Approximating Smiles: A Time Change Approach",
        "abstract": "We present a new method for analyzing the shape of implied volatility smiles The method is applicable to common semimartingale models, such as jump-diffusion, rough volatility, and infinite activity models. We use a moment-based formula to approximate the at-the-money skew and curvature. Additionally, we explicitly approximate the volatility skew and curvature under a time change framework. We derive characteristics of skewness and curvature and explain their implications for model selection based on the approximation. The accuracy of the short-term approximation results on models is tested via numerical methods and on empirical data. The method is then applied to the calibration problem.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2402.14674",
        "category": "q-fin",
        "title": "Doing AI: Algorithmic decision support as a human activity",
        "abstract": "Algorithmic decision support (ADS), using Machine-Learning-based AI, is becoming a major part of many processes. Organizations introduce ADS to improve decision-making and use available data, thereby possibly limiting deviations from the normative\"homo economicus\"and the biases that characterize human decision-making. However, a closer look at the development and use of ADS systems in organizational settings reveals that they necessarily involve a series of largely unspecified human decisions. They begin with deliberations for which decisions to use ADS, continue with choices while developing and deploying the ADS, and end with decisions on how to use the ADS output in an organization's operations. The paper presents an overview of these decisions and some relevant behavioral phenomena. It points out directions for further research, which is essential for correctly assessing the processes and their vulnerabilities. Understanding these behavioral aspects is important for successfully implementing ADS in organizations.",
        "references": [
            {
                "arxivId": "2107.03721",
                "title": "Demystifying the Draft EU Artificial Intelligence Act \u2014 Analysing the good, the bad, and the unclear elements of the proposed approach",
                "abstract": "In April 2021, the European Commission proposed a Regulation on Artificial Intelligence, known as the AI Act. We present an overview of the Act and analyse its implications, drawing on scholarship ranging from the study of contemporary AI practices to the structure of EU product safety regimes over the last four decades. Aspects of the AI Act, such as different rules for different risk-levels of AI, make sense. But we also find that some provisions of the draft AI Act have surprising legal implications, whilst others may be largely ineffective at achieving their stated goals. Several overarching aspects, including the enforcement regime and the effect of maximum harmonisation on the space for AI policy more generally, engender significant concern. These issues should be addressed as a priority in the legislative process."
            },
            {
                "arxivId": "2010.00828",
                "title": "Maximal benefits and possible detrimental effects of binary decision aids",
                "abstract": "Binary decision aids, such as alerts, are a simple and widely used form of automation. The formal analysis of a user\u2019s task performance with an aid sees the process as the combination of information from two detectors who both receive input about an event and evaluate it. The user\u2019s decisions are based on the output of the aid and on the information, the user obtains independently. We present a simple method for computing the maximal benefits a user can derive from a binary aid as a function of the user\u2019s and the aid\u2019s sensitivities. Combining the user and the aid often adds little to the performance the better detector could achieve alone. Also, if users assign non-optimal weights to the aid, performance may drop dramatically. Thus, the introduction of a valid aid can actually lower detection performance, compared to a more sensitive user working alone. Similarly, adding a user to a system with high sensitivity may lower its performance. System designers need to consider the potential adverse effects of introducing users or aids into systems."
            },
            {
                "arxivId": "2005.06057",
                "title": "Visual Analytics and Human Involvement in Machine Learning",
                "abstract": "The rapidly developing AI systems and applications still require human involvement in practically all parts of the analytics process. Human decisions are largely based on visualizations, providing data scientists details of data properties and the results of analytical procedures. Different visualizations are used in the different steps of the Machine Learning (ML) process. The decision which visualization to use depends on factors, such as the data domain, the data model and the step in the ML process. In this chapter, we describe the seven steps in the ML process and review different visualization techniques that are relevant for the different steps for different types of data, models and purposes."
            },
            {
                "arxivId": "2003.03541",
                "title": "A Human-Centered Review of Algorithms used within the U.S. Child Welfare System",
                "abstract": "The U.S. Child Welfare System (CWS) is charged with improving outcomes for foster youth; yet, they are overburdened and underfunded. To overcome this limitation, several states have turned towards algorithmic decision-making systems to reduce costs and determine better processes for improving CWS outcomes. Using a human-centered algorithmic design approach, we synthesize 50 peer-reviewed publications on computational systems used in CWS to assess how they were being developed, common characteristics of predictors used, as well as the target outcomes. We found that most of the literature has focused on risk assessment models but does not consider theoretical approaches (e.g., child-foster parent matching) nor the perspectives of caseworkers (e.g., case notes). Therefore, future algorithms should strive to be context-aware and theoretically robust by incorporating salient factors identified by past research. We provide the HCI community with research avenues for developing human-centered algorithms that redirect attention towards more equitable outcomes for CWS."
            },
            {
                "arxivId": "2001.09784",
                "title": "Algorithmic Fairness",
                "abstract": "An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness."
            },
            {
                "arxivId": "1908.09635",
                "title": "A Survey on Bias and Fairness in Machine Learning",
                "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2403.08678",
        "category": "q-fin",
        "title": "Path-dependency and leverage effect on capital return in periodic growth processes",
        "abstract": "Probability theory is applied to the finances of periodic growth processes. The expected value of the profit rate, on accrual basis, does not directly depend on divestments, neither on the capitalization path. The expected value of capitalization is path dependent. Because of the path-dependent capitalization, the return rate on capital is path-dependent, and the time-average return rate on capital differs from the expected value of the return rate on capital for the growth cycle. In the absence of intermediate divestments, the internal rate of return is path-independent, thereby differing from the expected value of the rate of return on capital. It is shown that the area-average of internal rate of return is not representative for the rate of return on capital within an estate. It is shown that the rotation cycle length maximizing the return rate on equity is independent of market interest rate. Correspondingly, from the viewpoint of wealth accumulation, the often-suggested dependency of suitable rotation length on discount rate appears to be a modeling artifact. Leverage effect enters the microeconomics of the growth processes through a separate leverage equation, where the leverage coefficient may reach positive or negative values. The leverage effect on the internal rate of return and the net present value are discussed. Both effects are solvable, resulting in incorrect estimates.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.05803",
        "category": "q-fin",
        "title": "Measuring Arbitrage Losses and Profitability of AMM Liquidity",
        "abstract": "This paper presents the results of a comprehensive empirical study of losses to arbitrageurs (following the formalization of loss-versus-rebalancing by [Milionis et al., 2022]) incurred by liquidity providers on automated market makers (AMMs). We show that those losses exceed the fees earned by liquidity providers across many of the largest AMM liquidity pools (on Uniswap). Remarkably, we also find that the Uniswap v2 pools are more profitable for passive LPs than their Uniswap v3 counterparts. We also investigate how arbitrage losses change with block times. As expected, arbitrage losses decrease when block production is faster. However, the rate of the decline varies significantly across different trading pairs. For instance, when comparing 100ms block times to Ethereum's current 12-second block times, the decrease in losses to arbitrageurs ranges between 20% to 70%, depending on the specific trading pair.",
        "references": [
            {
                "arxivId": "2307.02074",
                "title": "Arbitrageurs' profits, LVR, and sandwich attacks: batch trading as an AMM design response",
                "abstract": "We study a novel automated market maker design: the function maximizing AMM (FM-AMM). Our central assumption is that trades are batched before execution. Because of competition between arbitrageurs, the FM-AMM eliminates arbitrage profits (or LVR) and sandwich attacks, currently the two main problems in decentralized finance and blockchain design more broadly. We then consider 11 token pairs and use Binance price data to simulate the lower bound to the return of providing liquidity to an FM-AMM. Such a lower bound is, for the most part, slightly higher than the empirical returns of providing liquidity on Uniswap v3 (currently the dominant AMM)."
            },
            {
                "arxivId": "2305.14604",
                "title": "Automated Market Making and Arbitrage Profits in the Presence of Fees",
                "abstract": "We consider the impact of trading fees on the profits of arbitrageurs trading against an automated marker marker (AMM) or, equivalently, on the adverse selection incurred by liquidity providers due to arbitrage. We extend the model of Milionis et al. [2022] for a general class of two asset AMMs to both introduce fees and discrete Poisson block generation times. In our setting, we are able to compute the expected instantaneous rate of arbitrage profit in closed form. When the fees are low, in the fast block asymptotic regime, the impact of fees takes a particularly simple form: fees simply scale down arbitrage profits by the fraction of time that an arriving arbitrageur finds a profitable trade."
            },
            {
                "arxivId": "2208.06046",
                "title": "Automated Market Making and Loss-Versus-Rebalancing",
                "abstract": "Automated market making (AMM) protocols such as Uniswap have recently emerged as an alternative to the most common market structure for electronic trading, the limit order book. Relative to limit order books, AMMs are both more computationally efficient and do not require the participation of active market making intermediaries. As such, AMMs have emerged as the dominant market mechanism for trust-less decentralized exchanges (DEXs). We develop a model the underlying economics of AMMs from the perspective of their passive liquidity providers (LPs). Our central contribution is a\"Black-Scholes formula for AMMs\". Like the Black-Scholes formula, we consider the return to LPs once market risk has been hedged. We identify the main adverse selection cost incurred by LPs, which we call\"loss-versus-rebalancing\"(LVR, pronounced\"lever\"). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. In a continuous time Black-Scholes setting, we are able to derive closed-form expressions for this adverse selection cost, for all automated market makers, including constant function market makers and those featuring concentrated liquidity (e.g., Uniswap v3). Qualitatively, we highlight the main forces that drive AMM LP returns, including asset characteristics (volatility) and AMM characteristics (curvature/marginal liquidity). Quantitatively, we illustrate how our model's expressions for LP returns match actual LP returns for the Uniswap v2 WETH-USDC trading pair. Our model provides tradable insight into both the ex ante and ex post assessment of AMM LP investment decisions. LVR can also inform the design of the next generation of DEX market mechanisms -- in fact, in the short time since our work has been released,\"LVR mitigation\"has already emerged as the dominant challenge among practitioners in the AMM protocol designer community."
            },
            {
                "arxivId": "2205.08904",
                "title": "Risks and Returns of Uniswap V3 Liquidity Providers",
                "abstract": "Trade execution on Decentralized Exchanges (DEXes) is automatic and does not require individual buy and sell orders to be matched. Instead, liquidity aggregated in pools from individual liquidity providers enables trading between cryptocurrencies. The largest DEX measured by trading volume, Uniswap V3, promises a DEX design optimized for capital efficiency. However, Uniswap V3 requires far more decisions from liquidity providers than previous DEX designs. In this work, we develop a theoretical model to illustrate the choices faced by Uniswap V3 liquidity providers and their implications. Our model suggests that providing liquidity on Uniswap V3 is highly complex and requires many considerations from a user. Our supporting data analysis of the risks and returns of real Uniswap V3 liquidity providers underlines that liquidity providing in Uniswap V3 is incredibly complicated, and performances can vary wildly. While there are simple and profitable strategies for liquidity providers in liquidity pools characterized by negligible price volatilities, these strategies only yield modest returns. Instead, significant returns can only be obtained by accepting increased financial risks and at the cost of active management. Thus, providing liquidity has become a game reserved for sophisticated players with the introduction of Uniswap V3, where retail traders do not stand a chance."
            },
            {
                "arxivId": "2111.09192",
                "title": "Impermanent Loss in Uniswap v3",
                "abstract": "AMMs are autonomous smart contracts deployed on a blockchain that make markets between different assets that live on that chain. In this paper we are examining a specific class of AMMs called Constant Function Market Makers whose trading profile, ignoring fees, is determined by their bonding curve. This class of AMM suffers from what is commonly referred to as Impermanent Loss, which we have previously identified as the Gamma component of the associated self-financing trading strategy and which is the risk that LP providers wager against potential fee earnings. The recent Uniswap v3 release has popularized the concept of leveraged liquidity provision - wherein the trading range in which liquidity is provided is reduced and achieves a higher degree of capital efficiency through elimination of unused collateral. This leverage increases the fees earned, but it also increases the risk taken, ie the IL. Fee levels on Uniswap v3 are well publicized so, in this paper, we focus on calculating the IL. We found that for the 17 pools we analyzed, covering 43% of TVL and chosen by size, composite tokens and data availability, total fees earned since inception until the cut-off date was $199.3m. We also found that the total IL suffered by LPs during this period was USD 260.1m, meaning that in aggregate those LPs would have been better off by USD 60.8m had they simply HODLd."
            },
            {
                "arxivId": "2110.01368",
                "title": "Concentrated Liquidity in Automated Market Makers",
                "abstract": "We examine how the introduction of concentrated liquidity has changed the liquidity provision market in automated market makers such as Uniswap. To this end, we compare average liquidity provider returns from trading fees before and after its introduction. Furthermore, we quantify the performance of a number of fundamental concentrated liquidity strategies using historical trade data. We estimate their possible returns and evaluate which perform best for certain trading pairs and market conditions."
            },
            {
                "arxivId": "2105.13822",
                "title": "Behavior of Liquidity Providers in Decentralized Exchanges",
                "abstract": "Decentralized exchanges (DEXes) have introduced an innovative trading mechanism, where it is not necessary to match buy-orders and sell-orders to execute a trade. DEXes execute each trade individually, and the exchange rate is automatically determined by the ratio of assets reserved in the market. Therefore, apart from trading, financial players can also liquidity providers, benefiting from transaction fees from trades executed in DEXes. Although liquidity providers are essential for the functionality of DEXes, it is not clear how liquidity providers behave in such markets. In this paper, we aim to understand how liquidity providers react to market information and how they benefit from providing liquidity in DEXes. We measure the operations of liquidity providers on Uniswap and analyze how they determine their investment strategy based on market changes. We also reveal their returns and risks of investments in different trading pair categories, i.e., stable pairs, normal pairs, and exotic pairs. Further, we investigate the movement of liquidity between trading pools. To the best of our knowledge, this is the first work that systematically studies the behavior of liquidity providers in DEXes."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.12001",
        "category": "q-fin",
        "title": "Internet sentiment exacerbates intraday overtrading, evidence from A-Share market",
        "abstract": "Market fluctuations caused by overtrading are important components of systemic market risk. This study examines the effect of investor sentiment on intraday overtrading activities in the Chinese A-share market. Employing high-frequency sentiment indices inferred from social media posts on the Eastmoney forum Guba, the research focuses on constituents of the CSI 300 and CSI 500 indices over a period from 01/01/2018, to 12/30/2022. The empirical analysis indicates that investor sentiment exerts a significantly positive impact on intraday overtrading, with the influence being more pronounced among institutional investors relative to individual traders. Moreover, sentiment-driven overtrading is found to be more prevalent during bull markets as opposed to bear markets. Additionally, the effect of sentiment on overtrading is observed to be more pronounced among individual investors in large-cap stocks compared to small- and mid-cap stocks.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13163",
        "category": "q-fin",
        "title": "A national longitudinal dataset of skills taught in U.S. higher education curricula",
        "abstract": "Higher education plays a critical role in driving an innovative economy by equipping students with knowledge and skills demanded by the workforce. While researchers and practitioners have developed data systems to track detailed occupational skills, such as those established by the U.S. Department of Labor (DOL), much less effort has been made to document skill development in higher education at a similar granularity. Here, we fill this gap by presenting a longitudinal dataset of skills inferred from over three million course syllabi taught at nearly three thousand U.S. higher education institutions. To construct this dataset, we apply natural language processing to extract from course descriptions detailed workplace activities (DWAs) used by the DOL to describe occupations. We then aggregate these DWAs to create skill profiles for institutions and academic majors. Our dataset offers a large-scale representation of college-educated workers and their role in the economy. To showcase the utility of this dataset, we use it to 1) compare the similarity of skills taught and skills in the workforce according to the US Bureau of Labor Statistics, 2) estimate gender differences in acquired skills based on enrollment data, 3) depict temporal trends in the skills taught in social science curricula, and 4) connect college majors' skill distinctiveness to salary differences of graduates. Overall, this dataset can enable new research on the source of skills in the context of workforce development and provide actionable insights for shaping the future of higher education to meet evolving labor demands especially in the face of new technologies.",
        "references": [
            {
                "arxivId": "2303.10130",
                "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                "abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications."
            },
            {
                "arxivId": "2104.08727",
                "title": "GooAQ: Open Question Answering with Diverse Answer Types",
                "abstract": "While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a) in line with recent work, LM's strong performance on GooAQ's short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release GooAQ to facilitate further research on improving QA with diverse response types."
            },
            {
                "arxivId": "2102.07033",
                "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them",
                "abstract": "Abstract Open-domain Question Answering models that directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared with conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models fall short of the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5%, but trail RePAQ by over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) while retaining high accuracy. Lastly, we demonstrate RePAQ\u2019s strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to \u201cback-off\u201d to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone."
            },
            {
                "arxivId": "2003.07082",
                "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
                "abstract": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/."
            },
            {
                "arxivId": "1908.10084",
                "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
                "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
            },
            {
                "arxivId": "1705.05875",
                "title": "Small cities face greater impact from automation",
                "abstract": "The city has proved to be the most successful form of human agglomeration and provides wide employment opportunities for its dwellers. As advances in robotics and artificial intelligence revive concerns about the impact of automation on jobs, a question looms: how will automation affect employment in cities? Here, we provide a comparative picture of the impact of automation across US urban areas. Small cities will undertake greater adjustments, such as worker displacement and job content substitutions. We demonstrate that large cities exhibit increased occupational and skill specialization due to increased abundance of managerial and technical professions. These occupations are not easily automatable, and, thus, reduce the potential impact of automation in large cities. Our results pass several robustness checks including potential errors in the estimation of occupational automation and subsampling of occupations. Our study provides the first empirical law connecting two societal forces: urban agglomeration and automation's impact on employment."
            },
            {
                "arxivId": "1505.07907",
                "title": "Linking Economic Complexity, Institutions and Income Inequality",
                "abstract": null
            },
            {
                "arxivId": "1201.0490",
                "title": "Scikit-learn: Machine Learning in Python",
                "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            },
            {
                "arxivId": "1101.1707",
                "title": "The network structure of economic output",
                "abstract": null
            },
            {
                "arxivId": "0909.3890",
                "title": "The building blocks of economic complexity",
                "abstract": "For Adam Smith, wealth was related to the division of labor. As people and firms specialize in different activities, economic efficiency increases, suggesting that development is associated with an increase in the number of individual activities and with the complexity that emerges from the interactions between them. Here we develop a view of economic growth and development that gives a central role to the complexity of a country's economy by interpreting trade data as a bipartite network in which countries are connected to the products they export, and show that it is possible to quantify the complexity of a country's economy by characterizing the structure of this network. Furthermore, we show that the measures of complexity we derive are correlated with a country's level of income, and that deviations from this relationship are predictive of future growth. This suggests that countries tend to converge to the level of income dictated by the complexity of their productive structures, indicating that development efforts should focus on generating the conditions that would allow complexity to emerge to generate sustained growth and prosperity."
            },
            {
                "arxivId": "0803.0476",
                "title": "Fast unfolding of communities in large networks",
                "abstract": "We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13178",
        "category": "q-fin",
        "title": "The benefits and costs of agglomeration: insights from economics and complexity",
        "abstract": "There are many benefits and costs that come from people and firms clustering together in space. Agglomeration economies, in particular, are the manifestation of centripetal forces that make larger cities disproportionately more wealthy than smaller cities, pulling together individuals and firms in close physical proximity. Measuring agglomeration economies, however, is not easy, and the identification of its causes is still debated. Such association of productivity with size can arise from interactions that are facilitated by cities (\"positive externalities\"), but also from more productive individuals moving in and sorting into large cities (\"self-sorting\"). Under certain circumstances, even pure randomness can generate increasing returns to scale. In this chapter, we discuss some of the empirical observations, models, measurement challenges, and open question associated with the phenomenon of agglomeration economies. Furthermore, we discuss the implications of urban complexity theory, and in particular urban scaling, for the literature in agglomeration economies.",
        "references": [
            {
                "arxivId": "2111.08365",
                "title": "Mathematical models to explain the origin of urban scaling laws: a synthetic review",
                "abstract": "The quest for a theory of cities that could offer a quantitative and systematic approach to manage cities is at the top priority, given the challenges humanity faces due to the increasing urbanization and densification of cities. If such a theory is feasible, then its formulation must be in a mathematical way. As a contribution to organizing the mathematical ideas that deal with such a systematic way of understanding urban phenomena, we present this material, concentrating on one important aspect of what recently has been called the new science of cities. In this paper, we review the main mathematical models present in the literature that aim at explaining the origin and emergence of urban scaling. We intend to present the models, identify similarities and connections between them, and find situations in which different models lead to the same output. In addition, we report situations in which some ideas initially introduced in a particular model can also be introduced in another model, generating more diversification and increasing the scope of the models. The models treated in this paper explain urban scaling from different premises: from gravity ideas, passing through densification ideas and cites' geometry, to a hierarchical organization and socio-network properties. We also investigate scenarios in which these different fundamental ideas could be interpreted as similar -- where the similarity is likely but not obvious. Furthermore, in what concerns the gravity idea, we propose a general framework that includes all gravity models analyzed as a particular case."
            },
            {
                "arxivId": "2006.14140",
                "title": "Spatial interactions in urban scaling laws",
                "abstract": "Analyses of urban scaling laws assume that observations in different cities are independent of the existence of nearby cities. Here we introduce generative models and data-analysis methods that overcome this limitation by modelling explicitly the effect of interactions between individuals at different locations. Parameters that describe the scaling law and the spatial interactions are inferred from data simultaneously, allowing for rigorous (Bayesian) model comparison and overcoming the problem of defining the boundaries of urban regions. Results in five different datasets show that including spatial interactions typically leads to better models and a change in the exponent of the scaling law."
            },
            {
                "arxivId": "1910.07166",
                "title": "Evidence for localization and urbanization economies in urban scaling",
                "abstract": "We study the scaling of (i) numbers of workers and aggregate incomes by occupational categories against city size, and (ii) total incomes against numbers of workers in different occupations, across the functional metropolitan areas of Australia and the USA. The number of workers and aggregate incomes in specific high-income knowledge economy-related occupations and industries show increasing returns to scale by city size, showing that localization economies within particular industries account for superlinear effects. However, when total urban area incomes and/or gross domestic products are regressed using a generalized Cobb\u2013Douglas function against the number of workers in different occupations as labour inputs, constant returns to scale in productivity against city size are observed. This implies that the urbanization economies at the whole city level show linear scaling or constant returns to scale. Furthermore, industrial and occupational organizations, not population size, largely explain the observed productivity variable. The results show that some very specific industries and occupations contribute to the observed overall superlinearity. The findings suggest that it is not just size but also that it is the diversity of specific intra-city organization of economic and social activity and physical infrastructure that should be used to understand urban scaling behaviours."
            },
            {
                "arxivId": "1812.02842",
                "title": "Estimating the drivers of urban economic complexity and their connection to economic performance",
                "abstract": "Estimating the capabilities, or inputs of production, that drive and constrain the economic development of urban areas has remained a challenging goal. We posit that capabilities are instantiated in the complexity and sophistication of urban activities, the know-how of individual workers, and the city-wide collective know-how. We derive a model that indicates how the value of these three quantities can be inferred from the probability that an individual in a city is employed in a given urban activity. We illustrate how to estimate empirically these variables using data on employment across industries and metropolitan statistical areas in the USA. We then show how the functional form of the probability function derived from our theory is statistically superior when compared with competing alternative models, and that it explains well-known results in the urban scaling and economic complexity literature. Finally, we show how the quantities are associated with metrics of economic performance, suggesting our theory can provide testable implications for why some cities are more prosperous than others."
            },
            {
                "arxivId": "1802.00972",
                "title": "Unveiling relationships between crime and property in England and Wales via density scale-adjusted metrics and network tools",
                "abstract": "Scale-adjusted metrics (SAMs) are a significant achievement of the urban scaling hypothesis. SAMs remove the inherent biases of per capita measures computed in the absence of isometric allometries. However, this approach is limited to urban areas, while a large portion of the world\u2019s population still lives outside cities and rural areas dominate land use worldwide. Here, we extend the concept of SAMs to population density scale-adjusted metrics (DSAMs) to reveal relationships among different types of crime and property metrics. Our approach allows all human environments to be considered, avoids problems in the definition of urban areas, and accounts for the heterogeneity of population distributions within urban regions. By combining DSAMs, cross-correlation, and complex network analysis, we find that crime and property types have intricate and hierarchically organized relationships leading to some striking conclusions. Drugs and burglary had uncorrelated DSAMs and, to the extent property transaction values are indicators of affluence, twelve out of fourteen crime metrics showed no evidence of specifically targeting affluence. Burglary and robbery were the most connected in our network analysis and the modular structures suggest an alternative to \u201czero-tolerance\u201d policies by unveiling the crime and/or property types most likely to affect each other."
            },
            {
                "arxivId": "1604.07876",
                "title": "Explaining the prevalence, scaling and variance of urban phenomena",
                "abstract": null
            },
            {
                "arxivId": "1604.02872",
                "title": "Is this scaling nonlinear?",
                "abstract": "One of the most celebrated findings in complex systems in the last decade is that different indexes y (e.g. patents) scale nonlinearly with the population x of the cities in which they appear, i.e. y\u223cx\u03b2,\u03b2\u22601. More recently, the generality of this finding has been questioned in studies that used new databases and different definitions of city boundaries. In this paper, we investigate the existence of nonlinear scaling, using a probabilistic framework in which fluctuations are accounted for explicitly. In particular, we show that this allows not only to (i) estimate \u03b2 and confidence intervals, but also to (ii) quantify the evidence in favour of \u03b2\u22601 and (iii) test the hypothesis that the observations are compatible with the nonlinear scaling. We employ this framework to compare five different models to 15 different datasets and we find that the answers to points (i)\u2013(iii) crucially depend on the fluctuations contained in the data, on how they are modelled, and on the fact that the city sizes are heavy-tailed distributed."
            },
            {
                "arxivId": "1301.5919",
                "title": "The hypothesis of urban scaling: formalization, implications and challenges",
                "abstract": "There is strong expectation that cities, across time, culture and level of development, share much in common in terms of their form and function. Recently, attempts to formalize mathematically these expectations have led to the hypothesis of urban scaling, namely that certain properties of all cities change, on average, with their size in predictable scale-invariant ways. The emergence of these scaling relations depends on a few general properties of cities as social networks, co-located in space and time, that conceivably apply to a wide range of human settlements. Here, we discuss the present evidence for the hypothesis of urban scaling, some of the methodological issues dealing with proxy measurements and units of analysis and place these ndings in the context of other theories of cities and urban systems. We show that a large body of evidence about the scaling properties of cities indicates, in analogy to other complex systems, that they cannot be treated as extensive systems and discuss the consequences of these results for an emerging statistical theory of cities."
            },
            {
                "arxivId": "1301.1674",
                "title": "Constructing cities, deconstructing scaling laws",
                "abstract": "Cities can be characterized and modelled through different urban measures. Consistency within these observables is crucial in order to advance towards a science of cities. Bettencourt et al. have proposed that many of these urban measures can be predicted through universal scaling laws. We develop a framework to consistently define cities, using commuting to work and population density thresholds, and construct thousands of realizations of systems of cities with different boundaries for England and Wales. These serve as a laboratory for the scaling analysis of a large set of urban indicators. The analysis shows that population size alone does not provide us enough information to describe or predict the state of a city as previously proposed, indicating that the expected scaling laws are not corroborated. We found that most urban indicators scale linearly with city size, regardless of the definition of the urban boundaries. However, when nonlinear correlations are present, the exponent fluctuates considerably."
            },
            {
                "arxivId": "1105.5170",
                "title": "Modeling Users' Activity on Twitter Networks: Validation of Dunbar's Number",
                "abstract": "Microblogging and mobile devices appear to augment human social capabilities, which raises the question whether they remove cognitive or biological constraints on human communication. In this paper we analyze a dataset of Twitter conversations collected across six months involving 1.7 million individuals and test the theoretical cognitive limit on the number of stable social relationships known as Dunbar's number. We find that the data are in agreement with Dunbar's result; users can entertain a maximum of 100\u2013200 stable relationships. Thus, the \u2018economy of attention\u2019 is limited in the online world by cognitive and biological constraints as predicted by Dunbar's theory. We propose a simple model for users' behavior that includes finite priority queuing and time resources that reproduces the observed social behavior."
            },
            {
                "arxivId": "1102.4101",
                "title": "Scaling and Hierarchy in Urban Economies",
                "abstract": "In several recent publications, Bettencourt, West and collaborators claim that properties of cities such as gross economic production, personal income, numbers of patents filed, number of crimes committed, etc., show super-linear power-scaling with total population, while measures of resource use show sub-linear power-law scaling. Re-analysis of the gross economic production and personal income for cities in the United States, however, shows that the data cannot distinguish between power laws and other functional forms, including logarithmic growth, and that size predicts relatively little of the variation between cities. The striking appearance of scaling in previous work is largely artifact of using extensive quantities (city-wide totals) rather than intensive ones (per-capita rates). The remaining dependence of productivity on city size is explained by concentration of specialist service industries, with high value-added per worker, in larger cities, in accordance with the long-standing economic notion of the \"hierarchy of central places\"."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13189",
        "category": "q-fin",
        "title": "Use of two Public Distributed Ledgers to track the money of an economy",
        "abstract": "A tool to improve the effectiveness and the efficiency of public spending is proposed here. In the 19th century banknotes had a serial number. However, in modern days the use of digital transactions that do not use physical currency has opened the possibility to digitally track almost each cent of the economy. In this article a serial number or tracking number for each cent, pence or any other monetary unit of the economy is proposed. Then, almost all cents can be tracked by recording the transactions in a public distributed ledger, rather than recording the amount of the transaction, the information recorded in the block of the transaction is the actual serial number or tracking number for each cent that changes ownership. In order to keep the privacy of the transaction, only generic identification of private companies and individuals are recorded along with generic information about the concept of transaction, the region and the date/time. A secondary public distributed ledger whose blocks are identified by a hash reference that is recorded in the bank statement available to the payer and the payee allows for checking the accuracy of the first public distributed ledger by comparing the transactions made in one day, one region and one type of concept. However, the transactions made or received by the government are recorded with a much higher level of detail in the first ledger and a higher level of disclosure in the second ledger. The result is a tool that is able to accurately track public spending, to keep privacy of individuals and companies and to make statistical analysis and experiments or real tests in the economy of a country. This tool has the potential to assist public policymakers in demonstrating the societal benefits resulting from their policies, thereby enabling more informed decision-making for future policy endeavours.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13211",
        "category": "q-fin",
        "title": "Long-term forecasts of statewide travel demand patterns using large-scale mobile phone GPS data: A case study of Indiana",
        "abstract": "The growth in availability of large-scale GPS mobility data from mobile devices has the potential to aid traditional travel demand models (TDMs) such as the four-step planning model, but those processing methods are not commonly used in practice. In this study, we show the application of trip generation and trip distribution modeling using GPS data from smartphones in the state of Indiana. This involves extracting trip segments from the data and inferring the phone users' home locations, adjusting for data representativeness, and using a data-driven travel time-based cost function for the trip distribution model. The trip generation and interchange patterns in the state are modeled for 2025, 2035, and 2045. Employment sectors like industry and retail are observed to influence trip making behavior more than other sectors. The travel growth is predicted to be mostly concentrated in the suburban regions, with a small decline in the urban cores. Further, although the majority of the growth in trip flows over the years is expected to come from the corridors between the major urban centers of the state, relative interzonal trip flow growth will likely be uniformly spread throughout the state. We also validate our results with the forecasts of two travel demand models, finding a difference of 5-15% in overall trip counts. Our GPS data-based demand model will contribute towards augmenting the conventional statewide travel demand model developed by the state and regional planning agencies.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13291",
        "category": "q-fin",
        "title": "Liquidity Pool Design on Automated Market Makers",
        "abstract": "Automated market makers are a popular type of decentralized exchange in which users trade assets with each other directly and automatically through a liquidity pool and a fixed pricing function. The liquidity provider contributes to the liquidity pool by supplying assets to the pool and in return they earn transaction fees from traders who trade through the pool. We propose a model of optimal liquidity provision in which the risk-averse liquidity provider decides the investment proportion of wealth she would like to supply to the pool, trade in a centralized market, and consume in multiple periods. We derive the liquidity provider's optimal strategy by dynamic programming and numerically find the optimal liquidity pool that maximizes the liquidity provider's utility. Our findings indicate that the exchange rate volatility on the centralized market exerts a positive effect on the optimal transaction fee. Moreover, the optimal constant mean pricing formula is found to be related to the relative performance of the underlying assets on the centralized market.",
        "references": [
            {
                "arxivId": "2212.10035",
                "title": "Efficient Liquidity Providing via Margin Liquidity",
                "abstract": "The liquidity of the exchange is the most important factor when crytocurrency traders choose an exchange. However, the amount of liquidity provided by the liquidity providers in decentralized exchanges is insufficient when compared to centralized exchanges. This is because the liquidity providers in decentralized exchanges suffer from the risk of divergence loss inherent to the automated market making system. To this end, we introduce a new concept called margin liquidity and leverage this concept to propose a highly profitable margin liquidity-providing position. Then, we extend this margin liquidity-providing position to a virtual margin liquidity-providing position to alleviate the risk of divergence loss for the liquidity providers and encourage them to provide more liquidity to the pool. We show that our proposed margin liquidity is 8K times more capital efficient than the concentrated liquidity proposed in Uniswap V3."
            },
            {
                "arxivId": "2212.03340",
                "title": "Finding the Right Curve: Optimal Design of Constant Function Market Makers",
                "abstract": "Constant Function Market Makers (CFMMs) are a tool for creating exchange markets, have been deployed effectively in prediction markets, and are now especially prominent in the Decentralized Finance ecosystem. We show that for any set of beliefs about future asset prices, an optimal CFMM trading function exists that maximizes the fraction of trades that a CFMM can settle. We formulate a convex program to compute this optimal trading function. This program, therefore, gives a tractable framework for market-makers to compile their belief function on the future prices of the underlying assets into the trading function of a maximally capital-efficient CFMM. Our convex optimization framework further extends to capture the tradeoffs between fee revenue, arbitrage loss, and opportunity costs of liquidity providers. Analyzing the program shows how the consideration of profit and loss leads to a qualitatively different optimal trading function. Our model additionally explains the diversity of CFMM designs that appear in practice. We show that careful analysis of our convex program enables inference of a market-maker's beliefs about future asset prices, and show that these beliefs mirror the folklore intuition for several widely used CFMMs. Developing the program requires a new notion of the liquidity of a CFMM, and the core technical challenge is in the analysis of the KKT conditions of an optimization over an infinite-dimensional Banach space."
            },
            {
                "arxivId": "2206.04634",
                "title": "The Economics of Automated Market Makers",
                "abstract": "This paper studies the question whether automated market maker protocols such as Uniswap can sustainably retain a portion of their trading fees for the protocol. We approach the problem by modelling how to optimally choose a pool's take rate, i.e the fraction of fee revenue that remains with the protocol, in order to maximize the protocol's revenue. The model suggest that if AMMs have a portion of loyal trade volume, they can sustainably set a non-zero take rate, even without losing liquidity to competitors with a zero take rate. Furthermore, we determine the optimal take rate depending on a number of model parameters including how much loyal trade volume pools have and how high the competitors' take rates are."
            },
            {
                "arxivId": "2204.00464",
                "title": "Differential Liquidity Provision in Uniswap v3 and Implications for Contract Design\u2731",
                "abstract": "Decentralized exchanges (DEXs) provide a means for users to trade pairs of assets on-chain without the need of a trusted third party to effectuate a trade. Amongst these, constant function market maker (CFMM) DEXs such as Uniswap handle the most volume of trades between ERC-20 tokens. With the introduction of Uniswap v3, liquidity providers are given the option to differentially allocate liquidity to be used for trades that occur within specific price intervals. In this paper, we formalize the profit and loss that liquidity providers can earn when providing specific liquidity positions to a contract. With this in hand, we are able to compute optimal liquidity allocations for liquidity providers who hold beliefs over how prices evolve over time. Ultimately, we use this tool to shed light on the design question regarding how v3 contracts should partition price space for permissible liquidity allocations. Our results show that a richer space of potential partitions can simultaneously benefit both liquidity providers and traders."
            },
            {
                "arxivId": "2105.13510",
                "title": "A Note on Optimal Fees for Constant Function Market Makers",
                "abstract": "We suggest a framework to determine optimal trading fees for constant function market makers (CFMMs) in order to maximize liquidity provider returns. In a setting of multiple competing liquidity pools, we show that no race to the bottom occurs, but instead pure Nash equilibria of optimal fees exist. We theoretically prove the existence of these equilibria for pools using the constant product trade function used in popular CFMMs like Uniswap. We also numerically compute the equilibria for a number of examples and discuss the effects the equilibrium fees have on capital allocation among pools. Finally, we use our framework to compute optimal fees for real-world pools using past trade data."
            },
            {
                "arxivId": "2104.00446",
                "title": "Optimal Fees for Geometric Mean Market Makers",
                "abstract": null
            },
            {
                "arxivId": "2103.12732",
                "title": "SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM) Protocols",
                "abstract": "As an integral part of the decentralized finance (DeFi) ecosystem, decentralized exchanges (DEXs) with automated market maker (AMM) protocols have gained massive traction with the recently revived interest in blockchain and distributed ledger technology (DLT) in general. Instead of matching the buy and sell sides, automated market makers (AMMs) employ a peer-to-pool method and determine asset price algorithmically through a so-called conservation function. To facilitate the improvement and development of AMM-based decentralized exchanges (DEXs), we create the first systematization of knowledge in this area. We first establish a general AMM framework describing the economics and formalizing the system\u2019s state-space representation. We then employ our framework to systematically compare the top AMM protocols\u2019 mechanics, illustrating their conservation functions, as well as slippage and divergence loss functions. We further discuss security and privacy concerns, how they are enabled by AMM-based DEXs\u2019 inherent properties, and explore mitigating solutions. Finally, we conduct a comprehensive literature review on related work covering both DeFi and conventional market microstructure."
            },
            {
                "arxivId": "2105.02782",
                "title": "The Homogenous Properties of Automated Market Makers",
                "abstract": "Automated market makers (AMM) have grown to obtain significant market share within the cryptocurrency ecosystem, resulting in a proliferation of new products pursuing exotic strategies for horizontal differentiation. Yet, their theoretical properties are curiously homogeneous when a set of basic assumptions are met. In this paper, we start by presenting a universal approach to deriving a formula for liquidity provisioning for AMMs. Next, we show that the constant function market maker and token swap market maker models are theoretically equivalent when liquidity reserves are uniform. Proceeding with an examination of AMM market microstructure, we show how the non-linear price effect translates into slippage for traders and impermanent losses for liquidity providers. We proceed by showing how impermanent losses are a function of both volatility and market depth and discuss the implications of these findings within the context of the literature."
            },
            {
                "arxivId": "2003.10001",
                "title": "Improved Price Oracles: Constant Function Market Makers",
                "abstract": "Automated market makers, first popularized by Hanson's logarithmic market scoring rule (or LMSR) for prediction markets, have become important building blocks, called 'primitives,' for decentralized finance. A particularly useful primitive is the ability to measure the price of an asset, a problem often known as the pricing oracle problem. In this paper, we focus on the analysis of a very large class of automated market makers, called constant function market makers (or CFMMs) which includes existing popular market makers such as Uniswap, Balancer, and Curve, whose yearly transaction volume totals to billions of dollars. We give sufficient conditions such that, under fairly general assumptions, agents who interact with these constant function market makers are incentivized to correctly report the price of an asset and that they can do so in a computationally efficient way. We also derive several other useful properties that were previously not known. These include lower bounds on the total value of assets held by CFMMs and lower bounds guaranteeing that no agent can, by any set of trades, drain the reserves of assets held by a given CFMM."
            },
            {
                "arxivId": "1805.05077",
                "title": "Discrete Dividend Payments in Continuous Time",
                "abstract": "We propose a model in which dividend payments occur at regular, deterministic intervals in an otherwise continuous model. This contrasts traditional models where either the payment of continuous dividends is controlled or the dynamics are given by discrete time processes. Moreover, between two dividend payments, the structure allows for other types of control; we consider the possibility of equity issuance at any point in time. The value is characterized as the fixed point of an optimal control problem with periodic initial and terminal conditions. We prove the regularity and uniqueness of the corresponding dynamic programming equation and the convergence of an efficient numerical algorithm that we use to study the problem. The model enables us to find the loss caused by infrequent dividend payments. We show that under realistic parameter values, this loss varies from around 1%\u201324% depending on the state of the system and that using the optimal policy from the continuous problem further increases the loss."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13371",
        "category": "q-fin",
        "title": "On Risk-Sensitive Decision Making Under Uncertainty",
        "abstract": "This paper studies a risk-sensitive decision-making problem under uncertainty. It considers a decision-making process that unfolds over a fixed number of stages, in which a decision-maker chooses among multiple alternatives, some of which are deterministic and others are stochastic. The decision-maker's cumulative value is updated at each stage, reflecting the outcomes of the chosen alternatives. After formulating this as a stochastic control problem, we delineate the necessary optimality conditions for it. Two illustrative examples from optimal betting and inventory management are provided to support our theory.",
        "references": [
            {
                "arxivId": "2303.10417",
                "title": "On the Benefit of Nonlinear Control for Robust Logarithmic Growth: Coin Flipping Games as a Demonstration Case",
                "abstract": "The takeoff point for this letter is the voluminous body of literature addressing recursive betting games with expected logarithmic growth of wealth being the performance criterion. Whereas almost all existing papers involve use of linear feedback, the use of nonlinear control is conspicuously absent. This is epitomized by the large subset of this literature dealing with Kelly Betting. With this as the high-level motivation, we study the potential for use of nonlinear control in this framework. To this end, we consider a \u201cdemonstration case\u201d which is one of the simplest scenarios encountered in this line of research: repeated flips of a biased coin with probability of heads <inline-formula> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula>, and even-money payoff on each flip. First, we formulate a new robust nonlinear control problem which we believe is both simple to understand and apropos for dealing with concerns about distributional robustness; i.e., instead of assuming that p is perfectly known as in the case of the classical Kelly formulation, we begin with a bounding set <inline-formula> <tex-math notation=\"LaTeX\">${\\mathcal{ P}} \\subseteq [0{,}1]$ </tex-math></inline-formula> for this probability. Then, we provide a theorem, our main result, which gives a closed-form description of the optimal robust nonlinear controller and a corollary which establishes that it robustly outperforms linear controllers such as those found in the literature. A second, less significant, contribution of this letter bears upon the computability of our solution. For an n-flip game, whereas an admissible controller has <inline-formula> <tex-math notation=\"LaTeX\">$2^{\\mathrm{ n}}-1$ </tex-math></inline-formula> parameters, at the optimum only O(<inline-formula> <tex-math notation=\"LaTeX\">$\\rm n^{2}$ </tex-math></inline-formula>) of them turn out to be distinct. Finally, it is noted that the initial assumptions on payoffs and the use of the uniform distribution on <inline-formula> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula> are made mainly for simplicity of the exposition and compliance with length requirements for a Letter. Accordingly, this letter also includes a new Section with a discussion indicating how these assumptions can be relaxed."
            },
            {
                "arxivId": "2301.02754",
                "title": "On Frequency-Based Optimal Portfolio with Transaction Costs",
                "abstract": "R.O.C"
            },
            {
                "arxivId": "1610.02581",
                "title": "Variance-based Regularization with Convex Objectives",
                "abstract": "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13637",
        "category": "q-fin",
        "title": "Extremal cases of distortion risk measures with partial information",
        "abstract": "This paper considers the best- and worst-case of a general class of distortion risk measures when only partial information regarding the underlying distributions is available. Specifically, explicit sharp lower and upper bounds for a general class of distortion risk measures are derived based on the first two moments along with some shape information, such as symmetry/unimodality property of the underlying distributions. The proposed approach provides a unified framework for extremal problems of distortion risk measures.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13754",
        "category": "q-fin",
        "title": "Dispensing with optimal control: a new approach for the pricing and management of share buyback contracts",
        "abstract": "This paper introduces a novel methodology for the pricing and management of share buyback contracts, overcoming the limitations of traditional optimal control methods, which frequently encounter difficulties with high-dimensional state spaces and the intricacies of selecting appropriate risk penalty or risk aversion parameter. Our methodology applies optimized heuristic strategies to maximize the contract's value. The computation of this value utilizes classical methods typically used for pricing path-dependent Bermudan options. Additionally, our approach naturally leads to the formulation of a hedging strategy.",
        "references": [
            {
                "arxivId": "1312.5617",
                "title": "Accelerated Share Repurchase: pricing and execution strategy",
                "abstract": "In this article, we consider a specific optimal execution problem associated to accelerated share repurchase contracts. When firms want to repurchase their own shares, they often enter such a contract with a bank. The bank buys the shares for the firm and is paid the average market price over the execution period, the length of the period being decided upon by the bank during the buying process. Mathematically, the problem is new and related to both option pricing (Asian and Bermudan options) and optimal execution. We provide a model, along with associated numerical methods, to determine the optimal stopping time and the optimal buying strategy of the bank."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13768",
        "category": "q-fin",
        "title": "The Economics of Blockchain Governance: Evaluate Liquid Democracy on the Internet Computer",
        "abstract": "Decentralized Autonomous Organizations (DAOs), utilizing blockchain technology to enable collective governance, are a promising innovation. This research addresses the ongoing query in blockchain governance: How can DAOs optimize human cooperation? Focusing on the Network Nervous System (NNS), a comprehensive on-chain governance framework underpinned by the Internet Computer Protocol (ICP) and liquid democracy principles, we employ theoretical abstraction and simulations to evaluate its potential impact on cooperation and economic growth within DAOs. Our findings emphasize the significance of the NNS's staking mechanism, particularly the reward multiplier, in aligning individual short-term interests with the DAO's long-term prosperity. This study contributes to the understanding and effective design of blockchain-based governance systems.",
        "references": [
            {
                "arxivId": "2402.11170",
                "title": "Analyzing Reward Dynamics and Decentralization in Ethereum 2.0: An Advanced Data Engineering Workflow and Comprehensive Datasets for Proof-of-Stake Incentives",
                "abstract": "Ethereum 2.0, as the preeminent smart contract blockchain platform, guarantees the precise execution of applications without third-party intervention. At its core, this system leverages the Proof-of-Stake (PoS) consensus mechanism, which utilizes a stochastic process to select validators for block proposal and validation, consequently rewarding them for their contributions. However, the implementation of blockchain technology often diverges from its central tenet of decentralized consensus, presenting significant analytical challenges. Our study collects consensus reward data from the Ethereum Beacon chain and conducts a comprehensive analysis of reward distribution and evolution, categorizing them into attestation, proposer and sync committee rewards. To evaluate the degree of decentralization in PoS Ethereum, we apply several inequality indices, including the Shannon entropy, the Gini Index, the Nakamoto Coefficient, and the Herfindahl-Hirschman Index (HHI). Our comprehensive dataset is publicly available on Harvard Dataverse, and our analytical methodologies are accessible via GitHub, promoting open-access research. Additionally, we provide insights on utilizing our data for future investigations focused on assessing, augmenting, and refining the decentralization, security, and efficiency of blockchain systems."
            },
            {
                "arxivId": "2311.03530",
                "title": "DAO Decentralization: Voting-Bloc Entropy, Bribery, and Dark DAOs",
                "abstract": "Decentralized Autonomous Organizations (DAOs) use smart contracts to foster communities working toward common goals. Existing definitions of decentralization, however-the 'D' in DAO-fall short of capturing key properties characteristic of diverse and equitable participation. We propose a new metric called Voting-Bloc Entropy (VBE, pronounced ''vibe'') that formalizes a broad notion of decentralization in voting on DAO proposals. VBE measures the similarity of participants' utility functions across a set of proposals. We use VBE to prove a number of results about the decentralizing effects of vote delegation, proposal bundling, bribery, and quadratic voting. Our results lead to practical suggestions for enhancing DAO decentralization. One of our results highlights the risk of systemic bribery with increasing DAO decentralization. To show that this threat is realistic, we present the first practical realization of a Dark DAO, a proposed mechanism for privacy-preserving corruption of identity systems, including those used in DAO voting. Our Dark-DAO prototype uses trusted execution environments (TEEs) in the Oasis Sapphire blockchain for attacks on Ethereum DAOs. It demonstrates that Dark DAOs constitute a realistic future concern for DAO governance."
            },
            {
                "arxivId": "2311.14676",
                "title": "Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities",
                "abstract": "Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance standing as a critical determinant for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), wield a substantial impact on blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, a gap persists in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects -- Aave, Uniswap, Curve Dao, Aragon, Yearn.finance, Merit Circle, and Balancer -- placing a primary focus on discussions related to governance issues. Despite differing activity patterns, participants across these decentralized communities consistently express positive sentiments in their Discord discussions, indicating optimism towards governance decisions. Additionally, our research suggests a potential interplay between discussion intensity and sentiment dynamics, indicating that higher discussion volumes may contribute to more stable and positive emotions. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, with a specific emphasis on the decentralized blockchain governance ecosystem."
            },
            {
                "arxivId": "2212.06951",
                "title": "AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security",
                "abstract": "Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with #MEV and #Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on blockchain and social media platforms. Our study contributes to the literature at the interface of blockchain security, MEV solutions, and AI ethics."
            },
            {
                "arxivId": "2212.05632",
                "title": "Blockchain Network Analysis: A Comparative Study of Decentralized Banks",
                "abstract": "Decentralized finance (DeFi) is known for its unique mechanism design, which applies smart contracts to facilitate peer-to-peer transactions. The decentralized bank is a typical DeFi application. Ideally, a decentralized bank should be decentralized in the transaction. However, many recent studies have found that decentralized banks have not achieved a significant degree of decentralization. This research conducts a comparative study among mainstream decentralized banks. We apply core-periphery network features analysis using the transaction data from four decentralized banks, Liquity, Aave, MakerDao, and Compound. We extract six features and compare the banks' levels of decentralization cross-sectionally. According to the analysis results, we find that: 1) MakerDao and Compound are more decentralized in the transactions than Aave and Liquity. 2) Although decentralized banking transactions are supposed to be decentralized, the data show that four banks have primary external transaction core addresses such as Huobi, Coinbase, and Binance, etc. We also discuss four design features that might affect network decentralization. Our research contributes to the literature at the interface of decentralized finance, financial technology (Fintech), and social network analysis and inspires future protocol designs to live up to the promise of decentralized finance for a truly peer-to-peer transaction network."
            },
            {
                "arxivId": "1805.04548",
                "title": "DFINITY Technology Overview Series, Consensus System",
                "abstract": "The DFINITY blockchain computer provides a secure, performant and flexible consensus mechanism. At its core, DFINITY contains a decentralized randomness beacon which acts as a verifiable random function (VRF) that produces a stream of outputs over time. The novel technique behind the beacon relies on the existence of a unique-deterministic, non-interactive, DKG-friendly threshold signatures scheme. The only known examples of such a scheme are pairing-based and derived from BLS. \nThe DFINITY blockchain is layered on top of the DFINITY beacon and uses the beacon as its source of randomness for leader selection and leader ranking. A \"weight\" is attributed to a chain based on the ranks of the leaders who propose the blocks in the chain, and that weight is used to select between competing chains. The DFINITY blockchain is layered on top of the DFINITY beacon and uses the beacon as its source of randomness for leader selection and leader ranking blockchain is further hardened by a notarization process which dramatically improves the time to finality and eliminates the nothing-at-stake and selfish mining attacks. \nDFINITY consensus algorithm is made to scale through continuous quorum selections driven by the random beacon. In practice, DFINITY achieves block times of a few seconds and transaction finality after only two confirmations. The system gracefully handles temporary losses of network synchrony including network splits, while it is provably secure under synchrony."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13818",
        "category": "q-fin",
        "title": "Joint Liability Model with Adaptation to Climate Change",
        "abstract": "This paper extends the application of ESG score assessment methodologies from large corporations to individual farmers' production, within the context of climate change. Our proposal involves the integration of crucial agricultural sustainability variables into conventional personal credit evaluation frameworks, culminating in the formulation of a holistic sustainable credit rating referred to as the Environmental, Social, Economics (ESE) score. This ESE score is integrated into theoretical joint liability models, to gain valuable insights into optimal group sizes and individual-ESE score relationships. Additionally, we adopt a mean-variance utility function for farmers to effectively capture the risk associated with anticipated profits. Through a set of simulation exercises, the paper investigates the implications of incorporating ESE scores into credit evaluation systems, offering a nuanced comprehension of the repercussions under various climatic conditions.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13869",
        "category": "q-fin",
        "title": "A separate way to measure rate of return",
        "abstract": "Net profit is sometimes found from data for net operating surplus. We propose a way to find it from data for consumption, pay and market-value capital, and concomitantly to reveal the factor shares in consumption.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.13986",
        "category": "q-fin",
        "title": "Stochastic Volatility in Mean: Efficient Analysis by a Generalized Mixture Sampler",
        "abstract": "In this paper we consider the simulation-based Bayesian analysis of stochastic volatility in mean (SVM) models. Extending the highly efficient Markov chain Monte Carlo mixture sampler for the SV model proposed in Kim et al. (1998) and Omori et al. (2007), we develop an accurate approximation of the non-central chi-squared distribution as a mixture of thirty normal distributions. Under this mixture representation, we sample the parameters and latent volatilities in one block. We also detail a correction of the small approximation error by using additional Metropolis-Hastings steps. The proposed method is extended to the SVM model with leverage. The methodology and models are applied to excess holding yields in empirical studies, and the SVM model with leverage is shown to outperform competing volatility models based on marginal likelihoods.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14041",
        "category": "q-fin",
        "title": "Natural Capital as a Stock Option",
        "abstract": "The unfolding climate crisis is a physical manifestation of the damage that market economy, driven by the high intensity consumption of fossil fuels, has inflicted on the Earth System and on the stability conditions that were established by a complex conjugation of natural factors during the Holoecene. The magnitude of the human activities and its predatory nature is such that it is no longer possible to consider the Earth System and the services it provides for the habitability of the planet, the so-called natural capital, as an economical externality. Thus one is left with two main choices in what concerns the sustaintability of the planet's habitability: radical economic degrowth or highly efficient solutions to internalise the maintenance and the restoration of ecosystems and the services of the Earth System. It is proposed that an interesting strategy for the latter is to consider the natural capital as a stock option.",
        "references": [
            {
                "arxivId": "2401.07829",
                "title": "Cooling the Earth with $CO_2$ filled containers in space",
                "abstract": "We argue that geostationary (GEO) reflective containers filled with $CO_2$ could be used as shading devices to selectively cool areas on Earth's surface. This proposal would be an interesting addition to the recently discussed suggestion of dumping $CO_2$ to space through the well of a space lift \\cite{OB2023}. We also explore the possibility of producing propellants in GEO out of greenhouse gases expelled from the space lift. Finally, we discuss the much less effective idea of filtering the most prominent infrared bands of the incoming solar radiation using the $CO_2$ wrapped in transparent vessels."
            },
            {
                "arxivId": "2204.08955",
                "title": "Chaotic Behaviour of the Earth System in the Anthropocene",
                "abstract": "It is shown that the Earth System (ES) can, due to the impact of human activities, behave in a chaotic fashion. Our arguments are based on the assumption that the ES can be described by a Landau-Ginzburg model, which on its own allows for predicting that the ES evolves, through regular trajectories in the phase space, towards a Hothouse Earth scenario for a \ufb01nite amount of human-driven impact. Furthermore, we \ufb01nd that the equilibrium point for temperature \ufb02uctuations can exhibit bifurcations and a chaotic pattern if the human impact follows a logistic map."
            },
            {
                "arxivId": "2112.11404",
                "title": "Towards a Classification Scheme for the Rocky Planets based on Equilibrium Thermodynamic Considerations",
                "abstract": "\n A classification scheme for rocky planets is proposed, based on a description of the Earth System in terms of the Landau-Ginzburg Theory of phase transitions. Three major equilibrium states can be identified and the associated planetary states or phases are: Earth-like Holocene state; hot Venus-like state; cold Mars-like state. The scheme is based on an approach proposed to understand the Earth transition from the Holocene to the Anthropocene, driven by the impact of the human action on the Earth System. In the present work we identity the natural conditions that cause transformations on the planets forcing them into one of the states identified above. We discuss how the parameters that describe these transformations can be related with exoplanets observables. In analysing the relevant physical parameters, we were stroke by the similarities between Earth and Venus, and how likely is that the Anthropocene transition may lead to hot-house Earth scenario."
            },
            {
                "arxivId": "1907.10535",
                "title": "Towards a physically motivated planetary accounting framework",
                "abstract": "In this work, we present a physically motivated Planetary Accounting Framework for the Earth System. We show that the impact of the human activity in terms of the planetary boundary variables can be accounted for in our Landau\u2013Ginzburg phase transition physical formulation. We then use the interaction between climate change and ocean acidification mechanisms to exemplify the relation of the concentration and flux of substances of the planetary boundary variables, as proposed by the accounting framework of Meyer and Newman, with the underlying thermodynamic transformation, quantifiable by the Landau\u2013Ginzburg inspired model."
            },
            {
                "arxivId": "1811.05543",
                "title": "A phase space description of the Earth System in the Anthropocene",
                "abstract": "Based on a dynamic systems approach to the Landau-Ginzburg model, a phase space description of the Earth System (ES) in the transition to the Anthropocene is presented. It is shown that, for a finite amount of human-driven change, there is a stable equilibrium state that is an attractor of trajectories in the system's phase space and corresponds to a Hothouse Earth scenario. Using the interaction between the components of the ES, it is argued that, through the action of the Technosphere, mitigation strategies might arise for which the deviation of the ES temperature from the Holocene average temperature is smaller."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14115",
        "category": "q-fin",
        "title": "Pricing of European Calls with the Quantum Fourier Transform",
        "abstract": "The accurate valuation of financial derivatives plays a pivotal role in the finance industry. Although closed formulas for pricing are available for certain models and option types, exemplified by the European Call and Put options in the Black-Scholes Model, the use of either more complex models or more sophisticated options precludes the existence of such formulas, thereby requiring alternative approaches. The Monte Carlo simulation, an alternative approach effective in nearly all scenarios, has already been challenged by quantum computing techniques that leverage Amplitude Estimation. Despite its theoretical promise, this approach currently faces limitations due to the constraints of hardware in the Noisy Intermediate-Scale Quantum (NISQ) era. In this study, we introduce and analyze a quantum algorithm for pricing European call options across a broad spectrum of asset models. This method transforms a classical approach, which utilizes the Fast Fourier Transform (FFT), into a quantum algorithm, leveraging the efficiency of the Quantum Fourier Transform (QFT). Furthermore, we compare this novel algorithm with existing quantum algorithms for option pricing.",
        "references": [
            {
                "arxivId": "2304.08793",
                "title": "Quantum Architecture Search for Quantum Monte Carlo Integration via Conditional Parameterized Circuits with Application to Finance",
                "abstract": "Classical Monte Carlo algorithms can theoretically be sped up on a quantum computer by employing amplitude estimation (AE). To realize this, an efficient implementation of state-dependent functions is crucial. We develop a straightforward approach based on pretraining parameterized quantum circuits, and show how they can be transformed into their conditional variant, making them usable as a subroutine in an AE algorithm. To identify a suitable circuit, we propose a genetic optimization approach that combines variable ansatzes and data encoding. We apply our algorithm to the problem of pricing financial derivatives. At the expense of a costly pretraining process, this results in a quantum circuit implementing the derivatives' payoff function more efficiently than previously existing quantum algorithms. In particular, we compare the performance for European vanilla and basket options."
            },
            {
                "arxivId": "1905.02666",
                "title": "Option Pricing using Quantum Computers",
                "abstract": "We present a methodology to price options and portfolios of options on a gate-based quantum computer using amplitude estimation, an algorithm which provides a quadratic speedup compared to classical Monte Carlo methods. The options that we cover include vanilla options, multi-asset options and path-dependent options such as barrier options. We put an emphasis on the implementation of the quantum circuits required to build the input states and operators needed by amplitude estimation to price the different option types. Additionally, we show simulation results to highlight how the circuits that we implement price the different option contracts. Finally, we examine the performance of option pricing circuits on quantum hardware using the IBM Q Tokyo quantum device. We employ a simple, yet effective, error mitigation scheme that allows us to significantly reduce the errors arising from noisy two-qubit gates."
            },
            {
                "arxivId": "quant-ph/0005055",
                "title": "Quantum Amplitude Amplification and Estimation",
                "abstract": "Consider a Boolean function $\\chi: X \\to \\{0,1\\}$ that partitions set $X$ between its good and bad elements, where $x$ is good if $\\chi(x)=1$ and bad otherwise. Consider also a quantum algorithm $\\mathcal A$ such that $A |0\\rangle= \\sum_{x\\in X} \\alpha_x |x\\rangle$ is a quantum superposition of the elements of $X$, and let $a$ denote the probability that a good element is produced if $A |0\\rangle$ is measured. If we repeat the process of running $A$, measuring the output, and using $\\chi$ to check the validity of the result, we shall expect to repeat $1/a$ times on the average before a solution is found. *Amplitude amplification* is a process that allows to find a good $x$ after an expected number of applications of $A$ and its inverse which is proportional to $1/\\sqrt{a}$, assuming algorithm $A$ makes no measurements. This is a generalization of Grover's searching algorithm in which $A$ was restricted to producing an equal superposition of all members of $X$ and we had a promise that a single $x$ existed such that $\\chi(x)=1$. Our algorithm works whether or not the value of $a$ is known ahead of time. In case the value of $a$ is known, we can find a good $x$ after a number of applications of $A$ and its inverse which is proportional to $1/\\sqrt{a}$ even in the worst case. We show that this quadratic speedup can also be obtained for a large family of search problems for which good classical heuristics exist. Finally, as our main result, we combine ideas from Grover's and Shor's quantum algorithms to perform amplitude estimation, a process that allows to estimate the value of $a$. We apply amplitude estimation to the problem of *approximate counting*, in which we wish to estimate the number of $x\\in X$ such that $\\chi(x)=1$. We obtain optimal quantum algorithms in a variety of settings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14136",
        "category": "q-fin",
        "title": "Elicitability and identifiability of tail risk measures",
        "abstract": "Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk and Expected Shortfall being prime examples. They are induced by law-based risk measures, called their generators, evaluated on the tail distribution. This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively. As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile. The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile. For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures.",
        "references": [
            {
                "arxivId": "2208.08108",
                "title": "Characterizing M-estimators",
                "abstract": "\n We characterize the full classes of M-estimators for semiparametric models of general functionals by formally connecting the theory of consistent loss functions from forecast evaluation with the theory of M-estimation. This novel characterization result allows us to leverage existing results on loss functions known from the literature on forecast evaluation in estimation theory. We exemplify advantageous implications for the fields of robust, efficient, equivariant and Pareto-optimal M-estimation."
            },
            {
                "arxivId": "1312.1645",
                "title": "What is the Best Risk Measure in Practice? A Comparison of Standard Measures",
                "abstract": "Expected Shortfall (ES) has been widely accepted as a risk measure that is conceptually superior to Value-at-Risk (VaR). At the same time, however, it has been criticized for issues relating to backtesting. In particular, ES has been found not to be elicitable which means that backtesting for ES is less straight-forward than, e.g., backtesting for VaR. Expectiles have been suggested as potentially better alternatives to both ES and VaR. In this paper, we revisit commonly accepted desirable properties of risk measures like coherence, comonotonic additivity, robustness and elicitability. We check VaR, ES and Expectiles with regard to whether or not they enjoy these properties, with particular emphasis on Expectiles. We also consider their impact on capital allocation, an important issue in risk management. We find that, despite the caveats that apply to the estimation and backtesting of ES, it can be considered a good risk measure. In particular, there is no sufficient evidence to justify an all-inclusive replacement of ES by Expectiles in applications, especially as we provide an alternative way for backtesting of ES."
            },
            {
                "arxivId": "0912.0902",
                "title": "Making and Evaluating Point Forecasts",
                "abstract": "Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14137",
        "category": "q-fin",
        "title": "An Asymmetric Capital Asset Pricing Model",
        "abstract": "Providing a measure of market risk is an important issue for investors and financial institutions. However, the existing models for this purpose are per definition symmetric. The current paper introduces an asymmetric capital asset pricing model for measurement of the market risk. It explicitly accounts for the fact that falling prices determine the risk for a long position in the risky asset and the rising prices govern the risk for a short position. Thus, a position dependent market risk measure that is provided accords better with reality. The empirical application reveals that Apple stock is more volatile than the market only for the short seller. Surprisingly, the investor that has a long position in this stock is facing a lower volatility than the market. This property is not captured by the standard asset pricing model, which has important implications for the expected returns and hedging designs.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14141",
        "category": "q-fin",
        "title": "Competition and Collaboration in Crowdsourcing Communities: What happens when peers evaluate each other?",
        "abstract": "Crowdsourcing has evolved as an organizational approach to distributed problem solving and innovation. As contests are embedded in online communities and evaluation rights are assigned to the crowd, community members face a tension: they find themselves exposed to both competitive motives to win the contest prize and collaborative participation motives in the community. The competitive motive suggests they may evaluate rivals strategically according to their self-interest, the collaborative motive suggests they may evaluate their peers truthfully according to mutual interest. Using field data from Threadless on 38 million peer evaluations of more than 150,000 submissions across 75,000 individuals over 10 years and two natural experiments to rule out alternative explanations, we answer the question of how community members resolve this tension. We show that as their skill level increases, they become increasingly competitive and shift from using self-promotion to sabotaging their closest competitors. However, we also find signs of collaborative behavior when high-skilled members show leniency toward those community members who do not directly threaten their chance of winning. We explain how the individual-level use of strategic evaluations translates into important organizational-level outcomes by affecting the community structure through individuals' long-term participation. While low-skill targets of sabotage are less likely to participate in future contests, high-skill targets are more likely. This suggests a feedback loop between competitive evaluation behavior and future participation. These findings have important implications for the literature on crowdsourcing design, and the evolution and sustainability of crowdsourcing communities.",
        "references": [
            {
                "arxivId": "2104.08636",
                "title": "Avoiding the bullies: The resilience of cooperation among unequals",
                "abstract": "Can egalitarian norms or conventions survive the presence of dominant individuals who are ensured of victory in conflicts? We investigate the interaction of power asymmetry and partner choice in games of conflict over a contested resource. Previous models of cooperation do not include both power inequality and partner choice. Furthermore, models that do include power inequalities assume a static game where a bully\u2019s advantage does not change. They have therefore not attempted to model complex and realistic properties of social interaction. Here, we introduce three models to study the emergence and resilience of cooperation among unequals when interaction is random, when individuals can choose their partners, and where power asymmetries dynamically depend on accumulated payoffs. We find that the ability to avoid bullies with higher competitive ability afforded by partner choice mostly restores cooperative conventions and that the competitive hierarchy never forms. Partner choice counteracts the hyper dominance of bullies who are isolated in the network and eliminates the need for others to coordinate in a coalition. When competitive ability dynamically depends on cumulative payoffs, complex cycles of coupled network-strategy-rank changes emerge. Effective collaborators gain popularity (and thus power), adopt aggressive behavior, get isolated, and ultimately lose power. Neither the network nor behavior converge to a stable equilibrium. Despite the instability of power dynamics, the cooperative convention in the population remains stable overall and long-term inequality is completely eliminated. The interaction between partner choice and dynamic power asymmetry is crucial for these results: without partner choice, bullies cannot be isolated, and without dynamic power asymmetry, bullies do not lose their power even when isolated. We analytically identify a single critical point that marks a phase transition in all three iterations of our models. This critical point is where the first individual breaks from the convention and cycles start to emerge."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14252",
        "category": "q-fin",
        "title": "On a fundamental statistical edge principle",
        "abstract": "This paper establishes that conditioning the probability of execution of new orders on the self-generated historical trading information (HTI) of a trading strategy is a necessary condition for a statistical trading edge. It is shown, in particular, that, given any trading strategy S that does not use its own HTI, it is always possible to construct a new strategy S* that yields a systematically increasing improvement over S in terms of profit and loss (PnL) by using the self-generated HTI. This holds true under rather general conditions that are frequently met in practice, and it is proven through a decision mechanism specifically designed to formally prove this idea. Simulations and real-world trading evidence are included for validation and illustration, respectively.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14302",
        "category": "q-fin",
        "title": "A Global Minimum Tax for Large Firms Only: Implications for Tax Competition",
        "abstract": "The Global Minimum Tax (GMT) is applied only to firms above a certain size threshold. We set up a simple model of tax competition and profit shifting by heterogeneous multinational firms to evaluate the effects of this partial coverage of the GMT. A non-haven and a haven country are bound by the GMT rate for large multinationals, but can set tax rates for firms below the threshold non-cooperatively. We show that the introduction of the GMT with a moderate tax rate increases tax revenues in both the non-haven and the haven countries. Gradual increases in the GMT rate, however, trigger a sudden change in the tax competition equilibrium from a uniform to a split corporate tax rate, at which tax revenues in the non-haven country decline. In contrast, gradual increases in the coverage of the GMT never harm the non-haven country. We also discuss the quantitative effects of introducing a $15\\%$ GMT rate in a calibrated version of our model.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-23.json",
        "arxivId": "2404.14337",
        "category": "q-fin",
        "title": "Statistical Validation of Contagion Centrality in Financial Networks",
        "abstract": "In this paper, we introduce a novel centrality measure to evaluate shock propagation on financial networks capturing a notion of contagion and systemic risk contributions. In comparison to many popular centrality metrics (e.g., eigenvector centrality) which provide only a relative centrality between nodes, our proposed measure is in an absolute scale permitting comparisons of contagion risk over time. In addition, we provide a statistical validation method when the network is estimated from data, as is done in practice. This statistical test allows us to reliably assess the computed centrality values. We validate our methodology on simulated data and conduct empirical case studies using financial data. We find that our proposed centrality measure increases significantly during times of financial distress and is able to provide insights in to the (market implied) risk-levels of different firms and sectors.",
        "references": [
            {
                "arxivId": "1512.04460",
                "title": "Distress Propagation in Complex Networks: The Case of Non-Linear DebtRank",
                "abstract": "We consider a dynamical model of distress propagation on complex networks, which we apply to the study of financial contagion in networks of banks connected to each other by direct exposures. The model that we consider is an extension of the DebtRank algorithm, recently introduced in the literature. The mechanics of distress propagation is very simple: When a bank suffers a loss, distress propagates to its creditors, who in turn suffer losses, and so on. The original DebtRank assumes that losses are propagated linearly between connected banks. Here we relax this assumption and introduce a one-parameter family of non-linear propagation functions. As a case study, we apply this algorithm to a data-set of 183 European banks, and we study how the stability of the system depends on the non-linearity parameter under different stress-test scenarios. We find that the system is characterized by a transition between a regime where small shocks can be amplified and a regime where shocks do not propagate, and that the overall stability of the system increases between 2008 and 2013."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-24.json",
        "arxivId": "2203.09612",
        "category": "q-fin",
        "title": "Risk-Averse Markov Decision Processes through a Distributional Lens",
        "abstract": "By adopting a distributional viewpoint on law-invariant convex risk measures, we construct dynamics risk measures (DRMs) at the distributional level. We then apply these DRMs to investigate Markov decision processes, incorporating latent costs, random actions, and weakly continuous transition kernels. Furthermore, the proposed DRMs allow risk aversion to change dynamically. Under mild assumptions, we derive a dynamic programming principle and show the existence of an optimal policy in both finite and infinite time horizons. Moreover, we provide a sufficient condition for the optimality of deterministic actions. For illustration, we conclude the paper with examples from optimal liquidation with limit order books and autonomous driving.",
        "references": [
            {
                "arxivId": "2201.07656",
                "title": "Consistency of MLE for partially observed diffusions, with application in market microstructure modeling",
                "abstract": "This paper presents a tractable sufficient condition for the consistency of maximum likelihood estimators (MLEs) in partially observed diffusion models, stated in terms of stationary distribution of the associated fully observed diffusion, under the assumption that the set of unknown parameter values is finite. This sufficient condition is then verified in the context of a latent price model of market microstructure, yielding consistency of maximum likelihood estimators of the unknown parameters in this model. Finally, we compute the latter estimators using historical financial data taken from the NASDAQ exchange."
            },
            {
                "arxivId": "2112.13414",
                "title": "Reinforcement Learning with Dynamic Convex Risk Measures",
                "abstract": "We develop an approach for solving time-consistent risk-sensitive stochastic optimization problems using model-free reinforcement learning (RL). Specifically, we assume agents assess the risk of a sequence of random variables using dynamic convex risk measures. We employ a time-consistent dynamic programming principle to determine the value of a particular policy, and develop policy gradient update rules that aid in obtaining optimal policies. We further develop an actor-critic style algorithm using neural networks to optimize over policies. Finally, we demonstrate the performance and flexibility of our approach by applying it to three optimization problems: statistical arbitrage trading strategies, financial hedging, and obstacle avoidance robot control."
            },
            {
                "arxivId": "2012.04521",
                "title": "Minimizing spectral risk measures applied to Markov decision processes",
                "abstract": null
            },
            {
                "arxivId": "2010.07220",
                "title": "Markov decision processes with recursive risk measures",
                "abstract": null
            },
            {
                "arxivId": "1906.05113",
                "title": "A Survey of Autonomous Driving: Common Practices and Emerging Technologies",
                "abstract": "Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development."
            },
            {
                "arxivId": "1807.02300",
                "title": "Risk forms: representation, disintegration, and application to partially observable two-stage systems",
                "abstract": null
            },
            {
                "arxivId": "1710.11040",
                "title": "How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics",
                "abstract": null
            },
            {
                "arxivId": "1701.01327",
                "title": "Optimal Liquidation in a Level-I Limit Order Book for Large-Tick Stocks",
                "abstract": "We propose a framework to study the optimal liquidation strategy in a limit order book for large-tick stocks, with spread equal to one tick. All order book events (market orders, limit orders and cancellations) occur according to independent Poisson processes, with parameters depending on price move directions. Our goal is to maximise the expected terminal wealth of an agent who needs to liquidate her positions within a fixed time horizon. Assuming that the agent trades (through sell limit order or/and sell market order) only when the price moves, we model her liquidation procedure as a semi-Markov decision process, and compute the semi-Markov kernel using Laplace method in the language of queueing theory. The optimal liquidation policy is then solved by dynamic programming, and illustrated numerically."
            },
            {
                "arxivId": "1603.09030",
                "title": "A survey of time consistency of dynamic risk measures and dynamic performance measures in discrete time: LM-measure perspective",
                "abstract": null
            },
            {
                "arxivId": "1506.02188",
                "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach",
                "abstract": "In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach."
            },
            {
                "arxivId": "1105.3115",
                "title": "Dealing with the inventory risk: a solution to the market making problem",
                "abstract": null
            },
            {
                "arxivId": "1105.0247",
                "title": "LIQUIDATION IN LIMIT ORDER BOOKS WITH CONTROLLED INTENSITY",
                "abstract": "We consider a framework for solving optimal liquidation problems in limit order books. In particular, order arrivals are modeled as a point process whose intensity depends on the liquidation price. We set up a stochastic control problem in which the goal is to maximize the expected revenue from liquidating the entire position held. We solve this optimal liquidation problem for power\u2010law and exponential\u2010decay order book models explicitly and discuss several extensions. We also consider the continuous selling (or fluid) limit when the trading units are ever smaller and the intensity is ever larger. This limit provides an analytical approximation to the value function and the optimal solution. Using techniques from viscosity solutions we show that the discrete state problem and its optimal solution converge to the corresponding quantities in the continuous selling limit uniformly on compacts."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-24.json",
        "arxivId": "2212.09715",
        "category": "q-fin",
        "title": "Liquid Democracy. Two Experiments on Delegation in Voting",
        "abstract": "Proponents of participatory democracy praise Liquid Democracy: decisions are taken by referendum, but voters delegate their votes freely. When better informed voters are present, delegation can increase the probability of a correct decision. However, delegation must be used sparely because it reduces the information aggregated through voting. In two different experiments, we find that delegation underperforms both universal majority voting and the simpler option of abstention. In a tightly controlled lab experiment where the subjects' precision of information is conveyed in precise mathematical terms and very salient, the result is due to overdelegation. In a perceptual task run online where the precision of information is not known precisely, delegation remains very high and again underperforms both majority voting and abstention. In addition, subjects substantially overestimate the precision of the better informed voters, underlining that Liquid Democracy is fragile to multiple sources of noise. The paper makes an innovative methodological contribution by combining two very different experimental procedures: the study of voting rules would benefit from complementing controlled experiments with known precision of information with tests under ambiguity, a realistic assumption in many voting situations.",
        "references": [
            {
                "arxivId": "2306.03960",
                "title": "Information Aggregation with Delegation of Votes",
                "abstract": "Recent developments in blockchain technology have made possible greater progress on secure electronic voting, opening the way to better ways of democratic decision making. In this paper we formalise the features of ``liquid democracy'' which allows voters to delegate their votes to other voters, and we explore whether it improves information aggregation as compared to direct voting. We consider a two-alternative setup with truth-seeking voters (informed and uninformed) and partisan ones (leftists and rightists), and we show that delegation improves information aggregation in finite elections. We also propose a mechanism that further improves the information aggregation properties of delegation in private information settings, by guaranteeing that all vote transfers are from uninformed to informed truth-seeking voters. Delegation offers effective ways for truth-seeking uninformed voters to boost the vote-share of the alternative that matches the state of the world in all considered setups and hence deserves policy makers' attention."
            },
            {
                "arxivId": "1808.01906",
                "title": "The Fluid Mechanics of Liquid Democracy",
                "abstract": "Liquid democracy is the principle of making collective decisions by letting agents transitively delegate their votes. Despite its significant appeal, it has become apparent that a weakness of liquid democracy is that a small subset of agents may gain massive influence. To address this, we propose to change the current practice by allowing agents to specify multiple delegation options instead of just one. Much like in nature, where\u2014fluid mechanics teaches us\u2014liquid maintains an equal level in connected vessels, we seek to control the flow of votes in a way that balances influence as much as possible. Specifically, we analyze the problem of choosing delegations to approximately minimize the maximum number of votes entrusted to any agent by drawing connections to the literature on confluent flow. We also introduce a random graph model for liquid democracy and use it to demonstrate the benefits of our approach both theoretically and empirically."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-24.json",
        "arxivId": "2306.07731",
        "category": "q-fin",
        "title": "A Comparative Study of Factor Models for Different Periods of the Electricity Spot Price Market",
        "abstract": "Due to major shifts in European energy supply, a structural change can be observed in Austrian electricity spot price data starting from the second quarter of the year 2021 onward. In this work we study the performance of two different factor models for the electricity spot price in three different time periods. To this end, we consider three samples of EEX data for the Austrian base load electricity spot price, one from the pre-crises from 2018 to 2021, the second from the time of the crisis from 2021 to 2023 and the whole data from 2018 to 2023. For each of these samples, we investigate the fit of a classical 3-factor model with a Gaussian base signal and one positive and one negative jump signal and compare it with a 4-factor model to assess the effect of adding a second Gaussian base signal to the model. For the calibration of the models we develop a tailor-made Markov Chain Monte Carlo method based on Gibbs sampling. To evaluate the model adequacy, we provide simulations of the spot price as well as a posterior predictive check for the 3- and the 4-factor model. We find that the 4-factor model outperforms the 3-factor model in times of non-crises. In times of crises, the second Gaussian base signal does not lead to a better fit of the model. To the best of our knowledge, this is the first study regarding stochastic electricity spot price models in this new market environment. Hence, it serves as a solid base for future research.",
        "references": [
            {
                "arxivId": "1601.02900",
                "title": "Bayesian Calibration and Number of Jump Components in Electricity Spot Price Models",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-24.json",
        "arxivId": "2404.15079",
        "category": "q-fin",
        "title": "Cooperation, Correlation and Competition in Ergodic $N$-player Games and Mean-field Games of Singular Controls: A Case Study",
        "abstract": "We consider ergodic symmetric $N$-player and mean-field games of singular control in both cooperative and competitive settings. The state process dynamics of a representative player follow geometric Brownian motion, controlled additively through a nondecreasing process. Agents aim to maximize a long-time average reward functional with instantaneous profit of power type. The game shows strategic complementarities, in that the marginal profit function is increasing with respect to the dynamic average of the states of the other players, when $N<\\infty$, or with respect to the stationary mean of the players' distribution, in the mean-field case. In the mean-field formulation, we explicitly construct the solution to the mean-field control problem associated with central planner optimization, as well as Nash and coarse correlated equilibria (with singular and regular recommendations). Among our findings, we show that coarse correlated equilibria may exist even when Nash equilibria do not. Additionally, we show that a coarse correlated equilibrium with a regular (absolutely continuous) recommendation can outperform a Nash equilibrium where the equilibrium policy is of reflecting type (thus singularly continuous). Furthermore, we prove that the constructed mean-field control and mean-field equilibria can approximate the cooperative and competitive equilibria, respectively, in the corresponding game with $N$ players when $N$ is sufficiently large. To the best of our knowledge, this paper is the first to characterize coarse correlated equilibria, construct the explicit solution to an ergodic mean-field control problem, and provide approximation results for the related $N$-player game in the context of singular control games.",
        "references": [
            {
                "arxivId": "2404.07945",
                "title": "Existence of Optimal Stationary Singular Controls and Mean Field Game Equilibria",
                "abstract": "In this paper, we examine the stationary relaxed singular control problem within a multi-dimensional framework for a single agent, as well as its Mean Field Game (MFG) equivalent. We demonstrate that optimal relaxed controls exist for both maximization and minimization cases. These relaxed controls are defined by random measures across the state and control spaces, with the state process described as a solution to the associated martingale problem. By leveraging findings from [Kurtz-Stockbridge 2001], we establish the equivalence between the martingale problem and the stationary forward equation. This allows us to reformulate the relaxed control problem into a linear programming problem within the measure space. We prove the sequential compactness of these measures, thereby confirming the feasibility of achieving an optimal solution. Subsequently, our focus shifts to Mean Field Games. Drawing on insights from the single-agent problem and employing Kakutani--Glicksberg--Fan fixed point theorem, we derive the existence of a mean field game equilibria."
            },
            {
                "arxivId": "2402.09317",
                "title": "Extended mean-field games with multi-dimensional singular controls and non-linear jump impact",
                "abstract": "We establish a probabilistic framework for analysing extended mean-field games with multi-dimensional singular controls and state-dependent jump dynamics and costs. Two key challenges arise when analysing such games: the state dynamics may not depend continuously on the control and the reward function may not be u.s.c.~Both problems can be overcome by restricting the set of admissible singular controls to controls that can be approximated by continuous ones. We prove that the corresponding set of admissible weak controls is given by the weak solutions to a Marcus-type SDE and provide an explicit characterisation of the reward function. The reward function will in general only be u.s.c.~To address the lack of continuity we introduce a novel class of MFGs with a broader set of admissible controls, called MFGs of parametrisations. Parametrisations are laws of state/control processes that continuously interpolate jumps. We prove that the reward functional is continuous on the set of parametrisations, establish the existence of equilibria in MFGs of parametrisations, and show that the set of Nash equilibria in MFGs of parametrisations and in the underlying MFG with singular controls coincide. This shows that MFGs of parametrisations provide a canonical framework for analysing MFGs with singular controls and non-linear jump impact."
            },
            {
                "arxivId": "2401.17034",
                "title": "Multiple equilibria in mean-field game models for large oligopolies with strategic complementarities",
                "abstract": "We consider continuous-time mean-field stochastic games with strategic complementarities. The interaction between the representative productive firm and the population of rivals comes through the price at which the produced good is sold and the intensity of interaction is measured by a so-called\"strenght parameter\"$\\xi$. Via lattice-theoretic arguments we first prove existence of equilibria and provide comparative statics results when varying $\\xi$. A careful numerical study based on iterative schemes converging to suitable maximal and minimal equilibria allows then to study in relevant financial examples how the emergence of multiple equilibria is related to the strenght of the strategic interaction."
            },
            {
                "arxivId": "2311.04162",
                "title": "Coarse correlated equilibria in linear quadratic mean field games and application to an emission abatement game",
                "abstract": "Coarse correlated equilibria (CCE) are a good alternative to Nash equilibria (NE), as they arise more naturally as outcomes of learning algorithms and they may exhibit higher payoffs than NE. CCEs include a device which allows players' strategies to be correlated without any cooperation, only through information sent by a mediator. We develop a methodology to concretely compute mean field CCEs in a linear-quadratic mean field game framework. We compare their performance to mean field control solutions and mean field NE (usually named MFG solutions). Our approach is implemented in the mean field version of an emission abatement game between greenhouse gas emitters. In particular, we exhibit a simple and tractable class of mean field CCEs which allows to outperform very significantly the mean field NE payoff and abatement levels, bridging the gap between the mean field NE and the social optimum obtained by mean field control."
            },
            {
                "arxivId": "2303.16728",
                "title": "Coarse correlated equilibria for continuous time mean field games in open loop strategies",
                "abstract": "In the framework of continuous time symmetric stochastic differential games in open loop strategies, we introduce a generalization of mean field game solution, called coarse correlated solution. This can be seen as the analogue of a coarse correlated equilibrium in the $N$-player game, where a moderator randomly generates a strategy profile and asks the players to pre-commit to such strategies before disclosing them privately to each one of them; such a profile is a coarse correlated equilibrium if no player has an incentive to unilaterally deviate. We justify our definition by showing that a coarse correlated solution for the mean field game induces a sequence of approximate coarse correlated equilibria with vanishing error for the underlying $N$-player games. Existence of coarse correlated solutions for the mean field game is proved by means of a minimax theorem. An example with explicit solutions is discussed as well."
            },
            {
                "arxivId": "2212.12413",
                "title": "Strong solutions to submodular mean field games with common noise and related McKean-Vlasov FBSDEs",
                "abstract": "This paper studies multidimensional mean field games with common noise and the related system of McKean-Vlasov forward-backward stochastic differential equations deriving from the stochastic maximum principle. We first propose some structural conditions which are related to the submodularity of the underlying mean field game and are a sort of opposite version of the well known Lasry-Lions monotonicity. By reformulating the representative player minimization problem via the stochastic maximum principle, the submodularity conditions allow to prove comparison principles for the forward-backward system, which correspond to the monotonicity of the best reply map. Building on this property, existence of strong solutions is shown via Tarski's fixed point theorem, both for the mean field game and for the related McKean-Vlasov forward-backward system. In both cases, the set of solutions enjoys a lattice structure, with minimal and maximal solutions which can be constructed by iterating the best reply map or via the fictitious play algorithm."
            },
            {
                "arxivId": "1907.10968",
                "title": "Submodular mean field games: Existence and approximation of solutions",
                "abstract": "We study mean field games with scalar Ito-type dynamics and costs that are submodular with respect to a suitable order relation on the state and measure space. The submodularity assumption has a number of interesting consequences. Firstly, it allows us to prove existence of solutions via an application of Tarski's fixed point theorem, covering cases with discontinuous dependence on the measure variable. Secondly, it ensures that the set of solutions enjoys a lattice structure: in particular, there exist a minimal and a maximal solution. Thirdly, it guarantees that those two solutions can be obtained through a simple learning procedure based on the iterations of the best-response-map. The mean field game is first defined over ordinary stochastic controls, then extended to relaxed controls. Our approach allows also to treat a class of submodular mean field games with common noise in which the representative player at equilibrium interacts with the (conditional) mean of its state's distribution."
            },
            {
                "arxivId": "1606.03709",
                "title": "Mean Field Games of Timing and Models for Bank Runs",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2302.08987",
        "category": "q-fin",
        "title": "Firm-level supply chains to minimize unemployment and economic losses in rapid decarbonization scenarios",
        "abstract": null,
        "references": [
            {
                "arxivId": "2302.11451",
                "title": "Estimating the loss of economic predictability from aggregating firm-level production networks",
                "abstract": "Abstract To estimate the reaction of economies to political interventions or external disturbances, input\u2013output (IO) tables\u2014constructed by aggregating data into industrial sectors\u2014are extensively used. However, economic growth, robustness, and resilience crucially depend on the detailed structure of nonaggregated firm-level production networks (FPNs). Due to nonavailability of data, little is known about how much aggregated sector-based and detailed firm-level-based model predictions differ. Using a nearly complete nationwide FPN, containing 243,399 Hungarian firms with 1,104,141 supplier\u2013buyer relations, we self-consistently compare production losses on the aggregated industry-level production network (IPN) and the granular FPN. For this, we model the propagation of shocks of the same size on both, the IPN and FPN, where the latter captures relevant heterogeneities within industries. In a COVID-19 inspired scenario, we model the shock based on detailed firm-level data during the early pandemic. We find that using IPNs instead of FPNs leads to an underestimation of economic losses of up to 37%, demonstrating a natural limitation of industry-level IO models in predicting economic outcomes. We ascribe the large discrepancy to the significant heterogeneity of firms within industries: we find that firms within one sector only sell 23.5% to and buy 19.3% from the same industries on average, emphasizing the strong limitations of industrial sectors for representing the firms they include. Similar error levels are expected when estimating economic growth, CO2 emissions, and the impact of policy interventions with industry-level IO models. Granular data are key for reasonable predictions of dynamical economic systems."
            },
            {
                "arxivId": "2104.07260",
                "title": "Quantifying firm-level economic systemic risk from nation-wide supply networks",
                "abstract": null
            },
            {
                "arxivId": "1801.10515",
                "title": "Systemic risk-efficient asset allocations: Minimization of systemic risk as a network optimization problem",
                "abstract": null
            },
            {
                "arxivId": "1707.04870",
                "title": "Environmental impact assessment for climate change policy with the simulation-based integrated assessment model E3ME-FTT-GENIE",
                "abstract": null
            },
            {
                "arxivId": "1401.8026",
                "title": "Elimination of systemic risk in financial networks by means of a systemic risk transaction tax",
                "abstract": "Financial markets are exposed to systemic risk (SR), the risk that a major fraction of the system ceases to function, and collapses. It has recently become possible to quantify SR in terms of underlying financial networks where nodes represent financial institutions, and links capture the size and maturity of assets (loans), liabilities and other obligations, such as derivatives. We demonstrate that it is possible to quantify the share of SR that individual liabilities within a financial network contribute to the overall SR. We use empirical data of nationwide interbank liabilities to show that the marginal contribution to overall SR of liabilities for a given size varies by a factor of a thousand. We propose a tax on individual transactions that is proportional to their marginal contribution to overall SR. If a transaction does not increase SR, it is tax-free. With an agent-based model (ABM) (CRISIS macro-financial model), we demonstrate that the proposed \u2018Systemic Risk Tax\u2019 (SRT) leads to a self-organized restructuring of financial networks that are practically free of SR. The SRT can be seen as an insurance for the public against costs arising from cascading failure. ABM predictions are shown to be in remarkable agreement with the empirical data and can be used to understand the relation of credit risk and SR."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2403.16525",
        "category": "q-fin",
        "title": "Measuring Name Concentrations through Deep Learning",
        "abstract": "We propose a new deep learning approach for the quantification of name concentration risk in loan portfolios. Our approach is tailored for small portfolios and allows for both an actuarial as well as a mark-to-market definition of loss. The training of our neural network relies on Monte Carlo simulations with importance sampling which we explicitly formulate for the CreditRisk${+}$ and the ratings-based CreditMetrics model. Numerical results based on simulated as well as real data demonstrate the accuracy of our new approach and its superior performance compared to existing analytical methods for assessing name concentration risk in small and concentrated portfolios.",
        "references": [
            {
                "arxivId": "2311.13802",
                "title": "On the Relevance and Appropriateness of Name Concentration Risk Adjustments for Portfolios of Multilateral Development Banks",
                "abstract": "Sovereign loan portfolios of Multilateral Development Banks (MDBs) typically consist of only a small number of borrowers and hence are heavily exposed to single name concentration risk. Based on realistic MDB portfolios constructed from publicly available data, this paper quantifies the magnitude of the exposure to name concentration risk using exact Monte Carlo simulations. In comparing the exact adjustment for name concentration risk to its analytic approximation as currently applied by the major rating agency Standard&Poor's, we further investigate whether current capital adequacy frameworks for MDBs are overly conservative. Finally, we discuss the choice of appropriate model parameters and their impact on measures of name concentration risk."
            },
            {
                "arxivId": "2106.10024",
                "title": "Robust deep hedging",
                "abstract": "We study pricing and hedging under parameter uncertainty for a class of Markov processes which we call generalized affine processes and which includes the Black\u2013Scholes model as well as the constant elasticity of variance (CEV) model as special cases. Based on a general dynamic programming principle, we are able to link the associated nonlinear expectation to a variational form of the Kolmogorov equation which opens the door for fast numerical pricing in the robust framework. The main novelty of the paper is that we propose a deep hedging approach which efficiently solves the hedging problem under parameter uncertainty. We numerically evaluate this method on simulated and real data and show that the robust deep hedging outperforms existing hedging approaches in highly volatile periods."
            },
            {
                "arxivId": "cond-mat/0302402",
                "title": "Calculating credit risk capital charges with the one-factor model",
                "abstract": "Even in the simple one-factor credit portfolio model that underlies the Basel II regulatory capital rules coming into force in 2007, the exact contributions to credit value-at-risk can only be calculated with Monte-Carlo simulation or with approximation algorithms that often involve numerical integration. As this may require a lot of computational time, there is a need for approximate analytical formulae. In this note, we develop formulae according to two different approaches: the granularity adjustment approach initiated by M. Gordy and T. Wilde, and a semi-asymptotic approach. The application of the formulae is illustrated with a numerical example. Keywords: One-factor model, capital charge, granularity adjustment, quantile derivative."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2404.08136",
        "category": "q-fin",
        "title": "Exponentially Weighted Moving Models",
        "abstract": "An exponentially weighted moving model (EWMM) for a vector time series fits a new data model each time period, based on an exponentially fading loss function on past observed data. The well known and widely used exponentially weighted moving average (EWMA) is a special case that estimates the mean using a square loss function. For quadratic loss functions EWMMs can be fit using a simple recursion that updates the parameters of a quadratic function. For other loss functions, the entire past history must be stored, and the fitting problem grows in size as time increases. We propose a general method for computing an approximation of EWMM, which requires storing only a window of a fixed number of past samples, and uses an additional quadratic term to approximate the loss associated with the data before the window. This approximate EWMM relies on convex optimization, and solves problems that do not grow with time. We compare the estimates produced by our approximation with the estimates from the exact EWMM method.",
        "references": [
            {
                "arxivId": "1603.00943",
                "title": "CVXPY: A Python-Embedded Modeling Language for Convex Optimization",
                "abstract": "CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at http://www.cvxpy.org/ under the GPL license, along with documentation and examples."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2404.13964",
        "category": "q-fin",
        "title": "An Economic Solution to Copyright Challenges of Generative AI",
        "abstract": "Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media. There is growing concern that such systems may infringe on the copyright interests of training data contributors. To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content. The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics. This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance. Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training. Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners.",
        "references": [
            {
                "arxivId": "2302.04222",
                "title": "GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models",
                "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after\"fine-tuning\"on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply\"style cloaks\"to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%)."
            },
            {
                "arxivId": "2206.10013",
                "title": "Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments",
                "abstract": "We develop a new, principled algorithm for estimating the contribution of training data points to the behavior of a deep learning model, such as a specific prediction it makes. Our algorithm estimates the AME, a quantity that measures the expected (average) marginal effect of adding a data point to a subset of the training data, sampled from a given distribution. When subsets are sampled from the uniform distribution, the AME reduces to the well-known Shapley value. Our approach is inspired by causal inference and randomized experiments: we sample different subsets of the training data to train multiple submodels, and evaluate each submodel's behavior. We then use a LASSO regression to jointly estimate the AME of each data point, based on the subset compositions. Under sparsity assumptions ($k \\ll N$ datapoints have large AME), our estimator requires only $O(k\\log N)$ randomized submodel trainings, improving upon the best prior Shapley value estimators."
            },
            {
                "arxivId": "2205.15466",
                "title": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
                "abstract": "Data valuation has wide use cases in machine learning, including improving data quality and creating economic incentives for data sharing. This paper studies the robustness of data valuation to noisy model performance scores. Particularly, we find that the inherent randomness of the widely used stochastic gradient descent can cause existing data value notions (e.g., the Shapley value and the Leave-one-out error) to produce inconsistent data value rankings across different runs. To address this challenge, we introduce the concept of safety margin, which measures the robustness of a data value notion. We show that the Banzhaf value, a famous value notion that originated from cooperative game theory literature, achieves the largest safety margin among all semivalues (a class of value notions that satisfy crucial properties entailed by ML applications and include the famous Shapley value and Leave-one-out error). We propose an algorithm to efficiently estimate the Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation demonstrates that the Banzhaf value outperforms the existing semivalue-based data value notions on several ML tasks such as learning with weighted samples and noisy label detection. Overall, our study suggests that when the underlying ML algorithm is stochastic, the Banzhaf value is a promising alternative to the other semivalue-based data value schemes given its computational advantage and ability to robustly differentiate data quality."
            },
            {
                "arxivId": "2112.10752",
                "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
            },
            {
                "arxivId": "2107.06336",
                "title": "Improving Cooperative Game Theory-based Data Valuation via Data Utility Learning",
                "abstract": "The Shapley value (SV) and Least core (LC) are classic methods in cooperative game theory for cost/profit sharing problems. Both methods have recently been proposed as a principled solution for data valuation tasks, i.e., quantifying the contribution of individual datum in machine learning. However, both SV and LC suffer computational challenges due to the need for retraining models on combinatorially many data subsets. In this work, we propose to boost the efficiency in computing Shapley value or Least core by learning to estimate the performance of a learning algorithm on unseen data combinations. Theoretically, we derive bounds relating the error in the predicted learning performance to the approximation error in SV and LC. Empirically, we show that the proposed method can significantly improve the accuracy of SV and LC estimation."
            },
            {
                "arxivId": "2010.12082",
                "title": "A Multilinear Sampling Algorithm to Estimate Shapley Values",
                "abstract": "Shapley values are great analytical tools in game theory to measure the importance of a player in a game. Due to their axiomatic and desirable properties such as efficiency, they have become popular for feature importance analysis in data science and machine learning. However, the time complexity to compute Shapley values based on the original formula is exponential, and as the number of features increases, this becomes infeasible. Castro et al. [1] developed a sampling algorithm, to estimate Shapley values. In this work, we propose a new sampling method based on a multilinear extension technique as applied in game theory. The aim is to provide a more efficient (sampling) method for estimating Shapley values. Our method is applicable to any machine learning model, in particular for either multiclass classifications or regression problems. We apply the method to estimate Shapley values for multilayer perceptrons (MLPs) and through experimentation on two datasets, we demonstrate that our method provides more accurate estimations of the Shapley values by reducing the variance of the sampling statistics."
            },
            {
                "arxivId": "1902.10275",
                "title": "Towards Efficient Data Valuation Based on the Shapley Value",
                "abstract": "{\\em ``How much is my data worth?''} is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits among multiple data contributors and determining prospective compensation when data breaches happen. In this paper, we study the problem of \\emph{data valuation} by utilizing the Shapley value, a popular notion of value which originated in coopoerative game theory. The Shapley value defines a unique payoff scheme that satisfies many desiderata for the notion of data value. However, the Shapley value often requires \\emph{exponential} time to compute.\nTo meet this challenge, we propose a repertoire of efficient algorithms for approximating the Shapley value. We also demonstrate the value of each training instance for various benchmark datasets."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2404.15391",
        "category": "q-fin",
        "title": "Adaptive Mechanism Design using Multi-Agent Revealed Preferences",
        "abstract": "This paper constructs an algorithmic framework for adaptively achieving the mechanism design objective, finding a mechanism inducing socially optimal Nash equilibria, without knowledge of the utility functions of the agents. We consider a probing scheme where the designer can iteratively enact mechanisms and observe Nash equilibria responses. We first derive necessary and sufficient conditions, taking the form of linear program feasibility, for the existence of utility functions under which the empirical Nash equilibria responses are socially optimal. Then, we utilize this to construct a loss function with respect to the mechanism, and show that its global minimization occurs at mechanisms under which Nash equilibria system responses are also socially optimal. We develop a simulated annealing-based gradient algorithm, and prove that it converges in probability to this set of global minima, thus achieving adaptive mechanism design.",
        "references": [
            {
                "arxivId": "2310.02259",
                "title": "Towards An Analytical Framework for Potential Games",
                "abstract": "Potential game is an emerging notion and framework for studying multi-agent games, especially with heterogeneous agents. Up to date, potential games have been extensively studied mostly from the algorithmic aspect in approximating and computing the Nash equilibrium without verifying if the game is a potential game, due to the lack of analytical structure. In this paper, we aim to build an analytical framework for dynamic potential games. We prove that a game is a potential game if and only if each agent's value function can be decomposed as a potential function and a residual term that solely dependent on other agents' actions. This decomposition enables us to identify and analyze a new and important class of potential games called the distributed game. Moreover, by an appropriate notion of functional derivatives, we prove that a game is a potential game if the value function has a symmetric Jacobian. Consequently, for a general class of continuous-time stochastic games, their potential functions can be further characterized from both the probabilistic and the PDE approaches. The consistency of these two characterisations are shown in a class of linear-quadratic games."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2404.15478",
        "category": "q-fin",
        "title": "Algorithmic Market Making in Spot Precious Metals",
        "abstract": "The primary challenge of market making in spot precious metals is navigating the liquidity that is mainly provided by futures contracts. The Exchange for Physical (EFP) spread, which is the price difference between futures and spot, plays a pivotal role and exhibits multiple modes of relaxation corresponding to the diverse trading horizons of market participants. In this paper, we introduce a novel framework utilizing a nested Ornstein-Uhlenbeck process to model the EFP spread. We demonstrate the suitability of the framework for maximizing the expected P\\&L of a market maker while minimizing inventory risk across both spot and futures. Using a computationally efficient technique to approximate the solution of the Hamilton-Jacobi-Bellman equation associated with the corresponding stochastic optimal control problem, our methodology facilitates strategy optimization on demand in near real-time, paving the way for advanced algorithmic market making that capitalizes on the co-integration properties intrinsic to the precious metals sector.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-25.json",
        "arxivId": "2404.15495",
        "category": "q-fin",
        "title": "Correlations versus noise in the NFT market",
        "abstract": "The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market. To deepen the understanding of the dynamics of this market, in the current study, based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform, the degree of correlation in this market is examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix. It appears that correlation strength is lower here than that observed in previously studied markets. Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain. The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized.",
        "references": [
            {
                "arxivId": "2310.19747",
                "title": "Characteristics of price related fluctuations in Non-Fungible Token (NFT) market",
                "abstract": "A non-fungible token (NFT) market is a new trading invention based on the blockchain technology, which parallels the cryptocurrency market. In the present work, we study capitalization, floor price, the number of transactions, the inter-transaction times, and the transaction volume value of a few selected popular token collections. The results show that the fluctuations of all these quantities are characterized by heavy-tailed probability distribution functions, in most cases well described by the stretched exponentials, with a trace of power-law scaling at times, long-range memory, persistence, and in several cases even the fractal organization of fluctuations, mostly restricted to the larger fluctuations, however. We conclude that the NFT market-even though young and governed by somewhat different mechanisms of trading-shares several statistical properties with the regular financial markets. However, some differences are visible in the specific quantitative indicators."
            },
            {
                "arxivId": "2310.06844",
                "title": "Exploiting Unfair Advantages: Investigating Opportunistic Trading in the NFT Market",
                "abstract": "As cryptocurrency evolved, new financial instruments, such as lending and borrowing protocols, currency exchanges, fungible and non-fungible tokens (NFT), staking and mining protocols have emerged. A financial ecosystem built on top of a blockchain is supposed to be fair and transparent for each participating actor. Yet, there are sophisticated actors who turn their domain knowledge and market inefficiencies to their strategic advantage; thus extracting value from trades not accessible to others. This situation is further exacerbated by the fact that blockchain-based markets and decentralized finance (DeFi) instruments are mostly unregulated. Though a large body of work has already studied the unfairness of different aspects of DeFi and cryptocurrency trading, the economic intricacies of non-fungible token (NFT) trades necessitate further analysis and academic scrutiny. The trading volume of NFTs has skyrocketed in recent years. A single NFT trade worth over a million US dollars, or marketplaces making billions in revenue is not uncommon nowadays. While previous research indicated the presence of wrongdoings in the NFT market, to our knowledge, we are the first to study predatory trading practices, what we call opportunistic trading, in depth. Opportunistic traders are sophisticated actors who employ automated, high-frequency NFT trading strategies, which, oftentimes, are malicious, deceptive, or, at the very least, unfair. Such attackers weaponize their advanced technical knowledge and superior understanding of DeFi protocols to disrupt trades of unsuspecting users, and collect profits from economic situations that are inaccessible to ordinary users, in a\"supposedly\"fair market. In this paper, we explore three such broad classes of opportunistic strategies aiming to realize three distinct trading objectives, viz., acquire, instant profit generation, and loss minimization."
            },
            {
                "arxivId": "2306.17095",
                "title": "Decomposing cryptocurrency high-frequency price dynamics into recurring and noisy components.",
                "abstract": "This paper investigates the temporal patterns of activity in the cryptocurrency market with a focus on Bitcoin, Ethereum, Dogecoin, and WINkLink from January 2020 to December 2022. Market activity measures-logarithmic returns, volume, and transaction number, sampled every 10\u2009s, were divided into intraday and intraweek periods and then further decomposed into recurring and noise components via correlation matrix formalism. The key findings include the distinctive market behavior from traditional stock markets due to the nonexistence of trade opening and closing. This was manifested in three enhanced-activity phases aligning with Asian, European, and U.S. trading sessions. An intriguing pattern of activity surge in 15-min intervals, particularly at full hours, was also noticed, implying the potential role of algorithmic trading. Most notably, recurring bursts of activity in bitcoin and ether were identified to coincide with the release times of significant U.S. macroeconomic reports, such as Nonfarm payrolls, Consumer Price Index data, and Federal Reserve statements. The most correlated daily patterns of activity occurred in 2022, possibly reflecting the documented correlations with U.S. stock indices in the same period. Factors that are external to the inner market dynamics are found to be responsible for the repeatable components of the market dynamics, while the internal factors appear to be substantially random, which manifests itself in a good agreement between the empirical eigenvalue distributions in their bulk and the random-matrix theory predictions expressed by the Marchenko-Pastur distribution. The findings reported support the growing integration of cryptocurrencies into the global financial markets."
            },
            {
                "arxivId": "2306.13371",
                "title": "Fractal properties, information theory, and market efficiency",
                "abstract": null
            },
            {
                "arxivId": "2305.05751",
                "title": "What Is Mature and What Is Still Emerging in the Cryptocurrency Market?",
                "abstract": "In relation to the traditional financial markets, the cryptocurrency market is a recent invention and the trading dynamics of all its components are readily recorded and stored. This fact opens up a unique opportunity to follow the multidimensional trajectory of its development since inception up to the present time. Several main characteristics commonly recognized as financial stylized facts of mature markets were quantitatively studied here. In particular, it is shown that the return distributions, volatility clustering effects, and even temporal multifractal correlations for a few highest-capitalization cryptocurrencies largely follow those of the well-established financial markets. The smaller cryptocurrencies are somewhat deficient in this regard, however. They are also not as highly cross-correlated among themselves and with other financial markets as the large cryptocurrencies. Quite generally, the volume V impact on price changes R appears to be much stronger on the cryptocurrency market than in the mature stock markets, and scales as R(V)\u223cV\u03b1 with \u03b1\u22731."
            },
            {
                "arxivId": "2202.10623",
                "title": "On financial market correlation structures and diversification benefits across and within equity sectors",
                "abstract": null
            },
            {
                "arxivId": "2202.03866",
                "title": "NFT Wash Trading: Quantifying suspicious behaviour in NFT markets",
                "abstract": null
            },
            {
                "arxivId": "2112.06552",
                "title": "Cryptocurrency Market Consolidation in 2020\u20132021",
                "abstract": "Time series of price returns for 80 of the most liquid cryptocurrencies listed on Binance are investigated for the presence of detrended cross-correlations. A spectral analysis of the detrended correlation matrix and a topological analysis of the minimal spanning trees calculated based on this matrix are applied for different positions of a moving window. The cryptocurrencies become more strongly cross-correlated among themselves than they used to be before. The average cross-correlations increase with time on a specific time scale in a way that resembles the Epps effect amplification when going from past to present. The minimal spanning trees also change their topology and, for the short time scales, they become more centralized with increasing maximum node degrees, while for the long time scales they become more distributed, but also more correlated at the same time. Apart from the inter-market dependencies, the detrended cross-correlations between the cryptocurrency market and some traditional markets, like the stock markets, commodity markets, and Forex, are also analyzed. The cryptocurrency market shows higher levels of cross-correlations with the other markets during the same turbulent periods, in which it is strongly cross-correlated itself."
            },
            {
                "arxivId": "2108.09763",
                "title": "Community Detection in Cryptocurrencies with Potential Applications to Portfolio Diversification",
                "abstract": "In this paper, the cross-correlations of cryptocurrency returns are analysed. The paper examines one years worth of data for 146 cryptocurrencies from the period January 1 2019 to December 31 2019. The cross-correlations of these returns are firstly analysed by comparing eigenvalues and eigenvector components of the cross-correlation matrix C with Random Matrix Theory (RMT) assumptions. Results show that C deviates from these assumptions indicating that C contains genuine information about the correlations between the different cryptocurrencies. From here, Louvain community detection method is applied as a clustering mechanism and 15 community groupings are detected. Finally, PCA is completed on the standardised returns of each of these clusters to create a portfolio of cryptocurrencies for investment. This method selects a portfolio which contains a number of high value coins when compared back against their market ranking in the same year. In the interest of assessing continuity of the initial results, the method is also applied to a smaller dataset of the top 50 cryptocurrencies across three time periods of T = 125 days, which produces similar results. The results obtained in this paper show that these methods could be useful for constructing a portfolio of optimally performing cryptocurrencies."
            },
            {
                "arxivId": "2107.06659",
                "title": "Financial Return Distributions: Past, Present, and COVID-19",
                "abstract": "We analyze the price return distributions of currency exchange rates, cryptocurrencies, and contracts for differences (CFDs) representing stock indices, stock shares, and commodities. Based on recent data from the years 2017\u20132020, we model tails of the return distributions at different time scales by using power-law, stretched exponential, and q-Gaussian functions. We focus on the fitted function parameters and how they change over the years by comparing our results with those from earlier studies and find that, on the time horizons of up to a few minutes, the so-called \u201cinverse-cubic power-law\u201d still constitutes an appropriate global reference. However, we no longer observe the hypothesized universal constant acceleration of the market time flow that was manifested before in an ever faster convergence of empirical return distributions towards the normal distribution. Our results do not exclude such a scenario but, rather, suggest that some other short-term processes related to a current market situation alter market dynamics and may mask this scenario. Real market dynamics is associated with a continuous alternation of different regimes with different statistical properties. An example is the COVID-19 pandemic outburst, which had an enormous yet short-time impact on financial markets. We also point out that two factors\u2014speed of the market time flow and the asset cross-correlation magnitude\u2014while related (the larger the speed, the larger the cross-correlations on a given time scale), act in opposite directions with regard to the return distribution tails, which can affect the expected distribution convergence to the normal distribution."
            },
            {
                "arxivId": "2104.02318",
                "title": "Efficiency of communities and financial markets during the 2020 pandemic.",
                "abstract": "This paper investigates the relationship between the spread of the COVID-19 pandemic, the state of community activity, and the financial index performance across 20 countries. First, we analyze which countries behaved similarly in 2020 with respect to one of three multivariate time series: daily COVID-19 cases, Apple mobility data, and national equity index price. Next, we study the trajectories of all three of these attributes in conjunction to determine which exhibited greater similarity. Finally, we investigate whether country financial indices or mobility data responded more quickly to surges in COVID-19 cases. Our results indicate that mobility data and national financial indices exhibited the most similarity in their trajectories, with financial indices responding quicker. This suggests that financial market participants may have interpreted and responded to COVID-19 data more efficiently than governments. Furthermore, results imply that efforts to study community mobility data as a leading indicator for financial market performance during the pandemic were misguided."
            },
            {
                "arxivId": "1911.08944",
                "title": "Competition of noise and collectivity in global cryptocurrency trading: route to a self-contained market",
                "abstract": "Cross correlations in fluctuations of the daily exchange rates within the basket of the 100 highest-capitalization cryptocurrencies over the period October 1, 2015-March 31, 2019 are studied. The corresponding dynamics predominantly involve one leading eigenvalue of the correlation matrix, while the others largely coincide with those of Wishart random matrices. However, the magnitude of the principal eigenvalue, and thus the degree of collectivity, strongly depends on which cryptocurrency is used as a base. It is largest when the base is the most peripheral cryptocurrency; when more significant ones are taken into consideration, its magnitude systematically decreases, nevertheless preserving a sizable gap with respect to the random bulk, which in turn indicates that the organization of correlations becomes more heterogeneous. This finding provides a criterion for recognizing which currencies or cryptocurrencies play a dominant role in the global cryptomarket. The present study shows that over the period under consideration, the Bitcoin (BTC) predominates, hallmarking exchange rate dynamics at least as influential as the U.S. dollar (USD). Even more, the BTC started dominating around the year 2017, while other cryptocurrencies, such as the Ethereum and even Ripple, assumed similar trends. At the same time, the USD, an original value determinant for the cryptocurrency market, became increasingly disconnected, and its related characteristics eventually started approaching those of a fictitious currency. These results are strong indicators of incipient independence of the global cryptocurrency market, delineating a self-contained trade resembling the Forex."
            },
            {
                "arxivId": "1906.07491",
                "title": "Detecting correlations and triangular arbitrage opportunities in the Forex by means of multifractal detrended cross-correlations analysis",
                "abstract": null
            },
            {
                "arxivId": "1812.08548",
                "title": "Multifractal cross-correlations between the World Oil and other Financial Markets in 2012-2017",
                "abstract": null
            },
            {
                "arxivId": "1805.11909",
                "title": "Quantitative approach to multifractality induced by correlations and broad distribution of data",
                "abstract": null
            },
            {
                "arxivId": "1805.04750",
                "title": "Multifractal analysis of financial markets: a review",
                "abstract": "Multifractality is ubiquitously observed in complex natural and socioeconomic systems. Multifractal analysis provides powerful tools to understand the complex nonlinear nature of time series in diverse fields. Inspired by its striking analogy with hydrodynamic turbulence, from which the idea of multifractality originated, multifractal analysis of financial markets has bloomed, forming one of the main directions of econophysics. We review the multifractal analysis methods and multifractal models adopted in or invented for financial time series and their subtle properties, which are applicable to time series in other disciplines. We survey the cumulating evidence for the presence of multifractality in financial time series in different markets and at different time periods and discuss the sources of multifractality. The usefulness of multifractal analysis in quantifying market inefficiency, in supporting risk management and in developing other applications is presented. We finally discuss open problems and further directions of multifractal analysis."
            },
            {
                "arxivId": "1804.05916",
                "title": "Bitcoin market route to maturity? Evidence from return fluctuations, temporal correlations and multiscaling effects",
                "abstract": "Based on 1-min price changes recorded since year 2012, the fluctuation properties of the rapidly emerging Bitcoin market are assessed over chosen sub-periods, in terms of return distributions, volatility autocorrelation, Hurst exponents, and multiscaling effects. The findings are compared to the stylized facts of mature world markets. While early trading was affected by system-specific irregularities, it is found that over the months preceding April 2018 all these statistical indicators approach the features hallmarking maturity. This can be taken as an indication that the Bitcoin market, and possibly other cryptocurrencies, carry concrete potential of imminently becoming a regular market, alternative to the foreign exchange. Since high-frequency price data are available since the beginning of trading, the Bitcoin offers a unique window into the statistical characteristics of a market maturation trajectory."
            },
            {
                "arxivId": "1506.08692",
                "title": "Detrended fluctuation analysis made flexible to detect range of cross-correlated fluctuations.",
                "abstract": "The detrended cross-correlation coefficient \u03c1(DCCA) has recently been proposed to quantify the strength of cross-correlations on different temporal scales in bivariate, nonstationary time series. It is based on the detrended cross-correlation and detrended fluctuation analyses (DCCA and DFA, respectively) and can be viewed as an analog of the Pearson coefficient in the case of the fluctuation analysis. The coefficient \u03c1(DCCA) works well in many practical situations but by construction its applicability is limited to detection of whether two signals are generally cross-correlated, without the possibility to obtain information on the amplitude of fluctuations that are responsible for those cross-correlations. In order to introduce some related flexibility, here we propose an extension of \u03c1(DCCA) that exploits the multifractal versions of DFA and DCCA: multifractal detrended fluctuation analysis and multifractal detrended cross-correlation analysis, respectively. The resulting new coefficient \u03c1(q) not only is able to quantify the strength of correlations but also allows one to identify the range of detrended fluctuation amplitudes that are correlated in two signals under study. We show how the coefficient \u03c1(q) works in practical situations by applying it to stochastic time series representing processes with long memory: autoregressive and multiplicative ones. Such processes are often used to model signals recorded from complex systems and complex physical phenomena like turbulence, so we are convinced that this new measure can successfully be applied in time-series analysis. In particular, we present an example of such application to highly complex empirical data from financial markets. The present formulation can straightforwardly be extended to multivariate data in terms of the q-dependent counterpart of the correlation matrices and then to the network representation."
            },
            {
                "arxivId": "1503.02405",
                "title": "Detecting and interpreting distortions in hierarchical organization of complex time series.",
                "abstract": "Hierarchical organization is a cornerstone of complexity and multifractality constitutes its central quantifying concept. For model uniform cascades the corresponding singularity spectra are symmetric while those extracted from empirical data are often asymmetric. Using selected time series representing such diverse phenomena as price changes and intertransaction times in financial markets, sentence length variability in narrative texts, Missouri River discharge, and sunspot number variability as examples, we show that the resulting singularity spectra appear strongly asymmetric, more often left sided but in some cases also right sided. We present a unified view on the origin of such effects and indicate that they may be crucially informative for identifying the composition of the time series. One particularly intriguing case of this latter kind of asymmetry is detected in the daily reported sunspot number variability. This signals that either the commonly used famous Wolf formula distorts the real dynamics in expressing the largest sunspot numbers or, if not, that their dynamics is governed by a somewhat different mechanism."
            },
            {
                "arxivId": "0908.1089",
                "title": "The components of empirical multifractality in financial returns",
                "abstract": "We perform a systematic investigation on the components of the empirical multifractality of financial returns using the daily data of Dow Jones Industrial Average from 26 May 1896 to 27 April 2007 as an example. The temporal structure and fat-tailed distribution of the returns are considered as possible influence factors. The multifractal spectrum of the original return series is compared with those of four kinds of surrogate data: 1) shuffled data that contain no temporal correlation but have the same distribution, 2) surrogate data in which any nonlinear correlation is removed but the distribution and linear correlation are preserved, 3) surrogate data in which large positive and negative returns are replaced with small values, and 4) surrogate data generated from alternative fat-tailed distributions with the temporal correlation preserved. We find that all these factors have influence on the multifractal spectrum. We also find that the temporal structure (linear or nonlinear) has minor impact on the singularity width \u0394\u03b1 of the multifractal spectrum while the fat tails have major impact on \u0394\u03b1, which confirms the earlier results. In addition, the linear correlation is found to have only a horizontal translation effect on the multifractal spectrum in which the distance is approximately equal to the difference between its DFA scaling exponent and 0.5. Our method can also be applied to other financial or physical variables and other multifractal formalisms."
            },
            {
                "arxivId": "0907.2866",
                "title": "Quantitative features of multifractal subtleties in time series",
                "abstract": "Based on the Multifractal Detrended Fluctuation Analysis (MFDFA) and on the Wavelet Transform Modulus Maxima (WTMM) methods we investigate the origin of multifractality in the time series. Series fluctuating according to a qGaussian distribution, both uncorrelated and correlated in time, are used. For the uncorrelated series at the border (q=5/3) between the Gaussian and the Levy basins of attraction asymptotically we find a phase-like transition between monofractal and bifractal characteristics. This indicates that these may solely be the specific nonlinear temporal correlations that organize the series into a genuine multifractal hierarchy. For analyzing various features of multifractality due to such correlations, we use the model series generated from the binomial cascade as well as empirical series. Then, within the temporal ranges of well-developed power law correlations we find a fast convergence in all multifractal measures. Besides its practical significance this fact may reflect another manifestation of a conjectured q-generalized Central-Limit Theorem."
            },
            {
                "arxivId": "0810.1215",
                "title": "Scale free effects in world currency exchange network",
                "abstract": null
            },
            {
                "arxivId": "0803.0476",
                "title": "Fast unfolding of communities in large networks",
                "abstract": "We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks."
            },
            {
                "arxivId": "0708.4347",
                "title": "World currency exchange rate cross-correlations",
                "abstract": null
            },
            {
                "arxivId": "physics/0505074",
                "title": "The bulk of the stock market correlation matrix is not pure noise",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0412411",
                "title": "Detecting a currency's dominance or dependence using foreign exchange network trees.",
                "abstract": "In a system containing a large number of interacting stochastic processes, there will typically be many nonzero correlation coefficients. This makes it difficult to either visualize the system's interdependencies, or identify its dominant elements. Such a situation arises in foreign exchange (FX), which is the world's biggest market. Here we develop a network analysis of these correlations using minimum spanning trees (MSTs). We show that not only do the MSTs provide a meaningful representation of the global FX dynamics, but they also enable one to determine momentarily dominant and dependent currencies. We find that information about a country's geographical ties emerges from the raw exchange-rate data. Most importantly from a trading perspective, we discuss how to infer which currencies are \"in play\" during a particular period of time."
            },
            {
                "arxivId": "cond-mat/0403067",
                "title": "On the origin of power-law fluctuations in stock prices",
                "abstract": "We respond to the issues discussed by Farmer and Lillo (FL) related to our proposed approach to understanding the origin of power-law distributions in stock price fluctuations. First, we extend our previous analysis to 1000 US stocks and perform a new estimation of market impact that accounts for splitting of large orders and potential autocorrelations in the trade flow. Our new analysis shows clearly that price impact and volume are related by a square-root functional form of market impact for large volumes, in contrast to the claim of FL that this relationship increases as a power law with a smaller exponent. Since large orders are usually executed by splitting into smaller size trades, procedures used by FL give a downward bias for this power law exponent. Second, FL analyze 3 stocks traded on the London Stock Exchange, and solely on this basis they claim that the distribution of transaction volumes do not have a power-law tail for the London Stock Exchange. We perform new empirical analysis on transaction data for the 262 largest stocks listed in the London Stock Exchange, and find that the distribution of volume decays as a power-law with an exponent $\\approx 3/2$ -- in sharp contrast to FL's claim that the distribution of transaction volume does not have a power-law tail. Our exponent estimate of $\\approx 3/2$ is consistent with our previous results from the New York and Paris Stock Exchanges. We conclude that the available empirical evidence is consistent with our hypothesis on the origin of power-law fluctuations in stock prices."
            },
            {
                "arxivId": "cond-mat/0206130",
                "title": "Hierarchical organization in complex networks.",
                "abstract": "Many real networks in nature and society share two generic properties: they are scale-free and they display a high degree of clustering. We show that these two features are the consequence of a hierarchical organization, implying that small groups of nodes organize in a hierarchical manner into increasingly large groups, while maintaining a scale-free topology. In hierarchical networks, the degree of clustering characterizing the different groups follows a strict scaling law, which can be used to identify the presence of a hierarchical organization in real networks. We find that several real networks, such as the Worldwideweb, actor network, the Internet at the domain level, and the semantic web obey this scaling law, indicating that hierarchy is a fundamental characteristic of many complex systems."
            },
            {
                "arxivId": "physics/0202070",
                "title": "Multifractal Detrended Fluctuation Analysis of Nonstationary Time Series",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0108023",
                "title": "Random matrix approach to cross correlations in financial data.",
                "abstract": "We analyze cross correlations between price fluctuations of different stocks using methods of random matrix theory (RMT). Using two large databases, we calculate cross-correlation matrices C of returns constructed from (i) 30-min returns of 1000 US stocks for the 2-yr period 1994-1995, (ii) 30-min returns of 881 US stocks for the 2-yr period 1996-1997, and (iii) 1-day returns of 422 US stocks for the 35-yr period 1962-1996. We test the statistics of the eigenvalues lambda(i) of C against a \"null hypothesis\"--a random correlation matrix constructed from mutually uncorrelated time series. We find that a majority of the eigenvalues of C fall within the RMT bounds [lambda(-),lambda(+)] for the eigenvalues of random correlation matrices. We test the eigenvalues of C within the RMT bound for universal properties of random matrices and find good agreement with the results for the Gaussian orthogonal ensemble of random matrices-implying a large degree of randomness in the measured cross-correlation coefficients. Further, we find that the distribution of eigenvector components for the eigenvectors corresponding to the eigenvalues outside the RMT bound display systematic deviations from the RMT prediction. In addition, we find that these \"deviating eigenvectors\" are stable in time. We analyze the components of the deviating eigenvectors and find that the largest eigenvalue corresponds to an influence common to all stocks. Our analysis of the remaining deviating eigenvectors shows distinct groups, whose identities correspond to conventionally identified business sectors. Finally, we discuss applications to the construction of portfolios of stocks that have a stable ratio of risk to return."
            },
            {
                "arxivId": "cond-mat/0102402",
                "title": "Quantifying the dynamics of financial correlations",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/9905305",
                "title": "Scaling of the distribution of fluctuations of financial market indices.",
                "abstract": "We study the distribution of fluctuations of the S&P 500 index over a time scale deltat by analyzing three distinct databases. Database (i) contains approximately 1 200 000 records, sampled at 1-min intervals, for the 13-year period 1984-1996, database (ii) contains 8686 daily records for the 35-year period 1962-1996, and database (iii) contains 852 monthly records for the 71-year period 1926-1996. We compute the probability distributions of returns over a time scale deltat, where deltat varies approximately over a factor of 10(4)-from 1 min up to more than one month. We find that the distributions for deltat<or= 4 d (1560 min) are consistent with a power-law asymptotic behavior, characterized by an exponent alpha approximately 3, well outside the stable L\u00e9vy regime 0<alpha<2. To test the robustness of the S&P result, we perform a parallel analysis on two other financial market indices. Database (iv) contains 3560 daily records of the NIKKEI index for the 14-year period 1984-1997, and database (v) contains 4649 daily records of the Hang-Seng index for the 18-year period 1980-1997. We find estimates of alpha consistent with those describing the distribution of S&P 500 daily returns. One possible reason for the scaling of these distributions is the long persistence of the autocorrelation function of the volatility. For time scales longer than (deltat)x approximately 4 d, our results are consistent with a slow convergence to Gaussian behavior."
            },
            {
                "arxivId": "cond-mat/9902283",
                "title": "Universal and Nonuniversal Properties of Cross Correlations in Financial Time Series",
                "abstract": "We use methods of random matrix theory to analyze the cross-correlation matrix C of price changes of the largest 1000 US stocks for the 2-year period 1994-95. We find that the statistics of most of the eigenvalues in the spectrum of C agree with the predictions of random matrix theory, but there are deviations for a few of the largest eigenvalues. We find that C has the universal properties of the Gaussian orthogonal ensemble of random matrices. Furthermore, we analyze the eigenvectors of C through their inverse participation ratio and find eigenvectors with large inverse participation ratios at both edges of the eigenvalue spectrum--a situation reminiscent of results in localization theory."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2007.09320",
        "category": "q-fin",
        "title": "Convolution Bounds on Quantile Aggregation",
        "abstract": "Quantile aggregation with dependence uncertainty has a long history in probability theory with wide applications in problems in finance, risk management, statistics, and operations research. Using a recent result on inf-convolution of Range-Value-at-Risk, which includes Value-at-Risk and Expected Shortfall as special cases, we establish new analytical bounds which we call convolution bounds. These bounds are easy to compute, and we show that they are sharp in many relevant cases. We pay a special attention to the problem of quantile aggregation, and the convolution bounds help us to identify approximations for the extremal dependence structure. The convolution bound enjoys several advantages, including interpretability, tractability and theoretical properties. To the best of our knowledge, there is no other theoretical result on quantile aggregation which is not covered by the convolution bounds, and thus the convolution bounds are genuinely the best one available. The results can be applied to compute bounds on the distribution of the sum of random variables. Some applications to operations research are discussed.",
        "references": [
            {
                "arxivId": "2109.02730",
                "title": "Sorting with Teams",
                "abstract": "We fully solve a sorting problem with heterogeneous firms and multiple heterogeneous workers whose skills are imperfect substitutes. We show that optimal sorting, which we call mixed and countermonotonic, is comprised of two regions. In the first region, mediocre firms sort with mediocre workers and coworkers such that the output losses are equal across all these teams (mixing). In the second region, a high skill worker sorts with low skill coworkers and a high productivity firm (countermonotonicity). We characterize the equilibrium wages and firm values. Quantitatively, our model can generate the dispersion of earnings within and across US firms."
            },
            {
                "arxivId": "2007.14208",
                "title": "Admissible ways of merging p-values under arbitrary dependence",
                "abstract": "Methods of merging several p-values into a single p-value are important in their own right and widely used in multiple hypothesis testing. This paper is the first to systematically study the admissibility (in Wald's sense) of p-merging functions and their domination structure, without any assumptions on the dependence structure of the input p-values. As a technical tool we use the notion of e-values, which are alternatives to p-values recently promoted by several authors. We obtain several results on the representation of admissible p-merging functions via e-values and on (in)admissibility of existing p-merging functions. By introducing new admissible p-merging functions, we show that some classic merging methods can be strictly improved to enhance power without compromising validity under arbitrary dependence."
            },
            {
                "arxivId": "2007.12338",
                "title": "Ordering and inequalities for mixtures on risk aggregation",
                "abstract": "Aggregation sets, which represent model uncertainty due to unknown dependence, are an important object in the study of robust risk aggregation. In this paper, we investigate ordering relations between two aggregation sets for which the sets of marginals are related by two simple operations: distribution mixtures and quantile mixtures. Intuitively, these operations \u201chomogenize\u201d marginal distributions by making them similar. As a general conclusion from our results, more \u201chomogeneous\u201d marginals lead to a larger aggregation set, and thus more severe model uncertainty, although the situation for quantile mixtures is much more complicated than that for distribution mixtures. We proceed to study inequalities on the worst\u2010case values of risk measures in risk aggregation, which represent conservative calculation of regulatory capital. Among other results, we obtain an order relation on VaR under quantile mixture for marginal distributions with monotone densities. Numerical results are presented to visualize the theoretical results and further inspire some conjectures. Finally, we provide applications on portfolio diversification under dependence uncertainty and merging p\u2010values in multiple hypothesis testing, and discuss the connection of our results to joint mixability."
            },
            {
                "arxivId": "2006.15689",
                "title": "A Distributionally Robust Optimization Approach to the NASA Langley Uncertainty Quantification Challenge",
                "abstract": "We study a methodology to tackle the NASA Langley Uncertainty Quantification Challenge problem, based on an integration of robust optimization, more specifically a recent line of research known as distributionally robust optimization, and importance sampling in Monte Carlo simulation. The main computation machinery in this integrated methodology boils down to solving sampled linear programs. We will illustrate both our numerical performances and theoretical statistical guarantees obtained via connections to nonparametric hypothesis testing."
            },
            {
                "arxivId": "1905.12231",
                "title": "Multivariate Distributionally Robust Convex Regression under Absolute Error Loss",
                "abstract": "This paper proposes a novel non-parametric multidimensional convex regression estimator which is designed to be robust to adversarial perturbations in the empirical measure. We minimize over convex functions the maximum (over Wasserstein perturbations of the empirical measure) of the absolute regression errors. The inner maximization is solved in closed form resulting in a regularization penalty involves the norm of the gradient. We show consistency of our estimator and a rate of convergence of order $ \\widetilde{O}\\left( n^{-1/d}\\right) $, matching the bounds of alternative estimators based on square-loss minimization. Contrary to all of the existing results, our convergence rates hold without imposing compactness on the underlying domain and with no a priori bounds on the underlying convex function or its gradient norm."
            },
            {
                "arxivId": "1709.00641",
                "title": "Marginal and Dependence Uncertainty: Bounds, Optimal Transport, and Sharpness",
                "abstract": "Motivated by applications in model-free finance and quantitative risk management, we consider Fr\\'echet classes of multivariate distribution functions where additional information on the joint distribution is assumed, while uncertainty in the marginals is also possible. We derive optimal transport duality results for these Fr\\'echet classes that extend previous results in the related literature. These proofs are based on representation results for increasing convex functionals and the explicit computation of the conjugates. We show that the dual transport problem admits an explicit solution for the function $f=1_B$, where $B$ is a rectangular subset of $\\mathbb R^d$, and provide an intuitive geometric interpretation of this result. The improved Fr\\'echet--Hoeffding bounds provide ad-hoc upper bounds for these Fr\\'echet classes. We show that the improved Fr\\'echet--Hoeffding bounds are pointwise sharp for these classes in the presence of uncertainty in the marginals, while a counterexample yields that they are not pointwise sharp in the absence of uncertainty in the marginals, even in dimension 2. The latter result sheds new light on the improved Fr\\'echet--Hoeffding bounds, since Tankov [30] has showed that, under certain conditions, these bounds are sharp in dimension 2."
            },
            {
                "arxivId": "1704.02660",
                "title": "Centers of probability measures without the mean",
                "abstract": "In the recent years, the notion of mixability has been developed with applications to operations research, optimal transportation, and quantitative finance. An n-tuple of distributions is said to be jointly mixable if there exist n random variables following these distributions and adding up to a constant, called center, with probability one. When the n distributions are identical, we speak of complete mixability. If each distribution has finite mean, the center is obviously the sum of the means. In this paper, we investigate the set of centers of completely and jointly mixable distributions not having a finite mean. In addition to several results, we show the (possibly counterintuitive) fact that, for each $$n \\ge 2$$n\u22652, there exist n standard Cauchy random variables adding up to a constant C if and only if $$\\begin{aligned} |C|\\le \\frac{n\\,\\log (n-1)}{\\pi }. \\end{aligned}$$|C|\u2264nlog(n-1)\u03c0."
            },
            {
                "arxivId": "1703.06222",
                "title": "A unified treatment of multiple testing with prior knowledge using the p-filter",
                "abstract": "A significant literature studies ways of employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) \\emph{a priori} beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) multiple arbitrary partitions of the hypotheses into known (possibly overlapping) groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control. Then, we present a unified algorithmic framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)--(d) simultaneously, recovering a wide variety of common algorithms as special cases."
            },
            {
                "arxivId": "1610.05627",
                "title": "Robust Wasserstein profile inference and applications to machine learning",
                "abstract": "We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings."
            },
            {
                "arxivId": "1605.09074",
                "title": "Reconstructing Input Models in Stochastic Simulation",
                "abstract": "We investigate the problem of nonparametrically calibrating the input model in stochastic simulation, given only the availability of output data. While studies on simulation input uncertainty have focused on the use of input data, our inverse model calibration problem arises in various situations where resource and operational limitations hinder the direct observability of input data. We propose a moment-based, maximum entropy framework to infer the input model, by matching statistics between the simulation output and the real- world output. To bypass the methodological difficulties from the stochastic constraints in our formulation, we propose a stochastic quadratic penalty method that converts the problem into a sequence of least-square problems, where each element in the sequence can be solved by efficient stochastic approximation algorithms. We analyze the statistical properties of our method and demonstrate its ability to recover input models with numerical experiments."
            },
            {
                "arxivId": "1604.01446",
                "title": "Quantifying Distributional Model Risk Via Optimal Transport",
                "abstract": "This paper deals with the problem of quantifying the impact of model misspecification when computing general expected values of interest. The methodology that we propose is applicable in great generality, in particular, we provide examples involving path dependent expectations of stochastic processes. Our approach consists in computing bounds for the expectation of interest regardless of the probability measure used, as long as the measure lies within a prescribed tolerance measured in terms of a flexible class of distances from a suitable baseline model. These distances, based on optimal transportation between probability measures, include Wasserstein's distances as particular cases. The proposed methodology is well-suited for risk analysis, as we demonstrate with a number of applications. We also discuss how to estimate the tolerance region non-parametrically using Skorokhod-type embeddings in some of these applications."
            },
            {
                "arxivId": "1601.06858",
                "title": "On distributionally robust extreme value analysis",
                "abstract": null
            },
            {
                "arxivId": "1507.05609",
                "title": "Robust Analysis in Stochastic Simulation: Computation and Performance Guarantees",
                "abstract": "Any performance analysis based on stochastic simulation is subject to the errors inherent in misspecifying the modeling assumptions, particularly the input distributions. In situations with little support from data, we investigate the use of worst-case analysis to analyze these errors, by representing the partial, nonparametric knowledge of the input models via optimization constraints. We study the performance and robustness guarantees of this approach. We design and analyze a numerical scheme for solving a general class of simulation objectives and uncertainty specifications. The key steps involve a randomized discretization of the probability spaces, a simulable unbiased gradient estimator using a nonparametric analog of the likelihood ratio method, and a Frank-Wolfe (FW) variant of the stochastic approximation (SA) method (which we call FWSA) run on the space of input probability distributions. A convergence analysis for FWSA on non-convex problems is provided. We test the performance of our approach via several numerical examples."
            },
            {
                "arxivId": "1507.03293",
                "title": "Tail Analysis Without Parametric Models: A Worst-Case Perspective",
                "abstract": "A common bottleneck in evaluating extremal performance measures is that, because of their very nature, tail data are often very limited. The conventional approach selects the best probability distribution from tail data using parametric fitting, but the validity of the parametric choice can be difficult to verify. This paper describes an alternative based on the computation of worst-case bounds under the geometric premise of tail convexity, a feature shared by all common parametric tail distributions. We characterize the optimality structure of the resulting optimization problem, and demonstrate that the worst-case convex tail behavior is in a sense either extremely light tailed or extremely heavy tailed. We develop low-dimensional nonlinear programs that distinguish between the two cases and compute the worst-case bound. We numerically illustrate how the proposed approach can give more reliable performances than conventional parametric methods. The online appendix is available at https://doi.org/10.1287/..."
            },
            {
                "arxivId": "1512.03232",
                "title": "Extremal Dependence Concepts",
                "abstract": "The probabilistic characterization of the relationship between two or more random variables calls for a notion of dependence. Dependence modeling leads to mathematical and statistical challenges; recent developments in extremal dependence concepts have drawn a lot of attention in probability and its applications in several disciplines. The aim of this paper is to review various concepts of extremal positive and negative dependence, including several recently established results, reconstruct their history, link them to probabilistic optimization problems, and provide a list of open questions in this area. While the concept of extremal positive dependence is agreed upon for random vectors of arbitrary dimensions, various notions of extremal negative dependence arise when more than two random variables are involved. We review existing popular concepts of extremal negative dependence given in the literature and introduce a novel notion, which in a general sense includes the existing ones as particular cases. Even if much of the literature on dependence is actually focused on positive dependence, we show that negative dependence plays an equally important role in the solution of many optimization problems. While the most popular tool used nowadays to model dependence is that of a copula function, in this paper we use the equivalent concept of a set of rearrangements and this not only for historical reasons. Rearrangement functions describe the relationship between random variables in a completely deterministic way and this implies several advantages on the approximation of solutions in a broad class of optimization problems and allows a deeper understanding of dependence itself."
            },
            {
                "arxivId": "1505.05116",
                "title": "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2302.08302",
        "category": "q-fin",
        "title": "Stochastic control problems with state-reflections arising from relaxed benchmark tracking",
        "abstract": "This paper studies stochastic control problems motivated by optimal consumption with wealth benchmark tracking. The benchmark process is modeled by a combination of a geometric Brownian motion and a running maximum process, indicating its increasing trend in the long run. We consider a relaxed tracking formulation such that the wealth compensated by the injected capital always dominates the benchmark process. The stochastic control problem is to maximize the expected utility of consumption deducted by the cost of the capital injection under the dynamic floor constraint. By introducing two auxiliary state processes with reflections, an equivalent auxiliary control problem is formulated and studied, which leads to the HJB equation with two Neumann boundary conditions. We establish the existence of a unique classical solution to the dual PDE using some novel probabilistic representations involving the local time of some dual processes together with a tailor-made decomposition-homogenization technique. The proof of the verification theorem on the optimal feedback control can be carried out by some stochastic flow analysis and technical estimations of the optimal control.",
        "references": [
            {
                "arxivId": "2006.13661",
                "title": "Optimal Tracking Portfolio with a Ratcheting Capital Benchmark",
                "abstract": "This paper studies the finite horizon portfolio management by optimally tracking a ratcheting capital benchmark process. To formulate such an optimal tracking problem, we envision that the fund manager can dynamically inject capital into the portfolio account such that the total capital dominates the nondecreasing benchmark floor process at each intermediate time. The control problem is to minimize the cost of the accumulative capital injection. We first transform the original problem with floor constraints into an unconstrained control problem, however, under a running maximum cost. By identifying a controlled state process with reflection, we next transform the problem further into an equivalent auxiliary problem, which leads to a nonlinear Hamilton-Jacobi-Bellman (HJB) with a Neumann boundary condition. By employing the dual transform, the probabilistic representation approach and some stochastic flow arguments, the existence of the unique classical solution to the dual HJB is established. The verification theorem is carefully proved, which gives the complete characterization of the primal value function and the feedback optimal portfolio."
            },
            {
                "arxivId": "1802.03954",
                "title": "On Dynamic Programming Principle for Stochastic Control Under Expectation Constraints",
                "abstract": null
            },
            {
                "arxivId": "1312.2794",
                "title": "Penalization methods for the Skorokhod problem and reflecting SDEs with jumps",
                "abstract": "We study the problem of approximation of solutions of the Skorokhod problem and reflecting stochastic differential equations (SDEs) with jumps by sequences of solutions of equations with penalization terms. Applications to discrete approximation of weak and strong solutions of reflecting SDEs are given. Our proofs are based on new estimates for solutions of equations with penalization terms and the theory of convergence in the Jakubowski S-topology."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2308.01844",
        "category": "q-fin",
        "title": "A novel approach for quantum financial simulation and quantum state preparation",
        "abstract": null,
        "references": [
            {
                "arxivId": "2208.13372",
                "title": "Loading Probability Distributions in a Quantum circuit",
                "abstract": "Quantum circuits generating probability distributions has applications in several areas. Areas like \ufb01nance require quantum circuits that can generate distributions that mimic some given data pattern. Hamiltonian simulations require circuits that can initialize the wave function of a physical quantum system. These wave functions, in several cases, are identical to some very well known probability distributions. In this paper we discuss ways to construct parameterized quantum circuits that can generate both symmetric as well as asymmetric distributions. We follow the trajectory of quantum states as single and two qubit operations get applied to the system, and \ufb01nd out the best possible way to arrive at the desired distribution. The parameters are optimized by a variational solver. We present results from both simulators as well as real IBM quantum hardwares."
            },
            {
                "arxivId": "2202.11302",
                "title": "Optimal (controlled) quantum state preparation and improved unitary synthesis by quantum circuits with any number of ancillary qubits",
                "abstract": "<jats:p>As a cornerstone for many quantum linear algebraic and quantum machine learning algorithms, controlled quantum state preparation (CQSP) aims to provide the transformation of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo><mml:mo stretchy=\"false\">\u2192</mml:mo><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:msub><mml:mi>\u03c8</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo></mml:math> for all <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>i</mml:mi><mml:mo>\u2208</mml:mo><mml:mo fence=\"false\" stretchy=\"false\">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo fence=\"false\" stretchy=\"false\">}</mml:mo><mml:mi>k</mml:mi></mml:msup></mml:math> for the given <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math>-qubit states <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:msub><mml:mi>\u03c8</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo></mml:math>. In this paper, we construct a quantum circuit for implementing CQSP, with depth <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mn>2</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math> and size <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>O</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:math> for any given number <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>m</mml:mi></mml:math> of ancillary qubits. These bounds, which can also be viewed as a time-space tradeoff for the transformation, are optimal for any integer parameters <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>\u2265</mml:mo><mml:mn>0</mml:mn></mml:math> and <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi><mml:mo>\u2265</mml:mo><mml:mn>1</mml:mn></mml:math>. When <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>, the problem becomes the canonical quantum state preparation (QSP) problem with ancillary qubits, which asks for efficient implementations of the transformation <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mi>m</mml:mi></mml:msup><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo><mml:mo stretchy=\"false\">\u2192</mml:mo><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mi>\u03c8</mml:mi><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mi>m</mml:mi></mml:msup><mml:mo fence=\"false\" stretchy=\"false\">\u27e9</mml:mo></mml:math>. This problem has many applications with many investigations, yet its circuit complexity remains open. Our construction completely solves this problem, pinning down its depth complexity to <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi mathvariant=\"normal\">\u0398</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math> and its size complexity to <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi mathvariant=\"normal\">\u0398</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:math> for any <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>m</mml:mi></mml:math>. Another fundamental problem, unitary synthesis, asks to implement a general <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>n</mml:mi></mml:math>-qubit unitary by a quantum circuit. Previous work shows a lower bound of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi mathvariant=\"normal\">\u03a9</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mn>4</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math> and an upper bound of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>O</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:math> for <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">\u03a9</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msup><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:math> ancillary qubits. In this paper, we quadratically shrink this gap by presenting a quantum circuit of the depth of <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mn>2</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mi>n</mml:mi><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mn>1</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mn>2</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mn>3</mml:mn><mml:mi>n</mml:mi><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mn>1</mml:mn><mml:mrow class=\"MJX-TeXAtom-ORD\"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mtext>\u00a0</mml:mtext><mml:mtext>\u00a0</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math>.</jats:p>"
            },
            {
                "arxivId": "2201.11495",
                "title": "Quantum State Preparation with Optimal Circuit Depth: Implementations and Applications.",
                "abstract": "Quantum state preparation is an important subroutine for quantum computing. We show that any n-qubit quantum state can be prepared with a \u0398(n)-depth circuit using only single- and two-qubit gates, although with a cost of an exponential amount of ancillary qubits. On the other hand, for sparse quantum states with d\u2a7e2 nonzero entries, we can reduce the circuit depth to \u0398(log(nd)) with O(ndlogd) ancillary qubits. The algorithm for sparse states is exponentially faster than best-known results and the number of ancillary qubits is nearly optimal and only increases polynomially with the system size. We discuss applications of the results in different quantum computing tasks, such as Hamiltonian simulation, solving linear systems of equations, and realizing quantum random access memories, and find cases with exponential reductions of the circuit depth for all these three tasks. In particular, using our algorithm, we find a family of linear system solving problems enjoying exponential speedups, even compared to the best-known quantum and classical dequantization algorithms."
            },
            {
                "arxivId": "1308.6253",
                "title": "Quantum Simulation",
                "abstract": "Simulating quantum mechanics is known to be a difficult computational problem, especially when dealing with large systems. However, this difficulty may be overcome by using some controllable quantum system to study another less controllable or accessible quantum system, i.e., quantum simulation. Quantum simulation promises to have applications in the study of many problems in, e.g., condensed-matter physics, high-energy physics, atomic physics, quantum chemistry and cosmology. Quantum simulation could be implemented using quantum computers, but also with simpler, analog devices that would require less control, and therefore, would be easier to construct. A number of quantum systems such as neutral atoms, ions, polar molecules, electrons in semiconductors, superconducting circuits, nuclear spins and photons have been proposed as quantum simulators. This review outlines the main theoretical and experimental aspects of quantum simulation and emphasizes some of the challenges and promises of this fast-growing field."
            },
            {
                "arxivId": "0811.3171",
                "title": "Quantum algorithm for linear systems of equations.",
                "abstract": "Solving linear systems of equations is a common problem that arises both on its own and as a subroutine in more complex problems: given a matrix A and a vector b(-->), find a vector x(-->) such that Ax(-->) = b(-->). We consider the case where one does not need to know the solution x(-->) itself, but rather an approximation of the expectation value of some operator associated with x(-->), e.g., x(-->)(dagger) Mx(-->) for some matrix M. In this case, when A is sparse, N x N and has condition number kappa, the fastest known classical algorithms can find x(-->) and estimate x(-->)(dagger) Mx(-->) in time scaling roughly as N square root(kappa). Here, we exhibit a quantum algorithm for estimating x(-->)(dagger) Mx(-->) whose runtime is a polynomial of log(N) and kappa. Indeed, for small values of kappa [i.e., poly log(N)], we prove (using some common complexity-theoretic assumptions) that any classical algorithm for this problem generically requires exponentially more time than our quantum algorithm."
            },
            {
                "arxivId": "quant-ph/0306054",
                "title": "Spatial search by quantum walk",
                "abstract": "Grover's quantum search algorithm provides a way to speed up combinatorial search, but is not directly applicable to searching a physical database. Nevertheless, Aaronson and Ambainis showed that a database of N items laid out in d spatial dimensions can be searched in time of order {radical}(N) for d>2, and in time of order {radical}(N) poly(log N) for d=2. We consider an alternative search algorithm based on a continuous-time quantum walk on a graph. The case of the complete graph gives the continuous-time search algorithm of Farhi and Gutmann, and other previously known results can be used to show that {radical}(N) speedup can also be achieved on the hypercube. We show that full {radical}(N) speedup can be achieved on a d-dimensional periodic lattice for d>4. In d=4, the quantum walk search algorithm takes time of order {radical}(N) poly(log N), and in d<4, the algorithm does not provide substantial speedup."
            },
            {
                "arxivId": "quant-ph/0005055",
                "title": "Quantum Amplitude Amplification and Estimation",
                "abstract": "Consider a Boolean function $\\chi: X \\to \\{0,1\\}$ that partitions set $X$ between its good and bad elements, where $x$ is good if $\\chi(x)=1$ and bad otherwise. Consider also a quantum algorithm $\\mathcal A$ such that $A |0\\rangle= \\sum_{x\\in X} \\alpha_x |x\\rangle$ is a quantum superposition of the elements of $X$, and let $a$ denote the probability that a good element is produced if $A |0\\rangle$ is measured. If we repeat the process of running $A$, measuring the output, and using $\\chi$ to check the validity of the result, we shall expect to repeat $1/a$ times on the average before a solution is found. *Amplitude amplification* is a process that allows to find a good $x$ after an expected number of applications of $A$ and its inverse which is proportional to $1/\\sqrt{a}$, assuming algorithm $A$ makes no measurements. This is a generalization of Grover's searching algorithm in which $A$ was restricted to producing an equal superposition of all members of $X$ and we had a promise that a single $x$ existed such that $\\chi(x)=1$. Our algorithm works whether or not the value of $a$ is known ahead of time. In case the value of $a$ is known, we can find a good $x$ after a number of applications of $A$ and its inverse which is proportional to $1/\\sqrt{a}$ even in the worst case. We show that this quadratic speedup can also be obtained for a large family of search problems for which good classical heuristics exist. Finally, as our main result, we combine ideas from Grover's and Shor's quantum algorithms to perform amplitude estimation, a process that allows to estimate the value of $a$. We apply amplitude estimation to the problem of *approximate counting*, in which we wish to estimate the number of $x\\in X$ such that $\\chi(x)=1$. We obtain optimal quantum algorithms in a variety of settings."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2309.08431",
        "category": "q-fin",
        "title": "Decentralised Finance and Automated Market Making: Predictable Loss and Optimal Liquidity Provision",
        "abstract": "Constant product markets with concentrated liquidity (CL) are the most popular type of automated market makers. In this paper, we characterise the continuous-time wealth dynamics of strategic LPs who dynamically adjust their range of liquidity provision in CL pools. Their wealth results from fee income, the value of their holdings in the pool, and rebalancing costs. Next, we derive a self-financing and closed-form optimal liquidity provision strategy where the width of the LP's liquidity range is determined by the profitability of the pool (provision fees minus gas fees), the predictable losses (PL) of the LP's position, and concentration risk. Concentration risk refers to the decrease in fee revenue if the marginal exchange rate (akin to the midprice in a limit order book) in the pool exits the LP's range of liquidity. When the drift in the marginal rate is stochastic, we show how to optimally skew the range of liquidity to increase fee revenue and profit from the expected changes in the marginal rate. Finally, we use Uniswap v3 data to show that, on average, LPs have traded at a significant loss, and to show that the out-of-sample performance of our strategy is superior to the historical performance of LPs in the pool we consider.",
        "references": [
            {
                "arxivId": "2305.14604",
                "title": "Automated Market Making and Arbitrage Profits in the Presence of Fees",
                "abstract": "We consider the impact of trading fees on the profits of arbitrageurs trading against an automated marker marker (AMM) or, equivalently, on the adverse selection incurred by liquidity providers due to arbitrage. We extend the model of Milionis et al. [2022] for a general class of two asset AMMs to both introduce fees and discrete Poisson block generation times. In our setting, we are able to compute the expected instantaneous rate of arbitrage profit in closed form. When the fees are low, in the fast block asymptotic regime, the impact of fees takes a particularly simple form: fees simply scale down arbitrage profits by the fraction of time that an arriving arbitrageur finds a profitable trade."
            },
            {
                "arxivId": "2212.03340",
                "title": "Finding the Right Curve: Optimal Design of Constant Function Market Makers",
                "abstract": "Constant Function Market Makers (CFMMs) are a tool for creating exchange markets, have been deployed effectively in prediction markets, and are now especially prominent in the Decentralized Finance ecosystem. We show that for any set of beliefs about future asset prices, an optimal CFMM trading function exists that maximizes the fraction of trades that a CFMM can settle. We formulate a convex program to compute this optimal trading function. This program, therefore, gives a tractable framework for market-makers to compile their belief function on the future prices of the underlying assets into the trading function of a maximally capital-efficient CFMM. Our convex optimization framework further extends to capture the tradeoffs between fee revenue, arbitrage loss, and opportunity costs of liquidity providers. Analyzing the program shows how the consideration of profit and loss leads to a qualitatively different optimal trading function. Our model additionally explains the diversity of CFMM designs that appear in practice. We show that careful analysis of our convex program enables inference of a market-maker's beliefs about future asset prices, and show that these beliefs mirror the folklore intuition for several widely used CFMMs. Developing the program requires a new notion of the liquidity of a CFMM, and the core technical challenge is in the analysis of the KKT conditions of an optimization over an infinite-dimensional Banach space."
            },
            {
                "arxivId": "2208.06046",
                "title": "Automated Market Making and Loss-Versus-Rebalancing",
                "abstract": "Automated market making (AMM) protocols such as Uniswap have recently emerged as an alternative to the most common market structure for electronic trading, the limit order book. Relative to limit order books, AMMs are both more computationally efficient and do not require the participation of active market making intermediaries. As such, AMMs have emerged as the dominant market mechanism for trust-less decentralized exchanges (DEXs). We develop a model the underlying economics of AMMs from the perspective of their passive liquidity providers (LPs). Our central contribution is a\"Black-Scholes formula for AMMs\". Like the Black-Scholes formula, we consider the return to LPs once market risk has been hedged. We identify the main adverse selection cost incurred by LPs, which we call\"loss-versus-rebalancing\"(LVR, pronounced\"lever\"). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. In a continuous time Black-Scholes setting, we are able to derive closed-form expressions for this adverse selection cost, for all automated market makers, including constant function market makers and those featuring concentrated liquidity (e.g., Uniswap v3). Qualitatively, we highlight the main forces that drive AMM LP returns, including asset characteristics (volatility) and AMM characteristics (curvature/marginal liquidity). Quantitatively, we illustrate how our model's expressions for LP returns match actual LP returns for the Uniswap v2 WETH-USDC trading pair. Our model provides tradable insight into both the ex ante and ex post assessment of AMM LP investment decisions. LVR can also inform the design of the next generation of DEX market mechanisms -- in fact, in the short time since our work has been released,\"LVR mitigation\"has already emerged as the dominant challenge among practitioners in the AMM protocol designer community."
            },
            {
                "arxivId": "2205.08904",
                "title": "Risks and Returns of Uniswap V3 Liquidity Providers",
                "abstract": "Trade execution on Decentralized Exchanges (DEXes) is automatic and does not require individual buy and sell orders to be matched. Instead, liquidity aggregated in pools from individual liquidity providers enables trading between cryptocurrencies. The largest DEX measured by trading volume, Uniswap V3, promises a DEX design optimized for capital efficiency. However, Uniswap V3 requires far more decisions from liquidity providers than previous DEX designs. In this work, we develop a theoretical model to illustrate the choices faced by Uniswap V3 liquidity providers and their implications. Our model suggests that providing liquidity on Uniswap V3 is highly complex and requires many considerations from a user. Our supporting data analysis of the risks and returns of real Uniswap V3 liquidity providers underlines that liquidity providing in Uniswap V3 is incredibly complicated, and performances can vary wildly. While there are simple and profitable strategies for liquidity providers in liquidity pools characterized by negligible price volatilities, these strategies only yield modest returns. Instead, significant returns can only be obtained by accepting increased financial risks and at the cost of active management. Thus, providing liquidity has become a game reserved for sophisticated players with the introduction of Uniswap V3, where retail traders do not stand a chance."
            },
            {
                "arxivId": "2204.00464",
                "title": "Differential Liquidity Provision in Uniswap v3 and Implications for Contract Design\u2731",
                "abstract": "Decentralized exchanges (DEXs) provide a means for users to trade pairs of assets on-chain without the need of a trusted third party to effectuate a trade. Amongst these, constant function market maker (CFMM) DEXs such as Uniswap handle the most volume of trades between ERC-20 tokens. With the introduction of Uniswap v3, liquidity providers are given the option to differentially allocate liquidity to be used for trades that occur within specific price intervals. In this paper, we formalize the profit and loss that liquidity providers can earn when providing specific liquidity positions to a contract. With this in hand, we are able to compute optimal liquidity allocations for liquidity providers who hold beliefs over how prices evolve over time. Ultimately, we use this tool to shed light on the design question regarding how v3 contracts should partition price space for permissible liquidity allocations. Our results show that a richer space of potential partitions can simultaneously benefit both liquidity providers and traders."
            },
            {
                "arxivId": "2106.12033",
                "title": "Strategic Liquidity Provision in Uniswap v3",
                "abstract": "Uniswap v3 is the largest decentralized exchange for digital currencies. A novelty of its design is that it allows a liquidity provider (LP) to allocate liquidity to one or more closed intervals of the price of an asset instead of the full range of possible prices. An LP earns fee rewards proportional to the amount of its liquidity allocation when prices move in this interval. This induces the problem of {\\em strategic liquidity provision}: smaller intervals result in higher concentration of liquidity and correspondingly larger fees when the price remains in the interval, but with higher risk as prices may exit the interval leaving the LP with no fee rewards. Although reallocating liquidity to new intervals can mitigate this loss, it comes at a cost, as LPs must expend gas fees to do so. We formalize the dynamic liquidity provision problem and focus on a general class of strategies for which we provide a neural network-based optimization framework for maximizing LP earnings. We model a single LP that faces an exogenous sequence of price changes that arise from arbitrage and non-arbitrage trades in the decentralized exchange. We present experimental results informed by historical price data that demonstrate large improvements in LP earnings over existing allocation strategy baselines. Moreover we provide insight into qualitative differences in optimal LP behaviour in different economic environments."
            },
            {
                "arxivId": "2103.13773",
                "title": "Multi-asset Optimal Execution and Statistical Arbitrage Strategies under Ornstein-Uhlenbeck Dynamics",
                "abstract": "In recent years, academics, regulators, and market practitioners have increasingly addressed liquidity issues. Amongst the numerous problems addressed, the optimal execution of large orders is probably the one that has attracted the most research works, mainly in the case of single-asset portfolios. In practice, however, optimal execution problems often involve large portfolios comprising numerous assets, and models should consequently account for risks at the portfolio level. In this paper, we address multi-asset optimal execution in a model where prices have multivariate Ornstein-Uhlenbeck dynamics and where the agent maximizes the expected (exponential) utility of her PnL. We use the tools of stochastic optimal control and simplify the initial multidimensional Hamilton-JacobiBellman equation into a system of ordinary differential equations (ODEs) involving a Matrix Riccati ODE for which classical existence theorems do not apply. By using a priori estimates obtained thanks to optimal control tools, we nevertheless prove an existence and uniqueness result for the latter ODE, and then deduce a verification theorem that provides a rigorous solution to the execution problem. Using examples based on data from the foreign exchange and stock markets, we eventually illustrate our results and discuss their implications for both optimal execution and statistical arbitrage."
            },
            {
                "arxivId": "1911.03380",
                "title": "An Analysis of Uniswap Markets",
                "abstract": "Uniswap \u2014 and other constant product markets \u2014 appear to work well in practice despite their simplicity. In this paper, we give a simple formal analysis of constant product markets and their generalizations, showing that, under some common conditions, these markets must closely track the reference market price. We also show that Uniswap satisfies many other desirable properties and numerically demonstrate, via a large-scale agent-based simulation, that Uniswap is stable under a wide range of market conditions."
            },
            {
                "arxivId": "1908.03281",
                "title": "Latency and Liquidity Risk",
                "abstract": "Latency (i.e., time delay) in electronic markets affects the efficacy of liquidity taking strategies. During the time liquidity takers process information and send marketable limit orders (MLOs) to the exchange, the limit order book (LOB) might undergo updates, so there is no guarantee that MLOs are filled. We develop a latency-optimal trading strategy that improves the marksmanship of liquidity takers. The interaction between the LOB and MLOs is modelled as a marked point process. Each MLO specifies a price limit so the order can receive worse prices and quantities than those the liquidity taker targets if the updates in the LOB are against the interest of the trader. In our model, the liquidity taker balances the tradeoff between missing trades and the costs of walking the book. We employ techniques of variational analysis to obtain the optimal price limit of each MLO the agent sends. The price limit of a MLO is characterized as the solution to a new class of forward-backward stochastic differential equations (FBSDEs) driven by random measures. We prove the existence and uniqueness of the solution to the FBSDE and numerically solve it to illustrate the performance of the latency-optimal strategies."
            },
            {
                "arxivId": "1810.04383",
                "title": "Closed-form Approximations in Multi-asset Market Making",
                "abstract": "ABSTRACT A large proportion of market making models derive from the seminal model of Avellaneda and Stoikov. The numerical approximation of the value function and the optimal quotes in these models remains a challenge when the number of assets is large. In this article, we propose closed-form approximations for the value functions of many multi-asset extensions of the Avellaneda\u2013Stoikov model. These approximations or proxies can be used (i) as heuristic evaluation functions, (ii) as initial value functions in reinforcement learning algorithms, and/or (iii) directly to design quoting strategies through a greedy approach. Regarding the latter, our results lead to new and easily interpretable closed-form approximations for the optimal quotes, both in the finite-horizon case and in the asymptotic (ergodic) regime."
            },
            {
                "arxivId": "1804.04170",
                "title": "OPTIMAL LIQUIDATION UNDER STOCHASTIC PRICE IMPACT",
                "abstract": "We assume a continuous-time price impact model similar to that of Almgren\u2013Chriss but with the added assumption that the price impact parameters are stochastic processes modeled as correlated scalar Markov diffusions. In this setting, we develop trading strategies for a trader who desires to liquidate his inventory but faces price impact as a result of his trading. For a fixed trading horizon, we perform coefficient expansion on the Hamilton\u2013Jacobi\u2013Bellman (HJB) equation associated with the trader\u2019s value function. The coefficient expansion yields a sequence of partial differential equations that we solve to give closed-form approximations to the value function and optimal liquidation strategy. We examine some special cases of the optimal liquidation problem and give financial interpretations of the approximate liquidation strategies in these cases. Finally, we provide numerical examples to demonstrate the effectiveness of the approximations."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2402.03953",
        "category": "q-fin",
        "title": "Exploring the Impact: How Decentralized Exchange Designs Shape Traders' Behavior on Perpetual Future Contracts",
        "abstract": "In this paper, we analyze traders' behavior within both centralized exchanges (CEXs) and decentralized exchanges (DEXs), focusing on the volatility of Bitcoin prices and the trading activity of investors engaged in perpetual future contracts. We categorize the architecture of perpetual future exchanges into three distinct models, each exhibiting unique patterns of trader behavior in relation to trading volume, open interest, liquidation, and leverage. Our detailed examination of DEXs, especially those utilizing the Virtual Automated Market Making (VAMM) Model, uncovers a differential impact of open interest on long versus short positions. In exchanges which operate under the Oracle Pricing Model, we find that traders primarily act as price takers, with their trading actions reflecting direct responses to price movements of the underlying assets. Furthermore, our research highlights a significant propensity among less informed traders to overreact to positive news, as demonstrated by an increase in long positions. This study contributes to the understanding of market dynamics in digital asset exchanges, offering insights into the behavioral finance for future innovation of decentralized finance.",
        "references": [
            {
                "arxivId": "2212.06888",
                "title": "Fundamentals of Perpetual Futures",
                "abstract": "Perpetual futures -- swap contracts that never expire -- are the most popular derivative traded in cryptocurrency markets, with more than \\$100 billion traded daily. Perpetuals provide investors with leveraged exposure to cryptocurrencies, which does not require rollover or direct cryptocurrency holding. To keep the gap between perpetual futures and spot prices small, long position holders periodically pay short position holders a funding rate proportional to this gap. The funding rate incentivizes trades that tend to narrow the futures-spot gap. But unlike fixed-maturity futures, perpetuals are not guaranteed to converge to the spot price of their underlying asset at any time, and familiar no-arbitrage prices for perpetuals are not available, as the contracts have no expiry date to enforce arbitrage. Here, using a weaker notion of random-maturity arbitrage, we derive no-arbitrage prices for perpetual futures in frictionless markets and no-arbitrage bounds for markets with trading costs. These no-arbitrage prices provide a valuable benchmark for perpetual futures and simultaneously prescribe a strategy to exploit divergence from these fundamental values. Empirically, we find that deviations of crypto perpetual futures from no-arbitrage prices are considerably larger than those documented in traditional currency markets. These deviations comove across cryptocurrencies and diminish over time as crypto markets develop and become more efficient. A simple trading strategy generates large Sharpe ratios even for investors paying the highest trading costs on Binance, which is currently the largest crypto exchange by volume."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2403.05743",
        "category": "q-fin",
        "title": "Forecasting Electricity Market Signals via Generative AI",
        "abstract": "This paper presents a generative artificial intelligence approach to probabilistic forecasting of electricity market signals, such as real-time locational marginal prices and area control error signals. Inspired by the Wiener-Kallianpur innovation representation of nonparametric time series, we propose a weak innovation autoencoder architecture and a novel deep learning algorithm that extracts the canonical independent and identically distributed innovation sequence of the time series, from which future time series samples are generated. The validity of the proposed approach is established by proving that, under ideal training conditions, the generated samples have the same conditional probability distribution as that of the ground truth. Three applications involving highly dynamic and volatile time series in real-time market operations are considered: (i) locational marginal price forecasting for self-scheduled resources such as battery storage participants, (ii) interregional price spread forecasting for virtual bidders in interchange markets, and (iii) area control error forecasting for frequency regulations. Numerical studies based on market data from multiple independent system operators demonstrate the superior performance of the proposed generative forecaster over leading classical and modern machine learning techniques under both probabilistic and point forecasting metrics.",
        "references": [
            {
                "arxivId": "2403.06942",
                "title": "Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI",
                "abstract": "Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection. Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation sequence from CPOW time series, (ii) compresses the CPOW streaming data with innovation autoencoder and subband coding, and (iii) detects unknown faults and novel trends via nonparametric sequential hypothesis testing. Conclusion: This work argues that conventional monitoring using SCADA and phasor measurement unit (PMU) technologies is ill-suited for a future grid with deep penetration of inverter-based renewable generations and distributed energy resources. A monitoring system based on CPOW data streaming and AI data analytics should be the basic building blocks for situational awareness of a highly dynamic future grid."
            },
            {
                "arxivId": "2402.13870",
                "title": "Generative Probabilistic Time Series Forecasting and Applications in Grid Operations",
                "abstract": "Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques."
            },
            {
                "arxivId": "2301.03028",
                "title": "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement",
                "abstract": "Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE."
            },
            {
                "arxivId": "2106.12382",
                "title": "Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection",
                "abstract": "An innovations sequence of a time series is a sequence of independent and identically distributed random variables with which the original time series has a causal representation. The innovation at a time is statistically independent of the history of the time series. As such, it represents the new information contained at present but not in the past. Because of its simple probability structure, an innovations sequence is the most efficient signature of the original. Unlike the principle or independent component analysis representations, an innovations sequence preserves not only the complete statistical properties but also the temporal order of the original time series. An long-standing open problem is to find a computationally tractable way to extract an innovations sequence of non-Gaussian processes. This paper presents a deep learning approach, referred to as Innovations Autoencoder (IAE), that extracts innovations sequences using a causal convolutional neural network. An application of IAE to the one-class anomalous sequence detection problem with unknown anomaly and anomaly-free models is also presented."
            },
            {
                "arxivId": "2102.00431",
                "title": "Synergetic Learning of Heterogeneous Temporal Sequences for Multi-Horizon Probabilistic Forecasting",
                "abstract": "Time-series is ubiquitous across applications, such as transportation, finance and healthcare. Time-series is often influenced by external factors, especially in the form of asynchronous events, making forecasting difficult. However, existing models are mainly designated for either synchronous time-series or asynchronous event sequence, and can hardly provide a synthetic way to capture the relation between them. We propose Variational Synergetic Multi-Horizon Network (VSMHN), a novel deep conditional generative model. To learn complex correlations across heterogeneous sequences, a tailored encoder is devised to combine the advances in deep point processes models and variational recurrent neural networks. In addition, an aligned time coding and an auxiliary transition scheme are carefully devised for batched training on unaligned sequences. Our model can be trained effectively using stochastic variational inference and generates probabilistic predictions with Monte-Carlo simulation. Furthermore, our model produces accurate, sharp and more realistic probabilistic forecasts. We also show that modeling asynchronous event sequences is crucial for multi-horizon time-series forecasting."
            },
            {
                "arxivId": "2101.10460",
                "title": "Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate Time Series Forecasting",
                "abstract": "Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of computational burden and distribution modeling. Most previous work either makes simple distribution assumptions or abandons modeling cross-series correlations. A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when using deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factorization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By imposing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder. \n Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets, with gains sometimes as high as 50% for several standard metrics."
            },
            {
                "arxivId": "2012.05163",
                "title": "A Deep Learning Approach to Anomaly Sequence Detection for High-Resolution Monitoring of Power Systems",
                "abstract": "A deep learning approach is proposed to detect data and system anomalies using high-resolution continuous point-on-wave (CPOW) or phasor measurements. Both the anomaly and anomaly-free measurement models are assumed to have unknown temporal dependencies and probability distributions. Historical training samples are assumed for the anomaly-free model, while no training samples are available for the anomaly measurements. By transforming the anomaly-free observations into uniform independent and identically distributed sequences via a generative adversarial network, the proposed approach deploys a uniformity test for anomaly detection at the sensor level. A distributed detection scheme that combines sensor level detections at the control center is also proposed which combines local detections to form more reliable detections. Numerical results demonstrate significant improvement over the state-of-the-art solutions for various bad-data cases using real and synthetic CPOW and PMU data sets."
            },
            {
                "arxivId": "2002.06103",
                "title": "Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows",
                "abstract": "Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series."
            },
            {
                "arxivId": "1910.03002",
                "title": "High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes",
                "abstract": "Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting. However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several real-world datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model."
            },
            {
                "arxivId": "1903.12549",
                "title": "Probabilistic Forecasting of Sensory Data With Generative Adversarial Networks \u2013 ForGAN",
                "abstract": "Time series forecasting is one of the challenging problems for humankind. The traditional forecasting methods using mean regression models have severe shortcomings in reflecting real-world fluctuations. While new probabilistic methods rush to rescue, they fight with technical difficulties like quantile crossing or selecting a prior distribution. To meld the different strengths of these fields while avoiding their weaknesses, as well as, to push the boundary of the state-of-the-art, we introduce ForGAN \u2013 one step ahead probabilistic forecasting with generative adversarial networks. ForGAN utilizes the power of the conditional generative adversarial network to learn the data generating distribution and compute probabilistic forecasts from it. We argue how to evaluate ForGAN in opposition to regression methods. To investigate probabilistic forecasting of ForGAN, we create a new dataset and demonstrate our method abilities on it. This dataset will be made publicly available for comparison. Furthermore, we test ForGAN on two publicly available datasets, namely Mackey-Glass dataset and Internet traffic dataset (A5M), where the impressive performance of ForGAN demonstrate its high capability in forecasting future values."
            },
            {
                "arxivId": "1701.07875",
                "title": "Wasserstein GAN",
                "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions."
            },
            {
                "arxivId": "cond-mat/0201139",
                "title": "Long-range fractal correlations in literary corpora",
                "abstract": "In this paper, we analyze the fractal structure of long human language records by mapping large samples of texts onto time series. The particular mapping set up in this work is inspired on linguistic basis in the sense that is retains the word as the fundamental unit of communication. The results confirm that beyond the short-range correlations resulting from syntactic rules acting at sentence level, long-range structures emerge in large written language samples that give rise to long-range correlations in the use of words."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.11794",
        "category": "q-fin",
        "title": "Automated Social Science: Language Models as Scientist and Subjects",
        "abstract": "We present an approach for automatically generating and testing, in silico, social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.15489",
        "category": "q-fin",
        "title": "Multiblock MEV opportunities&protections in dynamic AMMs",
        "abstract": "Maximal Extractable Value (MEV) in Constant Function Market Making is fairly well understood. Does having dynamic weights, as found in liquidity boostrap pools (LBPs), Temporal-function market makers (TFMMs), and Replicating market makers (RMMs), introduce new attack vectors? In this paper we explore how inter-block weight changes can be analogous to trades, and can potentially lead to a multi-block MEV attack. New inter-block protections required to guard against this new attack vector are analysed. We also carry our a raft of numerical simulations, more than 450 million potential attack scenarios, showing both successful attacks and successful defense.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.16169",
        "category": "q-fin",
        "title": "Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds",
        "abstract": "This work develops a predictive model to identify potential targets of activist investment funds, which strategically acquire significant corporate stakes to drive operational and strategic improvements and enhance shareholder value. Predicting these targets is crucial for companies to mitigate intervention risks, for activists to select optimal targets, and for investors to capitalize on associated stock price gains. Our analysis utilizes data from the Russell 3000 index from 2016 to 2022. We tested 123 variations of models using different data imputation, oversampling, and machine learning methods, achieving a top AUC-ROC of 0.782. This demonstrates the model's effectiveness in identifying likely targets of activist funds. We applied the Shapley value method to determine the most influential factors in a company's susceptibility to activist investment. This interpretative approach provides clear insights into the driving forces behind activist targeting. Our model offers stakeholders a strategic tool for proactive corporate governance and investment strategy, enhancing understanding of the dynamics of activist investing.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.16295",
        "category": "q-fin",
        "title": "Joint calibration to SPX and VIX Derivative Markets with Composite Change of Time Models",
        "abstract": "The Chicago Board Options Exchange Volatility Index (VIX) is calculated from SPX options and derivatives of VIX are also traded in market, which leads to the so-called\"consistent modeling\"problem. This paper proposes a time-changed L\\'evy model for log price with a composite change of time structure to capture both features of the implied SPX volatility and the implied volatility of volatility. Consistent modeling is achieved naturally via flexible choices of jumps and leverage effects, as well as the composition of time changes. Many celebrated models are covered as special cases. From this model, we derive an explicit form of the characteristic function for the asset price (SPX) and the pricing formula for European options as well as VIX options. The empirical results indicate great competence of the proposed model in the problem of joint calibration of the SPX/VIX Markets.",
        "references": [
            {
                "arxivId": "1812.05859",
                "title": "Consistent time\u2010homogeneous modeling of SPX and VIX derivatives",
                "abstract": "This paper shows how to recover a stochastic volatility model (SVM) from a market model of the VIX futures term structure. Market models have more flexibility for fitting of curves than do SVMs, and therefore are better suited for pricing VIX futures and VIX derivatives. But the VIX itself is a derivative of the S&P500 (SPX) and it is common practice to price SPX derivatives using an SVM. Therefore, consistent modeling for both SPX and VIX should involve an SVM that can be obtained by inverting the market model. This paper's main result is a method for the recovery of a stochastic volatility function by solving an inverse problem where the input is the VIX function given by a market model. Analysis will show conditions necessary for there to be a unique solution to this inverse problem. The models are consistent if the recovered volatility function is non\u2010negative. Examples are presented to illustrate the theory, to highlight the issue of negativity in solutions, and to show the potential for inconsistency in non\u2010Markov settings."
            },
            {
                "arxivId": "1701.04260",
                "title": "On VIX futures in the rough Bergomi model",
                "abstract": "Abstract The rough Bergomi model introduced by Bayer et al. [Quant. Finance, 2015, 1\u201318] has been outperforming conventional Markovian stochastic volatility models by reproducing implied volatility smiles in a very realistic manner, in particular for short maturities. We investigate here the dynamics of the VIX and the forward variance curve generated by this model, and develop efficient pricing algorithms for VIX futures and options. We further analyse the validity of the rough Bergomi model to jointly describe the VIX and the SPX, and present a joint calibration algorithm based on the hybrid scheme by Bennedsen et al. [Finance Stoch., forthcoming]."
            },
            {
                "arxivId": "1410.3394",
                "title": "Volatility is rough",
                "abstract": "Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable timescale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault [Long memory in continuous-time stochastic volatility models. Math. Finance, 1998, 8(4), 291\u2013323]. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, . We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not a long memory process in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact."
            },
            {
                "arxivId": "1203.5903",
                "title": "Consistent Modelling of VIX and Equity Derivatives Using a 3/2 plus Jumps Model",
                "abstract": "Abstract The paper demonstrates that a pure-diffusion 3/2 model is able to capture the observed upward-sloping implied volatility skew in VIX options. This observation contradicts a common perception in the literature that jumps are required for the consistent modelling of equity and VIX derivatives. The pure-diffusion model, however, struggles to reproduce the smile in the implied volatilities of short-term index options. The pronounced implied volatility smile produces artificially inflated fitted parameters, resulting in unrealistically high VIX option implied volatilities. To remedy these shortcomings, jumps are introduced. The resulting model is able to better fit short-term index option implied volatilities while producing more realistic VIX option implied volatilities, without a loss in tractability."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.16449",
        "category": "q-fin",
        "title": "Analysis of market efficiency in main stock markets: using Karman-Filter as an approach",
        "abstract": "In this study, we utilize the Kalman-Filter analysis to assess market efficiency in major stock markets. The Kalman-Filter operates in two stages, assuming that the data contains a consistent trendline representing the true market value prior to being affected by noise. Unlike traditional methods, it can forecast stock price movements effectively. Our findings reveal significant portfolio returns in emerging markets such as Korea, Vietnam, and Malaysia, as well as positive returns in developed markets like the UK, Europe, Japan, and Hong Kong. This suggests that the Kalman-Filter-based price reversal indicator yields promising results across various market types.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.16467",
        "category": "q-fin",
        "title": "Riding Wavelets: A Method to Discover New Classes of Price Jumps",
        "abstract": "Cascades of events and extreme occurrences have garnered significant attention across diverse domains such as financial markets, seismology, and social physics. Such events can stem either from the internal dynamics inherent to the system (endogenous), or from external shocks (exogenous). The possibility of separating these two classes of events has critical implications for professionals in those fields. We introduce an unsupervised framework leveraging a representation of jump time-series based on wavelet coefficients and apply it to stock price jumps. In line with previous work, we recover the fact that the time-asymmetry of volatility is a major feature. Mean-reversion and trend are found to be two additional key features, allowing us to identify new classes of jumps. Furthermore, thanks to our wavelet-based representation, we investigate the reflexive properties of co-jumps, which occur when multiple stocks experience price jumps within the same minute. We argue that a significant fraction of co-jumps results from an endogenous contagion mechanism.",
        "references": [
            {
                "arxivId": "2308.01486",
                "title": "Path Shadowing Monte-Carlo",
                "abstract": "We introduce a Path Shadowing Monte-Carlo method, which provides prediction of future paths, given any generative model. At any given date, it averages future quantities over generated price paths whose past history matches, or `shadows', the actual (observed) history. We test our approach using paths generated from a maximum entropy model of financial prices, based on a recently proposed multi-scale analogue of the standard skewness and kurtosis called `Scattering Spectra'. This model promotes diversity of generated paths while reproducing the main statistical properties of financial prices, including stylized facts on volatility roughness. Our method yields state-of-the-art predictions for future realized volatility and allows one to determine conditional option smiles for the S\\&P500 that outperform both the current version of the Path-Dependent Volatility model and the option market itself."
            },
            {
                "arxivId": "2106.07040",
                "title": "Exogenous and endogenous price jumps belong to different dynamical classes",
                "abstract": "Synchronising a database of stock specific news with 5 years worth of order book data on 300 stocks, we show that abnormal price movements following news releases (exogenous) exhibit markedly different dynamical features from those arising spontaneously (endogenous). On average, large volatility fluctuations induced by exogenous events occur abruptly and are followed by a decaying power-law relaxation, while endogenous price jumps are characterized by progressively accelerating growth of volatility, also followed by a power-law relaxation, but slower than for exogenous jumps. Remarkably, our results are reminiscent of what is observed in different contexts, namely Amazon book sales and YouTube views. Finally, we show that fitting power-laws to individual volatility profiles allows one to classify large events into endogenous and exogenous dynamical classes, without relying on the news feed."
            },
            {
                "arxivId": "1701.07479",
                "title": "Epidemiological modelling of the 2005 French riots: a spreading wave and the role of contagion",
                "abstract": null
            },
            {
                "arxivId": "1505.00704",
                "title": "Collective synchronization and high frequency systemic instabilities in financial markets",
                "abstract": "We present some empirical evidence on the dynamics of price instabilities in financial markets and propose a new Hawkes modelling approach. Specifically, analysing the recent high frequency dynamics of a set of US stocks, we find that since 2001 the level of synchronization of large price movements across assets has significantly increased. We find that only a minor fraction of these systemic events can be connected with the release of pre-announced macroeconomic news. Finally, the larger is the multiplicity of the event\u2014i.e. how many assets have swung together\u2014the larger is the probability of a new event occurring in the near future, as well as its multiplicity. To reproduce these facts, due to the self- and cross-exciting nature of the event dynamics, we propose an approach based on Hawkes processes. For each event, we directly model the multiplicity as a multivariate point process, neglecting the identity of the specific assets. This allows us to introduce a parsimonious parametrization of the kernel of the process and to achieve a reliable description of the dynamics of large price movements for a high-dimensional portfolio."
            },
            {
                "arxivId": "1403.5227",
                "title": "Branching-ratio approximation for the self-exciting Hawkes process.",
                "abstract": "We introduce a model-independent approximation for the branching ratio of Hawkes self-exciting point processes. Our estimator requires knowing only the mean and variance of the event count in a sufficiently large time window, statistics that are readily obtained from empirical data. The method we propose greatly simplifies the estimation of the Hawkes branching ratio, recently proposed as a proxy for market endogeneity and formerly estimated using numerical likelihood maximization. We employ our method to support recent theoretical and experimental results indicating that the best fitting Hawkes model to describe S&P futures price changes is in fact critical (now and in the recent past) in light of the long memory of financial market activity."
            },
            {
                "arxivId": "1301.6141",
                "title": "Modelling systemic price cojumps with Hawkes factor models",
                "abstract": "Instabilities in the price dynamics of a large number of financial assets are a clear sign of systemic events. By investigating portfolios of highly liquid stocks, we find that there are a large number of high-frequency cojumps. We show that the dynamics of these jumps is described neither by a multivariate Poisson nor by a multivariate Hawkes model. We introduce a Hawkes one-factor model which is able to capture simultaneously the time clustering of jumps and the high synchronization of jumps across assets."
            },
            {
                "arxivId": "1211.1919",
                "title": "High-Frequency Trading Synchronizes Prices in Financial Markets",
                "abstract": "High-speed computerized trading, often called \"high-frequency trading\" (HFT), has increased dramatically in financial markets over the last decade. In the US and Europe, it now accounts for nearly one-half of all trades. Although evidence suggests that HFT contributes to the efficiency of markets, there are concerns it also adds to market instability, especially during times of stress. Currently, it is unclear how or why HFT produces these outcomes. In this paper, I use data from NASDAQ to show that HFT synchronizes prices in financial markets, making the values of related securities change contemporaneously. With a model, I demonstrate how price synchronization leads to increased efficiency: prices are more accurate and transaction costs are reduced. During times of stress, however, localized errors quickly propagate through the financial system if safeguards are not in place. In addition, there is potential for HFT to enforce incorrect relationships between securities, making prices more (or less) correlated than economic fundamentals warrant. This research highlights an important role that HFT plays in markets and helps answer several puzzling questions that previously seemed difficult to explain: why HFT is so prevalent, why HFT concentrates in certain securities and largely ignores others, and finally, how HFT can lower transaction costs yet still make profits."
            },
            {
                "arxivId": "1201.3572",
                "title": "Quantifying Reflexivity in Financial Markets: Towards a Prediction of Flash Crashes",
                "abstract": "We introduce a measure of activity of financial markets that provides a direct access to their level of endogeneity. This measure quantifies how much of price changes is due to endogenous feedback processes, as opposed to exogenous news. For this, we calibrate the self-excited conditional Poisson Hawkes model, which combines in a natural and parsimonious way exogenous influences with self-excited dynamics, to the E-mini S&P 500 futures contracts traded in the Chicago Mercantile Exchange from 1998 to 2010. We find that the level of endogeneity has increased significantly from 1998 to 2010, with only 70% in 1998 to less than 30% since 2007 of the price changes resulting from some revealed exogenous information. Analogous to nuclear plant safety measures concerned with avoiding \"criticality,\" our measure provides a direct quantification of the distance of the financial market from a critical state defined precisely as the limit of diverging trading activity in the absence of any external driving."
            },
            {
                "arxivId": "0803.2189",
                "title": "Robust dynamic classes revealed by measuring the response function of a social system",
                "abstract": "We study the relaxation response of a social system after endogenous and exogenous bursts of activity using the time series of daily views for nearly 5 million videos on YouTube. We find that most activity can be described accurately as a Poisson process. However, we also find hundreds of thousands of examples in which a burst of activity is followed by an ubiquitous power-law relaxation governing the timing of views. We find that these relaxation exponents cluster into three distinct classes and allow for the classification of collective human dynamics. This is consistent with an epidemic model on a social network containing two ingredients: a power-law distribution of waiting times between cause and action and an epidemic cascade of actions becoming the cause of future actions. This model is a conceptual extension of the fluctuation-dissipation theorem to social systems [Ruelle, D (2004) Phys Today 57:48\u201353] and [Roehner BM, et al., (2004) Int J Mod Phys C 15:809\u2013834], and provides a unique framework for the investigation of timing in complex systems."
            },
            {
                "arxivId": "0803.1769",
                "title": "Stock price jumps: news and volume play a minor role",
                "abstract": "In order to understand the origin of stock price jumps, we cross-correlate high-frequency time series of stock returns with different news feeds. We find that neither idiosyncratic news nor market wide news can explain the frequency and amplitude of price jumps. We find that the volatility patterns around jumps and around news are quite different: jumps are followed by increased volatility, whereas news tend on average to be followed by lower volatility levels. The shape of the volatility relaxation is also markedly different in the two cases. Finally, we provide direct evidence that large transaction volumes are_not_ responsible for large price jumps. We conjecture that most price jumps are induced by order flow fluctuations close to the point of vanishing liquidity."
            },
            {
                "arxivId": "0712.3929",
                "title": "Epileptic seizures: Quakes of the brain?",
                "abstract": "A dynamical analogy supported by five scale-free statistics (the Gutenberg-Richter distribution of event sizes, the distribution of interevent intervals, the Omori and inverse Omori laws, and the conditional waiting time until the next event) is shown to exist between two classes of seizures (\"focal\" in humans and generalized in animals) and earthquakes. Increments in excitatory interneuronal coupling in animals expose the system's dependence on this parameter and its dynamical transmutability: moderate increases lead to power-law behavior of seizure energy and interevent times, while marked ones to scale-free (power-law) coextensive with characteristic scales and events. The coextensivity of power law and characteristic size regimes is predicted by models of coupled heterogeneous threshold oscillators of relaxation and underscores the role of coupling strength in shaping the dynamics of these systems."
            },
            {
                "arxivId": "physics/0412171",
                "title": "Dynamics of book sales: endogenous versus exogenous shocks in complex networks.",
                "abstract": "We present an extensive study of the foreshock and aftershock signatures accompanying peaks of book sales. The time series of book sales are derived from the ranking system of Amazon.com. We present two independent ways of classifying peaks, one based on the acceleration pattern of sales and the other based on the exponent of the relaxation. They are found to be consistent and reveal the coexistence of two types of sales peaks: exogenous peaks occur abruptly and are followed by a power law relaxation, while endogenous sale peaks occur after a progressively accelerating power law growth followed by an approximately symmetrical power law relaxation which is slower than for exogenous peaks. We develop a simple epidemic model of buyers connected within a network of acquaintances which propagates rumors and opinions on books. The comparison between the predictions of the model and the empirical data confirms the validity of the model and suggests in addition that social networks have evolved to converge very close to criticality (here in the sense of critical branching processes of opinion spreading). We test in detail the evidence for a power law distribution of book sales and confirm a previous indirect study suggesting that the fraction of books (density distribution) P (S) of sales S is a power law P(S) approximately 1/ S(1+mu) with mu approximately equal to 2 ."
            },
            {
                "arxivId": "physics/0412026",
                "title": "Endogenous versus Exogenous Origins of Crises",
                "abstract": null
            },
            {
                "arxivId": "cond-mat/0310135",
                "title": "Endogenous Versus Exogenous Shocks in Complex Networks: An Empirical Test Using Book Sale Rankings",
                "abstract": "We study the precursory and recovery signatures accompanying shocks in complex networks, that we test on a unique database of the Amazon.com ranking of book sales. We find clear distinguishing signatures classifying two types of sales peaks. Exogenous peaks occur abruptly and are followed by a power law relaxation, while endogenous peaks occur after a progressively accelerating power law growth followed by an approximately symmetrical power law relaxation which is slower than for exogenous peaks. These results are rationalized quantitatively by a simple model of epidemic propagation of interactions with long memory within a network of acquaintances. The observed relaxation of sales implies that the sales dynamics is dominated by cascades rather than by the direct effects of news or advertisements, indicating that the social network is close to critical."
            },
            {
                "arxivId": "cond-mat/0206047",
                "title": "Endogenous versus exogenous shocks in systems with memory",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-26.json",
        "arxivId": "2404.16777",
        "category": "q-fin",
        "title": "Subset SSD for enhanced indexation with sector constraints",
        "abstract": "In this paper we apply second order stochastic dominance (SSD) to the problem of enhanced indexation with asset subset (sector) constraints. The problem we consider is how to construct a portfolio that is designed to outperform a given market index whilst having regard to the proportion of the portfolio invested in distinct market sectors. In our approach, subset SSD, the portfolio associated with each sector is treated in a SSD manner. In other words in subset SSD we actively try to find sector portfolios that SSD dominate their respective sector indices. However the proportion of the overall portfolio invested in each sector is not pre-specified, rather it is decided via optimisation. Computational results are given for our approach as applied to the S\\&P~500 over the period $29^{\\text{th}}$ August 2018 to $29^{\\text{th}}$ December 2023. This period, over 5 years, includes the Covid pandemic, which had a significant effect on stock prices. Our results indicate that the scaled version of our subset SSD approach significantly outperforms the S\\&P~500 over the period considered. Our approach also outperforms the standard SSD based approach to the problem.",
        "references": [
            {
                "arxivId": "2401.12669",
                "title": "New Approximate Stochastic Dominance Approaches for Enhanced Indexation Models",
                "abstract": "In this paper, we discuss portfolio selection strategies for Enhanced Indexation (EI), which are based on stochastic dominance relations. The goal is to select portfolios that stochastically dominate a given benchmark but that, at the same time, must generate some excess return with respect to a benchmark index. To achieve this goal, we propose a new methodology that selects portfolios using the ordered weighted average (OWA) operator, which generalizes previous approaches based on minimax selection rules and still leads to solving linear programming models. We also introduce a new type of approximate stochastic dominance rule and show that it implies the almost Second-order Stochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczynski (2012). We prove that our EI model based on OWA selects portfolios that dominate a given benchmark through this new form of stochastic dominance criterion. We test the performance of the obtained portfolios in an extensive empirical analysis based on real-world datasets. The computational results show that our proposed approach outperforms several SSD-based strategies widely used in the literature, as well as the global minimum variance portfolio."
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-29.json",
        "arxivId": "2104.13367",
        "category": "q-fin",
        "title": "A model of multiple hypothesis testing",
        "abstract": "Multiple hypothesis testing practices vary widely, without consensus on which are appropriate when. This paper provides an economic foundation for these practices designed to capture processes of scientific communication, such as regulatory approval on the basis of clinical trials. In studies of multiple treatments or sub-populations, adjustments may be appropriate depending on scale economies in the research production function, with control of classical notions of compound errors emerging in some but not all cases. In studies with multiple outcomes, indexing is appropriate and adjustments to test levels may be appropriate if the intended audience is heterogeneous. Data on actual costs in the drug approval process suggest both that some adjustment is warranted in that setting and that standard procedures are overly conservative.",
        "references": []
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-29.json",
        "arxivId": "2404.17008",
        "category": "q-fin",
        "title": "The TruEnd-procedure: Treating trailing zero-valued balances in credit data",
        "abstract": "A novel procedure is presented for finding the true but latent endpoints within the repayment histories of individual loans. The monthly observations beyond these true endpoints are false, largely due to operational failures that delay account closure, thereby corrupting some loans in the dataset with `false' observations. Detecting these false observations is difficult at scale since each affected loan history might have a different sequence of zero (or very small) month-end balances that persist towards the end. Identifying these trails of diminutive balances would require an exact definition of a\"small balance\", which can be found using our so-called TruEnd-procedure. We demonstrate this procedure and isolate the ideal small-balance definition using residential mortgages from a large South African bank. Evidently, corrupted loans are remarkably prevalent and have excess histories that are surprisingly long, which ruin the timing of certain risk events and compromise any subsequent time-to-event model such as survival analysis. Excess histories can be discarded using the ideal small-balance definition, which demonstrably improves the accuracy of both the predicted timing and severity of risk events, without materially impacting the monetary value of the portfolio. The resulting estimates of credit losses are lower and less biased, which augurs well for raising accurate credit impairments under the IFRS 9 accounting standard. Our work therefore addresses a pernicious data error, which highlights the pivotal role of data preparation in producing credible forecasts of credit risk.",
        "references": [
            {
                "arxivId": "2009.11064",
                "title": "Simulation-based optimisation of the timing of loan recovery across different portfolios",
                "abstract": null
            }
        ]
    },
    {
        "source_file": "merged_data_with_references_q-fin_2024-04-29.json",
        "arxivId": "2404.17497",
        "category": "q-fin",
        "title": "Merchants of Vulnerabilities: How Bug Bounty Programs Benefit Software Vendors",
        "abstract": "Software vulnerabilities enable exploitation by malicious hackers, compromising systems and data security. This paper examines bug bounty programs (BBPs) that incentivize ethical hackers to discover and responsibly disclose vulnerabilities to software vendors. Using game-theoretic models, we capture the strategic interactions between software vendors, ethical hackers, and malicious hackers. First, our analysis shows that software vendors can increase expected profits by participating in BBPs, explaining their growing adoption and the success of BBP platforms. Second, we find that vendors with BBPs will release software earlier, albeit with more potential vulnerabilities, as BBPs enable coordinated vulnerability disclosure and mitigation. Third, the optimal number of ethical hackers to invite to a BBP depends solely on the expected number of malicious hackers seeking exploitation. This optimal number of ethical hackers is lower than but increases with the expected malicious hacker count. Finally, higher bounties incentivize ethical hackers to exert more effort, thereby increasing the probability that they will discover severe vulnerabilities first while reducing the success probability of malicious hackers. These findings highlight BBPs' potential benefits for vendors beyond profitability. Earlier software releases are enabled by managing risks through coordinated disclosure. As cybersecurity threats evolve, BBP adoption will likely gain momentum, providing vendors with a valuable tool for enhancing security posture and stakeholder trust. Moreover, BBPs envelop vulnerability identification and disclosure into new market relationships and transactions, impacting software vendors' incentives regarding product security choices like release timing.",
        "references": [
            {
                "arxivId": "1812.00140",
                "title": "The Art, Science, and Engineering of Fuzzing: A Survey",
                "abstract": "Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective."
            }
        ]
    }
]